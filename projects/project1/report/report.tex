\documentclass[10pt,conference,compsocconf]{IEEEtran}

% Packages
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{cite}
\usepackage{hyperref}
	\def\sectionautorefname{Section}
	\def\subsectionautorefname{Section}
	\def\figureautorefname{Fig.}
\usepackage{graphicx}

% Formatting
\newcommand{\parabf}[1]{\vspace{1mm}\noindent\textbf{#1}}
% Formatting
\newcommand{\parait}[1]{\vspace{1mm}\noindent\textit{#1}}

% Macros
\newcommand{\Data}{\mathcal{D}}
\newcommand{\Deval}{\mathcal{D}_{\mathtt{eval}}}
\newcommand{\Dtrain}{\mathcal{D}_{\mathtt{train}}}
\newcommand{\Dtest}{\mathcal{D}_{\mathtt{test}}}
\newcommand{\Xtrain}{X_{\mathtt{train}}}
\newcommand{\ytrain}{\mathbf{y}_{\mathtt{train}}}
\newcommand{\Xeval}{X_{\mathtt{eval}}}
\newcommand{\yeval}{\mathbf{y}_{\mathtt{eval}}}
\newcommand{\Xtest}{X_{\mathtt{test}}}
\newcommand{\ytest}{\mathbf{y}_{\mathtt{test}}}
\newcommand{\ypred}[1]{\hat{\mathbf{y}}_{\mathtt{#1}}}

\newcommand{\features}{\mathbf{x}_i}
\newcommand{\target}{y_i}
\newcommand{\targetset}{\{ b, s \}}
\newcommand{\targetvector}{\mathbf{y}}
\newcommand{\weights}{\mathbf{w}}

\newcommand{\classifier}[2]{f_{#1, #2}}
\newcommand{\Train}{\mathtt{Algo}}
\newcommand{\LeastSquares}{\mathtt{LS}}
\newcommand{\Baseline}{\mathtt{Base}}
\newcommand{\Ridge}{\mathtt{Ridge}}
\newcommand{\LogReg}{\mathtt{LogReg}}


\begin{document}
\title{Project Report - Discovering the Higgs Particle}

\author{
  Francesco Intocci, Mattia Mariantoni, Theresa Stadler\\
  \textit{Department of Computer Science, EPFL, Switzerland}
}

\maketitle

\begin{abstract}
This short report summarises our attempt to build a robust and accurate predictive model to distinguish the decay signature of the Higgs boson from that of other fast decaying particles. We find that already simple linear classification methods achieve a better than baseline performance and that feature selection can further improve classification accuracy.
\end{abstract}


\section{Introduction}

The discovery of the Higgs boson~\cite{Higgs1964} at the Large Hadron Collider (LHC) in 2012~\cite{Aad2012} ended the hunt for the "God particle" that lasted for nearly 50 years~\cite{CERN}. A main challenge the scientists at the LHC had to overcome is that one can never directly observe a Higgs boson. The Higgs boson, like most particles, is unstable and, immediately after being produced, undergoes a process called particle decay. However, as collisions of protons produce a whole host of unstable particles, finding the unique \emph{decay signature} of the Higgs boson is a challenging classification task.

\subsection{Task Description}
In this short report, we describe our attempt to reproduce the search for the Higgs boson. Given access to an open-source dataset containing the decay signatures of collision events, we attempted to build an accurate and robust prediction model to distinguish the Higgs boson's decay trace from other particles.

\section{Background}

\subsection{Data Description and Processing}
\label{sec:data}
The data provided comprised 250,000 labelled training examples and 568,238 unlabelled test records generated by the ATLAS full detector simulator~\cite{HiggsML}.
We denote as $\Data = (X, \targetvector)$ a dataset where $X \in \mathbb{R}^{N \times D}$ is a matrix with the $i$-th row containing the $D$-dimensional feature vector $\features^T$ of the $i$-the record and $\targetvector$ is a column vector that contains the labels $\target \in \targetset$ of all $N$ records.       
In the original dataset, each feature vector $\features$ contained $d=30$ numerical features of which $29$ were continuous and $1$ discrete.  

\parabf{Feature selection.} \autoref{sec:results} presents experimental results on the classification performance across different feature sets. Here, we describe in short the different feature engineering strategies that were tested\footnote{See also \texttt{Run.ipynb} and \texttt{Data Exploration.ipynb} for details}.

\parait{MostInfo.} All models were tested on a reduced set of features that only pertained the 20 features with the highest linear correlation with the target variable.

\parait{ImputeJet.} $7$ out of the $29$ continuous data features were undefined for a large number of records ($177,457$ of $250,000$ labelled records) with \texttt{PRI\_jet\_num} $\leq 1$~\cite{OpenDataCERN}. In the original data, undefined values were encoded as $-999$. We ran all models on a dataset in which undefined value where imputed with the mean of defined values to avoid a potential skew of weights towards these outliers.

\parait{ExpandPoly.} To enable non-linear classification boundaries, we tested a polynomial basis expansion of the data with up to a maximum degree of $4$.

\parabf{Standardisation.} If not specified otherwise, all data was standardised to the mean and standard deviation (s.d.) of the \emph{current training set} before training. If cross-validation was applied, the mean and s.d. of the current training set were used to avoid optimism bias~\cite{Domingos2012}. 

\vspace*{-2mm}
\subsection{Classification Methods}
Formally, the task of this challenge was to find a \emph{binary classification function} $f: \mathbb{R}^d \rightarrow \targetset$ that takes as input a feature vector $\features \in \mathbb{R}^d$ and outputs a guess about the correct label $\hat{\target} \in \targetset$. To obtain $f$ we run a \emph{training algorithm} $\Train: (\Data, \theta) \rightarrow \classifier{\Train}{\Data}$ on a dataset $\Data$ to obtain a trained classifier $\classifier{\Train}{\Data}$ where $\theta$ is a set of training hyperparameters.\\
We describe in short the four main training algorithms implemented and tested as part of this project.

\parabf{$\Baseline$} The $\Baseline$ algorithm extracts relative class frequencies from its training set and given a non-labelled record outputs a random label according to

\vspace*{-2mm}
\begin{equation}
	\classifier{\Baseline}{\Data}(\features) = 
	\begin{cases}
	b &\mbox{with}\,  \pi_b = \frac{1}{N} \sum_{n=1}^N{\mathbbm{1}_{y_n = b}}\\
	s &\mbox{with}\,  \pi_s = \frac{1}{N} \sum_{n=1}^N{\mathbbm{1}_{y_n = s}}
	\end{cases}
\end{equation}

where $N = |\Data|$ is the size of the classifiers training set and $\pi_b, \pi_s$ are the probabilities of observing each class. 

\parabf{$\LeastSquares$} Given a dataset $\Data = (X, \targetvector)$, the linear least-squares algorithm $\LeastSquares$ outputs a model

\vspace*{-4mm}
\begin{equation}
	\classifier{\LeastSquares}{\Data}(\features) = g(\features^T \weights_{\LeastSquares})
\end{equation}

where $\weights_{\LeastSquares} = (X^T X)^{-1}X^T \targetvector$ is the weights vector that minimises the mean squared error (MSE) between true and predicted labels in the train set $\Data$ and $g$ is a threshold function that outputs $\hat{\target} = b$ if $\features^T \weights_{\LeastSquares} \leq 0$ and $\hat{\target} = s$ otherwise.

\parabf{$\Ridge$} The ridge regression algorithm $\Ridge$ is a variant of least-squares training in which the optimal weights vector is additionally constrained to minimise the $L_2$ norm of the weights vector $||\weights||_2^2$ with $\weights_\Ridge = (X^T X + 2N\lambda I)^{-1}X^T \targetvector$. $\lambda$ is a training hyperparameter that controls the trade-off between finding the optimal MSE and parameter shrinkage.

\parabf{$\LogReg$} A classifier trained under the logistic regression algorithm $\LogReg$ outputs $\hat{\target} = b$ if $\features^T \weights_{\LogReg} \leq \frac{1}{2}$ and $\hat{\target} = s$ otherwise. The optimal weights vector is found by minimising the negative log-likelihood (NLL) of the train set under a logistic regression model with

\vspace*{-4mm}
\begin{equation}
	\weights_\LogReg = \underset{\weights}{\text{argmin}} \sum_{i=1}^{N} \log [ 1 + \exp(\features^T\weights) ] - \target\features^T\weights
\end{equation} 

We implemented gradient descent (GD) on a variant of the NLL loss that is \emph{normalised to the size of the train set}. This enabled us to find the optimal step size $\gamma$ through cross-validation independent of the train set size. GD was initialised with a weights vector of all $0$s.

\subsection{Evaluation Setup}
The original challenge used a formal objective function referred to as \emph{approximate median significance}~\cite{HiggsML}. We used a simpler evaluation metric to select the best performing model. Each trained model was evaluated based on its \emph{prediction accuracy} on a hold-out set not used during training.\\
The prediction accuracy over a test set $\Dtest$ is defined as
\begin{equation}
	\mathtt{Accuracy} = \underset{\Dtest}{\mathbb{E}}[ \mathbbm{1}_{\target = \hat{\target}} ]
\end{equation}
where $\hat{\target} = \classifier{\Train}{\Dtrain}(\features)$ is the predictions produced by a trained model evaluated over the test set and $\mathbbm{1}_{\target = \hat{\target}}$ the indicator function equal to $1$ if $\target = \hat{\target}$ and $0$ otherwise.

\parabf{Evaluation set.} To avoid overfitting to the final test set and enable error analysis, a hold-out set was used for model selection. To create this evaluation set, the 250,000 labelled records contained in \texttt{train.csv} were split into a training set $\Dtrain = (\Xtrain, \ytrain)$ with $|\Dtrain| = 175,000$ and an evaluation set $\Deval = (\Xeval, \yeval)$ with $|\Deval| = 75,000$. In subsequent experiments, $\Dtrain$ was used to train a candidate classifier $\classifier{\Train}{\Dtrain}$. The performance of each trained classifier was then evaluated as the prediction accuracy on the hold-out set $\Deval$.

\parabf{Cross-validation for hyperparameter and feature selection.} As described above, some training algorithms take as input a set of hyperparameters $\theta$. To find the optimal set of hyperparameters $\theta^*$, $k$-fold cross-validation with $k=4$ over $\Dtrain$ was used. $\theta^*$ was selected as the parameter set with the \emph{minimum average misclassification rate} defined as $1 - \mathtt{Accuracy}$ over all $k$ test sets. The same procedure was used to select the best performing feature set for each model.\\

After model and hyperparameter selection, a final model was trained on the combined train set
$\classifier{\Train}{\{ \Dtrain, \Deval\}}$ and the test set predictions were submitted to the challenge platform. 

\vspace*{-3.5mm}
\section{Results}
\label{sec:results}

\begin{figure}
	\includegraphics[width=.5\textwidth]{compare_features_logreg.pdf}
	\caption{Prediction accuracy across feature sets for the best performing model $\LogReg$ trained under gradient descent with $\gamma = 0.01$}
	\label{fig:features}
	\setlength{\belowcaptionskip}{-10pt}
\end{figure}

\begin{figure}
	\includegraphics[width=.5\textwidth]{model_comparison_basic.pdf}
	\caption{Comparison of prediction accuracy on the pre-processed dataset $\Deval$.}
	\label{fig:compare}
	\setlength{\belowcaptionskip}{-1pt}
	\vspace*{-5mm}
\end{figure}

Cross-validation on $\Dtrain$ showed that expanding the dataset with a polynomial basis with maximum degree $4$ (labelled $\mathtt{ExpandPoly}$ in \autoref{fig:features}) yielded the highest accuracy across all classifiers. Average test accuracy increased from $72.7\%$ on the raw to $75.49\%$ on the modified dataset under a $\LogReg$ model with $\gamma = 0.01$. Removing uninformative features decreased average classification performance to $71.41\%$. Imputing undefined values did not significantly affect performance.\footnote{Results on all models and further detailed analysis in \texttt{Run.ipynb}}.

Based on these results, we compared the performance of all training algorithms on the expanded dataset. \autoref{fig:compare} shows the prediction accuracy evaluated over $\Deval$. Cross-validation over $\Dtrain$ was used to find the optimal step size $\gamma=0.01$ for $\LogReg$ training and the optimal trade-off parameter $\lambda=4.8 \cdot 10{-4}$ for ridge regression.\\
All models significantly outperformed the baseline $\Baseline$ with $54.95\%$. The logistic regression model $\LogReg$ reached the highest prediction accuracy with $71.32\%$. After training a model with the same parameter settings over the combined train set (all labelled records), test set accuracy reported by the challenge platform was $XX.X\%$. 

\vspace*{-2mm}
\section{Discussion}
Our experimental results highlight the complexity of the classification task at hand. None of the simplistic feature processing steps yielded a satisfying improvement in classification accuracy. Other feature selection strategies, such as removing redundant features with high cross-correlation, not discussed here, neither improved performance.
More work would be needed to understand the physical meaning of the data features to add more informative or remove redundant features.\\
The most surprising result we obtained is the relatively good performance of simple models such as the (regularised) MSE estimator $\Ridge$. It is unexpected that models trained via $\LogReg$ did not outperform this simple model with a larger margin. A likely explanation is that due to computational constraints $\LogReg$ training was terminated before  loss convergence.

\bibliography{references}
\bibliographystyle{IEEEtran}


\end{document}
