\documentclass[10pt,conference,compsocconf]{IEEEtran}

% Packages
\usepackage{amssymb}
\usepackage{cite}
\usepackage{hyperref}
	\def\sectionautorefname{Section}
	\def\subsectionautorefname{Section}
	\def\figureautorefname{Fig.}
\usepackage{graphicx}

% Formatting
\newcommand{\parabf}[1]{\vspace{1mm}\noindent\textbf{#1}}

% Macros
\newcommand{\classifier}[1]{f({#1})}
\newcommand{\Deval}{\mathcal{D}_{\mathtt{eval}}}
\newcommand{\Dtrain}{\mathcal{D}_{\mathtt{train}}}
\newcommand{\features}{\mathbf{x}_i}
\newcommand{\target}{y_i}

\begin{document}
\title{Project Report - Discovering the Higgs Particle}

\author{
  Names\\
  \textit{Department of Computer Science, EPFL, Switzerland}
}

\maketitle

\begin{abstract}
This short report summarises our attempt to 
\end{abstract}


\section{Introduction}

The discovery of the Higgs boson at the Large Hadron Collider (LHC) at CERN in 2012~\cite{Aad2012} caused a wide-range media hype around the world~\cite{Guardian}. The hunt for the "God particle", which plays a fundamental role in the standard model of particle physics, lasted for nearly 50 years, from Peter Higg's first publication that predicted its existence~\cite{Higgs1964} to the official confirmation by CERN scientists in 2013~\cite{CERN}.\\
There are many reasons why the search for the Higgs boson was one of the largest in modern science's history and lasted for half a decade. One of the many challenges the team of scientists working at the LHC had to overcome is that one can never directly observe a Higgs boson. The Higgs boson, like most particle types, is unstable and, immediately after being produced, undergoes a process called particle decay. This means that instead of searching for the particle itself, the CERN scientists conducted an endless number of experiments to identify the unique traces of its \emph{decay}. However, as collisions of protons produce a whole host of unstable particles, finding the "decay signature" of the Higgs boson is a challenging classification task.\\
In this short report, we describe an (amateur's) attempt to reproduce this uniquely fascinating search for the Higgs boson. 

\subsection{Task Description}
Given access to an open-source dataset containing the decay signatures of collision events, we attempted to build an accurate and robust prediction model to distinguish the Higgs boson's decay trace from other particles. The goal of this project was to explore different classification methods, feature engineering and selection techniques in the context of a real-world, challenging machine learning task. 

\section{Background}

\subsection{Data Description and Processing}
\label{sec:data}
The data provided comprised 250,000 labelled training examples (provided in the file \texttt{train.csv}) and 568,238 unlabelled test records (provided in the file \texttt{test.csv}) generated by the ATLAS full detector simulator~\cite{HiggsML}.
Each record in the dataset $\mathbf{r}_i = (\features, \target)$ consists of a $d$-dimensional feature vector $\features \in \mathbb{R}^d$ and a label $\target \in \{ b, s \}$.     


\parabf{Feature engineering and selection.} In the original dataset, each feature vector $\features$ contained $d=30$ numerical features of which $29$ were continuous and $1$ discrete.
To improve classification accuracy, we tested different feature selection and engineering techniques to enhance the original feature set.
Feature importance: Keep only top ten features with highest correlation with target variable. Remove redundant: Discard highly correlated features.

\subsection{Classification Methods}

\subsection{Evaluation}
While the original challenge used a formal objective function referred to as \emph{approximate median significance}~\cite{HiggsML}, we used two traditional evaluation metrics to select the best performing discriminant function. Each trained model was evaluated based on its \emph{prediction accuracy} and \emph{F1-score} on a test set not used during training.\\

\parabf{Evaluation set.} To avoid overfitting to the final test set, a hold-out set was used throughout all experiments for model evaluation. To create this evaluation set, the 250,000 labelled records contained in \texttt{train.csv} were split into a training set $\Dtrain$ with $|\Dtrain| = 175,000$ and an evaluation set $\Deval$ with $|\Deval| = 75,000$. In subsequent experiments, $\Dtrain$ was used to train a candidate classifier $\classifier{\Dtrain}$. The performance of each trained classifier was then evaluated as the prediction accuracy on the hold-out set $\Deval$.\\

\parabf{Cross-validation for hyperparameter and feature selection.} As described above, some training algorithms take as input a set of hyperparameters $\theta$. To find the optimal set of hyperparameters $\theta^*$, $k$-fold cross-validation with $k=5$ over $\Dtrain$ was used. The same procedure was used to select the top-ten most informative features using the methods described in \autoref{sec:data}.\\
  
After model and hyperparameter selection using the evaluation set, a final model was trained on the full training set ($\classifier{\{ \Dtrain, \Deval \}}$) and the test set predictions were submitted to the challenge platform. 

\section{Results}
Some beautiful plots here

\section{Discussion}
Say all the things we could have done but didn't do.

\bibliography{references}
\bibliographystyle{IEEEtran}


\end{document}
