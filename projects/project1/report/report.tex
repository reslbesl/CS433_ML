\documentclass[10pt,conference,compsocconf]{IEEEtran}

% Packages
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{cite}
\usepackage{hyperref}
	\def\sectionautorefname{Section}
	\def\subsectionautorefname{Section}
	\def\figureautorefname{Fig.}
\usepackage{graphicx}

% Formatting
\newcommand{\parabf}[1]{\vspace{1mm}\noindent\textbf{#1}}
% Formatting
\newcommand{\parait}[1]{\vspace{1mm}\noindent\textit{#1}}

% Macros
\newcommand{\Data}{\mathcal{D}}
\newcommand{\Deval}{\mathcal{D}_{\mathtt{eval}}}
\newcommand{\Dtrain}{\mathcal{D}_{\mathtt{train}}}
\newcommand{\Dtest}{\mathcal{D}_{\mathtt{test}}}
\newcommand{\Xtrain}{X_{\mathtt{train}}}
\newcommand{\ytrain}{\mathbf{y}_{\mathtt{train}}}
\newcommand{\Xeval}{X_{\mathtt{eval}}}
\newcommand{\yeval}{\mathbf{y}_{\mathtt{eval}}}
\newcommand{\Xtest}{X_{\mathtt{test}}}
\newcommand{\ytest}{\mathbf{y}_{\mathtt{test}}}
\newcommand{\ypred}[1]{\hat{\mathbf{y}}_{\mathtt{#1}}}

\newcommand{\features}{\mathbf{x}_i}
\newcommand{\target}{y_i}
\newcommand{\targetset}{\{ b, s \}}
\newcommand{\targetvector}{\mathbf{y}}
\newcommand{\weights}{\mathbf{w}}

\newcommand{\classifier}[2]{f_{#1, #2}}
\newcommand{\Train}{\mathtt{Algo}}
\newcommand{\LeastSquares}{\mathtt{LS}}
\newcommand{\Baseline}{\mathtt{Base}}
\newcommand{\Ridge}{\mathtt{Ridge}}
\newcommand{\LogReg}{\mathtt{LogReg}}


\begin{document}
\title{Project Report - Discovering the Higgs Particle}

\author{
  Names\\
  \textit{Department of Computer Science, EPFL, Switzerland}
}

\maketitle

\begin{abstract}
This short report summarises our attempt to build a robust and accurate predictive model to distinguish the decay signature of the Higgs boson from that of other fast decaying particles. We find that already simple linear classification methods achieve a better than baseline performance and that feature selection can further improve classification accuracy.
\end{abstract}


\section{Introduction}

The discovery of the Higgs boson at the Large Hadron Collider (LHC) at CERN in 2012~\cite{Aad2012} ended the hunt for the "God particle" which lasted for nearly 50 years, from Peter Higg's first publication that predicted its existence~\cite{Higgs1964} to the official confirmation by CERN scientists in 2013~\cite{CERN}.\\
One of the many challenges the team of scientists at the LHC had to overcome is that one can never directly observe a Higgs boson. The Higgs boson, like most particle types, is unstable and, immediately after being produced, undergoes a process called particle decay. This means that instead of searching for the particle itself, the CERN scientists conducted an endless number of experiments to identify the unique traces of its \emph{decay}. However, as collisions of protons produce a whole host of unstable particles, finding the \emph{decay} signature" of the Higgs boson is a challenging classification task.\\
In this short report, we describe an (amateur's) attempt to reproduce the uniquely fascinating search for the Higgs boson.

\subsection{Task Description}
Given access to an open-source dataset containing the decay signatures of collision events, we attempted to build an accurate and robust prediction model to distinguish the Higgs boson's decay trace from other particles. The goal of this project was to explore different classification methods, feature engineering and selection techniques in the context of a real-world, challenging machine learning task. 

\section{Background}

\subsection{Data Description and Processing}
\label{sec:data}
The data provided comprised 250,000 labelled training examples (provided in the file \texttt{train.csv}) and 568,238 unlabelled test records (provided in the file \texttt{test.csv}) generated by the ATLAS full detector simulator~\cite{HiggsML}.
Each record in the dataset $\mathbf{r}_i = (\features, \target)$ consists of a $d$-dimensional feature vector $\features \in \mathbb{R}^d$ and a label $\target \in \targetset$. We denote as $\Data = (X, \targetvector)$ a dataset where $X \in \mathbb{R}^{n \times d}$ is a matrix with the $i$-th row containing the feature vector $\features^T$ of record $\mathbf{r}_i$ and $\targetvector$ a column vector that contains the labels of all $n$ records.       
In the original dataset, each feature vector $\features$ contained $d=30$ numerical features of which $29$ were continuous and $1$ discrete.  

\parabf{Feature selection.} \autoref{sec:results} presents experimental results on the classification performance of different models across different feature sets. Here, we describe in short the different feature selection and processing strategies that were tested.

\parait{Most informative features.} All models were tested on a reduced set of features that only pertained the 20 features with the highest linear correlation with the target variable.

\parait{Undefined features.} $7$ out of the $29$ continuous data features were undefined for a large number of records ($177,457$ of $250,000$ labelled records) with \texttt{PRI\_jet\_num} $\leq 1$~\cite{OpenDataCERN}. In the original data, undefined values were encoded as $-999$. We decided not to impute the undefined values as they did not have any defined physical meaning. To avoid, however, that these outlier values skewed the learned model, we tested encoding these with a value closer to zero but still outside the range of defined values. We also assessed performance of a model with all potentially undefined variables removed.\\
Additionally, all classifiers were tested on a dataset in which the number of jets \texttt{PRI\_jet\_num} was replaced by a binary indicator variable that was $1$ if \texttt{PRI\_jet\_num} $>1$ and $0$ otherwise. Furthermore, three features with a high cross-correlation with other variables that had higher importance were removed as well. 

\parabf{Standardisation.} If not specified otherwise, all data was standardised to the mean and standard deviation (s.d.) of the \emph{current training set} before training. If cross-validation was applied, the mean and s.d. of the current training set were used to avoid optimism bias~\cite{Domingos2012}. 

\vspace*{-3.mm}
\subsection{Classification Methods}
Formally, the task of this challenge was to find a \emph{binary classification function}

\vspace*{-4mm}
\begin{equation}
	f: \mathbb{R}^d \rightarrow \targetset
\end{equation}

that takes as input a feature vector $\features \in \mathbb{R}^d$ and outputs a guess about the correct label $\target \in \targetset$. To obtain $f$ we run a \emph{training algorithm} $\Train$ on a dataset to obtain a trained classifier

\vspace*{-4mm}
\begin{equation}
	\Train: (\Data, \theta) \rightarrow \classifier{\Train}{\Data}
\end{equation}

where $\theta$ is a set of model-specific hyperparameters and we denote as $\classifier{\Train}{\Data}$ the classifier trained on dataset $\Data$ using training algorithm $\Train$.\\

We describe in short the four main training algorithms implemented and tested as part of this project.

\parabf{$\Baseline$} The $\Baseline$ algorithm extracts relative class frequencies from its training set and given a non-labelled record outputs a random label according to

\vspace*{-4mm}
\begin{equation}
	\classifier{\Baseline}{\Data}(\features) = 
	\begin{cases}
	b &\mbox{with}\,  \pi_b = \frac{1}{N} \sum_{n=1}^N{\mathbbm{1}_{y_n = b}}\\
	s &\mbox{with}\,  \pi_s = \frac{1}{N} \sum_{n=1}^N{\mathbbm{1}_{y_n = s}}
	\end{cases}
\end{equation}

where $N = |\Data|$ is the size of the classifiers training set and $\pi_b, \pi_s$ are the probabilities of observing each class. 

\parabf{$\LeastSquares$} Given a dataset $\Data = (X, \targetvector)$, the linear least-squares algorithm $\LeastSquares$ outputs a model

\vspace*{-4mm}
\begin{equation}
	\classifier{\LeastSquares}{\Data}(\features) = g(\features^T \weights_{\LeastSquares})
\end{equation}

where $\weights_{\LeastSquares} = (X^T X)^{-1}X^T \targetvector$ is the weights vector that minimises the mean squared error (MSE) between true and predicted labels in the train set $\Data$ and $g$ is a threshold function that outputs $\hat{\target} = b$ if $\features^T \weights_{\LeastSquares} \leq 0$ and $\hat{\target} = s$ otherwise.

\parabf{$\Ridge$} The ridge regression algorithm $\Ridge$ is a variant of least-squares training in which the optimal weights vector is additionally constrained to minimise the $L_2$ norm of the weights vector $||\weights||_2^2$ with $\weights_\Ridge = (X^T X + 2N\lambda I)^{-1}X^T \targetvector$. $\lambda$ is a training hyperparameter that controls the trade-off between finding the optimal MSE and parameter shrinkage to avoid overfitting to the train set.

\parabf{$\LogReg$} A classifier trained under the logistic regression algorithm $\LogReg$ outputs $\hat{\target} = b$ if $\features^T \weights_{\LogReg} \leq \frac{1}{2}$ and $\hat{\target} = s$ otherwise. The optimal weights vector is found by minimising the negative log-likelihood of the train set under a logistic regression model with

\vspace*{-4mm}
\begin{equation}
	\weights_\LogReg = \underset{\weights}{\text{argmin}} \sum_{i=1}^{N} \log [ 1 + \exp(\features^T\weights) ] - \target\features^T\weights
\end{equation} 


\subsection{Evaluation Setup}
While the original challenge used a formal objective function referred to as \emph{approximate median significance}~\cite{HiggsML}, we used a simpler evaluation metric to select the best performing discriminant function. Each trained model was evaluated based on its \emph{prediction accuracy} on a hold-out set not used during training.\\
The prediction accuracy over a test set $\Dtest = (\Xtest, \ytest)$ is defined as
\begin{equation}
	\mathtt{Accuracy} = \underset{\Dtest}{\mathbb{E}}[ \mathbbm{1}_{\target = \hat{\target}} ]
\end{equation}
where $\hat{\target} = \classifier{\Train}{\Dtrain}(\features)$ is the predictions produced by a trained model evaluated over the test set and $\mathbbm{1}_{\target = \hat{\target}}$ the indicator function equal to $1$ if $\target = \hat{\target}$ and $0$ otherwise.

\parabf{Evaluation set.} To avoid overfitting to the final test set and enable detailed error analysis, a hold-out set was used throughout all experiments for model evaluation. To create this evaluation set, the 250,000 labelled records contained in \texttt{train.csv} were split into a training set $\Dtrain = (\Xtrain, \ytrain)$ with $|\Dtrain| = 175,000$ and an evaluation set $\Deval = (\Xeval, \yeval)$ with $|\Deval| = 75,000$. In subsequent experiments, $\Dtrain$ was used to train a candidate classifier $\classifier{\Train}{\Dtrain}$. The performance of each trained classifier was then evaluated as the prediction accuracy on the hold-out set $\Deval$.

\parabf{Cross-validation for hyperparameter and feature selection.} As described above, some training algorithms take as input a set of hyperparameters $\theta$. To find the optimal set of hyperparameters $\theta^*$, $k$-fold cross-validation with $k=4$ over $\Dtrain$ was used. $\theta^*$ was selected as the parameter set with the \emph{minimum average misclassification rate} defined as $1 - \mathtt{Accuracy}$ over all $k$ test sets. The same procedure was used to select the best performing feature set for each model.\\

After model and hyperparameter selection using the evaluation set, a final model was trained on the full training set
$\classifier{\Train}{\{ \Dtrain, \Deval\}}$ and the test set predictions were submitted to the challenge platform. 

\section{Results}
\label{sec:results}

\begin{figure}
	\includegraphics[width=.5\textwidth]{compare_features_ridge.pdf}
	\caption{Prediction accuracy across feature sets for a ridge regression model}
	\vspace*{-5mm}
	\setlength{\belowcaptionskip}{-10pt}
\end{figure}

\section{Discussion}
Say all the things we could have done but didn't do.

\bibliography{references}
\bibliographystyle{IEEEtran}


\end{document}
