\documentclass[10pt,conference,compsocconf]{IEEEtran}

% Packages
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{cite}
\usepackage{hyperref}
	\def\sectionautorefname{Section}
	\def\subsectionautorefname{Section}
	\def\figureautorefname{Fig.}
\usepackage{graphicx}

% Formatting
\newcommand{\parabf}[1]{\vspace{1mm}\noindent\textbf{#1}}
% Formatting
\newcommand{\parait}[1]{\vspace{1mm}\noindent\textit{#1}}

% Macros
\newcommand{\Data}{\mathcal{D}}
\newcommand{\Deval}{\mathcal{D}_{\mathtt{eval}}}
\newcommand{\Dtrain}{\mathcal{D}_{\mathtt{train}}}
\newcommand{\Dtest}{\mathcal{D}_{\mathtt{test}}}
\newcommand{\Xtrain}{X_{\mathtt{train}}}
\newcommand{\ytrain}{\mathbf{y}_{\mathtt{train}}}
\newcommand{\Xeval}{X_{\mathtt{eval}}}
\newcommand{\yeval}{\mathbf{y}_{\mathtt{eval}}}
\newcommand{\Xtest}{X_{\mathtt{test}}}
\newcommand{\ytest}{\mathbf{y}_{\mathtt{test}}}
\newcommand{\ypred}[1]{\hat{\mathbf{y}}_{\mathtt{#1}}}

\newcommand{\features}{\mathbf{x}_i}
\newcommand{\target}{y_i}
\newcommand{\targetset}{\{ b, s \}}
\newcommand{\targetvector}{\mathbf{y}}
\newcommand{\weights}{\mathbf{w}}

\newcommand{\classifier}[2]{f_{#1, #2}}
\newcommand{\Train}{\mathtt{Algo}}
\newcommand{\LeastSquares}{\mathtt{LS}}
\newcommand{\Baseline}{\mathtt{Base}}
\newcommand{\Ridge}{\mathtt{Ridge}}
\newcommand{\LogReg}{\mathtt{LogReg}}


\begin{document}
\title{Project Report - Discovering the Higgs Particle}

\author{
  Names\\
  \textit{Department of Computer Science, EPFL, Switzerland}
}

\maketitle

\begin{abstract}
This short report summarises our attempt to build a robust and accurate predictive model to distinguish the decay signature of the Higgs boson from that of other fast decaying particles. We find that already simple linear classification methods achieve a better than baseline performance and that feature selection can further improve classification accuracy.
\end{abstract}


\section{Introduction}

The discovery of the Higgs boson~\cite{Higgs1964} at the Large Hadron Collider (LHC) in 2012~\cite{Aad2012} ended the hunt for the "God particle" which lasted for nearly 50 years~\cite{CERN}. One of the many challenges the team of scientists at the LHC had to overcome is that one can never directly observe a Higgs boson. The Higgs boson, like most particle types, is unstable and, immediately after being produced, undergoes a process called particle decay. However, as collisions of protons produce a whole host of unstable particles, finding the unique \emph{decay signature} of the Higgs boson is a challenging classification task.\\
In this short report, we describe our attempt to reproduce the uniquely fascinating search for the Higgs boson.

\subsection{Task Description}
Given access to an open-source dataset containing the decay signatures of collision events, we attempted to build an accurate and robust prediction model to distinguish the Higgs boson's decay trace from other particles. The goal of this project was to explore different classification methods, feature engineering and selection techniques in the context of a real-world machine learning task. 

\section{Background}

\subsection{Data Description and Processing}
\label{sec:data}
The data provided comprised 250,000 labelled training examples and 568,238 unlabelled test records generated by the ATLAS full detector simulator~\cite{HiggsML}.
We denote as $\Data = (X, \targetvector)$ a dataset where $X \in \mathbb{R}^{N \times D}$ is a matrix with the $i$-th row containing the $D$-dimensional feature vector $\features^T$ of the $i$-the record and $\targetvector$ is a column vector that contains the labels $\target \in \targetset$ of all $N$ records.       
In the original dataset, each feature vector $\features$ contained $d=30$ numerical features of which $29$ were continuous and $1$ discrete.  

\parabf{Feature selection.} \autoref{sec:results} presents experimental results on the classification performance of different models across different feature sets. Here, we describe in short the different feature selection and processing strategies that were tested.

\parait{Most informative features.} All models were tested on a reduced set of features that only pertained the 20 features with the highest linear correlation with the target variable.

\parait{Undefined features.} $7$ out of the $29$ continuous data features were undefined for a large number of records ($177,457$ of $250,000$ labelled records) with \texttt{PRI\_jet\_num} $\leq 1$~\cite{OpenDataCERN}. In the original data, undefined values were encoded as $-999$. We decided not to impute the undefined values as they did not have any defined physical meaning. To avoid, however, that these outlier values skewed the learned model, we tested encoding these with a value closer to zero but still outside the range of defined values.\\
Additionally, we tested a dataset in which the number of jets \texttt{PRI\_jet\_num} was replaced by a binary indicator variable that was $1$ if \texttt{PRI\_jet\_num} $>1$ and $0$ otherwise to reduce redundant information encoding. Three features with a high cross-correlation with other variables that had higher importance were removed from this set as well.

\parabf{Standardisation.} If not specified otherwise, all data was standardised to the mean and standard deviation (s.d.) of the \emph{current training set} before training. If cross-validation was applied, the mean and s.d. of the current training set were used to avoid optimism bias~\cite{Domingos2012}. 

\vspace*{-3.mm}
\subsection{Classification Methods}
Formally, the task of this challenge was to find a \emph{binary classification function}

\vspace*{-4mm}
\begin{equation}
	f: \mathbb{R}^d \rightarrow \targetset
\end{equation}

that takes as input a feature vector $\features \in \mathbb{R}^d$ and outputs a guess about the correct label $\target \in \targetset$. To obtain $f$ we run a \emph{training algorithm} $\Train: (\Data, \theta) \rightarrow \classifier{\Train}{\Data}$ on a dataset $\Data$ to obtain a trained classifier $\classifier{\Train}{\Data}$ where $\theta$ is a set of training hyperparameters.\\

We describe in short the four main training algorithms implemented and tested as part of this project.

\parabf{$\Baseline$} The $\Baseline$ algorithm extracts relative class frequencies from its training set and given a non-labelled record outputs a random label according to

\vspace*{-2mm}
\begin{equation}
	\classifier{\Baseline}{\Data}(\features) = 
	\begin{cases}
	b &\mbox{with}\,  \pi_b = \frac{1}{N} \sum_{n=1}^N{\mathbbm{1}_{y_n = b}}\\
	s &\mbox{with}\,  \pi_s = \frac{1}{N} \sum_{n=1}^N{\mathbbm{1}_{y_n = s}}
	\end{cases}
\end{equation}

where $N = |\Data|$ is the size of the classifiers training set and $\pi_b, \pi_s$ are the probabilities of observing each class. 

\parabf{$\LeastSquares$} Given a dataset $\Data = (X, \targetvector)$, the linear least-squares algorithm $\LeastSquares$ outputs a model

\vspace*{-4mm}
\begin{equation}
	\classifier{\LeastSquares}{\Data}(\features) = g(\features^T \weights_{\LeastSquares})
\end{equation}

where $\weights_{\LeastSquares} = (X^T X)^{-1}X^T \targetvector$ is the weights vector that minimises the mean squared error (MSE) between true and predicted labels in the train set $\Data$ and $g$ is a threshold function that outputs $\hat{\target} = b$ if $\features^T \weights_{\LeastSquares} \leq 0$ and $\hat{\target} = s$ otherwise.

\parabf{$\Ridge$} The ridge regression algorithm $\Ridge$ is a variant of least-squares training in which the optimal weights vector is additionally constrained to minimise the $L_2$ norm of the weights vector $||\weights||_2^2$ with $\weights_\Ridge = (X^T X + 2N\lambda I)^{-1}X^T \targetvector$. $\lambda$ is a training hyperparameter that controls the trade-off between finding the optimal MSE and parameter shrinkage.

\parabf{$\LogReg$} A classifier trained under the logistic regression algorithm $\LogReg$ outputs $\hat{\target} = b$ if $\features^T \weights_{\LogReg} \leq \frac{1}{2}$ and $\hat{\target} = s$ otherwise. The optimal weights vector is found by minimising the negative log-likelihood (NLL) of the train set under a logistic regression model with

\vspace*{-4mm}
\begin{equation}
	\weights_\LogReg = \underset{\weights}{\text{argmin}} \sum_{i=1}^{N} \log [ 1 + \exp(\features^T\weights) ] - \target\features^T\weights
\end{equation} 

We implemented traditional gradient descent on a variant of the NLL loss function that was \emph{normalised to the size of the train set}. This enabled us to find the optimal step size $\gamma$ through cross-validation independent of the train set size.

\subsection{Evaluation Setup}
The original challenge used a formal objective function referred to as \emph{approximate median significance}~\cite{HiggsML}. We used a simpler evaluation metric to select the best performing discriminant function. Each trained model was evaluated based on its \emph{prediction accuracy} on a hold-out set not used during training.\\
The prediction accuracy over a test set $\Dtest$ is defined as
\begin{equation}
	\mathtt{Accuracy} = \underset{\Dtest}{\mathbb{E}}[ \mathbbm{1}_{\target = \hat{\target}} ]
\end{equation}
where $\hat{\target} = \classifier{\Train}{\Dtrain}(\features)$ is the predictions produced by a trained model evaluated over the test set and $\mathbbm{1}_{\target = \hat{\target}}$ the indicator function equal to $1$ if $\target = \hat{\target}$ and $0$ otherwise.

\parabf{Evaluation set.} To avoid overfitting to the final test set and enable error analysis, a hold-out set was used for model selection. To create this evaluation set, the 250,000 labelled records contained in \texttt{train.csv} were split into a training set $\Dtrain = (\Xtrain, \ytrain)$ with $|\Dtrain| = 175,000$ and an evaluation set $\Deval = (\Xeval, \yeval)$ with $|\Deval| = 75,000$. In subsequent experiments, $\Dtrain$ was used to train a candidate classifier $\classifier{\Train}{\Dtrain}$. The performance of each trained classifier was then evaluated as the prediction accuracy on the hold-out set $\Deval$.

\parabf{Cross-validation for hyperparameter and feature selection.} As described above, some training algorithms take as input a set of hyperparameters $\theta$. To find the optimal set of hyperparameters $\theta^*$, $k$-fold cross-validation with $k=4$ over $\Dtrain$ was used. $\theta^*$ was selected as the parameter set with the \emph{minimum average misclassification rate} defined as $1 - \mathtt{Accuracy}$ over all $k$ test sets. The same procedure was used to select the best performing feature set for each model.\\

After model and hyperparameter selection, a final model was trained on the combined train set
$\classifier{\Train}{\{ \Dtrain, \Deval\}}$ and the test set predictions were submitted to the challenge platform. 

\vspace*{-3.5mm}
\section{Results}
\label{sec:results}

\begin{figure}
	\includegraphics[width=.5\textwidth]{compare_features_logreg.pdf}
	\caption{Prediction accuracy across feature sets for the best performing model $\LogReg$ trained under gradient descent with $\gamma = 0.01$}
	\label{fig:features}
	\setlength{\belowcaptionskip}{-10pt}
\end{figure}

\begin{figure}
	\includegraphics[width=.5\textwidth]{model_comparison_basic.pdf}
	\caption{Comparison of prediction accuracy on the pre-processed dataset $\Deval$.}
	\label{fig:compare}
	\setlength{\belowcaptionskip}{-1pt}
	\vspace*{-5mm}
\end{figure}

Cross-validation on $\Dtrain$ showed that encoding the number of jets as a binary variable and removing features with high cross-correlations (labelled DeCorrelate in \autoref{fig:features}) yielded the highest accuracy across all classifiers. Average test accuracy increased from $71.86\%$ on the raw to $71.98\%$ on the modified dataset under a logistic regression model with $\gamma = 0.01$. Removing uninformative features decreased average classification performance from $71.86\%$ to $70.87\%$.

Based on these results, we compared the performance of different model training algorithms on the pre-processed dataset. \autoref{fig:compare} shows the prediction accuracy of all models evaluated over $\Deval$. Cross-validation over $\Dtrain$ was used to find the optimal step size $\gamma=0.01$ for $\LogReg$ training and the optimal trade-off parameter $\lambda=2.9 \cdot 10{-9}$ for ridge regression.\\
All models significantly outperformed the baseline $\Baseline$ with $54.95\%$. The logistic regression model $\LogReg$ reached the highest prediction accuracy with $72.51\%$. After training a model with the same parameter settings over the combined train set (all labelled records), test set accuracy as reported by the challenge platform for this model was $72.7\%$. 

\section{Discussion}
First and foremost, our experimental results highlight the complexity of the classification task at hand. None of the simplistic feature processing steps applied yielded a satisfying improvement in classification accuracy. More work would be needed to understand the physical meaning of the data features to add more informative or remove redundant features.\\
The most surprising result we obtained is the relatively good performance of simple models such as the (regularised) MSE estimator $\Ridge$. It is unexpected that models trained via $\LogReg$ did not outperform this simple model with a larger margin. A likely explanation, however, is that due to computational constraints we did not run enough iterations to optimise the compute-intense gradient descent method. Optimisation was terminated when the loss had converged up to a threshold of $10^{-7}$ (see \texttt{README} for further details). A longer search could result in improved performance.\\


\bibliography{references}
\bibliographystyle{IEEEtran}


\end{document}
