\documentclass[10pt,conference,compsocconf]{IEEEtran}

% Packages
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{cite}
\usepackage{hyperref}
	\def\sectionautorefname{Section}
	\def\subsectionautorefname{Section}
	\def\figureautorefname{Fig.}
\usepackage{graphicx}

% Formatting
\newcommand{\parabf}[1]{\vspace{1mm}\noindent\textbf{#1}}

% Macros
\newcommand{\Data}{\mathcal{D}}
\newcommand{\Deval}{\mathcal{D}_{\mathtt{eval}}}
\newcommand{\Dtrain}{\mathcal{D}_{\mathtt{train}}}
\newcommand{\Dtest}{\mathcal{D}_{\mathtt{test}}}
\newcommand{\Xtrain}{X_{\mathtt{train}}}
\newcommand{\ytrain}{\mathbf{y}_{\mathtt{train}}}
\newcommand{\Xeval}{X_{\mathtt{eval}}}
\newcommand{\yeval}{\mathbf{y}_{\mathtt{eval}}}
\newcommand{\Xtest}{X_{\mathtt{test}}}
\newcommand{\ytest}{\mathbf{y}_{\mathtt{test}}}
\newcommand{\ypred}[1]{\hat{\mathbf{y}}_{\mathtt{#1}}}

\newcommand{\features}{\mathbf{x}_i}
\newcommand{\target}{y_i}
\newcommand{\targetset}{\{ b, s \}}
\newcommand{\targetvector}{\mathbf{y}}
\newcommand{\weights}{\mathbf{w}}

\newcommand{\classifier}[2]{f_{#1, #2}}
\newcommand{\Train}{\mathtt{Algo}}
\newcommand{\LS}{\mathtt{LS}}
\newcommand{\Base}{\mathtt{Base}}

\begin{document}
\title{Project Report - Discovering the Higgs Particle}

\author{
  Names\\
  \textit{Department of Computer Science, EPFL, Switzerland}
}

\maketitle

\begin{abstract}
This short report summarises our attempt to 
\end{abstract}


\section{Introduction}

The discovery of the Higgs boson at the Large Hadron Collider (LHC) at CERN in 2012~\cite{Aad2012} caused a wide-range media hype around the world~\cite{Guardian}. The hunt for the "God particle", which plays a fundamental role in the standard model of particle physics, lasted for nearly 50 years, from Peter Higg's first publication that predicted its existence~\cite{Higgs1964} to the official confirmation by CERN scientists in 2013~\cite{CERN}.\\
One of the many challenges the team of scientists at the LHC had to overcome is that one can never directly observe a Higgs boson. The Higgs boson, like most particle types, is unstable and, immediately after being produced, undergoes a process called particle decay. This means that instead of searching for the particle itself, the CERN scientists conducted an endless number of experiments to identify the unique traces of its \emph{decay}. However, as collisions of protons produce a whole host of unstable particles, finding the "decay signature" of the Higgs boson is a challenging classification task.\\
In this short report, we describe an (amateur's) attempt to reproduce the uniquely fascinating search for the Higgs boson.

\subsection{Task Description}
Given access to an open-source dataset containing the decay signatures of collision events, we attempted to build an accurate and robust prediction model to distinguish the Higgs boson's decay trace from other particles. The goal of this project was to explore different classification methods, feature engineering and selection techniques in the context of a real-world, challenging machine learning task. 

\section{Background}

\subsection{Data Description and Processing}
\label{sec:data}
The data provided comprised 250,000 labelled training examples (provided in the file \texttt{train.csv}) and 568,238 unlabelled test records (provided in the file \texttt{test.csv}) generated by the ATLAS full detector simulator~\cite{HiggsML}.
Each record in the dataset $\mathbf{r}_i = (\features, \target)$ consists of a $d$-dimensional feature vector $\features \in \mathbb{R}^d$ and a label $\target \in \targetset$. We denote as $\Data = (X, \targetvector)$ a dataset where $X \in \mathbb{R}^{n \times d}$ is a matrix with the $i$-th row containing the feature vectors $\features^T$ of record $\mathbf{r}_i$ and $\targetvector$ a column vector that contains the labels of all $n$ records.       
In the original dataset, each feature vector $\features$ contained $d=30$ numerical features of which $29$ were continuous and $1$ discrete.

\subsection{Classification Methods}
Formally, the task of this challenge was to find a \emph{binary classification function}

\begin{equation}
	f: \mathbb{R}^d \rightarrow \targetset
\end{equation}

that takes as input a feature vector $\features$ and outputs a guess about the correct label $\target \in \targetset$. To obtain $f$ we run a \emph{training algorithm} $\Train$ on a dataset to obtain a trained classifier

\begin{equation}
	\Train: (\Data, \theta) \rightarrow \classifier{\Train}{\Data}
\end{equation}

where $\theta$ is a set of model-specific hyperparameters and we denote as $\classifier{\Train}{\Data}$ the classifier trained on dataset $\Data$ using training algorithm $\Train$.\\

We describe in short the five different training algorithms that we tested.

\parabf{$\mathtt{Baseline}$}

\parabf{$\mathtt{LinearLS}$.} Given a dataset $\Data = (X, \targetvector)$, the linear least-squares algorithm ($\LS$) outputs a model

\begin{equation}
	\classifier{\LS}{\Data}(\features) = g(\features^T \weights_{\LS})
\end{equation}

where $\weights_{\LS} = (X^T X)^{-1}X^T \targetvector$ is the weights vector that minimises the mean squared error (MSE) between the true and predicted label and $g$ is a threshold function that outputs $\hat{\target} = b$ if $\features^T \weights_{\LS} \leq 0$ and $\hat{\target} = s$ otherwise.

\subsection{Evaluation Setup}
While the original challenge used a formal objective function referred to as \emph{approximate median significance}~\cite{HiggsML}, we used a simpler evaluation metrics to select the best performing discriminant function. Each trained model was evaluated based on its \emph{prediction accuracy} on a test set not used during training.\\
The prediction accuracy over a test set $\Dtest = (\Xtest, \ytest)$ is defined as
\begin{equation}
	\mathtt{Accuracy} = \underset{\Dtest}{\mathbb{E}}[ \mathbbm{1}_{\ytest = \ypred{test}} ]
\end{equation}
where $\ypred{test} = \classifier{\Train}{\Dtrain}(\Xtest)$ is a vector of predictions produced by a trained model evaluated over the test set and $\mathbbm{1}_{\ytest = \ypred{test}}$ the indicator function equal to $1$ if $\target = \hat{\target}$ and $0$ otherwise.

\parabf{Evaluation set.} To avoid overfitting to the final test set, a hold-out set was used throughout all experiments for model evaluation. To create this evaluation set, the 250,000 labelled records contained in \texttt{train.csv} were split into a training set $\Dtrain = (\Xtrain, \ytrain)$ with $|\Dtrain| = 175,000$ and an evaluation set $\Deval = (\Xeval, \yeval)$ with $|\Deval| = 75,000$. In subsequent experiments, $\Dtrain$ was used to train a candidate classifier $\classifier{\Train}{\Dtrain}$. The performance of each trained classifier was then evaluated as the prediction accuracy on the hold-out set $\Deval$.\\

\parabf{Cross-validation for hyperparameter and feature selection.} As described above, some training algorithms take as input a set of hyperparameters $\theta$. To find the optimal set of hyperparameters $\theta^*$, $k$-fold cross-validation with $k=5$ over $\Dtrain$ was used. The same procedure was used to select the top-ten most informative features using the methods described in \autoref{sec:data}.\\
  
After model and hyperparameter selection using the evaluation set, a final model was trained on the full training set
($\classifier{\Train}{\{ \Dtrain, \Deval\}}$) and the test set predictions were submitted to the challenge platform. 

\section{Results}
Some beautiful plots here

\section{Discussion}
Say all the things we could have done but didn't do.

\bibliography{references}
\bibliographystyle{IEEEtran}


\end{document}
