{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mattia\\anaconda3;C:\\Users\\Mattia\\anaconda3\\Library\\mingw-w64\\bin;C:\\Users\\Mattia\\anaconda3\\Library\\usr\\bin;C:\\Users\\Mattia\\anaconda3\\Library\\bin;C:\\Users\\Mattia\\anaconda3\\Scripts;C:\\Users\\Mattia\\anaconda3\\bin;C:\\Users\\Mattia\\anaconda3\\condabin;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0;C:\\Windows\\System32\\OpenSSH;C:\\Program Files\\MATLAB\\R2020a\\bin;C:\\Program Files\\Git\\cmd;C:\\Program Files\\Git\\mingw64\\bin;C:\\Program Files\\Git\\usr\\bin;C:\\Users\\Mattia\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\Mattia\\AppData\\Local\\GitHubDesktop\\bin;C:\\Users\\Mattia\\AppData\\Local\\Programs\\MiKTeX 2.9\\miktex\\bin\\x64;\\Users\\Mattia\\AppData\\Local\\Programs\\MiKTeX 2.9\\miktex\\bin\\x64;\\Users\\Mattia\\AppData\\Local\\Programs\\MiKTeX 2.9\\miktex\\bin\\x64;\\Users\\Mattia\\AppData\\Local\\Programs\\MiKTeX 2.9\\miktex\\bin\\x64;\\Users\\Mattia\\AppData\\Local\\Programs\\MiKTeX 2.9\\miktex\\bin\\x64;\\Users\\Mattia\\AppData\\Local\\Programs\\MiKTeX 2.9\\miktex\\bin\\x64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('../scripts/')\n",
    "\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + r'\\Users\\Mattia\\AppData\\Local\\Programs\\MiKTeX 2.9\\miktex\\bin\\x64'\n",
    "print(os.getenv(\"PATH\"))\n",
    "\n",
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "from implementation_variants import *\n",
    "from plots import *\n",
    "\n",
    "SEED = 42\n",
    "DATA_PATH = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/train.csv')\n",
    "df_train = df_train.set_index('Id')\n",
    "df_train_x = df_train[list(filter(lambda c: c != 'Prediction', list(df_train)))]\n",
    "\n",
    "FEATURE_NAMES = list(df_train_x)\n",
    "FEATURE_NAMES=FEATURE_NAMES+[str(i) for i in range(180-len(FEATURE_NAMES))]\n",
    "\n",
    " # Load train data\n",
    "y, x, ids = load_csv_data(path.join(DATA_PATH, 'train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(30):\n",
    "    meani=x[:,i][x[:,i] != -999.0].mean()\n",
    "    x[:,i][x[:,i] == -999.0]=meani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # train samples: 175000\n",
      " # eval samples: 75000\n"
     ]
    }
   ],
   "source": [
    "x=np.hstack((x,x**2,x**3,x**4,x**5,x**6))\n",
    "\n",
    "# Split into train and evaluation set\n",
    "(x_train, y_train), (x_eval, y_eval) = train_eval_split(y, x, split_ratio=.7, seed=1) #alternatives: CV or adding validation and testing\n",
    "\n",
    "print(f' # train samples: {len(y_train)}\\n # eval samples: {len(y_eval)}')\n",
    "\n",
    "num_dim = x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(features_to_remove):\n",
    "    feat_idx = [FEATURE_NAMES.index(f) for f in features_to_remove]\n",
    "    mask = np.ones(len(FEATURE_NAMES)).astype(bool)\n",
    "    mask[feat_idx] = False\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEAST_INFO = ['DER_mass_vis', 'DER_deltar_tau_lep','DER_pt_tot','PRI_tau_eta','PRI_tau_phi',\n",
    "              'PRI_lep_pt','PRI_lep_eta','PRI_lep_phi','PRI_met','PRI_met_phi']\n",
    "\n",
    "JET_NOT_DEFINED = ['DER_lep_eta_centrality', 'DER_prodeta_jet_jet', 'DER_mass_jet_jet', 'DER_deltaeta_jet_jet', \n",
    "                      'PRI_jet_subleading_eta', 'PRI_jet_subleading_phi', 'PRI_jet_subleading_pt']\n",
    "\n",
    "DER_FEATURES = list(filter(lambda c: 'DER' in c, list(FEATURE_NAMES)))\n",
    "\n",
    "feature_masks = [generate_mask([]), generate_mask(LEAST_INFO)]\n",
    "\n",
    "labels = ['Raw', 'MostInfo', 'ImputeJet', 'DeCorrelate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_features_visualisation(acc_train, acc_test, labels, model):\n",
    "    arr_train = acc_train.flatten()\n",
    "    arr_test = acc_test.flatten()\n",
    "\n",
    "    arr_features = np.concatenate([np.repeat(labels, acc_train.shape[1]), np.repeat(labels, acc_train.shape[1])])\n",
    "    arr_labels = np.concatenate([np.repeat('Train', len(arr_train)), np.repeat('Test', len(arr_test))])\n",
    "    arr_acc = np.concatenate([arr_train, arr_test])\n",
    "\n",
    "    plt_data = pd.DataFrame({'Feature Set':arr_features, 'Accuracy':arr_acc, 'Set':arr_labels})\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8,3.5))\n",
    "    ax = sns.boxplot(x='Feature Set', y='Accuracy', hue='Set', data=plt_data)\n",
    "    ax.set(ylabel='$\\mathtt{Accuracy}$', title=f'{model}');\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Least-Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_least_squares(y_train, x_train, y_eval, x_eval, feature_mask):\n",
    "    # Remove unwanted features\n",
    "    x_train = x_train[:, feature_mask]\n",
    "    x_eval = x_eval[:, feature_mask]\n",
    "    \n",
    "    # Standardise to training mean and s.d.\n",
    "    x_train, mean_x, std_x = standardise(x_train)\n",
    "    x_eval = standardise_to_fixed(x_eval, mean_x, std_x)\n",
    "    \n",
    "    # Run training\n",
    "    w, loss = least_squares(y_train, x_train)\n",
    "#     print(f'Training loss: {loss}')\n",
    "\n",
    "    # Get training accuracy\n",
    "    acc_tr  = eval_model(y_train, x_train, w, thresh=0)\n",
    "#     print(f'Training accuracy: {acc_tr}')\n",
    "\n",
    "    # Get accuracy on evaluation set\n",
    "    acc_te = eval_model(y_eval, x_eval, w, thresh=0)\n",
    "    print(f'Test Accuracy: {acc_te}')\n",
    "    \n",
    "    return w, loss, acc_tr, acc_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Set:  Raw\n",
      "Test Accuracy: 0.7517714285714285\n",
      "Test Accuracy: 0.7493714285714286\n",
      "Test Accuracy: 0.7536457142857143\n",
      "Test Accuracy: 0.7512914285714286\n",
      "Feature Set:  MostInfo\n",
      "Test Accuracy: 0.7523657142857143\n",
      "Test Accuracy: 0.74976\n",
      "Test Accuracy: 0.7532342857142857\n",
      "Test Accuracy: 0.75184\n",
      "Feature Set: Impute\n",
      "Test Accuracy: 0.7517714285714285\n",
      "Test Accuracy: 0.7493714285714286\n",
      "Test Accuracy: 0.7536457142857143\n",
      "Test Accuracy: 0.7512914285714286\n",
      "Feature Set: Indicator\n",
      "Test Accuracy: 0.7514057142857142\n",
      "Test Accuracy: 0.7481828571428571\n",
      "Test Accuracy: 0.7523885714285714\n",
      "Test Accuracy: 0.7508342857142857\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x252 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "k_fold = 4\n",
    "\n",
    "# Hyperparameter optimisation\n",
    "acc_train = np.empty((len(feature_masks)+2, k_fold), float)\n",
    "acc_test = np.empty((len(feature_masks)+2, k_fold), float)\n",
    "    \n",
    "for i, features in enumerate(feature_masks):\n",
    "    print('Feature Set: ', labels[i])\n",
    "    k = 0\n",
    "    for train_split, test_split in k_fold_iter(y_train, x_train, k_fold):\n",
    "            # Run evaluation\n",
    "            w, loss, acc_tr, acc_te = run_least_squares(train_split[1], train_split[0], \n",
    "                                                        test_split[1], test_split[0], features)\n",
    "            acc_train[i, k] = acc_tr\n",
    "            acc_test[i, k] = acc_te\n",
    "            \n",
    "            k+=1\n",
    "            \n",
    "print('Feature Set: Impute')\n",
    "col_index = [FEATURE_NAMES.index(i) for i in JET_NOT_DEFINED]\n",
    "x_train_imp = x_train.copy()\n",
    "x_train_imp[x_train_imp == -999] = -100\n",
    "\n",
    "x_eval_imp = x_eval.copy()\n",
    "x_eval_imp[x_eval_imp == -999] = -100\n",
    "\n",
    "features = generate_mask([])\n",
    "\n",
    "k = 0\n",
    "for train_split, test_split in k_fold_iter(y_train, x_train_imp, k_fold):\n",
    "        # Run evaluation\n",
    "        w, loss, acc_tr, acc_te = run_least_squares(train_split[1], train_split[0], \n",
    "                                                    test_split[1], test_split[0], features)\n",
    "        acc_train[-2, k] = acc_tr\n",
    "        acc_test[-2, k] = acc_te\n",
    "\n",
    "        k+=1\n",
    "        \n",
    "print('Feature Set: Indicator')\n",
    "JET_NUM_IDX = FEATURE_NAMES.index('PRI_jet_num')\n",
    "\n",
    "x_ind = (x_train[:, JET_NUM_IDX] > 1).astype(int)\n",
    "# x_train_imp = np.delete(x_train, JET_NUM_IDX, axis=1)\n",
    "x_train_imp = np.concatenate([x_train, np.expand_dims(x_ind, axis=1)], axis=1)\n",
    "\n",
    "features = generate_mask(['DER_deltaeta_jet_jet', 'DER_prodeta_jet_jet'])\n",
    "features = np.concatenate([features, [True]])\n",
    "\n",
    "k = 0\n",
    "for train_split, test_split in k_fold_iter(y_train, x_train_imp, k_fold):\n",
    "        # Run evaluation\n",
    "        w, loss, acc_tr, acc_te = run_least_squares(train_split[1], train_split[0], \n",
    "                                                    test_split[1], test_split[0], features)\n",
    "        acc_train[-1, k] = acc_tr\n",
    "        acc_test[-1, k] = acc_te\n",
    "\n",
    "        k+=1\n",
    "            \n",
    "fig = compare_features_visualisation(acc_train, acc_test, labels, '$\\mathtt{LeastSquares}$')\n",
    "fig.savefig('../report/compare_features_ls.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Ridge Regresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ridge(y_train, x_train, y_eval, x_eval, feature_mask):\n",
    "    # Remove unwanted features\n",
    "    x_train = x_train[:, feature_mask]\n",
    "    x_eval = x_eval[:, feature_mask]\n",
    "    \n",
    "    # Standardise to training mean and s.d.\n",
    "    x_train, mean_x, std_x = standardise(x_train)\n",
    "    x_eval = standardise_to_fixed(x_eval, mean_x, std_x)\n",
    "    \n",
    "    lambdas = np.logspace(-8, -1, 10)\n",
    "    k_fold = 4\n",
    "\n",
    "    # Hyperparameter optimisation\n",
    "    acc_train = np.empty((len(lambdas), k_fold), float)\n",
    "    acc_test = np.empty((len(lambdas), k_fold), float)\n",
    "\n",
    "    for l, lambda_ in enumerate(lambdas):\n",
    "        k = 0\n",
    "        for train_split, test_split in k_fold_iter(y_train, x_train, k_fold):\n",
    "            # Train\n",
    "            w, loss = ridge_regression(train_split[1], train_split[0], lambda_)\n",
    "            acc_tr = eval_model(train_split[1], train_split[0], w)\n",
    "            acc_train[l, k] = acc_tr\n",
    "\n",
    "            # Test\n",
    "            acc_te = eval_model(test_split[1], test_split[0], w)\n",
    "            acc_test[l, k] = acc_te\n",
    "\n",
    "            k += 1\n",
    "#     fig = cross_validation_visualization(lambdas, 1-acc_train, 1-acc_test)\n",
    "\n",
    "    avg_acc_test = np.mean(acc_test, axis=1)\n",
    "    lambda_opt_ridge = lambdas[np.argmax(avg_acc_test)]\n",
    "\n",
    "    print('Maximum test accuracy {} with lambda {}'.format(np.max(avg_acc_test), lambda_opt_ridge))\n",
    "    \n",
    "    # Run training\n",
    "    w, loss = ridge_regression(y_train, x_train, lambda_opt_ridge)\n",
    "#     print(f'Training loss: {loss_ridge}')\n",
    "\n",
    "    acc_tr = eval_model(y_train, x_train, w)\n",
    "#     print(f'Training accuracy: {acc}')\n",
    "\n",
    "    acc_te = eval_model(y_eval, x_eval, w)\n",
    "    print(f'Test accuracy: {acc_te}')\n",
    "    \n",
    "    \n",
    "    return w, loss, acc_tr, acc_te\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Set Raw\n",
      "Maximum test accuracy 0.7419617822747775 with lambda 0.0004641588833612782\n",
      "Test accuracy: 0.74544\n",
      "Maximum test accuracy 0.7413293916859686 with lambda 0.0027825594022071257\n",
      "Test accuracy: 0.7404114285714286\n",
      "Maximum test accuracy 0.7402169937827623 with lambda 0.0027825594022071257\n",
      "Test accuracy: 0.74208\n",
      "Maximum test accuracy 0.7470056686578082 with lambda 1.2915496650148827e-05\n",
      "Test accuracy: 0.7490971428571429\n",
      "\n",
      "Feature Set MostInfo\n",
      "Maximum test accuracy 0.742434170425454 with lambda 0.0004641588833612782\n",
      "Test accuracy: 0.7456\n",
      "Maximum test accuracy 0.7418703523101304 with lambda 0.0004641588833612782\n",
      "Test accuracy: 0.7434514285714285\n",
      "Maximum test accuracy 0.7399731805437034 with lambda 0.0027825594022071257\n",
      "Test accuracy: 0.7425371428571429\n",
      "Maximum test accuracy 0.7477447275387052 with lambda 1.2915496650148827e-05\n",
      "Test accuracy: 0.7499428571428571\n",
      "\n",
      "Feature Set: ImputeJet\n",
      "Maximum test accuracy 0.7419617822747775 with lambda 0.0004641588833612782\n",
      "Test accuracy: 0.74544\n",
      "Maximum test accuracy 0.7413293916859686 with lambda 0.0027825594022071257\n",
      "Test accuracy: 0.7404114285714286\n",
      "Maximum test accuracy 0.7402169937827623 with lambda 0.0027825594022071257\n",
      "Test accuracy: 0.74208\n",
      "Maximum test accuracy 0.7470056686578082 with lambda 1.2915496650148827e-05\n",
      "Test accuracy: 0.7490971428571429\n",
      "\n",
      "Feature Set: DeCorrelate\n",
      "Maximum test accuracy 0.7413674875045715 with lambda 0.0004641588833612782\n",
      "Test accuracy: 0.7451885714285714\n",
      "Maximum test accuracy 0.7405750944776301 with lambda 0.0027825594022071257\n",
      "Test accuracy: 0.7402285714285715\n",
      "Maximum test accuracy 0.7393636474460563 with lambda 0.0027825594022071257\n",
      "Test accuracy: 0.7405257142857142\n",
      "Maximum test accuracy 0.7462132756308668 with lambda 1.2915496650148827e-05\n",
      "Test accuracy: 0.7496685714285715\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x252 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "k_fold = 4\n",
    "\n",
    "# Hyperparameter optimisation\n",
    "acc_train = np.empty((len(labels), k_fold), float)\n",
    "acc_test = np.empty((len(labels), k_fold), float)\n",
    "    \n",
    "for i, features in enumerate(feature_masks):\n",
    "    print('\\nFeature Set', labels[i])\n",
    "    k = 0\n",
    "    for train_split, test_split in k_fold_iter(y_train, x_train, k_fold):\n",
    "            # Run evaluation\n",
    "            w, loss, acc_tr, acc_te = run_ridge(train_split[1], train_split[0], \n",
    "                                                        test_split[1], test_split[0], features)\n",
    "            acc_train[i, k] = acc_tr\n",
    "            acc_test[i, k] = acc_te\n",
    "            \n",
    "            k+=1\n",
    "            \n",
    "print('\\nFeature Set: ImputeJet')\n",
    "col_index = [FEATURE_NAMES.index(i) for i in JET_NOT_DEFINED]\n",
    "x_train_imp = x_train.copy()\n",
    "x_train_imp[x_train_imp == -999] = -100\n",
    "\n",
    "x_eval_imp = x_eval.copy()\n",
    "x_eval_imp[x_eval_imp == -999] = -100\n",
    "\n",
    "features = generate_mask([])\n",
    "\n",
    "k = 0\n",
    "for train_split, test_split in k_fold_iter(y_train, x_train_imp, k_fold):\n",
    "        # Run evaluation\n",
    "        w, loss, acc_tr, acc_te = run_ridge(train_split[1], train_split[0], \n",
    "                                                    test_split[1], test_split[0], features)\n",
    "        acc_train[-2, k] = acc_tr\n",
    "        acc_test[-2, k] = acc_te\n",
    "\n",
    "        k+=1\n",
    "        \n",
    "print('\\nFeature Set: DeCorrelate')\n",
    "JET_NUM_IDX = FEATURE_NAMES.index('PRI_jet_num')\n",
    "\n",
    "x_ind = (x_train[:, JET_NUM_IDX] > 1).astype(int)\n",
    "# x_train_imp = np.delete(x_train, JET_NUM_IDX, axis=1)\n",
    "x_train_imp = np.concatenate([x_train, np.expand_dims(x_ind, axis=1)], axis=1)\n",
    "\n",
    "features = generate_mask(['DER_deltaeta_jet_jet', 'DER_prodeta_jet_jet'])\n",
    "features = np.concatenate([features, [True]])\n",
    "\n",
    "k = 0\n",
    "for train_split, test_split in k_fold_iter(y_train, x_train_imp, k_fold):\n",
    "        # Run evaluation\n",
    "        w, loss, acc_tr, acc_te = run_ridge(train_split[1], train_split[0], \n",
    "                                                    test_split[1], test_split[0], features)\n",
    "        acc_train[-1, k] = acc_tr\n",
    "        acc_test[-1, k] = acc_te\n",
    "\n",
    "        k+=1\n",
    "            \n",
    "fig = compare_features_visualisation(acc_train, acc_test, labels, '$\\mathtt{Ridge}$')\n",
    "fig.savefig('../report/compare_features_ridge.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_logistic(y_train, x_train, y_eval, x_eval, feature_mask):\n",
    "    # Remove unwanted features\n",
    "    x_train = x_train[:, feature_mask]\n",
    "    x_eval = x_eval[:, feature_mask]\n",
    "    \n",
    "    # Standardise to training mean and s.d.\n",
    "    x_train, mean_x, std_x = standardise(x_train)\n",
    "    x_eval = standardise_to_fixed(x_eval, mean_x, std_x)\n",
    "\n",
    "    tx_train = np.c_[np.ones(x_train.shape[0]), x_train]\n",
    "    tx_eval = np.c_[np.ones(x_eval.shape[0]), x_eval]\n",
    "    \n",
    "    gamma = 0.01\n",
    "    max_iters = 10000\n",
    "    w_initial = np.ones(tx_train.shape[1])\n",
    "\n",
    "    # Run gradient descent \n",
    "    w, loss = logistic_regression_mean(y_train, tx_train, w_initial, max_iters, gamma, threshold=1e-6)\n",
    "#     print(f'Training loss: {loss_lr}')\n",
    "\n",
    "    acc_tr = eval_model(y_train, tx_train, w, thresh=0.5)\n",
    "#     print(f'Training accuracy: {acc}')\n",
    "\n",
    "    acc_te = eval_model(y_eval, tx_eval, w, thresh=0.5)\n",
    "    print(f'Testing accuracy: {acc_te}')\n",
    "    \n",
    "    return w, loss, acc_tr, acc_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Set:  Raw\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-15aeb0349149>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[1;31m# Run evaluation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             w, loss, acc_tr, acc_te = run_logistic(train_split[1], train_split[0], \n\u001b[1;32m---> 13\u001b[1;33m                                                         test_split[1], test_split[0], features)\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[0macc_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0macc_tr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0macc_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0macc_te\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-59-322d77d19981>\u001b[0m in \u001b[0;36mrun_logistic\u001b[1;34m(y_train, x_train, y_eval, x_eval, feature_mask)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# Run gradient descent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_regression_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_initial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;31m#     print(f'Training loss: {loss_lr}')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Machine learning\\CS433_ML\\projects\\project1\\scripts\\implementation_variants.py\u001b[0m in \u001b[0;36mlogistic_regression_mean\u001b[1;34m(y, tx, initial_w, max_iters, gamma, threshold, verbose)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[1;31m# Compute new loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_loss_logreg_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Machine learning\\CS433_ML\\projects\\project1\\scripts\\costs.py\u001b[0m in \u001b[0;36mcompute_loss_logreg_mean\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcompute_loss_logreg_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_loss_logreg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Machine learning\\CS433_ML\\projects\\project1\\scripts\\costs.py\u001b[0m in \u001b[0;36mcompute_loss_logreg\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcompute_loss_logreg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;34m\"\"\"Compute the loss under a logistic regression model (negative log likelihood) with class labels {0, 1}.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Class labels must be encoded as {0, 1}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k_fold = 4\n",
    "\n",
    "# Hyperparameter optimisation\n",
    "acc_train = np.empty((len(feature_masks)+2, k_fold), float)\n",
    "acc_test = np.empty((len(feature_masks)+2, k_fold), float)\n",
    "    \n",
    "for i, features in enumerate(feature_masks):\n",
    "    print('\\nFeature Set: ', labels[i])\n",
    "    k = 0\n",
    "    for train_split, test_split in k_fold_iter(y_train, x_train, k_fold):\n",
    "            # Run evaluation\n",
    "            w, loss, acc_tr, acc_te = run_logistic(train_split[1], train_split[0], \n",
    "                                                        test_split[1], test_split[0], features)\n",
    "            acc_train[i, k] = acc_tr\n",
    "            acc_test[i, k] = acc_te\n",
    "            \n",
    "            k+=1\n",
    "            \n",
    "print('\\nFeature Set: Impute')\n",
    "col_index = [FEATURE_NAMES.index(i) for i in JET_NOT_DEFINED]\n",
    "x_train_imp = x_train.copy()\n",
    "x_train_imp[x_train_imp == -999] = -100\n",
    "\n",
    "x_eval_imp = x_eval.copy()\n",
    "x_eval_imp[x_eval_imp == -999] = -100\n",
    "\n",
    "features = generate_mask([])\n",
    "\n",
    "k = 0\n",
    "for train_split, test_split in k_fold_iter(y_train, x_train_imp, k_fold):\n",
    "        # Run evaluation\n",
    "        w, loss, acc_tr, acc_te = run_logistic(train_split[1], train_split[0], \n",
    "                                                    test_split[1], test_split[0], features)\n",
    "        acc_train[-2, k] = acc_tr\n",
    "        acc_test[-2, k] = acc_te\n",
    "\n",
    "        k+=1\n",
    "        \n",
    "print('\\nFeature Set: Indicator')\n",
    "JET_NUM_IDX = FEATURE_NAMES.index('PRI_jet_num')\n",
    "\n",
    "x_ind = (x_train[:, JET_NUM_IDX] > 1).astype(int)\n",
    "# x_train_imp = np.delete(x_train, JET_NUM_IDX, axis=1)\n",
    "x_train_imp = np.concatenate([x_train, np.expand_dims(x_ind, axis=1)], axis=1)\n",
    "\n",
    "features = generate_mask(['DER_deltaeta_jet_jet', 'DER_prodeta_jet_jet'])\n",
    "features = np.concatenate([features, [True]])\n",
    "\n",
    "k = 0\n",
    "for train_split, test_split in k_fold_iter(y_train, x_train_imp, k_fold):\n",
    "        # Run evaluation\n",
    "        w, loss, acc_tr, acc_te = run_logistic(train_split[1], train_split[0], \n",
    "                                                    test_split[1], test_split[0], features)\n",
    "        acc_train[-1, k] = acc_tr\n",
    "        acc_test[-1, k] = acc_te\n",
    "\n",
    "        k+=1\n",
    "            \n",
    "fig = compare_features_visualisation(acc_train, acc_test, labels, '$\\mathtt{LogReg}$')\n",
    "fig.savefig('../report/compare_features_logreg.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare model performance on best performing feature set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(175000, 179)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JET_NUM_IDX = FEATURE_NAMES.index('PRI_jet_num')\n",
    "\n",
    "def feature_transform(x):\n",
    "    x_ind = (x[:, JET_NUM_IDX] > 1).astype(int)\n",
    "    x = np.concatenate([x, np.expand_dims(x_ind, axis=1)], axis=1)\n",
    "\n",
    "    features = generate_mask(['DER_deltaeta_jet_jet', 'DER_prodeta_jet_jet'])\n",
    "    features = np.concatenate([features, [True]])\n",
    "\n",
    "    x = x[:, features]\n",
    "    \n",
    "    return x\n",
    "\n",
    "fx_train = feature_transform(x_train)\n",
    "fx_eval = feature_transform(x_eval)\n",
    "\n",
    "fx_train, mu_x, sigma_x = standardise(fx_train)\n",
    "fx_eval = standardise_to_fixed(fx_eval, mu_x, sigma_x)\n",
    "\n",
    "tx_train = np.c_[np.ones(fx_train.shape[0]), fx_train]\n",
    "tx_eval = np.c_[np.ones(fx_eval.shape[0]), fx_eval]\n",
    "\n",
    "fx_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.5495733333333334\n"
     ]
    }
   ],
   "source": [
    "# Get baseline frequency of the two classes in training data\n",
    "prior_probs = [sum(y_train == 1)/len(y_train), sum(y_train == 0)/len(y_train)]\n",
    "\n",
    "# Test error on evaluation set\n",
    "y_eval_pred = np.random.choice([1., 0], size=len(y_eval), p=prior_probs)\n",
    "\n",
    "acc_baseline = get_accuracy(y_eval_pred, y_eval)\n",
    "\n",
    "print(f'Testing Accuracy: {acc_baseline}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train LeastSquares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.13826596181052767\n",
      "Training accuracy: 0.7304742857142857\n",
      "Test Accuracy: 0.7284\n"
     ]
    }
   ],
   "source": [
    "# Get linear least-squares model\n",
    "w_ls, loss_ls = least_squares(y_train, fx_train)\n",
    "print(f'Training loss: {loss_ls}')\n",
    "\n",
    "# Get training accuracy\n",
    "acc_tr  = eval_model(y_train, fx_train, w_ls, thresh=0)\n",
    "print(f'Training accuracy: {acc_tr}')\n",
    "\n",
    "# Get accuracy on evaluation set\n",
    "acc_ls = eval_model(y_eval, fx_eval, w_ls, thresh=0)\n",
    "print(f'Test Accuracy: {acc_ls}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda: 1e-09\n",
      "lambda: 2.9763514416313133e-09\n",
      "lambda: 8.858667904100832e-09\n",
      "lambda: 2.6366508987303555e-08\n",
      "lambda: 7.847599703514607e-08\n",
      "lambda: 2.3357214690901212e-07\n",
      "lambda: 6.951927961775605e-07\n",
      "lambda: 2.06913808111479e-06\n",
      "lambda: 6.158482110660255e-06\n",
      "lambda: 1.8329807108324338e-05\n",
      "lambda: 5.4555947811685143e-05\n",
      "lambda: 0.0001623776739188721\n",
      "lambda: 0.0004832930238571752\n",
      "lambda: 0.0014384498882876629\n",
      "lambda: 0.004281332398719387\n",
      "lambda: 0.012742749857031322\n",
      "lambda: 0.03792690190732246\n",
      "lambda: 0.11288378916846883\n",
      "lambda: 0.33598182862837744\n",
      "lambda: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Run cross val\n",
    "lambdas = np.logspace(-9, 0, 20)\n",
    "k_fold = 4\n",
    "\n",
    "# Hyperparameter optimisation\n",
    "acc_train = np.empty((len(lambdas), k_fold), float)\n",
    "acc_test = np.empty((len(lambdas), k_fold), float)\n",
    "\n",
    "for l, lambda_ in enumerate(lambdas):\n",
    "    print(f'lambda: {lambda_}')\n",
    "    k = 0\n",
    "    for train_split, test_split in k_fold_iter(y_train, fx_train, k_fold):\n",
    "        # Train\n",
    "        w, loss = ridge_regression(train_split[1], train_split[0], lambda_)\n",
    "        acc_tr = eval_model(train_split[1], train_split[0], w)\n",
    "        acc_train[l, k] = acc_tr\n",
    "\n",
    "        # Test\n",
    "        acc_te = eval_model(test_split[1], test_split[0], w)\n",
    "        acc_test[l, k] = acc_te\n",
    "\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum test accuracy 0.72644 with lambda 1e-09\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = cross_validation_visualization(lambdas, 1-acc_train, 1-acc_test)\n",
    "\n",
    "avg_acc_test = np.mean(acc_test, axis=1)\n",
    "lambda_opt_ridge = lambdas[np.argmax(avg_acc_test)]\n",
    "\n",
    "print('Maximum test accuracy {} with lambda {}'.format(np.max(avg_acc_test), lambda_opt_ridge))\n",
    "\n",
    "fig.savefig('../report/ridge_crossval.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.13933829612385162\n",
      "Training accuracy: 0.7264914285714286\n",
      "Test accuracy: 0.7260133333333333\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "w_ridge, loss_ridge = ridge_regression(y_train, fx_train, lambda_opt_ridge)\n",
    "print(f'Training loss: {loss_ridge}')\n",
    "\n",
    "acc = eval_model(y_train, fx_train, w_ridge)\n",
    "print(f'Training accuracy: {acc}')\n",
    "\n",
    "acc_ridge = eval_model(y_eval, fx_eval, w_ridge)\n",
    "print(f'Test accuracy: {acc_ridge}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss convergence:Terminate because loss did not change by more than threshold.\n",
      "Loss convergence:Terminate because loss did not change by more than threshold.\n",
      "Loss convergence:Terminate because loss did not change by more than threshold.\n",
      "Loss convergence:Terminate because loss did not change by more than threshold.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-c755f1dfb9eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m# Train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         w, loss = logistic_regression_mean(train_split[1], train_split[0], w_initial, \n\u001b[1;32m---> 17\u001b[1;33m                                            max_iters, gamma, threshold=1e-6, verbose=False)\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0macc_tr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_split\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_split\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthresh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Machine learning\\CS433_ML\\projects\\project1\\scripts\\implementation_variants.py\u001b[0m in \u001b[0;36mlogistic_regression_mean\u001b[1;34m(y, tx, initial_w, max_iters, gamma, threshold, verbose)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[1;31m# Compute new loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_loss_logreg_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Machine learning\\CS433_ML\\projects\\project1\\scripts\\costs.py\u001b[0m in \u001b[0;36mcompute_loss_logreg_mean\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcompute_loss_logreg_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_loss_logreg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Machine learning\\CS433_ML\\projects\\project1\\scripts\\costs.py\u001b[0m in \u001b[0;36mcompute_loss_logreg\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcompute_loss_logreg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;34m\"\"\"Compute the loss under a logistic regression model (negative log likelihood) with class labels {0, 1}.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Class labels must be encoded as {0, 1}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run cross-validation to find optimal step-size\n",
    "gammas = np.logspace(-7, -2, 5)\n",
    "k_fold = 4\n",
    "\n",
    "max_iters = 5000\n",
    "w_initial = np.ones(tx_train.shape[1])\n",
    "\n",
    "# Hyperparameter optimisation\n",
    "acc_train = np.empty((len(gammas), k_fold), float)\n",
    "acc_test = np.empty((len(gammas), k_fold), float)\n",
    "\n",
    "for g, gamma in enumerate(gammas):\n",
    "    k = 0\n",
    "    for train_split, test_split in k_fold_iter(y_train, tx_train, k_fold):\n",
    "        # Train\n",
    "        w, loss = logistic_regression_mean(train_split[1], train_split[0], w_initial, \n",
    "                                           max_iters, gamma, threshold=1e-6, verbose=False)\n",
    "        \n",
    "        acc_tr = eval_model(train_split[1], train_split[0], w, thresh=0.5)\n",
    "        acc_train[g, k] = acc_tr\n",
    "\n",
    "        # Test\n",
    "        acc_te = eval_model(test_split[1], test_split[0], w, thresh=0.5)\n",
    "        acc_test[g, k] = acc_te\n",
    "\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum test accuracy 0.7230571428571428 with gamma 0.01\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = cross_validation_visualization(gammas, 1-acc_train, 1-acc_test)\n",
    "fig.savefig('../report/crossval_lr.pdf', bbox_inches='tight')\n",
    "avg_acc_test = np.mean(acc_test, axis=1)\n",
    "gamma_opt_lr = gammas[np.argmax(avg_acc_test)]\n",
    "\n",
    "print('Maximum test accuracy {} with gamma {}'.format(np.max(avg_acc_test), gamma_opt_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent (0/14999): loss=4.9399464656485925, gradient=1.1070150667360843\n",
      "Gradient Descent (100/14999): loss=3.744638955603192, gradient=1.0757625234785748\n",
      "Gradient Descent (200/14999): loss=2.660183871882455, gradient=0.9939685748542625\n",
      "Gradient Descent (300/14999): loss=1.8358315281073783, gradient=0.8008442314450439\n",
      "Gradient Descent (400/14999): loss=1.373341534216789, gradient=0.5664264578414981\n",
      "Gradient Descent (500/14999): loss=1.1283284058888108, gradient=0.4414843709133819\n",
      "Gradient Descent (600/14999): loss=0.9608052299096556, gradient=0.3813570647365515\n",
      "Gradient Descent (700/14999): loss=0.834100959239845, gradient=0.33095041231472316\n",
      "Gradient Descent (800/14999): loss=0.7403573840109454, gradient=0.2818153614972203\n",
      "Gradient Descent (900/14999): loss=0.6735010928901823, gradient=0.23613902384085597\n",
      "Gradient Descent (1000/14999): loss=0.6270053126903851, gradient=0.19614762202692698\n",
      "Gradient Descent (1100/14999): loss=0.5950191683662266, gradient=0.16260812861450613\n",
      "Gradient Descent (1200/14999): loss=0.5729789948998237, gradient=0.1352718223081525\n",
      "Gradient Descent (1300/14999): loss=0.5576363695049985, gradient=0.11327327997296995\n",
      "Gradient Descent (1400/14999): loss=0.5467934531249612, gradient=0.09565434975145502\n",
      "Gradient Descent (1500/14999): loss=0.5389923063535039, gradient=0.0815362735522871\n",
      "Gradient Descent (1600/14999): loss=0.5332700941306313, gradient=0.07019321279257965\n",
      "Gradient Descent (1700/14999): loss=0.528987489186593, gradient=0.06104532180738103\n",
      "Gradient Descent (1800/14999): loss=0.5257161139205274, gradient=0.05363487686138744\n",
      "Gradient Descent (1900/14999): loss=0.5231657282822834, gradient=0.047602496364440736\n",
      "Gradient Descent (2000/14999): loss=0.5211372603256591, gradient=0.04266620588522998\n",
      "Gradient Descent (2100/14999): loss=0.5194924021346482, gradient=0.03860460839301821\n",
      "Gradient Descent (2200/14999): loss=0.5181337794978406, gradient=0.03524368361349481\n",
      "Gradient Descent (2300/14999): loss=0.5169919067042482, gradient=0.03244637204180193\n",
      "Gradient Descent (2400/14999): loss=0.5160165282005124, gradient=0.03010437242163754\n",
      "Gradient Descent (2500/14999): loss=0.5151708154757387, gradient=0.028131749840285093\n",
      "Gradient Descent (2600/14999): loss=0.5144274352597575, gradient=0.026460001737582003\n",
      "Gradient Descent (2700/14999): loss=0.5137658544404596, gradient=0.025034267275724272\n",
      "Gradient Descent (2800/14999): loss=0.5131704702971358, gradient=0.023810415873717974\n",
      "Gradient Descent (2900/14999): loss=0.5126292974326444, gradient=0.02275280425400025\n",
      "Gradient Descent (3000/14999): loss=0.5121330344757822, gradient=0.02183253955021163\n",
      "Gradient Descent (3100/14999): loss=0.5116743928682533, gradient=0.021026125654577104\n",
      "Gradient Descent (3200/14999): loss=0.5112476086336527, gradient=0.02031440094711704\n",
      "Gradient Descent (3300/14999): loss=0.5108480833805242, gradient=0.019681699006761175\n",
      "Gradient Descent (3400/14999): loss=0.5104721176146044, gradient=0.019115181333074965\n",
      "Gradient Descent (3500/14999): loss=0.5101167107104211, gradient=0.018604303907318626\n",
      "Gradient Descent (3600/14999): loss=0.5097794095274673, gradient=0.018140388775425544\n",
      "Gradient Descent (3700/14999): loss=0.5094581928807719, gradient=0.017716278677444568\n",
      "Gradient Descent (3800/14999): loss=0.5091513826887157, gradient=0.017326057783237283\n",
      "Gradient Descent (3900/14999): loss=0.5088575751458738, gradient=0.016964825337528196\n",
      "Gradient Descent (4000/14999): loss=0.5085755870516259, gradient=0.016628511835618608\n",
      "Gradient Descent (4100/14999): loss=0.5083044136971903, gradient=0.016313729501718093\n",
      "Gradient Descent (4200/14999): loss=0.5080431956300867, gradient=0.016017650504586953\n",
      "Gradient Descent (4300/14999): loss=0.5077911922815553, gradient=0.015737907645667824\n",
      "Gradient Descent (4400/14999): loss=0.5075477609316904, gradient=0.015472513281868456\n",
      "Gradient Descent (4500/14999): loss=0.5073123398492723, gradient=0.0152197930623364\n",
      "Gradient Descent (4600/14999): loss=0.5070844347136689, gradient=0.014978331712545707\n",
      "Gradient Descent (4700/14999): loss=0.5068636076295454, gradient=0.014746928624593278\n",
      "Gradient Descent (4800/14999): loss=0.5066494681991954, gradient=0.014524561436263754\n",
      "Gradient Descent (4900/14999): loss=0.5064416662347906, gradient=0.014310356123658228\n",
      "Gradient Descent (5000/14999): loss=0.5062398857829953, gradient=0.014103562409060823\n",
      "Gradient Descent (5100/14999): loss=0.5060438402039401, gradient=0.013903533509964467\n",
      "Gradient Descent (5200/14999): loss=0.5058532681005028, gradient=0.01370970943694577\n",
      "Gradient Descent (5300/14999): loss=0.5056679299358833, gradient=0.013521603195512698\n",
      "Gradient Descent (5400/14999): loss=0.5054876052103755, gradient=0.013338789366700236\n",
      "Gradient Descent (5500/14999): loss=0.5053120900941066, gradient=0.013160894638344477\n",
      "Gradient Descent (5600/14999): loss=0.5051411954329209, gradient=0.012987589937894515\n",
      "Gradient Descent (5700/14999): loss=0.5049747450607517, gradient=0.012818583881777479\n",
      "Gradient Descent (5800/14999): loss=0.5048125743646604, gradient=0.012653617308507118\n",
      "Gradient Descent (5900/14999): loss=0.5046545290589476, gradient=0.012492458705183881\n",
      "Gradient Descent (6000/14999): loss=0.5045004641329291, gradient=0.012334900371603676\n",
      "Gradient Descent (6100/14999): loss=0.5043502429435164, gradient=0.01218075519435839\n",
      "Gradient Descent (6200/14999): loss=0.5042037364290073, gradient=0.01202985392627424\n",
      "Gradient Descent (6300/14999): loss=0.504060822424754, gradient=0.011882042885269629\n",
      "Gradient Descent (6400/14999): loss=0.5039213850647852, gradient=0.011737182002011294\n",
      "Gradient Descent (6500/14999): loss=0.5037853142562604, gradient=0.011595143158248094\n",
      "Gradient Descent (6600/14999): loss=0.5036525052158792, gradient=0.011455808767924259\n",
      "Gradient Descent (6700/14999): loss=0.5035228580592154, gradient=0.011319070561542738\n",
      "Gradient Descent (6800/14999): loss=0.5033962774354476, gradient=0.011184828541104997\n",
      "Gradient Descent (6900/14999): loss=0.5032726722011835, gradient=0.011052990078576993\n",
      "Gradient Descent (7000/14999): loss=0.5031519551280974, gradient=0.01092346913544768\n",
      "Gradient Descent (7100/14999): loss=0.5030340426399182, gradient=0.010796185584740802\n",
      "Gradient Descent (7200/14999): loss=0.5029188545750056, gradient=0.01067106461996333\n",
      "Gradient Descent (7300/14999): loss=0.5028063139713154, gradient=0.010548036238046735\n",
      "Gradient Descent (7400/14999): loss=0.5026963468710287, gradient=0.010427034785459973\n",
      "Gradient Descent (7500/14999): loss=0.5025888821425176, gradient=0.010307998558426875\n",
      "Gradient Descent (7600/14999): loss=0.5024838513176467, gradient=0.010190869449632175\n",
      "Gradient Descent (7700/14999): loss=0.5023811884426881, gradient=0.010075592635002998\n",
      "Gradient Descent (7800/14999): loss=0.5022808299413614, gradient=0.009962116295151762\n",
      "Gradient Descent (7900/14999): loss=0.502182714488708, gradient=0.009850391366896739\n",
      "Gradient Descent (8000/14999): loss=0.5020867828946726, gradient=0.00974037132096912\n",
      "Gradient Descent (8100/14999): loss=0.5019929779964086, gradient=0.00963201196259346\n",
      "Gradient Descent (8200/14999): loss=0.501901244558449, gradient=0.009525271252111943\n",
      "Gradient Descent (8300/14999): loss=0.5018115291799811, gradient=0.009420109143229017\n",
      "Gradient Descent (8400/14999): loss=0.5017237802085585, gradient=0.009316487436793728\n",
      "Gradient Descent (8500/14999): loss=0.5016379476596586, gradient=0.00921436964832477\n",
      "Gradient Descent (8600/14999): loss=0.5015539831415583, gradient=0.00911372088772599\n",
      "Gradient Descent (8700/14999): loss=0.5014718397850639, gradient=0.009014507749846027\n",
      "Gradient Descent (8800/14999): loss=0.5013914721776752, gradient=0.008916698214710328\n",
      "Gradient Descent (8900/14999): loss=0.501312836301811, gradient=0.008820261556402977\n",
      "Gradient Descent (9000/14999): loss=0.5012358894767639, gradient=0.008725168259702654\n",
      "Gradient Descent (9100/14999): loss=0.5011605903040794, gradient=0.008631389943686325\n",
      "Gradient Descent (9200/14999): loss=0.5010868986160928, gradient=0.008538899291607761\n",
      "Gradient Descent (9300/14999): loss=0.501014775427375, gradient=0.008447669986438896\n",
      "Gradient Descent (9400/14999): loss=0.5009441828888691, gradient=0.008357676651531715\n",
      "Gradient Descent (9500/14999): loss=0.500875084244516, gradient=0.008268894795919106\n",
      "Gradient Descent (9600/14999): loss=0.5008074437901863, gradient=0.008181300763825679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent (9700/14999): loss=0.5007412268347543, gradient=0.008094871688005649\n",
      "Gradient Descent (9800/14999): loss=0.5006763996631624, gradient=0.008009585446565057\n",
      "Gradient Descent (9900/14999): loss=0.5006129295013376, gradient=0.007925420622961078\n",
      "Gradient Descent (10000/14999): loss=0.5005507844828353, gradient=0.007842356468902077\n",
      "Gradient Descent (10100/14999): loss=0.5004899336170957, gradient=0.007760372869899766\n",
      "Gradient Descent (10200/14999): loss=0.500430346759205, gradient=0.007679450313249064\n",
      "Gradient Descent (10300/14999): loss=0.500371994581067, gradient=0.00759956985823273\n",
      "Gradient Descent (10400/14999): loss=0.5003148485438939, gradient=0.007520713108367252\n",
      "Gradient Descent (10500/14999): loss=0.5002588808719375, gradient=0.007442862185523497\n",
      "Gradient Descent (10600/14999): loss=0.5002040645273809, gradient=0.007365999705770883\n",
      "Gradient Descent (10700/14999): loss=0.5001503731863235, gradient=0.007290108756807825\n",
      "Gradient Descent (10800/14999): loss=0.5000977812157974, gradient=0.0072151728768530795\n",
      "Gradient Descent (10900/14999): loss=0.5000462636517494, gradient=0.007141176034884269\n",
      "Gradient Descent (11000/14999): loss=0.49999579617794004, gradient=0.007068102612119192\n",
      "Gradient Descent (11100/14999): loss=0.49994635510570334, gradient=0.006995937384645254\n",
      "Gradient Descent (11200/14999): loss=0.49989791735452427, gradient=0.006924665507109805\n",
      "Gradient Descent (11300/14999): loss=0.4998504604333852, gradient=0.006854272497392133\n",
      "Gradient Descent (11400/14999): loss=0.49980396242284625, gradient=0.006784744222184129\n",
      "Gradient Descent (11500/14999): loss=0.4997584019578137, gradient=0.006716066883412921\n",
      "Gradient Descent (11600/14999): loss=0.49971375821096964, gradient=0.006648227005444264\n",
      "Gradient Descent (11700/14999): loss=0.4996700108768213, gradient=0.0065812114230104635\n",
      "Gradient Descent (11800/14999): loss=0.4996271401563453, gradient=0.006515007269811104\n",
      "Gradient Descent (11900/14999): loss=0.4995851267421951, gradient=0.0064496019677392335\n",
      "Gradient Descent (12000/14999): loss=0.49954395180444283, gradient=0.006384983216689175\n",
      "Gradient Descent (12100/14999): loss=0.49950359697683483, gradient=0.006321138984905885\n",
      "Gradient Descent (12200/14999): loss=0.49946404434353286, gradient=0.006258057499838709\n",
      "Gradient Descent (12300/14999): loss=0.4994252764263187, gradient=0.006195727239465415\n",
      "Gradient Descent (12400/14999): loss=0.4993872761722451, gradient=0.006134136924055117\n",
      "Gradient Descent (12500/14999): loss=0.4993500269417079, gradient=0.006073275508340784\n",
      "Gradient Descent (12600/14999): loss=0.49931351249692363, gradient=0.006013132174074832\n",
      "Gradient Descent (12700/14999): loss=0.49927771699079637, gradient=0.0059536963229426644\n",
      "Gradient Descent (12800/14999): loss=0.499242624956153, gradient=0.005894957569811553\n",
      "Gradient Descent (12900/14999): loss=0.49920822129533576, gradient=0.0058369057362933395\n",
      "Gradient Descent (13000/14999): loss=0.4991744912701327, gradient=0.005779530844601613\n",
      "Gradient Descent (13100/14999): loss=0.4991414204920372, gradient=0.005722823111685022\n",
      "Gradient Descent (13200/14999): loss=0.49910899491281924, gradient=0.005666772943619881\n",
      "Gradient Descent (13300/14999): loss=0.4990772008153964, gradient=0.005611370930246562\n",
      "Gradient Descent (13400/14999): loss=0.49904602480499555, gradient=0.0055566078400350275\n",
      "Gradient Descent (13500/14999): loss=0.4990154538005897, gradient=0.00550247461516613\n",
      "Gradient Descent (13600/14999): loss=0.49898547502660306, gradient=0.00544896236681623\n",
      "Gradient Descent (13700/14999): loss=0.49895607600487096, gradient=0.0053960623706333696\n",
      "Gradient Descent (13800/14999): loss=0.49892724454684834, gradient=0.005343766062394298\n",
      "Gradient Descent (13900/14999): loss=0.4988989687460536, gradient=0.005292065033832256\n",
      "Gradient Descent (14000/14999): loss=0.4988712369707427, gradient=0.005240951028626163\n",
      "Gradient Descent (14100/14999): loss=0.4988440378568027, gradient=0.005190415938542428\n",
      "Gradient Descent (14200/14999): loss=0.49881736030085755, gradient=0.005140451799721291\n",
      "Gradient Descent (14300/14999): loss=0.4987911934535784, gradient=0.0050910507891000515\n",
      "Gradient Descent (14400/14999): loss=0.49876552671319213, gradient=0.00504220522096611\n",
      "Gradient Descent (14500/14999): loss=0.4987403497191788, gradient=0.00499390754363309\n",
      "Gradient Descent (14600/14999): loss=0.49871565234615434, gradient=0.004946150336234122\n",
      "Gradient Descent (14700/14999): loss=0.4986914246979293, gradient=0.004898926305626006\n",
      "Gradient Descent (14800/14999): loss=0.49866765710173905, gradient=0.004852228283399279\n",
      "Gradient Descent (14900/14999): loss=0.4986443401026408, gradient=0.004806049222988788\n",
      "Training loss: 0.49862169105964604\n",
      "Training accuracy: 0.7255085714285714\n",
      "Testing accuracy: 0.72516\n"
     ]
    }
   ],
   "source": [
    "max_iters = 15000\n",
    "w_initial = np.ones(tx_train.shape[1])\n",
    "\n",
    "# Run gradient descent \n",
    "w_lr, loss_lr = logistic_regression_mean(y_train, tx_train, w_initial, max_iters, gamma_opt_lr, \n",
    "                                    threshold=1e-7, verbose=True)\n",
    "print(f'Training loss: {loss_lr}')\n",
    "\n",
    "acc = eval_model(y_train, tx_train, w_lr, thresh=0.5)\n",
    "print(f'Training accuracy: {acc}')\n",
    "\n",
    "acc_lr = eval_model(y_eval, tx_eval, w_lr, thresh=0.5)\n",
    "print(f'Testing accuracy: {acc_lr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare model performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy = [acc_baseline, acc_ls, acc_ridge, acc_lr]\n",
    "labels = ['$\\mathtt{Base}$', '$\\mathtt{LeastSquares}$', '$\\mathtt{Ridge}$',  \n",
    "          '$\\mathtt{Logistic}$']\n",
    "\n",
    "fig = model_comparison_visualization(accuracy, labels, acc_baseline)\n",
    "fig.savefig('../report/model_comparison_basic.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final training and test set prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all labelled records\n",
    "y, x, ids = load_csv_data(path.join(DATA_PATH, 'train.csv'))\n",
    "\n",
    "for i in range(30):\n",
    "    meani=x[:,i][x[:,i] != -999.0].mean()\n",
    "    x[:,i][x[:,i] == -999.0]=meani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_NAMES = list(df_train_x)\n",
    "FEATURE_NAMES=FEATURE_NAMES+[str(i) for i in range(30-len(FEATURE_NAMES))]\n",
    "\n",
    "# Apply feature transform\n",
    "fx = feature_transform(x)\n",
    "\n",
    "fx=np.hstack((fx,fx**2,fx**3,fx**4,fx**5,fx**6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise to mean and s.d.\n",
    "fx, mu_x, sigma_x = standardise(fx)\n",
    "\n",
    "# Add offset term\n",
    "tx = np.c_[np.ones(len(y)), fx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent (0/9999): loss=0.45560576772123523, gradient=0.010194141353334176\n",
      "Gradient Descent (0/9999): loss=0.45560576772123523, gradient=0.010194141353334176, w=[-1.06144914e+00  8.96702524e-01 -7.21073060e-01 -1.69983165e-01\n",
      "  4.21566509e-01 -1.07124563e-01  1.32660199e+00 -1.31423324e-01\n",
      "  8.72208535e-02 -3.82024356e-01  2.06149537e-01  1.02022626e-01\n",
      "  1.06304907e+00 -3.50993321e-04  6.38027246e-02  5.77119579e-01\n",
      "  1.33144657e-02  9.01194817e-02 -1.46251427e-01  4.17595058e-02\n",
      "  8.78517392e-02 -1.02154916e-01  4.87152088e-01  8.48690265e-03\n",
      "  6.08681268e-02  2.52683595e-01  2.38780653e-02  2.66522078e-02\n",
      "  3.25799529e-03  2.66214724e-02 -8.45789581e-01  5.09364830e-02\n",
      " -1.33494518e+00  9.36600544e-02  4.56634120e-02  4.25057211e-01\n",
      "  9.45714072e-01 -2.41559297e-01  2.20188025e-02  1.44732026e-01\n",
      "  5.24698352e-02 -4.92715602e-01 -7.67560464e-02  5.08570817e-02\n",
      "  1.20536079e-01 -1.64348544e-01  7.02292582e-02  3.72556012e-01\n",
      "  5.30927298e-02 -1.98255915e-01 -1.22065909e-01 -2.56629867e-01\n",
      "  3.70099700e-01  1.03049388e-01 -1.25281679e-01  1.96355494e-01\n",
      " -2.60751017e-03 -1.13833305e-01  2.66214724e-02 -6.20501028e-01\n",
      "  2.76120475e-01 -4.93892817e-01  7.59594919e-01  2.65226959e-01\n",
      " -1.02474905e-01  1.00133638e+00 -1.36990690e-01 -3.62438767e-02\n",
      "  4.44166493e-02  2.49992538e-02 -3.13414541e-01 -2.51930503e-02\n",
      " -4.35346257e-02 -1.25998239e-01 -2.83164757e-02 -6.23266896e-02\n",
      "  9.13195997e-01 -5.25279250e-02 -1.15256129e-01 -8.02947577e-02\n",
      " -3.05596524e-01 -4.55869112e-02 -5.25522271e-02 -2.14526311e-01\n",
      " -5.24990411e-02 -3.48567356e-02 -4.14565841e-03  2.66214724e-02\n",
      " -4.09361118e-02  4.48952989e-02  1.08467788e-01  9.62134885e-01\n",
      "  5.17941858e-01 -3.48998553e-01  1.00368469e+00  3.41296664e-03\n",
      " -6.40249542e-02  3.79535857e-03  1.53151692e-02  2.22159737e-01\n",
      " -6.72678580e-02 -2.69463632e-02 -1.29443413e-01 -6.56134723e-02\n",
      " -3.11596109e-02  9.90007974e-01 -2.52541770e-02  1.23594325e-02\n",
      " -3.45692362e-02 -6.99466161e-02  3.17580360e-02 -1.66071144e-02\n",
      " -9.45210023e-02  1.34661110e-02  1.80568663e-03  9.70899913e-03\n",
      "  2.66214724e-02  2.87140478e-01 -5.68796057e-02  2.87930152e-01\n",
      "  9.95950556e-01  7.02221499e-01 -4.07884441e-01  1.00405915e+00\n",
      "  5.80716974e-02  2.41175388e-02 -6.82926124e-02  1.47340242e-02\n",
      "  4.49657313e-01  2.68322003e-02 -4.71962873e-03 -4.13619526e-02\n",
      "  1.19697125e-02 -8.59933035e-03  1.00129308e+00  1.60559955e-02\n",
      "  6.31427787e-02  4.45491441e-03  1.81722163e-01  4.45107267e-02\n",
      "  1.33483808e-02  4.69325224e-02  3.70047772e-02  1.48030922e-02\n",
      " -2.76488650e-03  2.66214724e-02  4.34180708e-01 -3.22254719e-02\n",
      "  3.48693211e-01  1.00217054e+00  8.11646017e-01 -3.56435230e-01\n",
      "  1.00411438e+00  7.94368785e-02  1.36745469e-01 -6.01570113e-02\n",
      "  1.91213227e-02  5.32770987e-01  2.09732284e-02 -6.51304103e-03\n",
      "  6.44227212e-02  3.94935249e-02 -1.51179901e-02  1.00340407e+00\n",
      " -1.75051536e-02  8.75995964e-02  3.44884320e-02  3.52938030e-01\n",
      " -9.95256168e-02 -4.47300402e-02  1.59217218e-01 -4.73334433e-02\n",
      "  8.26098194e-03  8.34860112e-03  2.66214724e-02]\n",
      "Gradient Descent (5/9999): loss=0.4556015515548064, gradient=0.01018943258385102\n",
      "Gradient Descent (10/9999): loss=0.4555973400599607, gradient=0.010184710884408147\n",
      "Gradient Descent (15/9999): loss=0.45559313324051653, gradient=0.010179973948408274\n",
      "Gradient Descent (20/9999): loss=0.4555889311024205, gradient=0.010175219346329575\n",
      "Gradient Descent (25/9999): loss=0.45558473365385543, gradient=0.010170444521727296\n",
      "Gradient Descent (30/9999): loss=0.45558054090535127, gradient=0.010165646787561317\n",
      "Gradient Descent (35/9999): loss=0.45557635286989795, gradient=0.01016082332294042\n",
      "Gradient Descent (40/9999): loss=0.45557216956306235, gradient=0.01015597117038703\n",
      "Gradient Descent (45/9999): loss=0.45556799100310497, gradient=0.010151087233738509\n",
      "Gradient Descent (50/9999): loss=0.45556381721109646, gradient=0.010146168276814997\n",
      "Gradient Descent (55/9999): loss=0.4555596482110386, gradient=0.010141210922997523\n",
      "Gradient Descent (60/9999): loss=0.45555548402997875, gradient=0.01013621165587428\n",
      "Gradient Descent (65/9999): loss=0.455551324698127, gradient=0.010131166821126768\n",
      "Gradient Descent (70/9999): loss=0.455547170248967, gradient=0.010126072629841228\n",
      "Gradient Descent (75/9999): loss=0.4555430207193645, gradient=0.01012092516344239\n",
      "Gradient Descent (80/9999): loss=0.4555388761496681, gradient=0.010115720380457854\n",
      "Gradient Descent (85/9999): loss=0.45553473658380245, gradient=0.010110454125328737\n",
      "Gradient Descent (90/9999): loss=0.45553060206935286, gradient=0.010105122139487433\n",
      "Gradient Descent (95/9999): loss=0.4555264726576357, gradient=0.010099720074923949\n",
      "Gradient Descent (100/9999): loss=0.4555223484037555, gradient=0.010094243510456796\n",
      "Gradient Descent (105/9999): loss=0.4555182293666464, gradient=0.010088687970913831\n",
      "Gradient Descent (110/9999): loss=0.4555141156090912, gradient=0.010083048949409083\n",
      "Gradient Descent (115/9999): loss=0.4555100071977218, gradient=0.010077321932873827\n",
      "Gradient Descent (120/9999): loss=0.4555059042029909, gradient=0.010071502430961917\n",
      "Gradient Descent (125/9999): loss=0.4555018066991181, gradient=0.010065586008400927\n",
      "Gradient Descent (130/9999): loss=0.45549771476400364, gradient=0.010059568320798885\n",
      "Gradient Descent (135/9999): loss=0.45549362847910974, gradient=0.010053445153843264\n",
      "Gradient Descent (140/9999): loss=0.455489547929303, gradient=0.01004721246574232\n",
      "Gradient Descent (145/9999): loss=0.4554854732026608, gradient=0.010040866432660481\n",
      "Gradient Descent (150/9999): loss=0.4554814043902326, gradient=0.010034403496790035\n",
      "Gradient Descent (155/9999): loss=0.45547734158576325, gradient=0.010027820416581627\n",
      "Gradient Descent (160/9999): loss=0.45547328488536687, gradient=0.010021114318531749\n",
      "Gradient Descent (165/9999): loss=0.45546923438715975, gradient=0.010014282749795858\n",
      "Gradient Descent (170/9999): loss=0.45546519019084725, gradient=0.010007323730770207\n",
      "Gradient Descent (175/9999): loss=0.4554611523972678, gradient=0.010000235806665205\n",
      "Gradient Descent (180/9999): loss=0.45545712110789366, gradient=0.009993018096988488\n",
      "Gradient Descent (185/9999): loss=0.455453096424297, gradient=0.009985670341769571\n",
      "Gradient Descent (190/9999): loss=0.4554490784475771, gradient=0.009978192943300766\n",
      "Gradient Descent (195/9999): loss=0.45544506727776174, gradient=0.009970587002145013\n",
      "Gradient Descent (200/9999): loss=0.4554410630131839, gradient=0.009962854346178109\n",
      "Gradient Descent (205/9999): loss=0.4554370657498434, gradient=0.0099549975514947\n",
      "Gradient Descent (210/9999): loss=0.4554330755807603, gradient=0.00994701995411828\n",
      "Gradient Descent (215/9999): loss=0.45542909259533076, gradient=0.00993892565161508\n",
      "Gradient Descent (220/9999): loss=0.4554251168786918, gradient=0.009930719493920364\n",
      "Gradient Descent (225/9999): loss=0.45542114851110804, gradient=0.009922407062936582\n",
      "Gradient Descent (230/9999): loss=0.45541718756738775, gradient=0.00991399464075043\n",
      "Gradient Descent (235/9999): loss=0.4554132341163389, gradient=0.009905489166628964\n",
      "Gradient Descent (240/9999): loss=0.45540928822027527, gradient=0.009896898183281065\n",
      "Gradient Descent (245/9999): loss=0.45540534993457926, gradient=0.009888229773195525\n",
      "Gradient Descent (250/9999): loss=0.4554014193073293, gradient=0.009879492486175695\n",
      "Gradient Descent (255/9999): loss=0.45539749637899885, gradient=0.009870695259467861\n",
      "Gradient Descent (260/9999): loss=0.455393581182231, gradient=0.009861847332112532\n",
      "Gradient Descent (265/9999): loss=0.4553896737416915, gradient=0.009852958155322382\n",
      "Gradient Descent (270/9999): loss=0.4553857740740006, gradient=0.009844037300800258\n",
      "Gradient Descent (275/9999): loss=0.45538188218774717, gradient=0.009835094368948494\n",
      "Gradient Descent (280/9999): loss=0.455377998083577, gradient=0.009826138898887848\n",
      "Gradient Descent (285/9999): loss=0.4553741217543559, gradient=0.009817180282100992\n",
      "Gradient Descent (290/9999): loss=0.45537025318540286, gradient=0.009808227681352512\n",
      "Gradient Descent (295/9999): loss=0.4553663923547819, gradient=0.009799289956318947\n",
      "Gradient Descent (300/9999): loss=0.4553625392336531, gradient=0.009790375597107076\n",
      "Gradient Descent (305/9999): loss=0.45535869378666705, gradient=0.009781492666554575\n",
      "Gradient Descent (310/9999): loss=0.4553548559723986, gradient=0.009772648751913337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent (315/9999): loss=0.4553510257438113, gradient=0.009763850926220749\n",
      "Gradient Descent (320/9999): loss=0.45534720304874166, gradient=0.009755105719386653\n",
      "Gradient Descent (325/9999): loss=0.4553433878303967, gradient=0.009746419098766964\n",
      "Gradient Descent (330/9999): loss=0.45533958002785585, gradient=0.009737796458773866\n",
      "Gradient Descent (335/9999): loss=0.4553357795765712, gradient=0.009729242618887189\n",
      "Gradient Descent (340/9999): loss=0.4553319864088586, gradient=0.009720761829288764\n",
      "Gradient Descent (345/9999): loss=0.45532820045437283, gradient=0.009712357783239579\n",
      "Gradient Descent (350/9999): loss=0.45532442164056725, gradient=0.009704033635257769\n",
      "Gradient Descent (355/9999): loss=0.45532064989312565, gradient=0.009695792024130992\n",
      "Gradient Descent (360/9999): loss=0.4553168851363703, gradient=0.009687635099804224\n",
      "Gradient Descent (365/9999): loss=0.4553131272936425, gradient=0.009679564553219347\n",
      "Gradient Descent (370/9999): loss=0.45530937628765045, gradient=0.00967158164824001\n",
      "Gradient Descent (375/9999): loss=0.45530563204079, gradient=0.009663687254868181\n",
      "Gradient Descent (380/9999): loss=0.4553018944754314, gradient=0.009655881883043494\n",
      "Gradient Descent (385/9999): loss=0.45529816351417796, gradient=0.0096481657164072\n",
      "Gradient Descent (390/9999): loss=0.4552944390800944, gradient=0.009640538645504723\n",
      "Gradient Descent (395/9999): loss=0.4552907210969072, gradient=0.00963300029999302\n",
      "Gradient Descent (400/9999): loss=0.455287009489177, gradient=0.009625550079505798\n",
      "Gradient Descent (405/9999): loss=0.45528330418244767, gradient=0.009618187182910827\n",
      "Gradient Descent (410/9999): loss=0.4552796051033709, gradient=0.009610910635768416\n",
      "Gradient Descent (415/9999): loss=0.4552759121798086, gradient=0.009603719315864078\n",
      "Gradient Descent (420/9999): loss=0.45527222534091655, gradient=0.009596611976747167\n",
      "Gradient Descent (425/9999): loss=0.4552685445172108, gradient=0.009589587269253633\n",
      "Gradient Descent (430/9999): loss=0.4552648696406168, gradient=0.009582643761032857\n",
      "Gradient Descent (435/9999): loss=0.4552612006445043, gradient=0.009575779954129133\n",
      "Gradient Descent (440/9999): loss=0.45525753746371206, gradient=0.009568994300694439\n",
      "Gradient Descent (445/9999): loss=0.45525388003455797, gradient=0.009562285216927417\n",
      "Gradient Descent (450/9999): loss=0.45525022829484313, gradient=0.009555651095346892\n",
      "Gradient Descent (455/9999): loss=0.45524658218384567, gradient=0.009549090315516312\n",
      "Gradient Descent (460/9999): loss=0.45524294164230844, gradient=0.009542601253340421\n",
      "Gradient Descent (465/9999): loss=0.4552393066124205, gradient=0.00953618228905617\n",
      "Gradient Descent (470/9999): loss=0.45523567703779455, gradient=0.009529831814038754\n",
      "Gradient Descent (475/9999): loss=0.4552320528634391, gradient=0.009523548236539704\n",
      "Gradient Descent (480/9999): loss=0.45522843403572993, gradient=0.009517329986469404\n",
      "Gradient Descent (485/9999): loss=0.45522482050237517, gradient=0.009511175519329751\n",
      "Gradient Descent (490/9999): loss=0.4552212122123832, gradient=0.009505083319396068\n",
      "Gradient Descent (495/9999): loss=0.455217609116025, gradient=0.009499051902239881\n",
      "Gradient Descent (500/9999): loss=0.45521401116479865, gradient=0.009493079816676697\n",
      "Gradient Descent (505/9999): loss=0.45521041831139164, gradient=0.009487165646215353\n",
      "Gradient Descent (510/9999): loss=0.45520683050964367, gradient=0.009481308010078323\n",
      "Gradient Descent (515/9999): loss=0.45520324771450954, gradient=0.00947550556385503\n",
      "Gradient Descent (520/9999): loss=0.4551996698820215, gradient=0.009469756999843603\n",
      "Gradient Descent (525/9999): loss=0.45519609696925367, gradient=0.009464061047130627\n",
      "Gradient Descent (530/9999): loss=0.45519252893428597, gradient=0.009458416471451825\n",
      "Gradient Descent (535/9999): loss=0.4551889657361682, gradient=0.009452822074871994\n",
      "Gradient Descent (540/9999): loss=0.45518540733488705, gradient=0.009447276695317208\n",
      "Gradient Descent (545/9999): loss=0.4551818536913326, gradient=0.009441779205988106\n",
      "Gradient Descent (550/9999): loss=0.4551783047672653, gradient=0.00943632851467875\n",
      "Gradient Descent (555/9999): loss=0.4551747605252848, gradient=0.009430923563022676\n",
      "Gradient Descent (560/9999): loss=0.45517122092880047, gradient=0.009425563325683636\n",
      "Gradient Descent (565/9999): loss=0.4551676859420014, gradient=0.009420246809506818\n",
      "Gradient Descent (570/9999): loss=0.455164155529829, gradient=0.009414973052642849\n",
      "Gradient Descent (575/9999): loss=0.4551606296579483, gradient=0.009409741123655708\n",
      "Gradient Descent (580/9999): loss=0.4551571082927236, gradient=0.009404550120623054\n",
      "Gradient Descent (585/9999): loss=0.45515359140119205, gradient=0.009399399170236305\n",
      "Gradient Descent (590/9999): loss=0.4551500789510405, gradient=0.009394287426906445\n",
      "Gradient Descent (595/9999): loss=0.4551465709105811, gradient=0.009389214071879941\n",
      "Gradient Descent (600/9999): loss=0.45514306724872994, gradient=0.009384178312368943\n",
      "Gradient Descent (605/9999): loss=0.45513956793498545, gradient=0.009379179380698007\n",
      "Gradient Descent (610/9999): loss=0.45513607293940794, gradient=0.00937421653347004\n",
      "Gradient Descent (615/9999): loss=0.4551325822325992, gradient=0.0093692890507528\n",
      "Gradient Descent (620/9999): loss=0.455129095785685, gradient=0.009364396235286863\n",
      "Gradient Descent (625/9999): loss=0.45512561357029563, gradient=0.00935953741171621\n",
      "Gradient Descent (630/9999): loss=0.45512213555854947, gradient=0.009354711925841253\n",
      "Gradient Descent (635/9999): loss=0.4551186617230359, gradient=0.009349919143894806\n",
      "Gradient Descent (640/9999): loss=0.4551151920367998, gradient=0.009345158451840786\n",
      "Gradient Descent (645/9999): loss=0.45511172647332593, gradient=0.009340429254695264\n",
      "Gradient Descent (650/9999): loss=0.45510826500652424, gradient=0.009335730975869653\n",
      "Gradient Descent (655/9999): loss=0.4551048076107162, gradient=0.009331063056535608\n",
      "Gradient Descent (660/9999): loss=0.45510135426062165, gradient=0.00932642495501073\n",
      "Gradient Descent (665/9999): loss=0.45509790493134505, gradient=0.00932181614616511\n",
      "Gradient Descent (670/9999): loss=0.45509445959836425, gradient=0.009317236120847465\n",
      "Gradient Descent (675/9999): loss=0.4550910182375174, gradient=0.009312684385330732\n",
      "Gradient Descent (680/9999): loss=0.4550875808249933, gradient=0.009308160460776196\n",
      "Gradient Descent (685/9999): loss=0.45508414733731867, gradient=0.00930366388271566\n",
      "Gradient Descent (690/9999): loss=0.45508071775134984, gradient=0.00929919420055115\n",
      "Gradient Descent (695/9999): loss=0.4550772920442611, gradient=0.009294750977071166\n",
      "Gradient Descent (700/9999): loss=0.4550738701935363, gradient=0.009290333787983376\n",
      "Gradient Descent (705/9999): loss=0.4550704521769586, gradient=0.009285942221462956\n",
      "Gradient Descent (710/9999): loss=0.45506703797260345, gradient=0.009281575877715918\n",
      "Gradient Descent (715/9999): loss=0.45506362755882906, gradient=0.009277234368557227\n",
      "Gradient Descent (720/9999): loss=0.4550602209142678, gradient=0.009272917317002869\n",
      "Gradient Descent (725/9999): loss=0.4550568180178196, gradient=0.009268624356875657\n",
      "Gradient Descent (730/9999): loss=0.45505341884864486, gradient=0.009264355132424186\n",
      "Gradient Descent (735/9999): loss=0.4550500233861561, gradient=0.009260109297954497\n",
      "Gradient Descent (740/9999): loss=0.45504663161001246, gradient=0.009255886517474133\n",
      "Gradient Descent (745/9999): loss=0.455043243500112, gradient=0.009251686464348072\n",
      "Gradient Descent (750/9999): loss=0.45503985903658656, gradient=0.009247508820966172\n",
      "Gradient Descent (755/9999): loss=0.45503647819979565, gradient=0.009243353278421925\n",
      "Gradient Descent (760/9999): loss=0.45503310097032, gradient=0.009239219536201923\n",
      "Gradient Descent (765/9999): loss=0.4550297273289567, gradient=0.009235107301885818\n",
      "Gradient Descent (770/9999): loss=0.4550263572567139, gradient=0.009231016290856481\n",
      "Gradient Descent (775/9999): loss=0.4550229907348056, gradient=0.00922694622602008\n",
      "Gradient Descent (780/9999): loss=0.45501962774464727, gradient=0.00922289683753564\n",
      "Gradient Descent (785/9999): loss=0.4550162682678499, gradient=0.009218867862553718\n",
      "Gradient Descent (790/9999): loss=0.45501291228621776, gradient=0.00921485904496443\n",
      "Gradient Descent (795/9999): loss=0.45500955978174173, gradient=0.009210870135153875\n",
      "Gradient Descent (800/9999): loss=0.4550062107365969, gradient=0.009206900889769032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent (805/9999): loss=0.45500286513313826, gradient=0.009202951071491047\n",
      "Gradient Descent (810/9999): loss=0.4549995229538958, gradient=0.00919902044881627\n",
      "Gradient Descent (815/9999): loss=0.45499618418157295, gradient=0.009195108795845076\n",
      "Gradient Descent (820/9999): loss=0.45499284879904117, gradient=0.009191215892078107\n",
      "Gradient Descent (825/9999): loss=0.45498951678933747, gradient=0.009187341522219897\n",
      "Gradient Descent (830/9999): loss=0.4549861881356615, gradient=0.009183485475989298\n",
      "Gradient Descent (835/9999): loss=0.4549828628213714, gradient=0.009179647547936928\n",
      "Gradient Descent (840/9999): loss=0.4549795408299821, gradient=0.009175827537269058\n",
      "Gradient Descent (845/9999): loss=0.4549762221451611, gradient=0.009172025247678019\n",
      "Gradient Descent (850/9999): loss=0.45497290675072694, gradient=0.009168240487178722\n",
      "Gradient Descent (855/9999): loss=0.45496959463064585, gradient=0.009164473067951177\n",
      "Gradient Descent (860/9999): loss=0.4549662857690293, gradient=0.009160722806188968\n",
      "Gradient Descent (865/9999): loss=0.45496298015013165, gradient=0.009156989521953146\n",
      "Gradient Descent (870/9999): loss=0.4549596777583477, gradient=0.009153273039031722\n",
      "Gradient Descent (875/9999): loss=0.4549563785782108, gradient=0.009149573184804438\n",
      "Gradient Descent (880/9999): loss=0.4549530825943895, gradient=0.009145889790112577\n",
      "Gradient Descent (885/9999): loss=0.45494978979168693, gradient=0.009142222689133839\n",
      "Gradient Descent (890/9999): loss=0.4549465001550375, gradient=0.009138571719261836\n",
      "Gradient Descent (895/9999): loss=0.45494321366950596, gradient=0.00913493672099036\n",
      "Gradient Descent (900/9999): loss=0.4549399303202847, gradient=0.009131317537802115\n",
      "Gradient Descent (905/9999): loss=0.45493665009269196, gradient=0.009127714016061708\n",
      "Gradient Descent (910/9999): loss=0.4549333729721707, gradient=0.009124126004912877\n",
      "Gradient Descent (915/9999): loss=0.4549300989442864, gradient=0.009120553356179773\n",
      "Gradient Descent (920/9999): loss=0.45492682799472506, gradient=0.009116995924272151\n",
      "Gradient Descent (925/9999): loss=0.4549235601092922, gradient=0.009113453566094347\n",
      "Gradient Descent (930/9999): loss=0.4549202952739115, gradient=0.009109926140957873\n",
      "Gradient Descent (935/9999): loss=0.4549170334746225, gradient=0.009106413510497539\n",
      "Gradient Descent (940/9999): loss=0.454913774697579, gradient=0.009102915538591069\n",
      "Gradient Descent (945/9999): loss=0.45491051892904916, gradient=0.009099432091281922\n",
      "Gradient Descent (950/9999): loss=0.4549072661554126, gradient=0.00909596303670524\n",
      "Gradient Descent (955/9999): loss=0.4549040163631596, gradient=0.009092508245016999\n",
      "Gradient Descent (960/9999): loss=0.45490076953888975, gradient=0.009089067588326036\n",
      "Gradient Descent (965/9999): loss=0.45489752566931096, gradient=0.009085640940628885\n",
      "Gradient Descent (970/9999): loss=0.4548942847412381, gradient=0.009082228177747382\n",
      "Gradient Descent (975/9999): loss=0.45489104674159125, gradient=0.009078829177269047\n",
      "Gradient Descent (980/9999): loss=0.45488781165739545, gradient=0.009075443818489827\n",
      "Gradient Descent (985/9999): loss=0.4548845794757793, gradient=0.009072071982359363\n",
      "Gradient Descent (990/9999): loss=0.45488135018397363, gradient=0.009068713551428688\n",
      "Gradient Descent (995/9999): loss=0.45487812376930997, gradient=0.009065368409800214\n",
      "Gradient Descent (1000/9999): loss=0.4548749002192212, gradient=0.009062036443079699\n",
      "Gradient Descent (1000/9999): loss=0.4548749002192212, gradient=0.009062036443079699, w=[-1.06750820e+00  9.18946155e-01 -7.36320921e-01 -1.67444404e-01\n",
      "  4.33700705e-01 -9.85713934e-02  1.34251250e+00 -1.34342145e-01\n",
      "  8.57897677e-02 -3.95983755e-01  2.04378924e-01  9.91669797e-02\n",
      "  1.06753849e+00 -2.88716795e-04  6.05786558e-02  5.71176518e-01\n",
      "  1.32241447e-02  8.61762218e-02 -1.43607252e-01  3.99264537e-02\n",
      "  8.49297778e-02 -1.03603392e-01  4.88243903e-01  8.10166968e-03\n",
      "  5.70813527e-02  2.44345681e-01  2.38147512e-02  2.51333439e-02\n",
      "  1.87997260e-03  2.67404325e-02 -8.68065686e-01  6.78488042e-02\n",
      " -1.35405703e+00  7.79641409e-02  3.97719787e-02  4.19746934e-01\n",
      "  9.46281120e-01 -2.43813138e-01  3.78651071e-02  1.43843429e-01\n",
      "  5.48331590e-02 -4.98823285e-01 -7.67489771e-02  4.65859241e-02\n",
      "  1.20543758e-01 -1.64881239e-01  6.52036445e-02  3.62451238e-01\n",
      "  5.11208905e-02 -2.06470519e-01 -1.22484694e-01 -2.62284162e-01\n",
      "  3.73222716e-01  9.81010020e-02 -1.20399239e-01  2.02116083e-01\n",
      " -3.87046091e-03 -1.10373662e-01  2.67404325e-02 -6.29744424e-01\n",
      "  2.96906700e-01 -5.00006828e-01  7.52953350e-01  2.46031680e-01\n",
      " -1.12443717e-01  1.00120336e+00 -1.25937154e-01 -2.31470901e-02\n",
      "  4.52449145e-02  2.67519308e-02 -3.10395427e-01 -2.49652955e-02\n",
      " -4.31804541e-02 -1.19589058e-01 -2.81991657e-02 -6.19563503e-02\n",
      "  9.10292715e-01 -5.21437446e-02 -1.12427655e-01 -8.03221924e-02\n",
      " -3.04361558e-01 -4.43169566e-02 -5.22273792e-02 -2.09032440e-01\n",
      " -5.19222426e-02 -3.48168933e-02  8.55995365e-03  2.67404325e-02\n",
      " -3.54669595e-02  4.27465491e-02  1.02769569e-01  9.60833089e-01\n",
      "  4.98504841e-01 -3.55003717e-01  1.00366341e+00  1.79854191e-02\n",
      " -6.34492933e-02  4.69528370e-03  1.57044224e-02  2.18420948e-01\n",
      " -6.78604482e-02 -2.70904811e-02 -1.26642961e-01 -6.56763612e-02\n",
      " -3.11442665e-02  9.89441277e-01 -2.48809361e-02  2.03370996e-02\n",
      " -3.44607193e-02 -7.01974439e-02  3.06426942e-02 -1.69195103e-02\n",
      " -9.64220921e-02  1.27067590e-02  1.87511876e-03  1.58516021e-02\n",
      "  2.67404325e-02  2.96210516e-01 -7.30505992e-02  2.78496159e-01\n",
      "  9.95670166e-01  6.87370030e-01 -4.07471996e-01  1.00406610e+00\n",
      "  6.49985734e-02  1.81361401e-02 -6.82323912e-02  1.38259885e-02\n",
      "  4.37551373e-01  2.66310019e-02 -2.82825336e-03 -4.58329610e-02\n",
      "  1.18392061e-02 -6.17122436e-03  1.00117414e+00  1.70580610e-02\n",
      "  6.51688758e-02  4.60874231e-03  1.73288017e-01  4.35640651e-02\n",
      "  1.56873530e-02  3.96394025e-02  3.63151736e-02  1.56810093e-02\n",
      " -6.97263585e-03  2.67404325e-02  4.41689007e-01 -5.32503856e-02\n",
      "  3.36760604e-01  1.00210268e+00  8.01155253e-01 -3.50503975e-01\n",
      "  1.00412576e+00  7.65414146e-02  1.28360976e-01 -5.86545206e-02\n",
      "  1.71129007e-02  5.16234180e-01  2.16224432e-02 -3.68505831e-03\n",
      "  5.38700733e-02  3.98417828e-02 -1.18036895e-02  1.00338102e+00\n",
      " -1.65365305e-02  8.05058841e-02  3.46566247e-02  3.38141499e-01\n",
      " -1.00862690e-01 -4.17487930e-02  1.49275681e-01 -5.10908854e-02\n",
      "  8.55556969e-03 -3.11838115e-03  2.67404325e-02]\n",
      "Gradient Descent (1005/9999): loss=0.4548716795212385, gradient=0.009058717538330704\n",
      "Gradient Descent (1010/9999): loss=0.45486846166299194, gradient=0.00905541158403068\n",
      "Gradient Descent (1015/9999): loss=0.4548652466322084, gradient=0.009052118470029371\n",
      "Gradient Descent (1020/9999): loss=0.45486203441671136, gradient=0.009048838087508797\n",
      "Gradient Descent (1025/9999): loss=0.4548588250044196, gradient=0.00904557032894527\n",
      "Gradient Descent (1030/9999): loss=0.45485561838334665, gradient=0.009042315088072985\n",
      "Gradient Descent (1035/9999): loss=0.45485241454159947, gradient=0.009039072259849422\n",
      "Gradient Descent (1040/9999): loss=0.45484921346737794, gradient=0.009035841740422249\n",
      "Gradient Descent (1045/9999): loss=0.45484601514897416, gradient=0.009032623427097776\n",
      "Gradient Descent (1050/9999): loss=0.45484281957477074, gradient=0.009029417218310966\n",
      "Gradient Descent (1055/9999): loss=0.4548396267332412, gradient=0.009026223013596698\n",
      "Gradient Descent (1060/9999): loss=0.45483643661294826, gradient=0.00902304071356259\n",
      "Gradient Descent (1065/9999): loss=0.45483324920254325, gradient=0.009019870219862988\n",
      "Gradient Descent (1070/9999): loss=0.45483006449076624, gradient=0.009016711435174235\n",
      "Gradient Descent (1075/9999): loss=0.45482688246644337, gradient=0.009013564263171098\n",
      "Gradient Descent (1080/9999): loss=0.4548237031184882, gradient=0.009010428608504427\n",
      "Gradient Descent (1085/9999): loss=0.4548205264358996, gradient=0.009007304376779837\n",
      "Gradient Descent (1090/9999): loss=0.45481735240776183, gradient=0.009004191474537416\n",
      "Gradient Descent (1095/9999): loss=0.4548141810232428, gradient=0.009001089809232497\n",
      "Gradient Descent (1100/9999): loss=0.45481101227159465, gradient=0.008997999289217339\n",
      "Gradient Descent (1105/9999): loss=0.45480784614215236, gradient=0.008994919823723687\n",
      "Gradient Descent (1110/9999): loss=0.45480468262433305, gradient=0.008991851322846337\n",
      "Gradient Descent (1115/9999): loss=0.4548015217076356, gradient=0.008988793697527272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent (1120/9999): loss=0.4547983633816391, gradient=0.008985746859540875\n",
      "Gradient Descent (1125/9999): loss=0.45479520763600406, gradient=0.008982710721479699\n",
      "Gradient Descent (1130/9999): loss=0.45479205446046933, gradient=0.00897968519674098\n",
      "Gradient Descent (1135/9999): loss=0.4547889038448546, gradient=0.008976670199513932\n",
      "Gradient Descent (1140/9999): loss=0.4547857557790554, gradient=0.008973665644767512\n",
      "Gradient Descent (1145/9999): loss=0.45478261025304695, gradient=0.008970671448238913\n",
      "Gradient Descent (1150/9999): loss=0.4547794672568808, gradient=0.008967687526422664\n",
      "Gradient Descent (1155/9999): loss=0.454776326780685, gradient=0.008964713796560147\n",
      "Gradient Descent (1160/9999): loss=0.4547731888146637, gradient=0.008961750176629773\n",
      "Gradient Descent (1165/9999): loss=0.4547700533490961, gradient=0.0089587965853375\n",
      "Gradient Descent (1170/9999): loss=0.45476692037433575, gradient=0.008955852942107984\n",
      "Gradient Descent (1175/9999): loss=0.4547637898808111, gradient=0.008952919167076035\n",
      "Gradient Descent (1180/9999): loss=0.45476066185902325, gradient=0.008949995181078496\n",
      "Gradient Descent (1185/9999): loss=0.4547575362995468, gradient=0.00894708090564659\n",
      "Gradient Descent (1190/9999): loss=0.45475441319302795, gradient=0.00894417626299856\n",
      "Gradient Descent (1195/9999): loss=0.4547512925301854, gradient=0.008941281176032639\n",
      "Gradient Descent (1200/9999): loss=0.45474817430180847, gradient=0.008938395568320448\n",
      "Gradient Descent (1205/9999): loss=0.4547450584987574, gradient=0.008935519364100511\n",
      "Gradient Descent (1210/9999): loss=0.4547419451119621, gradient=0.008932652488272201\n",
      "Gradient Descent (1215/9999): loss=0.45473883413242194, gradient=0.008929794866389916\n",
      "Gradient Descent (1220/9999): loss=0.4547357255512055, gradient=0.00892694642465741\n",
      "Gradient Descent (1225/9999): loss=0.45473261935944964, gradient=0.008924107089922565\n",
      "Gradient Descent (1230/9999): loss=0.4547295155483585, gradient=0.008921276789671988\n",
      "Gradient Descent (1235/9999): loss=0.45472641410920395, gradient=0.008918455452026249\n",
      "Gradient Descent (1240/9999): loss=0.4547233150333242, gradient=0.00891564300573493\n",
      "Gradient Descent (1245/9999): loss=0.45472021831212395, gradient=0.008912839380172144\n",
      "Gradient Descent (1250/9999): loss=0.45471712393707325, gradient=0.008910044505331886\n",
      "Gradient Descent (1255/9999): loss=0.45471403189970716, gradient=0.008907258311823766\n",
      "Gradient Descent (1260/9999): loss=0.45471094219162506, gradient=0.008904480730868834\n",
      "Gradient Descent (1265/9999): loss=0.4547078548044907, gradient=0.00890171169429529\n",
      "Gradient Descent (1270/9999): loss=0.4547047697300311, gradient=0.008898951134534724\n",
      "Gradient Descent (1275/9999): loss=0.45470168696003593, gradient=0.008896198984618\n",
      "Gradient Descent (1280/9999): loss=0.45469860648635757, gradient=0.00889345517817147\n",
      "Gradient Descent (1285/9999): loss=0.45469552830091015, gradient=0.008890719649413237\n",
      "Gradient Descent (1290/9999): loss=0.4546924523956691, gradient=0.008887992333149438\n",
      "Gradient Descent (1295/9999): loss=0.45468937876267046, gradient=0.008885273164770573\n",
      "Gradient Descent (1300/9999): loss=0.4546863073940108, gradient=0.008882562080247939\n",
      "Gradient Descent (1305/9999): loss=0.45468323828184626, gradient=0.008879859016130049\n",
      "Gradient Descent (1310/9999): loss=0.45468017141839256, gradient=0.008877163909539076\n",
      "Gradient Descent (1315/9999): loss=0.4546771067959238, gradient=0.008874476698167462\n",
      "Gradient Descent (1320/9999): loss=0.45467404440677234, gradient=0.008871797320274287\n",
      "Gradient Descent (1325/9999): loss=0.45467098424332886, gradient=0.008869125714681887\n",
      "Gradient Descent (1330/9999): loss=0.45466792629804026, gradient=0.008866461820772456\n",
      "Gradient Descent (1335/9999): loss=0.45466487056341093, gradient=0.008863805578484419\n",
      "Gradient Descent (1340/9999): loss=0.45466181703200115, gradient=0.008861156928309223\n",
      "Gradient Descent (1345/9999): loss=0.45465876569642716, gradient=0.008858515811287675\n",
      "Gradient Descent (1350/9999): loss=0.45465571654936004, gradient=0.008855882169006624\n",
      "Gradient Descent (1355/9999): loss=0.454652669583526, gradient=0.008853255943595394\n",
      "Gradient Descent (1360/9999): loss=0.4546496247917054, gradient=0.008850637077722462\n",
      "Gradient Descent (1365/9999): loss=0.45464658216673254, gradient=0.00884802551459175\n",
      "Gradient Descent (1370/9999): loss=0.4546435417014943, gradient=0.008845421197939287\n",
      "Gradient Descent (1375/9999): loss=0.45464050338893136, gradient=0.008842824072029645\n",
      "Gradient Descent (1380/9999): loss=0.4546374672220361, gradient=0.008840234081652262\n",
      "Gradient Descent (1385/9999): loss=0.45463443319385277, gradient=0.008837651172118141\n",
      "Gradient Descent (1390/9999): loss=0.45463140129747726, gradient=0.008835075289255931\n",
      "Gradient Descent (1395/9999): loss=0.4546283715260563, gradient=0.008832506379408514\n",
      "Gradient Descent (1400/9999): loss=0.45462534387278697, gradient=0.008829944389429321\n",
      "Gradient Descent (1405/9999): loss=0.4546223183309165, gradient=0.00882738926667858\n",
      "Gradient Descent (1410/9999): loss=0.45461929489374137, gradient=0.008824840959019703\n",
      "Gradient Descent (1415/9999): loss=0.4546162735546076, gradient=0.008822299414815485\n",
      "Gradient Descent (1420/9999): loss=0.45461325430690913, gradient=0.008819764582924381\n",
      "Gradient Descent (1425/9999): loss=0.4546102371440889, gradient=0.008817236412696714\n",
      "Gradient Descent (1430/9999): loss=0.454607222059637, gradient=0.00881471485397082\n",
      "Gradient Descent (1435/9999): loss=0.454604209047091, gradient=0.0088121998570693\n",
      "Gradient Descent (1440/9999): loss=0.4546011981000353, gradient=0.008809691372795003\n",
      "Gradient Descent (1445/9999): loss=0.4545981892121006, gradient=0.00880718935242723\n",
      "Gradient Descent (1450/9999): loss=0.4545951823769637, gradient=0.008804693747717824\n",
      "Gradient Descent (1455/9999): loss=0.4545921775883466, gradient=0.008802204510887125\n",
      "Gradient Descent (1460/9999): loss=0.45458917484001693, gradient=0.008799721594620079\n",
      "Gradient Descent (1465/9999): loss=0.45458617412578695, gradient=0.008797244952062137\n",
      "Gradient Descent (1470/9999): loss=0.45458317543951265, gradient=0.00879477453681544\n",
      "Gradient Descent (1475/9999): loss=0.4545801787750942, gradient=0.00879231030293451\n",
      "Gradient Descent (1480/9999): loss=0.4545771841264754, gradient=0.008789852204922347\n",
      "Gradient Descent (1485/9999): loss=0.454574191487643, gradient=0.008787400197726327\n",
      "Gradient Descent (1490/9999): loss=0.45457120085262576, gradient=0.008784954236734059\n",
      "Gradient Descent (1495/9999): loss=0.45456821221549526, gradient=0.00878251427776931\n",
      "Gradient Descent (1500/9999): loss=0.45456522557036494, gradient=0.008780080277087815\n",
      "Gradient Descent (1505/9999): loss=0.45456224091138936, gradient=0.008777652191373178\n",
      "Gradient Descent (1510/9999): loss=0.45455925823276416, gradient=0.008775229977732701\n",
      "Gradient Descent (1515/9999): loss=0.45455627752872574, gradient=0.008772813593693175\n",
      "Gradient Descent (1520/9999): loss=0.45455329879355033, gradient=0.008770402997196738\n",
      "Gradient Descent (1525/9999): loss=0.45455032202155465, gradient=0.008767998146596656\n",
      "Gradient Descent (1530/9999): loss=0.45454734720709455, gradient=0.008765599000653122\n",
      "Gradient Descent (1535/9999): loss=0.45454437434456474, gradient=0.0087632055185291\n",
      "Gradient Descent (1540/9999): loss=0.4545414034283991, gradient=0.008760817659786023\n",
      "Gradient Descent (1545/9999): loss=0.4545384344530699, gradient=0.008758435384379717\n",
      "Gradient Descent (1550/9999): loss=0.4545354674130869, gradient=0.008756058652656027\n",
      "Gradient Descent (1555/9999): loss=0.4545325023029979, gradient=0.00875368742534671\n",
      "Gradient Descent (1560/9999): loss=0.4545295391173881, gradient=0.008751321663565239\n",
      "Gradient Descent (1565/9999): loss=0.454526577850879, gradient=0.008748961328802493\n",
      "Gradient Descent (1570/9999): loss=0.4545236184981296, gradient=0.008746606382922675\n",
      "Gradient Descent (1575/9999): loss=0.45452066105383443, gradient=0.00874425678815903\n",
      "Gradient Descent (1580/9999): loss=0.45451770551272375, gradient=0.008741912507109671\n",
      "Gradient Descent (1585/9999): loss=0.45451475186956436, gradient=0.00873957350273345\n",
      "Gradient Descent (1590/9999): loss=0.45451180011915726, gradient=0.008737239738345722\n",
      "Gradient Descent (1595/9999): loss=0.4545088502563387, gradient=0.008734911177614251\n",
      "Gradient Descent (1600/9999): loss=0.45450590227597965, gradient=0.008732587784554968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent (1605/9999): loss=0.45450295617298503, gradient=0.008730269523527954\n",
      "Gradient Descent (1610/9999): loss=0.45450001194229395, gradient=0.008727956359233261\n",
      "Gradient Descent (1615/9999): loss=0.454497069578879, gradient=0.008725648256706828\n",
      "Gradient Descent (1620/9999): loss=0.45449412907774583, gradient=0.00872334518131636\n",
      "Gradient Descent (1625/9999): loss=0.45449119043393316, gradient=0.008721047098757364\n",
      "Gradient Descent (1630/9999): loss=0.45448825364251266, gradient=0.008718753975048968\n",
      "Gradient Descent (1635/9999): loss=0.45448531869858816, gradient=0.008716465776530066\n",
      "Gradient Descent (1640/9999): loss=0.4544823855972953, gradient=0.008714182469855131\n",
      "Gradient Descent (1645/9999): loss=0.4544794543338021, gradient=0.008711904021990449\n",
      "Gradient Descent (1650/9999): loss=0.4544765249033072, gradient=0.008709630400209962\n",
      "Gradient Descent (1655/9999): loss=0.45447359730104103, gradient=0.00870736157209148\n",
      "Gradient Descent (1660/9999): loss=0.45447067152226506, gradient=0.008705097505512757\n",
      "Gradient Descent (1665/9999): loss=0.4544677475622707, gradient=0.008702838168647552\n",
      "Gradient Descent (1670/9999): loss=0.4544648254163802, gradient=0.008700583529961874\n",
      "Gradient Descent (1675/9999): loss=0.45446190507994594, gradient=0.008698333558210121\n",
      "Gradient Descent (1680/9999): loss=0.45445898654834943, gradient=0.008696088222431252\n",
      "Gradient Descent (1685/9999): loss=0.4544560698170026, gradient=0.008693847491945147\n",
      "Gradient Descent (1690/9999): loss=0.4544531548813458, gradient=0.008691611336348737\n",
      "Gradient Descent (1695/9999): loss=0.4544502417368491, gradient=0.008689379725512434\n",
      "Gradient Descent (1700/9999): loss=0.45444733037901064, gradient=0.008687152629576396\n",
      "Gradient Descent (1705/9999): loss=0.45444442080335756, gradient=0.008684930018946912\n",
      "Gradient Descent (1710/9999): loss=0.4544415130054448, gradient=0.008682711864292788\n",
      "Gradient Descent (1715/9999): loss=0.45443860698085564, gradient=0.008680498136541868\n",
      "Gradient Descent (1720/9999): loss=0.45443570272520084, gradient=0.008678288806877389\n",
      "Gradient Descent (1725/9999): loss=0.45443280023411886, gradient=0.008676083846734588\n",
      "Gradient Descent (1730/9999): loss=0.454429899503275, gradient=0.008673883227797134\n",
      "Gradient Descent (1735/9999): loss=0.4544270005283619, gradient=0.008671686921993839\n",
      "Gradient Descent (1740/9999): loss=0.454424103305099, gradient=0.00866949490149514\n",
      "Gradient Descent (1745/9999): loss=0.4544212078292321, gradient=0.008667307138709892\n",
      "Gradient Descent (1750/9999): loss=0.4544183140965334, gradient=0.008665123606281857\n",
      "Gradient Descent (1755/9999): loss=0.45441542210280117, gradient=0.008662944277086639\n",
      "Gradient Descent (1760/9999): loss=0.4544125318438596, gradient=0.008660769124228314\n",
      "Gradient Descent (1765/9999): loss=0.45440964331555844, gradient=0.008658598121036223\n",
      "Gradient Descent (1770/9999): loss=0.4544067565137729, gradient=0.008656431241061895\n",
      "Gradient Descent (1775/9999): loss=0.4544038714344034, gradient=0.008654268458075834\n",
      "Gradient Descent (1780/9999): loss=0.4544009880733756, gradient=0.00865210974606449\n",
      "Gradient Descent (1785/9999): loss=0.45439810642663947, gradient=0.008649955079227133\n",
      "Gradient Descent (1790/9999): loss=0.45439522649017, gradient=0.008647804431972939\n",
      "Gradient Descent (1795/9999): loss=0.4543923482599663, gradient=0.008645657778917903\n",
      "Gradient Descent (1800/9999): loss=0.45438947173205185, gradient=0.008643515094882029\n",
      "Gradient Descent (1805/9999): loss=0.4543865969024743, gradient=0.008641376354886276\n",
      "Gradient Descent (1810/9999): loss=0.45438372376730446, gradient=0.008639241534149847\n",
      "Gradient Descent (1815/9999): loss=0.4543808523226374, gradient=0.008637110608087277\n",
      "Gradient Descent (1820/9999): loss=0.45437798256459144, gradient=0.008634983552305658\n",
      "Gradient Descent (1825/9999): loss=0.4543751144893078, gradient=0.008632860342601913\n",
      "Gradient Descent (1830/9999): loss=0.4543722480929513, gradient=0.008630740954960109\n",
      "Gradient Descent (1835/9999): loss=0.4543693833717089, gradient=0.008628625365548688\n",
      "Gradient Descent (1840/9999): loss=0.4543665203217916, gradient=0.008626513550717991\n",
      "Gradient Descent (1845/9999): loss=0.45436365893943087, gradient=0.008624405486997487\n",
      "Gradient Descent (1850/9999): loss=0.4543607992208823, gradient=0.008622301151093394\n",
      "Gradient Descent (1855/9999): loss=0.45435794116242256, gradient=0.008620200519885967\n",
      "Gradient Descent (1860/9999): loss=0.45435508476035147, gradient=0.008618103570427233\n",
      "Gradient Descent (1865/9999): loss=0.454352230010989, gradient=0.008616010279938304\n",
      "Gradient Descent (1870/9999): loss=0.45434937691067856, gradient=0.008613920625807217\n",
      "Gradient Descent (1875/9999): loss=0.4543465254557837, gradient=0.008611834585586361\n",
      "Gradient Descent (1880/9999): loss=0.4543436756426899, gradient=0.008609752136990254\n",
      "Gradient Descent (1885/9999): loss=0.45434082746780385, gradient=0.008607673257893196\n",
      "Gradient Descent (1890/9999): loss=0.454337980927553, gradient=0.008605597926327042\n",
      "Gradient Descent (1895/9999): loss=0.45433513601838615, gradient=0.008603526120478966\n",
      "Gradient Descent (1900/9999): loss=0.45433229273677217, gradient=0.008601457818689248\n",
      "Gradient Descent (1905/9999): loss=0.45432945107920075, gradient=0.008599392999449108\n",
      "Gradient Descent (1910/9999): loss=0.45432661104218214, gradient=0.008597331641398658\n",
      "Gradient Descent (1915/9999): loss=0.45432377262224694, gradient=0.008595273723324746\n",
      "Gradient Descent (1920/9999): loss=0.45432093581594535, gradient=0.008593219224158948\n",
      "Gradient Descent (1925/9999): loss=0.4543181006198479, gradient=0.008591168122975525\n",
      "Gradient Descent (1930/9999): loss=0.4543152670305449, gradient=0.008589120398989474\n",
      "Gradient Descent (1935/9999): loss=0.4543124350446468, gradient=0.00858707603155452\n",
      "Gradient Descent (1940/9999): loss=0.45430960465878284, gradient=0.008585035000161292\n",
      "Gradient Descent (1945/9999): loss=0.454306775869602, gradient=0.008582997284435356\n",
      "Gradient Descent (1950/9999): loss=0.4543039486737726, gradient=0.00858096286413543\n",
      "Gradient Descent (1955/9999): loss=0.4543011230679824, gradient=0.008578931719151538\n",
      "Gradient Descent (1960/9999): loss=0.4542982990489375, gradient=0.008576903829503218\n",
      "Gradient Descent (1965/9999): loss=0.4542954766133636, gradient=0.008574879175337825\n",
      "Gradient Descent (1970/9999): loss=0.4542926557580046, gradient=0.008572857736928721\n",
      "Gradient Descent (1975/9999): loss=0.45428983647962334, gradient=0.008570839494673717\n",
      "Gradient Descent (1980/9999): loss=0.45428701877500105, gradient=0.008568824429093281\n",
      "Gradient Descent (1985/9999): loss=0.4542842026409376, gradient=0.00856681252082904\n",
      "Gradient Descent (1990/9999): loss=0.45428138807425095, gradient=0.008564803750642058\n",
      "Gradient Descent (1995/9999): loss=0.4542785750717771, gradient=0.008562798099411382\n",
      "Gradient Descent (2000/9999): loss=0.45427576363037026, gradient=0.008560795548132459\n",
      "Gradient Descent (2000/9999): loss=0.45427576363037026, gradient=0.008560795548132459, w=[-1.07341591e+00  9.40661889e-01 -7.50764531e-01 -1.64253626e-01\n",
      "  4.46906806e-01 -8.98057158e-02  1.35758139e+00 -1.36678786e-01\n",
      "  8.45388596e-02 -4.08546450e-01  2.02168213e-01  9.61466844e-02\n",
      "  1.07062867e+00 -2.69973202e-04  5.75758117e-02  5.66830227e-01\n",
      "  1.31380836e-02  8.25161186e-02 -1.41635400e-01  3.82184100e-02\n",
      "  8.37097076e-02 -1.04778602e-01  4.90412083e-01  7.68503863e-03\n",
      "  5.35958843e-02  2.35968173e-01  2.36963216e-02  2.37534031e-02\n",
      "  6.75134304e-04  2.68012481e-02 -8.90164401e-01  8.13406666e-02\n",
      " -1.37225123e+00  6.33158717e-02  3.47458573e-02  4.14225126e-01\n",
      "  9.46453784e-01 -2.47263227e-01  5.13101093e-02  1.42779626e-01\n",
      "  5.71264613e-02 -5.04415698e-01 -7.67241137e-02  4.26500465e-02\n",
      "  1.19493728e-01 -1.65363933e-01  6.05984442e-02  3.52534174e-01\n",
      "  4.93663450e-02 -2.14394577e-01 -1.22706063e-01 -2.66797937e-01\n",
      "  3.75930475e-01  9.34817690e-02 -1.15615734e-01  2.06920058e-01\n",
      " -5.20746285e-03 -1.08421661e-01  2.68012481e-02 -6.38856767e-01\n",
      "  3.15014180e-01 -5.05692894e-01  7.46459390e-01  2.27541277e-01\n",
      " -1.22171030e-01  1.00101923e+00 -1.17085633e-01 -1.23008207e-02\n",
      "  4.58008476e-02  2.85031860e-02 -3.05917596e-01 -2.47523714e-02\n",
      " -4.28110633e-02 -1.14504013e-01 -2.80795912e-02 -6.15479970e-02\n",
      "  9.07436519e-01 -5.17527771e-02 -1.10788590e-01 -8.01542493e-02\n",
      " -3.02071248e-01 -4.30697340e-02 -5.18968819e-02 -2.01843084e-01\n",
      " -5.13391867e-02 -3.47308202e-02  1.92713121e-02  2.68012481e-02\n",
      " -3.00922859e-02  4.10622774e-02  9.72146717e-02  9.59541082e-01\n",
      "  4.79439956e-01 -3.60512919e-01  1.00363600e+00  3.04315892e-02\n",
      " -6.33267140e-02  5.69448273e-03  1.61453451e-02  2.15847399e-01\n",
      " -6.84534292e-02 -2.72336583e-02 -1.23875632e-01 -6.57208934e-02\n",
      " -3.11631907e-02  9.88880678e-01 -2.45336812e-02  2.66180431e-02\n",
      " -3.41389822e-02 -6.95597317e-02  2.95599872e-02 -1.72364574e-02\n",
      " -9.48808045e-02  1.18679876e-02  1.92332212e-03  2.08631981e-02\n",
      "  2.68012481e-02  3.05055283e-01 -8.62230603e-02  2.69124794e-01\n",
      "  9.95389351e-01  6.72674420e-01 -4.06474931e-01  1.00407204e+00\n",
      "  7.04700070e-02  1.28764719e-02 -6.83296312e-02  1.30134608e-02\n",
      "  4.26190881e-01  2.64259196e-02 -1.10129730e-03 -4.93862865e-02\n",
      "  1.17233657e-02 -3.92465240e-03  1.00105570e+00  1.79462924e-02\n",
      "  6.58364124e-02  4.99371204e-03  1.65523480e-01  4.26541438e-02\n",
      "  1.77959635e-02  3.67649559e-02  3.56892853e-02  1.65195596e-02\n",
      " -1.12796152e-02  2.68012481e-02  4.48937073e-01 -6.95751128e-02\n",
      "  3.24864751e-01  1.00203426e+00  7.90721350e-01 -3.44009226e-01\n",
      "  1.00413674e+00  7.29017185e-02  1.21290059e-01 -5.68889297e-02\n",
      "  1.52371718e-02  5.00162155e-01  2.22532286e-02 -1.07366195e-03\n",
      "  4.45856421e-02  4.01843587e-02 -8.81278676e-03  1.00335776e+00\n",
      " -1.57260842e-02  7.26266851e-02  3.50695106e-02  3.23822231e-01\n",
      " -1.01821730e-01 -3.89922110e-02  1.44251491e-01 -5.42689822e-02\n",
      "  8.85717825e-03 -1.39039383e-02  2.68012481e-02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent (2005/9999): loss=0.4542729537469027, gradient=0.008558796077915628\n",
      "Gradient Descent (2010/9999): loss=0.454270145418264, gradient=0.008556799669984658\n",
      "Gradient Descent (2015/9999): loss=0.45426733864136215, gradient=0.00855480630567531\n",
      "Gradient Descent (2020/9999): loss=0.4542645334131217, gradient=0.008552815966433909\n",
      "Gradient Descent (2025/9999): loss=0.4542617297304859, gradient=0.008550828633815944\n",
      "Gradient Descent (2030/9999): loss=0.45425892759041425, gradient=0.008548844289484762\n",
      "Gradient Descent (2035/9999): loss=0.4542561269898843, gradient=0.008546862915210144\n",
      "Gradient Descent (2040/9999): loss=0.4542533279258905, gradient=0.008544884492867087\n",
      "Gradient Descent (2045/9999): loss=0.45425053039544394, gradient=0.008542909004434511\n",
      "Gradient Descent (2050/9999): loss=0.45424773439557325, gradient=0.008540936431993917\n",
      "Gradient Descent (2055/9999): loss=0.45424493992332343, gradient=0.008538966757728323\n",
      "Gradient Descent (2060/9999): loss=0.4542421469757563, gradient=0.008536999963920927\n",
      "Gradient Descent (2065/9999): loss=0.4542393555499506, gradient=0.008535036032954026\n",
      "Gradient Descent (2070/9999): loss=0.4542365656430013, gradient=0.008533074947307802\n",
      "Gradient Descent (2075/9999): loss=0.4542337772520198, gradient=0.008531116689559233\n",
      "Gradient Descent (2080/9999): loss=0.4542309903741339, gradient=0.00852916124238105\n",
      "Gradient Descent (2085/9999): loss=0.4542282050064874, gradient=0.008527208588540548\n",
      "Gradient Descent (2090/9999): loss=0.45422542114624115, gradient=0.00852525871089869\n",
      "Gradient Descent (2095/9999): loss=0.4542226387905704, gradient=0.00852331159240892\n",
      "Gradient Descent (2100/9999): loss=0.454219857936668, gradient=0.008521367216116304\n",
      "Gradient Descent (2105/9999): loss=0.4542170785817419, gradient=0.008519425565156526\n",
      "Gradient Descent (2110/9999): loss=0.4542143007230155, gradient=0.008517486622754861\n",
      "Gradient Descent (2115/9999): loss=0.4542115243577285, gradient=0.008515550372225355\n",
      "Gradient Descent (2120/9999): loss=0.45420874948313567, gradient=0.008513616796969851\n",
      "Gradient Descent (2125/9999): loss=0.4542059760965077, gradient=0.008511685880477156\n",
      "Gradient Descent (2130/9999): loss=0.45420320419513055, gradient=0.008509757606322091\n",
      "Gradient Descent (2135/9999): loss=0.45420043377630515, gradient=0.008507831958164843\n",
      "Gradient Descent (2140/9999): loss=0.45419766483734825, gradient=0.008505908919749922\n",
      "Gradient Descent (2145/9999): loss=0.4541948973755913, gradient=0.00850398847490552\n",
      "Gradient Descent (2150/9999): loss=0.454192131388381, gradient=0.008502070607542721\n",
      "Gradient Descent (2155/9999): loss=0.454189366873079, gradient=0.008500155301654719\n",
      "Gradient Descent (2160/9999): loss=0.4541866038270616, gradient=0.008498242541316108\n",
      "Gradient Descent (2165/9999): loss=0.45418384224772046, gradient=0.008496332310682123\n",
      "Gradient Descent (2170/9999): loss=0.4541810821324615, gradient=0.008494424593988087\n",
      "Gradient Descent (2175/9999): loss=0.4541783234787054, gradient=0.008492519375548561\n",
      "Gradient Descent (2180/9999): loss=0.4541755662838874, gradient=0.008490616639756891\n",
      "Gradient Descent (2185/9999): loss=0.4541728105454574, gradient=0.008488716371084412\n",
      "Gradient Descent (2190/9999): loss=0.4541700562608797, gradient=0.008486818554079952\n",
      "Gradient Descent (2195/9999): loss=0.4541673034276323, gradient=0.008484923173369215\n",
      "Gradient Descent (2200/9999): loss=0.45416455204320855, gradient=0.00848303021365417\n",
      "Gradient Descent (2205/9999): loss=0.4541618021051152, gradient=0.00848113965971256\n",
      "Gradient Descent (2210/9999): loss=0.4541590536108731, gradient=0.008479251496397357\n",
      "Gradient Descent (2215/9999): loss=0.4541563065580174, gradient=0.008477365708636239\n",
      "Gradient Descent (2220/9999): loss=0.4541535609440974, gradient=0.008475482281431076\n",
      "Gradient Descent (2225/9999): loss=0.45415081676667596, gradient=0.00847360119985751\n",
      "Gradient Descent (2230/9999): loss=0.45414807402332946, gradient=0.008471722449064464\n",
      "Gradient Descent (2235/9999): loss=0.4541453327116486, gradient=0.008469846014273684\n",
      "Gradient Descent (2240/9999): loss=0.45414259282923736, gradient=0.008467971880779364\n",
      "Gradient Descent (2245/9999): loss=0.45413985437371357, gradient=0.008466100033947684\n",
      "Gradient Descent (2250/9999): loss=0.45413711734270834, gradient=0.008464230459216468\n",
      "Gradient Descent (2255/9999): loss=0.4541343817338662, gradient=0.008462363142094816\n",
      "Gradient Descent (2260/9999): loss=0.45413164754484536, gradient=0.008460498068162714\n",
      "Gradient Descent (2265/9999): loss=0.45412891477331707, gradient=0.008458635223070698\n",
      "Gradient Descent (2270/9999): loss=0.4541261834169657, gradient=0.00845677459253961\n",
      "Gradient Descent (2275/9999): loss=0.4541234534734893, gradient=0.008454916162360151\n",
      "Gradient Descent (2280/9999): loss=0.4541207249405984, gradient=0.008453059918392732\n",
      "Gradient Descent (2285/9999): loss=0.454117997816017, gradient=0.008451205846567117\n",
      "Gradient Descent (2290/9999): loss=0.4541152720974817, gradient=0.008449353932882191\n",
      "Gradient Descent (2295/9999): loss=0.45411254778274246, gradient=0.008447504163405737\n",
      "Gradient Descent (2300/9999): loss=0.4541098248695615, gradient=0.008445656524274153\n",
      "Gradient Descent (2305/9999): loss=0.454107103355714, gradient=0.008443811001692305\n",
      "Gradient Descent (2310/9999): loss=0.4541043832389882, gradient=0.008441967581933281\n",
      "Gradient Descent (2315/9999): loss=0.45410166451718437, gradient=0.008440126251338251\n",
      "Gradient Descent (2320/9999): loss=0.45409894718811594, gradient=0.008438286996316236\n",
      "Gradient Descent (2325/9999): loss=0.45409623124960846, gradient=0.008436449803344018\n",
      "Gradient Descent (2330/9999): loss=0.4540935166994995, gradient=0.008434614658965984\n",
      "Gradient Descent (2335/9999): loss=0.45409080353564013, gradient=0.008432781549793957\n",
      "Gradient Descent (2340/9999): loss=0.4540880917558924, gradient=0.008430950462507152\n",
      "Gradient Descent (2345/9999): loss=0.4540853813581316, gradient=0.008429121383852031\n",
      "Gradient Descent (2350/9999): loss=0.4540826723402448, gradient=0.00842729430064221\n",
      "Gradient Descent (2355/9999): loss=0.4540799647001312, gradient=0.00842546919975841\n",
      "Gradient Descent (2360/9999): loss=0.4540772584357014, gradient=0.008423646068148419\n",
      "Gradient Descent (2365/9999): loss=0.45407455354487947, gradient=0.008421824892826996\n",
      "Gradient Descent (2370/9999): loss=0.4540718500256, gradient=0.008420005660875871\n",
      "Gradient Descent (2375/9999): loss=0.45406914787581, gradient=0.008418188359443692\n",
      "Gradient Descent (2380/9999): loss=0.4540664470934682, gradient=0.008416372975746074\n",
      "Gradient Descent (2385/9999): loss=0.45406374767654517, gradient=0.00841455949706553\n",
      "Gradient Descent (2390/9999): loss=0.4540610496230229, gradient=0.008412747910751526\n",
      "Gradient Descent (2395/9999): loss=0.45405835293089514, gradient=0.008410938204220519\n",
      "Gradient Descent (2400/9999): loss=0.4540556575981669, gradient=0.00840913036495596\n",
      "Gradient Descent (2405/9999): loss=0.4540529636228553, gradient=0.008407324380508339\n",
      "Gradient Descent (2410/9999): loss=0.45405027100298817, gradient=0.008405520238495336\n",
      "Gradient Descent (2415/9999): loss=0.4540475797366049, gradient=0.008403717926601745\n",
      "Gradient Descent (2420/9999): loss=0.4540448898217564, gradient=0.008401917432579644\n",
      "Gradient Descent (2425/9999): loss=0.4540422012565045, gradient=0.00840011874424852\n",
      "Gradient Descent (2430/9999): loss=0.4540395140389227, gradient=0.008398321849495278\n",
      "Gradient Descent (2435/9999): loss=0.45403682816709456, gradient=0.008396526736274432\n",
      "Gradient Descent (2440/9999): loss=0.454034143639116, gradient=0.008394733392608178\n",
      "Gradient Descent (2445/9999): loss=0.4540314604530929, gradient=0.008392941806586533\n",
      "Gradient Descent (2450/9999): loss=0.4540287786071425, gradient=0.008391151966367538\n",
      "Gradient Descent (2455/9999): loss=0.4540260980993928, gradient=0.008389363860177294\n",
      "Gradient Descent (2460/9999): loss=0.45402341892798287, gradient=0.008387577476310209\n",
      "Gradient Descent (2465/9999): loss=0.4540207410910617, gradient=0.00838579280312917\n",
      "Gradient Descent (2470/9999): loss=0.45401806458678967, gradient=0.008384009829065658\n",
      "Gradient Descent (2475/9999): loss=0.45401538941333824, gradient=0.00838222854261999\n",
      "Gradient Descent (2480/9999): loss=0.45401271556888795, gradient=0.008380448932361449\n",
      "Gradient Descent (2485/9999): loss=0.45401004305163106, gradient=0.008378670986928578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent (2490/9999): loss=0.45400737185976997, gradient=0.008376894695029323\n",
      "Gradient Descent (2495/9999): loss=0.454004701991517, gradient=0.008375120045441244\n",
      "Gradient Descent (2500/9999): loss=0.4540020334450953, gradient=0.008373347027011774\n",
      "Gradient Descent (2505/9999): loss=0.45399936621873815, gradient=0.008371575628658427\n",
      "Gradient Descent (2510/9999): loss=0.45399670031068884, gradient=0.008369805839369044\n",
      "Gradient Descent (2515/9999): loss=0.45399403571920083, gradient=0.008368037648202058\n",
      "Gradient Descent (2520/9999): loss=0.45399137244253773, gradient=0.00836627104428671\n",
      "Gradient Descent (2525/9999): loss=0.45398871047897327, gradient=0.00836450601682327\n",
      "Gradient Descent (2530/9999): loss=0.45398604982679086, gradient=0.008362742555083425\n",
      "Gradient Descent (2535/9999): loss=0.4539833904842838, gradient=0.00836098064841042\n",
      "Gradient Descent (2540/9999): loss=0.4539807324497556, gradient=0.0083592202862194\n",
      "Gradient Descent (2545/9999): loss=0.45397807572151877, gradient=0.008357461457997649\n",
      "Gradient Descent (2550/9999): loss=0.45397542029789656, gradient=0.008355704153304907\n",
      "Gradient Descent (2555/9999): loss=0.4539727661772209, gradient=0.008353948361773661\n",
      "Gradient Descent (2560/9999): loss=0.45397011335783377, gradient=0.008352194073109368\n",
      "Gradient Descent (2565/9999): loss=0.4539674618380866, gradient=0.00835044127709085\n",
      "Gradient Descent (2570/9999): loss=0.45396481161634017, gradient=0.008348689963570508\n",
      "Gradient Descent (2575/9999): loss=0.4539621626909647, gradient=0.008346940122474694\n",
      "Gradient Descent (2580/9999): loss=0.4539595150603398, gradient=0.00834519174380397\n",
      "Gradient Descent (2585/9999): loss=0.45395686872285446, gradient=0.008343444817633428\n",
      "Gradient Descent (2590/9999): loss=0.45395422367690647, gradient=0.00834169933411301\n",
      "Gradient Descent (2595/9999): loss=0.4539515799209031, gradient=0.008339955283467837\n",
      "Gradient Descent (2600/9999): loss=0.4539489374532608, gradient=0.008338212655998482\n",
      "Gradient Descent (2605/9999): loss=0.4539462962724047, gradient=0.008336471442081304\n",
      "Gradient Descent (2610/9999): loss=0.4539436563767688, gradient=0.00833473163216882\n",
      "Gradient Descent (2615/9999): loss=0.4539410177647968, gradient=0.008332993216789961\n",
      "Gradient Descent (2620/9999): loss=0.4539383804349403, gradient=0.00833125618655041\n",
      "Gradient Descent (2625/9999): loss=0.4539357443856601, gradient=0.00832952053213295\n",
      "Gradient Descent (2630/9999): loss=0.45393310961542566, gradient=0.008327786244297735\n",
      "Gradient Descent (2635/9999): loss=0.4539304761227153, gradient=0.008326053313882717\n",
      "Gradient Descent (2640/9999): loss=0.45392784390601537, gradient=0.008324321731803806\n",
      "Gradient Descent (2645/9999): loss=0.4539252129638215, gradient=0.008322591489055394\n",
      "Gradient Descent (2650/9999): loss=0.4539225832946368, gradient=0.008320862576710483\n",
      "Gradient Descent (2655/9999): loss=0.4539199548969741, gradient=0.008319134985921134\n",
      "Gradient Descent (2660/9999): loss=0.4539173277693534, gradient=0.008317408707918732\n",
      "Gradient Descent (2665/9999): loss=0.4539147019103032, gradient=0.008315683734014357\n",
      "Gradient Descent (2670/9999): loss=0.45391207731836086, gradient=0.008313960055599015\n",
      "Gradient Descent (2675/9999): loss=0.453909453992071, gradient=0.008312237664144009\n",
      "Gradient Descent (2680/9999): loss=0.4539068319299871, gradient=0.008310516551201288\n",
      "Gradient Descent (2685/9999): loss=0.4539042111306702, gradient=0.008308796708403635\n",
      "Gradient Descent (2690/9999): loss=0.45390159159268934, gradient=0.008307078127465133\n",
      "Gradient Descent (2695/9999): loss=0.4538989733146215, gradient=0.008305360800181315\n",
      "Gradient Descent (2700/9999): loss=0.45389635629505165, gradient=0.008303644718429551\n",
      "Gradient Descent (2705/9999): loss=0.45389374053257225, gradient=0.008301929874169318\n",
      "Gradient Descent (2710/9999): loss=0.45389112602578385, gradient=0.008300216259442529\n",
      "Gradient Descent (2715/9999): loss=0.4538885127732942, gradient=0.00829850386637374\n",
      "Gradient Descent (2720/9999): loss=0.4538859007737187, gradient=0.008296792687170481\n",
      "Gradient Descent (2725/9999): loss=0.4538832900256807, gradient=0.008295082714123552\n",
      "Gradient Descent (2730/9999): loss=0.45388068052781066, gradient=0.008293373939607268\n",
      "Gradient Descent (2735/9999): loss=0.45387807227874616, gradient=0.008291666356079681\n",
      "Gradient Descent (2740/9999): loss=0.4538754652771326, gradient=0.00828995995608289\n",
      "Gradient Descent (2745/9999): loss=0.4538728595216226, gradient=0.008288254732243346\n",
      "Gradient Descent (2750/9999): loss=0.4538702550108753, gradient=0.008286550677271968\n",
      "Gradient Descent (2755/9999): loss=0.45386765174355825, gradient=0.008284847783964473\n",
      "Gradient Descent (2760/9999): loss=0.4538650497183444, gradient=0.008283146045201582\n",
      "Gradient Descent (2765/9999): loss=0.4538624489339152, gradient=0.008281445453949248\n",
      "Gradient Descent (2770/9999): loss=0.4538598493889583, gradient=0.008279746003258887\n",
      "Gradient Descent (2775/9999): loss=0.45385725108216807, gradient=0.00827804768626753\n",
      "Gradient Descent (2780/9999): loss=0.4538546540122463, gradient=0.008276350496198095\n",
      "Gradient Descent (2785/9999): loss=0.4538520581779011, gradient=0.008274654426359496\n",
      "Gradient Descent (2790/9999): loss=0.4538494635778472, gradient=0.008272959470146975\n",
      "Gradient Descent (2795/9999): loss=0.4538468702108062, gradient=0.008271265621042053\n",
      "Gradient Descent (2800/9999): loss=0.4538442780755062, gradient=0.008269572872612891\n",
      "Gradient Descent (2805/9999): loss=0.45384168717068163, gradient=0.008267881218514354\n",
      "Gradient Descent (2810/9999): loss=0.4538390974950734, gradient=0.008266190652488176\n",
      "Gradient Descent (2815/9999): loss=0.45383650904742867, gradient=0.00826450116836306\n",
      "Gradient Descent (2820/9999): loss=0.4538339218265015, gradient=0.008262812760054868\n",
      "Gradient Descent (2825/9999): loss=0.4538313358310511, gradient=0.008261125421566679\n",
      "Gradient Descent (2830/9999): loss=0.4538287510598438, gradient=0.008259439146988888\n",
      "Gradient Descent (2835/9999): loss=0.4538261675116518, gradient=0.008257753930499344\n",
      "Gradient Descent (2840/9999): loss=0.45382358518525284, gradient=0.008256069766363408\n",
      "Gradient Descent (2845/9999): loss=0.453821004079431, gradient=0.008254386648934001\n",
      "Gradient Descent (2850/9999): loss=0.45381842419297663, gradient=0.00825270457265167\n",
      "Gradient Descent (2855/9999): loss=0.45381584552468524, gradient=0.008251023532044648\n",
      "Gradient Descent (2860/9999): loss=0.4538132680733584, gradient=0.008249343521728878\n",
      "Gradient Descent (2865/9999): loss=0.4538106918378035, gradient=0.00824766453640804\n",
      "Gradient Descent (2870/9999): loss=0.4538081168168335, gradient=0.008245986570873531\n",
      "Gradient Descent (2875/9999): loss=0.4538055430092669, gradient=0.008244309620004511\n",
      "Gradient Descent (2880/9999): loss=0.4538029704139277, gradient=0.008242633678767795\n",
      "Gradient Descent (2885/9999): loss=0.45380039902964553, gradient=0.00824095874221792\n",
      "Gradient Descent (2890/9999): loss=0.4537978288552553, gradient=0.008239284805497063\n",
      "Gradient Descent (2895/9999): loss=0.4537952598895969, gradient=0.008237611863834976\n",
      "Gradient Descent (2900/9999): loss=0.45379269213151596, gradient=0.008235939912548883\n",
      "Gradient Descent (2905/9999): loss=0.45379012557986337, gradient=0.008234268947043464\n",
      "Gradient Descent (2910/9999): loss=0.45378756023349487, gradient=0.008232598962810724\n",
      "Gradient Descent (2915/9999): loss=0.45378499609127104, gradient=0.008230929955429817\n",
      "Gradient Descent (2920/9999): loss=0.453782433152058, gradient=0.008229261920567039\n",
      "Gradient Descent (2925/9999): loss=0.45377987141472653, gradient=0.00822759485397561\n",
      "Gradient Descent (2930/9999): loss=0.4537773108781526, gradient=0.008225928751495505\n",
      "Gradient Descent (2935/9999): loss=0.4537747515412164, gradient=0.008224263609053317\n",
      "Gradient Descent (2940/9999): loss=0.45377219340280356, gradient=0.008222599422662088\n",
      "Gradient Descent (2945/9999): loss=0.45376963646180357, gradient=0.008220936188421051\n",
      "Gradient Descent (2950/9999): loss=0.4537670807171116, gradient=0.00821927390251546\n",
      "Gradient Descent (2955/9999): loss=0.45376452616762664, gradient=0.008217612561216366\n",
      "Gradient Descent (2960/9999): loss=0.45376197281225217, gradient=0.008215952160880294\n",
      "Gradient Descent (2965/9999): loss=0.45375942064989666, gradient=0.008214292697949134\n",
      "Gradient Descent (2970/9999): loss=0.4537568696794725, gradient=0.008212634168949715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent (2975/9999): loss=0.45375431989989673, gradient=0.008210976570493564\n",
      "Gradient Descent (2980/9999): loss=0.4537517713100903, gradient=0.008209319899276682\n",
      "Gradient Descent (2985/9999): loss=0.45374922390897837, gradient=0.008207664152079084\n",
      "Gradient Descent (2990/9999): loss=0.453746677695491, gradient=0.008206009325764568\n",
      "Gradient Descent (2995/9999): loss=0.4537441326685612, gradient=0.008204355417280346\n",
      "Gradient Descent (3000/9999): loss=0.4537415888271266, gradient=0.008202702423656639\n",
      "Gradient Descent (3000/9999): loss=0.4537415888271266, gradient=0.008202702423656639, w=[-1.07933816e+00  9.61808959e-01 -7.63174627e-01 -1.60561644e-01\n",
      "  4.60546579e-01 -8.09501313e-02  1.37198927e+00 -1.38396765e-01\n",
      "  8.34313642e-02 -4.19723756e-01  1.99596029e-01  9.29595746e-02\n",
      "  1.07253757e+00 -2.85008204e-04  5.47878164e-02  5.64197491e-01\n",
      "  1.30584468e-02  7.91273736e-02 -1.40054918e-01  3.66313758e-02\n",
      "  8.37537694e-02 -1.05753461e-01  4.92890242e-01  7.24901187e-03\n",
      "  5.03699790e-02  2.27958572e-01  2.35386792e-02  2.24736052e-02\n",
      " -4.76046144e-04  2.68407058e-02 -9.12137768e-01  9.29473915e-02\n",
      " -1.38968395e+00  4.95774915e-02  3.05153850e-02  4.08578523e-01\n",
      "  9.46440249e-01 -2.51409095e-01  6.28518902e-02  1.41522180e-01\n",
      "  5.93511667e-02 -5.09656919e-01 -7.66811292e-02  3.90185917e-02\n",
      "  1.17930962e-01 -1.65805162e-01  5.63717705e-02  3.42881665e-01\n",
      "  4.77920520e-02 -2.21822455e-01 -1.22832526e-01 -2.70566818e-01\n",
      "  3.78275726e-01  8.91693911e-02 -1.11138072e-01  2.10893365e-01\n",
      " -6.55449623e-03 -1.07440182e-01  2.68407058e-02 -6.47859429e-01\n",
      "  3.30430177e-01 -5.11030240e-01  7.40164156e-01  2.09733877e-01\n",
      " -1.31667443e-01  1.00081006e+00 -1.09589806e-01 -3.21386635e-03\n",
      "  4.62391528e-02  3.02499214e-02 -3.00517500e-01 -2.45415599e-02\n",
      " -4.24288924e-02 -1.10484652e-01 -2.79596953e-02 -6.11199888e-02\n",
      "  9.04629924e-01 -5.13524810e-02 -1.09597496e-01 -7.99242157e-02\n",
      " -2.98862633e-01 -4.18425969e-02 -5.15543321e-02 -1.94463111e-01\n",
      " -5.07515381e-02 -3.46291526e-02  2.86300998e-02  2.68407058e-02\n",
      " -2.47981117e-02  3.78620147e-02  9.17737004e-02  9.58275964e-01\n",
      "  4.60747420e-01 -3.65607672e-01  1.00360594e+00  4.15376666e-02\n",
      " -6.32995764e-02  6.71488162e-03  1.66291684e-02  2.13915799e-01\n",
      " -6.90554072e-02 -2.73784042e-02 -1.21443097e-01 -6.57663954e-02\n",
      " -3.11986643e-02  9.88326137e-01 -2.42108893e-02  3.20342082e-02\n",
      " -3.37615118e-02 -6.81227213e-02  2.85013256e-02 -1.75440665e-02\n",
      " -9.24003096e-02  1.09756710e-02  1.99219422e-03  2.49656696e-02\n",
      "  2.68407058e-02  3.13713785e-01 -9.99319931e-02  2.59804308e-01\n",
      "  9.95112283e-01  6.58140117e-01 -4.05010959e-01  1.00407771e+00\n",
      "  7.49930294e-02  8.66710779e-03 -6.83917796e-02  1.22821381e-02\n",
      "  4.15193738e-01  2.62299043e-02  4.74151733e-04 -5.26452611e-02\n",
      "  1.16164135e-02 -1.87295755e-03  1.00093799e+00  1.87353143e-02\n",
      "  6.57416376e-02  5.43553371e-03  1.58300525e-01  4.17786108e-02\n",
      "  1.97145481e-02  3.52748707e-02  3.51173471e-02  1.73037947e-02\n",
      " -1.59431464e-02  2.68407058e-02  4.55977608e-01 -8.58979525e-02\n",
      "  3.13000114e-01  1.00196642e+00  7.80349108e-01 -3.37075786e-01\n",
      "  1.00414772e+00  6.87223259e-02  1.15885707e-01 -5.49787271e-02\n",
      "  1.34747696e-02  4.84295318e-01  2.28544283e-02  1.33718859e-03\n",
      "  3.58640842e-02  4.04995445e-02 -6.08908727e-03  1.00333462e+00\n",
      " -1.50513326e-02  6.42566201e-02  3.55417210e-02  3.09833551e-01\n",
      " -1.02470393e-01 -3.64269079e-02  1.40838469e-01 -5.69158105e-02\n",
      "  9.19043931e-03 -2.46459885e-02  2.68407058e-02]\n",
      "Gradient Descent (3005/9999): loss=0.4537390461701291, gradient=0.00820105034200636\n",
      "Gradient Descent (3010/9999): loss=0.4537365046965138, gradient=0.008199399169524653\n",
      "Gradient Descent (3015/9999): loss=0.4537339644052301, gradient=0.008197748903488544\n",
      "Gradient Descent (3020/9999): loss=0.4537314252952311, gradient=0.008196099541256455\n",
      "Gradient Descent (3025/9999): loss=0.4537288873654736, gradient=0.008194451080267808\n",
      "Gradient Descent (3030/9999): loss=0.45372635061491795, gradient=0.0081928035180426\n",
      "Gradient Descent (3035/9999): loss=0.4537238150425286, gradient=0.00819115685218082\n",
      "Gradient Descent (3040/9999): loss=0.4537212806472727, gradient=0.008189511080362115\n",
      "Gradient Descent (3045/9999): loss=0.4537187474281217, gradient=0.008187866200345138\n",
      "Gradient Descent (3050/9999): loss=0.4537162153840501, gradient=0.008186222209967172\n",
      "Gradient Descent (3055/9999): loss=0.4537136845140359, gradient=0.008184579107143541\n",
      "Gradient Descent (3060/9999): loss=0.4537111548170603, gradient=0.008182936889867017\n",
      "Gradient Descent (3065/9999): loss=0.4537086262921077, gradient=0.008181295556207389\n",
      "Gradient Descent (3070/9999): loss=0.4537060989381663, gradient=0.008179655104310781\n",
      "Gradient Descent (3075/9999): loss=0.4537035727542267, gradient=0.008178015532399115\n",
      "Gradient Descent (3080/9999): loss=0.45370104773928294, gradient=0.00817637683876951\n",
      "Gradient Descent (3085/9999): loss=0.4536985238923323, gradient=0.008174739021793685\n",
      "Gradient Descent (3090/9999): loss=0.4536960012123748, gradient=0.008173102079917314\n",
      "Gradient Descent (3095/9999): loss=0.45369347969841334, gradient=0.008171466011659394\n",
      "Gradient Descent (3100/9999): loss=0.45369095934945386, gradient=0.00816983081561157\n",
      "Gradient Descent (3105/9999): loss=0.45368844016450527, gradient=0.008168196490437553\n",
      "Gradient Descent (3110/9999): loss=0.45368592214257886, gradient=0.008166563034872353\n",
      "Gradient Descent (3115/9999): loss=0.45368340528268924, gradient=0.00816493044772163\n",
      "Gradient Descent (3120/9999): loss=0.4536808895838533, gradient=0.008163298727860991\n",
      "Gradient Descent (3125/9999): loss=0.4536783750450904, gradient=0.00816166787423529\n",
      "Gradient Descent (3130/9999): loss=0.453675861665423, gradient=0.00816003788585788\n",
      "Gradient Descent (3135/9999): loss=0.4536733494438757, gradient=0.008158408761809881\n",
      "Gradient Descent (3140/9999): loss=0.4536708383794759, gradient=0.008156780501239464\n",
      "Gradient Descent (3145/9999): loss=0.453668328471253, gradient=0.008155153103361037\n",
      "Gradient Descent (3150/9999): loss=0.45366581971823877, gradient=0.008153526567454538\n",
      "Gradient Descent (3155/9999): loss=0.4536633121194684, gradient=0.00815190089286459\n",
      "Gradient Descent (3160/9999): loss=0.45366080567397793, gradient=0.008150276078999787\n",
      "Gradient Descent (3165/9999): loss=0.4536583003808063, gradient=0.008148652125331812\n",
      "Gradient Descent (3170/9999): loss=0.4536557962389946, gradient=0.008147029031394689\n",
      "Gradient Descent (3175/9999): loss=0.4536532932475863, gradient=0.008145406796783952\n",
      "Gradient Descent (3180/9999): loss=0.45365079140562664, gradient=0.008143785421155781\n",
      "Gradient Descent (3185/9999): loss=0.4536482907121631, gradient=0.00814216490422618\n",
      "Gradient Descent (3190/9999): loss=0.45364579116624515, gradient=0.008140545245770183\n",
      "Gradient Descent (3195/9999): loss=0.45364329276692417, gradient=0.00813892644562094\n",
      "Gradient Descent (3200/9999): loss=0.45364079551325337, gradient=0.00813730850366886\n",
      "Gradient Descent (3205/9999): loss=0.45363829940428807, gradient=0.008135691419860747\n",
      "Gradient Descent (3210/9999): loss=0.45363580443908547, gradient=0.008134075194198925\n",
      "Gradient Descent (3215/9999): loss=0.4536333106167044, gradient=0.008132459826740384\n",
      "Gradient Descent (3220/9999): loss=0.4536308179362056, gradient=0.00813084531759585\n",
      "Gradient Descent (3225/9999): loss=0.45362832639665124, gradient=0.008129231666928869\n",
      "Gradient Descent (3230/9999): loss=0.4536258359971055, gradient=0.008127618874954948\n",
      "Gradient Descent (3235/9999): loss=0.4536233467366341, gradient=0.008126006941940635\n",
      "Gradient Descent (3240/9999): loss=0.4536208586143045, gradient=0.008124395868202587\n",
      "Gradient Descent (3245/9999): loss=0.45361837162918545, gradient=0.008122785654106618\n",
      "Gradient Descent (3250/9999): loss=0.4536158857803472, gradient=0.008121176300066896\n",
      "Gradient Descent (3255/9999): loss=0.4536134010668619, gradient=0.00811956780654484\n",
      "Gradient Descent (3260/9999): loss=0.45361091748780297, gradient=0.008117960174048354\n",
      "Gradient Descent (3265/9999): loss=0.4536084350422449, gradient=0.008116353403130732\n",
      "Gradient Descent (3270/9999): loss=0.45360595372926416, gradient=0.008114747494389837\n",
      "Gradient Descent (3275/9999): loss=0.4536034735479381, gradient=0.008113142448467094\n",
      "Gradient Descent (3280/9999): loss=0.45360099449734564, gradient=0.00811153826604658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent (3285/9999): loss=0.4535985165765671, gradient=0.008109934947854048\n",
      "Gradient Descent (3290/9999): loss=0.4535960397846834, gradient=0.008108332494655946\n",
      "Gradient Descent (3295/9999): loss=0.45359356412077734, gradient=0.008106730907258528\n",
      "Gradient Descent (3300/9999): loss=0.4535910895839329, gradient=0.008105130186506854\n",
      "Gradient Descent (3305/9999): loss=0.4535886161732348, gradient=0.00810353033328384\n",
      "Gradient Descent (3310/9999): loss=0.4535861438877693, gradient=0.008101931348509325\n",
      "Gradient Descent (3315/9999): loss=0.4535836727266234, gradient=0.00810033323313906\n",
      "Gradient Descent (3320/9999): loss=0.4535812026888854, gradient=0.008098735988163795\n",
      "Gradient Descent (3325/9999): loss=0.45357873377364444, gradient=0.008097139614608301\n",
      "Gradient Descent (3330/9999): loss=0.45357626597999096, gradient=0.008095544113530436\n",
      "Gradient Descent (3335/9999): loss=0.45357379930701636, gradient=0.00809394948602013\n",
      "Gradient Descent (3340/9999): loss=0.45357133375381237, gradient=0.008092355733198484\n",
      "Gradient Descent (3345/9999): loss=0.4535688693194725, gradient=0.008090762856216785\n",
      "Gradient Descent (3350/9999): loss=0.45356640600309056, gradient=0.008089170856255618\n",
      "Gradient Descent (3355/9999): loss=0.45356394380376164, gradient=0.008087579734523776\n",
      "Gradient Descent (3360/9999): loss=0.4535614827205815, gradient=0.008085989492257455\n",
      "Gradient Descent (3365/9999): loss=0.45355902275264653, gradient=0.008084400130719241\n",
      "Gradient Descent (3370/9999): loss=0.45355656389905447, gradient=0.008082811651197131\n",
      "Gradient Descent (3375/9999): loss=0.4535541061589034, gradient=0.008081224055003678\n",
      "Gradient Descent (3380/9999): loss=0.45355164953129207, gradient=0.008079637343475027\n",
      "Gradient Descent (3385/9999): loss=0.4535491940153203, gradient=0.008078051517969911\n",
      "Gradient Descent (3390/9999): loss=0.45354673961008846, gradient=0.008076466579868824\n",
      "Gradient Descent (3395/9999): loss=0.45354428631469756, gradient=0.008074882530573035\n",
      "Gradient Descent (3400/9999): loss=0.45354183412824944, gradient=0.008073299371503676\n",
      "Gradient Descent (3405/9999): loss=0.4535393830498468, gradient=0.008071717104100845\n",
      "Gradient Descent (3410/9999): loss=0.453536933078592, gradient=0.00807013572982268\n",
      "Gradient Descent (3415/9999): loss=0.4535344842135897, gradient=0.008068555250144478\n",
      "Gradient Descent (3420/9999): loss=0.453532036453943, gradient=0.008066975666557766\n",
      "Gradient Descent (3425/9999): loss=0.4535295897987577, gradient=0.008065396980569434\n",
      "Gradient Descent (3430/9999): loss=0.45352714424713897, gradient=0.008063819193700819\n",
      "Gradient Descent (3435/9999): loss=0.4535246997981929, gradient=0.00806224230748686\n",
      "Gradient Descent (3440/9999): loss=0.4535222564510255, gradient=0.008060666323475215\n",
      "Gradient Descent (3445/9999): loss=0.4535198142047443, gradient=0.008059091243225389\n",
      "Gradient Descent (3450/9999): loss=0.4535173730584568, gradient=0.00805751706830785\n",
      "Gradient Descent (3455/9999): loss=0.4535149330112708, gradient=0.008055943800303207\n",
      "Gradient Descent (3460/9999): loss=0.453512494062295, gradient=0.008054371440801423\n",
      "Gradient Descent (3465/9999): loss=0.45351005621063833, gradient=0.008052799991400822\n",
      "Gradient Descent (3470/9999): loss=0.4535076194554101, gradient=0.008051229453707452\n",
      "Gradient Descent (3475/9999): loss=0.4535051837957205, gradient=0.008049659829334112\n",
      "Gradient Descent (3480/9999): loss=0.4535027492306796, gradient=0.008048091119899577\n",
      "Gradient Descent (3485/9999): loss=0.4535003157593981, gradient=0.00804652332702788\n",
      "Gradient Descent (3490/9999): loss=0.453497883380987, gradient=0.00804495645234744\n",
      "Gradient Descent (3495/9999): loss=0.4534954520945582, gradient=0.008043390497490251\n",
      "Gradient Descent (3500/9999): loss=0.4534930218992232, gradient=0.00804182546409118\n",
      "Gradient Descent (3505/9999): loss=0.4534905927940946, gradient=0.008040261353787161\n",
      "Gradient Descent (3510/9999): loss=0.4534881647782849, gradient=0.008038698168216429\n",
      "Gradient Descent (3515/9999): loss=0.4534857378509071, gradient=0.008037135909017775\n",
      "Gradient Descent (3520/9999): loss=0.45348331201107467, gradient=0.008035574577829876\n",
      "Gradient Descent (3525/9999): loss=0.4534808872579013, gradient=0.008034014176290443\n",
      "Gradient Descent (3530/9999): loss=0.4534784635905012, gradient=0.008032454706035582\n",
      "Gradient Descent (3535/9999): loss=0.4534760410079884, gradient=0.008030896168699094\n",
      "Gradient Descent (3540/9999): loss=0.45347361950947784, gradient=0.008029338565911738\n",
      "Gradient Descent (3545/9999): loss=0.4534711990940849, gradient=0.008027781899300526\n",
      "Gradient Descent (3550/9999): loss=0.45346877976092415, gradient=0.008026226170488128\n",
      "Gradient Descent (3555/9999): loss=0.453466361509112, gradient=0.008024671381092125\n",
      "Gradient Descent (3560/9999): loss=0.4534639443377643, gradient=0.008023117532724361\n",
      "Gradient Descent (3565/9999): loss=0.45346152824599695, gradient=0.008021564626990359\n",
      "Gradient Descent (3570/9999): loss=0.4534591132329272, gradient=0.00802001266548862\n",
      "Gradient Descent (3575/9999): loss=0.4534566992976713, gradient=0.00801846164980998\n",
      "Gradient Descent (3580/9999): loss=0.45345428643934677, gradient=0.008016911581537087\n",
      "Gradient Descent (3585/9999): loss=0.4534518746570712, gradient=0.008015362462243741\n",
      "Gradient Descent (3590/9999): loss=0.4534494639499622, gradient=0.008013814293494272\n",
      "Gradient Descent (3595/9999): loss=0.45344705431713794, gradient=0.008012267076843039\n",
      "Gradient Descent (3600/9999): loss=0.45344464575771676, gradient=0.008010720813833744\n",
      "Gradient Descent (3605/9999): loss=0.45344223827081725, gradient=0.008009175505999033\n",
      "Gradient Descent (3610/9999): loss=0.45343983185555836, gradient=0.008007631154859791\n",
      "Gradient Descent (3615/9999): loss=0.45343742651105967, gradient=0.008006087761924685\n",
      "Gradient Descent (3620/9999): loss=0.4534350222364403, gradient=0.008004545328689622\n",
      "Gradient Descent (3625/9999): loss=0.4534326190308204, gradient=0.008003003856637238\n",
      "Gradient Descent (3630/9999): loss=0.45343021689331947, gradient=0.008001463347236436\n",
      "Gradient Descent (3635/9999): loss=0.4534278158230588, gradient=0.007999923801941769\n",
      "Gradient Descent (3640/9999): loss=0.45342541581915824, gradient=0.007998385222193101\n",
      "Gradient Descent (3645/9999): loss=0.45342301688073955, gradient=0.007996847609415065\n",
      "Gradient Descent (3650/9999): loss=0.45342061900692354, gradient=0.007995310965016598\n",
      "Gradient Descent (3655/9999): loss=0.4534182221968321, gradient=0.007993775290390524\n",
      "Gradient Descent (3660/9999): loss=0.4534158264495868, gradient=0.007992240586913099\n",
      "Gradient Descent (3665/9999): loss=0.4534134317643101, gradient=0.007990706855943615\n",
      "Gradient Descent (3670/9999): loss=0.4534110381401247, gradient=0.007989174098823899\n",
      "Gradient Descent (3675/9999): loss=0.45340864557615307, gradient=0.007987642316878034\n",
      "Gradient Descent (3680/9999): loss=0.45340625407151874, gradient=0.007986111511411889\n",
      "Gradient Descent (3685/9999): loss=0.4534038636253449, gradient=0.007984581683712723\n",
      "Gradient Descent (3690/9999): loss=0.4534014742367554, gradient=0.007983052835048875\n",
      "Gradient Descent (3695/9999): loss=0.4533990859048746, gradient=0.00798152496666937\n",
      "Gradient Descent (3700/9999): loss=0.45339669862882687, gradient=0.007979998079803551\n",
      "Gradient Descent (3705/9999): loss=0.45339431240773714, gradient=0.007978472175660802\n",
      "Gradient Descent (3710/9999): loss=0.4533919272407305, gradient=0.007976947255430125\n",
      "Gradient Descent (3715/9999): loss=0.4533895431269323, gradient=0.00797542332027992\n",
      "Gradient Descent (3720/9999): loss=0.453387160065469, gradient=0.007973900371357652\n",
      "Gradient Descent (3725/9999): loss=0.4533847780554662, gradient=0.00797237840978949\n",
      "Gradient Descent (3730/9999): loss=0.45338239709605094, gradient=0.00797085743668012\n",
      "Gradient Descent (3735/9999): loss=0.45338001718635035, gradient=0.007969337453112397\n",
      "Gradient Descent (3740/9999): loss=0.4533776383254915, gradient=0.007967818460147111\n",
      "Gradient Descent (3745/9999): loss=0.4533752605126025, gradient=0.007966300458822729\n",
      "Gradient Descent (3750/9999): loss=0.4533728837468114, gradient=0.007964783450155137\n",
      "Gradient Descent (3755/9999): loss=0.45337050802724665, gradient=0.007963267435137434\n",
      "Gradient Descent (3760/9999): loss=0.4533681333530376, gradient=0.007961752414739695\n",
      "Gradient Descent (3765/9999): loss=0.45336575972331355, gradient=0.007960238389908711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent (3770/9999): loss=0.45336338713720437, gradient=0.007958725361567878\n",
      "Gradient Descent (3775/9999): loss=0.4533610155938403, gradient=0.007957213330616893\n",
      "Gradient Descent (3780/9999): loss=0.4533586450923523, gradient=0.007955702297931676\n",
      "Gradient Descent (3785/9999): loss=0.4533562756318714, gradient=0.007954192264364083\n",
      "Gradient Descent (3790/9999): loss=0.4533539072115295, gradient=0.00795268323074184\n",
      "Gradient Descent (3795/9999): loss=0.45335153983045884, gradient=0.007951175197868338\n",
      "Gradient Descent (3800/9999): loss=0.45334917348779147, gradient=0.00794966816652245\n",
      "Gradient Descent (3805/9999): loss=0.4533468081826611, gradient=0.007948162137458466\n",
      "Gradient Descent (3810/9999): loss=0.45334444391420103, gradient=0.007946657111405884\n",
      "Gradient Descent (3815/9999): loss=0.4533420806815458, gradient=0.007945153089069361\n",
      "Gradient Descent (3820/9999): loss=0.45333971848382953, gradient=0.007943650071128574\n",
      "Gradient Descent (3825/9999): loss=0.45333735732018776, gradient=0.007942148058238058\n",
      "Gradient Descent (3830/9999): loss=0.45333499718975606, gradient=0.007940647051027212\n",
      "Gradient Descent (3835/9999): loss=0.4533326380916707, gradient=0.0079391470501001\n",
      "Gradient Descent (3840/9999): loss=0.45333028002506837, gradient=0.007937648056035503\n",
      "Gradient Descent (3845/9999): loss=0.45332792298908703, gradient=0.007936150069386694\n",
      "Gradient Descent (3850/9999): loss=0.4533255669828637, gradient=0.007934653090681514\n",
      "Gradient Descent (3855/9999): loss=0.45332321200553743, gradient=0.007933157120422193\n",
      "Gradient Descent (3860/9999): loss=0.4533208580562473, gradient=0.007931662159085417\n",
      "Gradient Descent (3865/9999): loss=0.453318505134133, gradient=0.007930168207122222\n",
      "Gradient Descent (3870/9999): loss=0.45331615323833485, gradient=0.007928675264957967\n",
      "Gradient Descent (3875/9999): loss=0.45331380236799373, gradient=0.007927183332992293\n",
      "Gradient Descent (3880/9999): loss=0.45331145252225136, gradient=0.007925692411599173\n",
      "Gradient Descent (3885/9999): loss=0.45330910370024974, gradient=0.007924202501126793\n",
      "Gradient Descent (3890/9999): loss=0.4533067559011319, gradient=0.007922713601897675\n",
      "Gradient Descent (3895/9999): loss=0.45330440912404163, gradient=0.00792122571420857\n",
      "Gradient Descent (3900/9999): loss=0.4533020633681227, gradient=0.007919738838330513\n",
      "Gradient Descent (3905/9999): loss=0.4532997186325199, gradient=0.007918252974508865\n",
      "Gradient Descent (3910/9999): loss=0.4532973749163791, gradient=0.007916768122963295\n",
      "Gradient Descent (3915/9999): loss=0.4532950322188469, gradient=0.00791528428388781\n",
      "Gradient Descent (3920/9999): loss=0.4532926905390693, gradient=0.007913801457450817\n",
      "Gradient Descent (3925/9999): loss=0.4532903498761949, gradient=0.007912319643795173\n",
      "Gradient Descent (3930/9999): loss=0.4532880102293721, gradient=0.00791083884303819\n",
      "Gradient Descent (3935/9999): loss=0.45328567159774963, gradient=0.007909359055271698\n",
      "Gradient Descent (3940/9999): loss=0.45328333398047765, gradient=0.007907880280562168\n",
      "Gradient Descent (3945/9999): loss=0.4532809973767067, gradient=0.00790640251895068\n",
      "Gradient Descent (3950/9999): loss=0.4532786617855886, gradient=0.007904925770453094\n",
      "Gradient Descent (3955/9999): loss=0.4532763272062753, gradient=0.007903450035060003\n",
      "Gradient Descent (3960/9999): loss=0.4532739936379198, gradient=0.007901975312736977\n",
      "Gradient Descent (3965/9999): loss=0.4532716610796763, gradient=0.007900501603424523\n",
      "Gradient Descent (3970/9999): loss=0.4532693295306992, gradient=0.00789902890703822\n",
      "Gradient Descent (3975/9999): loss=0.453266998990144, gradient=0.007897557223468809\n",
      "Gradient Descent (3980/9999): loss=0.45326466945716687, gradient=0.007896086552582336\n",
      "Gradient Descent (3985/9999): loss=0.4532623409309254, gradient=0.00789461689422019\n",
      "Gradient Descent (3990/9999): loss=0.4532600134105774, gradient=0.007893148248199297\n",
      "Gradient Descent (3995/9999): loss=0.4532576868952815, gradient=0.007891680614312148\n",
      "Gradient Descent (4000/9999): loss=0.45325536138419775, gradient=0.007890213992327033\n",
      "Gradient Descent (4000/9999): loss=0.45325536138419775, gradient=0.007890213992327033, w=[-1.08504540e+00  9.82222605e-01 -7.73977330e-01 -1.56623205e-01\n",
      "  4.74264744e-01 -7.21320782e-02  1.38587677e+00 -1.39766751e-01\n",
      "  8.23589235e-02 -4.29983672e-01  1.96847126e-01  8.96275809e-02\n",
      "  1.07353535e+00 -3.22243462e-04  5.21966781e-02  5.62773059e-01\n",
      "  1.29817804e-02  7.59878321e-02 -1.38930082e-01  3.51648336e-02\n",
      "  8.46379618e-02 -1.06545774e-01  4.95268504e-01  6.80752336e-03\n",
      "  4.73773133e-02  2.20464800e-01  2.33485062e-02  2.12824790e-02\n",
      " -1.64951850e-03  2.68987929e-02 -9.34103033e-01  1.03488644e-01\n",
      " -1.40653455e+00  3.65684127e-02  2.69866304e-02  4.02858858e-01\n",
      "  9.46298556e-01 -2.56059647e-01  7.26903616e-02  1.40098160e-01\n",
      "  6.15213834e-02 -5.14590323e-01 -7.66127720e-02  3.56686470e-02\n",
      "  1.16016353e-01 -1.66202950e-01  5.24920598e-02  3.33518328e-01\n",
      "  4.63715665e-02 -2.28841084e-01 -1.22882131e-01 -2.73898891e-01\n",
      "  3.80289906e-01  8.51455815e-02 -1.06888617e-01  2.14154008e-01\n",
      " -7.88851661e-03 -1.07155299e-01  2.68987929e-02 -6.56794010e-01\n",
      "  3.44750253e-01 -5.16082195e-01  7.34057039e-01  1.92561798e-01\n",
      " -1.40959580e-01  1.00058436e+00 -1.03010524e-01  4.38357114e-03\n",
      "  4.67029493e-02  3.19968412e-02 -2.94451893e-01 -2.43281724e-02\n",
      " -4.20362820e-02 -1.07030797e-01 -2.78389787e-02 -6.06793303e-02\n",
      "  9.01876338e-01 -5.09436215e-02 -1.08584016e-01 -7.96592760e-02\n",
      " -2.94914244e-01 -4.06367336e-02 -5.12010163e-02 -1.87145933e-01\n",
      " -5.01611702e-02 -3.45191413e-02  3.70890670e-02  2.68987929e-02\n",
      " -1.95776116e-02  3.47373679e-02  8.64286292e-02  9.57039090e-01\n",
      "  4.42407480e-01 -3.70368752e-01  1.00357383e+00  5.17970963e-02\n",
      " -6.33913295e-02  7.72124972e-03  1.71521683e-02  2.12396284e-01\n",
      " -6.96616006e-02 -2.75211652e-02 -1.18928166e-01 -6.58147768e-02\n",
      " -3.12423495e-02  9.87777570e-01 -2.39047715e-02  3.70306511e-02\n",
      " -3.33648536e-02 -6.59715564e-02  2.74368780e-02 -1.78382285e-02\n",
      " -8.95266059e-02  1.00555580e-02  2.08744196e-03  2.85381444e-02\n",
      "  2.68987929e-02  3.22216950e-01 -1.12801101e-01  2.50527988e-01\n",
      "  9.94839247e-01  6.43758940e-01 -4.03188551e-01  1.00408279e+00\n",
      "  7.89864610e-02  5.23299725e-03 -6.83033955e-02  1.16209316e-02\n",
      "  4.04404876e-01  2.60461069e-02  1.91122122e-03 -5.53659779e-02\n",
      "  1.15181385e-02 -6.38088834e-06  1.00082060e+00  1.94323772e-02\n",
      "  6.53201677e-02  5.89045232e-03  1.51576422e-01  4.09300743e-02\n",
      "  2.14611844e-02  3.44587577e-02  3.45894105e-02  1.80297950e-02\n",
      " -2.07280277e-02  2.68987929e-02  4.62852538e-01 -1.01070204e-01\n",
      "  3.01163212e-01  1.00189889e+00  7.70034356e-01 -3.29818518e-01\n",
      "  1.00415826e+00  6.43135597e-02  1.11706537e-01 -5.29946560e-02\n",
      "  1.18082089e-02  4.68533952e-01  2.34298749e-02  3.56791838e-03\n",
      "  2.78194828e-02  4.07836013e-02 -3.59875578e-03  1.00331114e+00\n",
      " -1.44859618e-02  5.57432901e-02  3.60244737e-02  2.96146206e-01\n",
      " -1.02883018e-01 -3.40342579e-02  1.38238902e-01 -5.90766774e-02\n",
      "  9.55169818e-03 -3.52387457e-02  2.68987929e-02]\n",
      "Gradient Descent (4005/9999): loss=0.45325303687648677, gradient=0.007888748381988027\n",
      "Gradient Descent (4010/9999): loss=0.45325071337130984, gradient=0.00788728378301527\n",
      "Gradient Descent (4015/9999): loss=0.45324839086782953, gradient=0.007885820195104969\n",
      "Gradient Descent (4020/9999): loss=0.4532460693652094, gradient=0.007884357617929666\n",
      "Gradient Descent (4025/9999): loss=0.4532437488626135, gradient=0.007882896051138228\n",
      "Gradient Descent (4030/9999): loss=0.453241429359207, gradient=0.007881435494356182\n",
      "Gradient Descent (4035/9999): loss=0.4532391108541561, gradient=0.007879975947185678\n",
      "Gradient Descent (4040/9999): loss=0.45323679334662786, gradient=0.007878517409205798\n",
      "Gradient Descent (4045/9999): loss=0.4532344768357902, gradient=0.007877059879972604\n",
      "Gradient Descent (4050/9999): loss=0.45323216132081245, gradient=0.007875603359019377\n",
      "Gradient Descent (4055/9999): loss=0.4532298468008644, gradient=0.007874147845856723\n",
      "Gradient Descent (4060/9999): loss=0.45322753327511717, gradient=0.007872693339972798\n",
      "Gradient Descent (4065/9999): loss=0.45322522074274246, gradient=0.007871239840833443\n",
      "Gradient Descent (4070/9999): loss=0.4532229092029133, gradient=0.007869787347882365\n",
      "Gradient Descent (4075/9999): loss=0.45322059865480424, gradient=0.007868335860541342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent (4080/9999): loss=0.45321828909758965, gradient=0.007866885378210358\n",
      "Gradient Descent (4085/9999): loss=0.45321598053044576, gradient=0.007865435900267842\n",
      "Gradient Descent (4090/9999): loss=0.45321367295254994, gradient=0.007863987426070836\n",
      "Gradient Descent (4095/9999): loss=0.45321136636308007, gradient=0.007862539954955124\n",
      "Gradient Descent (4100/9999): loss=0.45320906076121537, gradient=0.007861093486235543\n",
      "Gradient Descent (4105/9999): loss=0.45320675614613615, gradient=0.007859648019206099\n",
      "Gradient Descent (4110/9999): loss=0.45320445251702396, gradient=0.007858203553140167\n",
      "Gradient Descent (4115/9999): loss=0.453202149873061, gradient=0.007856760087290728\n",
      "Gradient Descent (4120/9999): loss=0.45319984821343084, gradient=0.007855317620890536\n",
      "Gradient Descent (4125/9999): loss=0.4531975475373181, gradient=0.007853876153152363\n",
      "Gradient Descent (4130/9999): loss=0.4531952478439087, gradient=0.00785243568326918\n",
      "Gradient Descent (4135/9999): loss=0.45319294913238933, gradient=0.007850996210414372\n",
      "Gradient Descent (4140/9999): loss=0.4531906514019479, gradient=0.007849557733741923\n",
      "Gradient Descent (4145/9999): loss=0.4531883546517739, gradient=0.007848120252386712\n",
      "Gradient Descent (4150/9999): loss=0.45318605888105706, gradient=0.007846683765464636\n",
      "Gradient Descent (4155/9999): loss=0.45318376408898925, gradient=0.007845248272072886\n",
      "Gradient Descent (4160/9999): loss=0.45318147027476263, gradient=0.007843813771290144\n",
      "Gradient Descent (4165/9999): loss=0.45317917743757136, gradient=0.00784238026217681\n",
      "Gradient Descent (4170/9999): loss=0.4531768855766096, gradient=0.007840947743775218\n",
      "Gradient Descent (4175/9999): loss=0.4531745946910743, gradient=0.007839516215109905\n",
      "Gradient Descent (4180/9999): loss=0.45317230478016224, gradient=0.007838085675187751\n",
      "Gradient Descent (4185/9999): loss=0.4531700158430718, gradient=0.007836656122998295\n",
      "Gradient Descent (4190/9999): loss=0.4531677278790028, gradient=0.007835227557513924\n",
      "Gradient Descent (4195/9999): loss=0.453165440887156, gradient=0.007833799977690077\n",
      "Gradient Descent (4200/9999): loss=0.4531631548667334, gradient=0.00783237338246555\n",
      "Gradient Descent (4205/9999): loss=0.4531608698169383, gradient=0.007830947770762667\n",
      "Gradient Descent (4210/9999): loss=0.4531585857369754, gradient=0.007829523141487518\n",
      "Gradient Descent (4215/9999): loss=0.4531563026260499, gradient=0.007828099493530267\n",
      "Gradient Descent (4220/9999): loss=0.4531540204833692, gradient=0.007826676825765278\n",
      "Gradient Descent (4225/9999): loss=0.45315173930814157, gradient=0.007825255137051429\n",
      "Gradient Descent (4230/9999): loss=0.4531494590995761, gradient=0.007823834426232338\n",
      "Gradient Descent (4235/9999): loss=0.4531471798568837, gradient=0.00782241469213661\n",
      "Gradient Descent (4240/9999): loss=0.45314490157927617, gradient=0.007820995933578016\n",
      "Gradient Descent (4245/9999): loss=0.4531426242659671, gradient=0.007819578149355823\n",
      "Gradient Descent (4250/9999): loss=0.45314034791617025, gradient=0.007818161338255001\n",
      "Gradient Descent (4255/9999): loss=0.4531380725291021, gradient=0.007816745499046457\n",
      "Gradient Descent (4260/9999): loss=0.45313579810397925, gradient=0.007815330630487257\n",
      "Gradient Descent (4265/9999): loss=0.4531335246400202, gradient=0.00781391673132091\n",
      "Gradient Descent (4270/9999): loss=0.4531312521364445, gradient=0.007812503800277617\n",
      "Gradient Descent (4275/9999): loss=0.4531289805924733, gradient=0.007811091836074497\n",
      "Gradient Descent (4280/9999): loss=0.45312671000732835, gradient=0.007809680837415802\n",
      "Gradient Descent (4285/9999): loss=0.4531244403802334, gradient=0.007808270802993245\n",
      "Gradient Descent (4290/9999): loss=0.4531221717104133, gradient=0.007806861731486169\n",
      "Gradient Descent (4295/9999): loss=0.45311990399709406, gradient=0.007805453621561839\n",
      "Gradient Descent (4300/9999): loss=0.4531176372395033, gradient=0.0078040464718756655\n",
      "Gradient Descent (4305/9999): loss=0.45311537143686975, gradient=0.007802640281071474\n",
      "Gradient Descent (4310/9999): loss=0.4531131065884231, gradient=0.00780123504778171\n",
      "Gradient Descent (4315/9999): loss=0.4531108426933952, gradient=0.007799830770627773\n",
      "Gradient Descent (4320/9999): loss=0.45310857975101865, gradient=0.007798427448220141\n",
      "Gradient Descent (4325/9999): loss=0.45310631776052734, gradient=0.0077970250791587785\n",
      "Gradient Descent (4330/9999): loss=0.45310405672115683, gradient=0.007795623662033215\n",
      "Gradient Descent (4335/9999): loss=0.45310179663214395, gradient=0.007794223195422912\n",
      "Gradient Descent (4340/9999): loss=0.4530995374927268, gradient=0.007792823677897448\n",
      "Gradient Descent (4345/9999): loss=0.45309727930214466, gradient=0.007791425108016849\n",
      "Gradient Descent (4350/9999): loss=0.4530950220596381, gradient=0.007790027484331719\n",
      "Gradient Descent (4355/9999): loss=0.45309276576444973, gradient=0.007788630805383585\n",
      "Gradient Descent (4360/9999): loss=0.4530905104158227, gradient=0.00778723506970511\n",
      "Gradient Descent (4365/9999): loss=0.453088256013002, gradient=0.007785840275820314\n",
      "Gradient Descent (4370/9999): loss=0.4530860025552339, gradient=0.007784446422244903\n",
      "Gradient Descent (4375/9999): loss=0.45308375004176554, gradient=0.007783053507486403\n",
      "Gradient Descent (4380/9999): loss=0.4530814984718463, gradient=0.007781661530044542\n",
      "Gradient Descent (4385/9999): loss=0.45307924784472625, gradient=0.0077802704884113475\n",
      "Gradient Descent (4390/9999): loss=0.453076998159657, gradient=0.007778880381071526\n",
      "Gradient Descent (4395/9999): loss=0.45307474941589165, gradient=0.007777491206502622\n",
      "Gradient Descent (4400/9999): loss=0.45307250161268453, gradient=0.007776102963175318\n",
      "Gradient Descent (4405/9999): loss=0.45307025474929147, gradient=0.007774715649553627\n",
      "Gradient Descent (4410/9999): loss=0.4530680088249695, gradient=0.007773329264095233\n",
      "Gradient Descent (4415/9999): loss=0.4530657638389769, gradient=0.007771943805251589\n",
      "Gradient Descent (4420/9999): loss=0.453063519790574, gradient=0.007770559271468293\n",
      "Gradient Descent (4425/9999): loss=0.4530612766790216, gradient=0.00776917566118527\n",
      "Gradient Descent (4430/9999): loss=0.4530590345035826, gradient=0.0077677929728370305\n",
      "Gradient Descent (4435/9999): loss=0.45305679326352083, gradient=0.007766411204852921\n",
      "Gradient Descent (4440/9999): loss=0.45305455295810143, gradient=0.007765030355657315\n",
      "Gradient Descent (4445/9999): loss=0.4530523135865916, gradient=0.0077636504236699355\n",
      "Gradient Descent (4450/9999): loss=0.453050075148259, gradient=0.007762271407306036\n",
      "Gradient Descent (4455/9999): loss=0.4530478376423732, gradient=0.007760893304976657\n",
      "Gradient Descent (4460/9999): loss=0.45304560106820524, gradient=0.0077595161150888605\n",
      "Gradient Descent (4465/9999): loss=0.4530433654250273, gradient=0.007758139836045982\n",
      "Gradient Descent (4470/9999): loss=0.4530411307121126, gradient=0.007756764466247866\n",
      "Gradient Descent (4475/9999): loss=0.4530388969287365, gradient=0.007755390004091057\n",
      "Gradient Descent (4480/9999): loss=0.45303666407417503, gradient=0.007754016447969107\n",
      "Gradient Descent (4485/9999): loss=0.4530344321477062, gradient=0.007752643796272799\n",
      "Gradient Descent (4490/9999): loss=0.4530322011486088, gradient=0.0077512720473902706\n",
      "Gradient Descent (4495/9999): loss=0.45302997107616344, gradient=0.007749901199707431\n",
      "Gradient Descent (4500/9999): loss=0.45302774192965184, gradient=0.007748531251608043\n",
      "Gradient Descent (4505/9999): loss=0.4530255137083572, gradient=0.007747162201474022\n",
      "Gradient Descent (4510/9999): loss=0.45302328641156375, gradient=0.007745794047685629\n",
      "Gradient Descent (4515/9999): loss=0.4530210600385578, gradient=0.007744426788621737\n",
      "Gradient Descent (4520/9999): loss=0.45301883458862635, gradient=0.007743060422660052\n",
      "Gradient Descent (4525/9999): loss=0.4530166100610582, gradient=0.007741694948177316\n",
      "Gradient Descent (4530/9999): loss=0.453014386455143, gradient=0.00774033036354952\n",
      "Gradient Descent (4535/9999): loss=0.4530121637701724, gradient=0.007738966667152207\n",
      "Gradient Descent (4540/9999): loss=0.4530099420054388, gradient=0.007737603857360615\n",
      "Gradient Descent (4545/9999): loss=0.4530077211602363, gradient=0.007736241932549888\n",
      "Gradient Descent (4550/9999): loss=0.4530055012338602, gradient=0.007734880891095398\n",
      "Gradient Descent (4555/9999): loss=0.4530032822256073, gradient=0.007733520731372859\n",
      "Gradient Descent (4560/9999): loss=0.45300106413477553, gradient=0.007732161451758589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent (4565/9999): loss=0.45299884696066456, gradient=0.007730803050629721\n",
      "Gradient Descent (4570/9999): loss=0.45299663070257445, gradient=0.007729445526364435\n",
      "Gradient Descent (4575/9999): loss=0.45299441535980783, gradient=0.007728088877342158\n",
      "Gradient Descent (4580/9999): loss=0.4529922009316678, gradient=0.007726733101943737\n",
      "Gradient Descent (4585/9999): loss=0.45298998741745916, gradient=0.007725378198551744\n",
      "Gradient Descent (4590/9999): loss=0.45298777481648783, gradient=0.007724024165550599\n",
      "Gradient Descent (4595/9999): loss=0.45298556312806126, gradient=0.007722671001326766\n",
      "Gradient Descent (4600/9999): loss=0.45298335235148773, gradient=0.0077213187042690975\n",
      "Gradient Descent (4605/9999): loss=0.45298114248607757, gradient=0.007719967272768874\n",
      "Gradient Descent (4610/9999): loss=0.4529789335311419, gradient=0.007718616705220079\n",
      "Gradient Descent (4615/9999): loss=0.45297672548599344, gradient=0.007717267000019641\n",
      "Gradient Descent (4620/9999): loss=0.45297451834994557, gradient=0.007715918155567545\n",
      "Gradient Descent (4625/9999): loss=0.4529723121223139, gradient=0.007714570170267124\n",
      "Gradient Descent (4630/9999): loss=0.45297010680241445, gradient=0.007713223042525155\n",
      "Gradient Descent (4635/9999): loss=0.45296790238956525, gradient=0.007711876770752153\n",
      "Gradient Descent (4640/9999): loss=0.4529656988830851, gradient=0.007710531353362529\n",
      "Gradient Descent (4645/9999): loss=0.4529634962822944, gradient=0.007709186788774733\n",
      "Gradient Descent (4650/9999): loss=0.4529612945865145, gradient=0.007707843075411507\n",
      "Gradient Descent (4655/9999): loss=0.4529590937950684, gradient=0.007706500211700048\n",
      "Gradient Descent (4660/9999): loss=0.4529568939072799, gradient=0.00770515819607223\n",
      "Gradient Descent (4665/9999): loss=0.4529546949224744, gradient=0.007703817026964699\n",
      "Gradient Descent (4670/9999): loss=0.45295249683997835, gradient=0.007702476702819183\n",
      "Gradient Descent (4675/9999): loss=0.4529502996591197, gradient=0.007701137222082528\n",
      "Gradient Descent (4680/9999): loss=0.45294810337922753, gradient=0.007699798583207027\n",
      "Gradient Descent (4685/9999): loss=0.4529459079996316, gradient=0.007698460784650475\n",
      "Gradient Descent (4690/9999): loss=0.4529437135196637, gradient=0.007697123824876429\n",
      "Gradient Descent (4695/9999): loss=0.452941519938657, gradient=0.007695787702354305\n",
      "Gradient Descent (4700/9999): loss=0.45293932725594455, gradient=0.007694452415559645\n",
      "Gradient Descent (4705/9999): loss=0.45293713547086206, gradient=0.007693117962974202\n",
      "Gradient Descent (4710/9999): loss=0.4529349445827453, gradient=0.0076917843430861195\n",
      "Gradient Descent (4715/9999): loss=0.4529327545909321, gradient=0.007690451554390168\n",
      "Gradient Descent (4720/9999): loss=0.45293056549476135, gradient=0.007689119595387835\n",
      "Gradient Descent (4725/9999): loss=0.4529283772935726, gradient=0.0076877884645874745\n",
      "Gradient Descent (4730/9999): loss=0.4529261899867068, gradient=0.007686458160504554\n",
      "Gradient Descent (4735/9999): loss=0.4529240035735065, gradient=0.0076851286816617325\n",
      "Gradient Descent (4740/9999): loss=0.45292181805331494, gradient=0.0076838000265890445\n",
      "Gradient Descent (4745/9999): loss=0.4529196334254765, gradient=0.007682472193824045\n",
      "Gradient Descent (4750/9999): loss=0.45291744968933717, gradient=0.007681145181911986\n",
      "Gradient Descent (4755/9999): loss=0.4529152668442432, gradient=0.007679818989405928\n",
      "Gradient Descent (4760/9999): loss=0.45291308488954296, gradient=0.0076784936148669046\n",
      "Gradient Descent (4765/9999): loss=0.4529109038245855, gradient=0.007677169056864075\n",
      "Gradient Descent (4770/9999): loss=0.45290872364872076, gradient=0.007675845313974859\n",
      "Gradient Descent (4775/9999): loss=0.4529065443613001, gradient=0.007674522384785062\n",
      "Gradient Descent (4780/9999): loss=0.4529043659616762, gradient=0.007673200267889034\n",
      "Gradient Descent (4785/9999): loss=0.45290218844920227, gradient=0.0076718789618898066\n",
      "Gradient Descent (4790/9999): loss=0.4529000118232331, gradient=0.007670558465399202\n",
      "Gradient Descent (4795/9999): loss=0.4528978360831242, gradient=0.007669238777037993\n",
      "Gradient Descent (4800/9999): loss=0.4528956612282323, gradient=0.007667919895435986\n",
      "Gradient Descent (4805/9999): loss=0.4528934872579151, gradient=0.007666601819232231\n",
      "Gradient Descent (4810/9999): loss=0.45289131417153183, gradient=0.0076652845470750255\n",
      "Gradient Descent (4815/9999): loss=0.45288914196844204, gradient=0.007663968077622162\n",
      "Gradient Descent (4820/9999): loss=0.452886970648007, gradient=0.007662652409540952\n",
      "Gradient Descent (4825/9999): loss=0.45288480020958816, gradient=0.007661337541508388\n",
      "Gradient Descent (4830/9999): loss=0.4528826306525492, gradient=0.0076600234722112425\n",
      "Gradient Descent (4835/9999): loss=0.45288046197625353, gradient=0.0076587102003462\n",
      "Gradient Descent (4840/9999): loss=0.45287829418006664, gradient=0.007657397724619931\n",
      "Gradient Descent (4845/9999): loss=0.4528761272633543, gradient=0.007656086043749245\n",
      "Gradient Descent (4850/9999): loss=0.4528739612254836, gradient=0.00765477515646112\n",
      "Gradient Descent (4855/9999): loss=0.4528717960658227, gradient=0.007653465061492891\n",
      "Gradient Descent (4860/9999): loss=0.45286963178374035, gradient=0.0076521557575923126\n",
      "Gradient Descent (4865/9999): loss=0.4528674683786066, gradient=0.007650847243517615\n",
      "Gradient Descent (4870/9999): loss=0.45286530584979234, gradient=0.007649539518037698\n",
      "Gradient Descent (4875/9999): loss=0.45286314419666945, gradient=0.007648232579932113\n",
      "Gradient Descent (4880/9999): loss=0.45286098341861086, gradient=0.007646926427991214\n",
      "Gradient Descent (4885/9999): loss=0.4528588235149901, gradient=0.007645621061016239\n",
      "Gradient Descent (4890/9999): loss=0.4528566644851821, gradient=0.007644316477819366\n",
      "Gradient Descent (4895/9999): loss=0.452854506328562, gradient=0.007643012677223824\n",
      "Gradient Descent (4900/9999): loss=0.4528523490445065, gradient=0.007641709658064005\n",
      "Gradient Descent (4905/9999): loss=0.4528501926323932, gradient=0.007640407419185415\n",
      "Gradient Descent (4910/9999): loss=0.45284803709160015, gradient=0.0076391059594448875\n",
      "Gradient Descent (4915/9999): loss=0.45284588242150636, gradient=0.007637805277710601\n",
      "Gradient Descent (4920/9999): loss=0.452843728621492, gradient=0.007636505372862114\n",
      "Gradient Descent (4925/9999): loss=0.452841575690938, gradient=0.007635206243790472\n",
      "Gradient Descent (4930/9999): loss=0.45283942362922597, gradient=0.007633907889398276\n",
      "Gradient Descent (4935/9999): loss=0.45283727243573874, gradient=0.007632610308599716\n",
      "Gradient Descent (4940/9999): loss=0.4528351221098592, gradient=0.0076313135003206346\n",
      "Gradient Descent (4945/9999): loss=0.452832972650972, gradient=0.007630017463498593\n",
      "Gradient Descent (4950/9999): loss=0.45283082405846203, gradient=0.007628722197082929\n",
      "Gradient Descent (4955/9999): loss=0.45282867633171525, gradient=0.007627427700034784\n",
      "Gradient Descent (4960/9999): loss=0.4528265294701183, gradient=0.007626133971327183\n",
      "Gradient Descent (4965/9999): loss=0.4528243834730583, gradient=0.007624841009945036\n",
      "Gradient Descent (4970/9999): loss=0.45282223833992397, gradient=0.007623548814885236\n",
      "Gradient Descent (4975/9999): loss=0.45282009407010393, gradient=0.007622257385156639\n",
      "Gradient Descent (4980/9999): loss=0.452817950662988, gradient=0.007620966719780153\n",
      "Gradient Descent (4985/9999): loss=0.45281580811796673, gradient=0.007619676817788742\n",
      "Gradient Descent (4990/9999): loss=0.4528136664344315, gradient=0.007618387678227467\n",
      "Gradient Descent (4995/9999): loss=0.452811525611774, gradient=0.007617099300153539\n",
      "Gradient Descent (5000/9999): loss=0.45280938564938733, gradient=0.007615811682636242\n",
      "Gradient Descent (5000/9999): loss=0.45280938564938733, gradient=0.007615811682636242, w=[-1.09050633e+00  1.00195148e+00 -7.83671039e-01 -1.52537223e-01\n",
      "  4.87961827e-01 -6.34233799e-02  1.39930932e+00 -1.40947286e-01\n",
      "  8.12976617e-02 -4.39508056e-01  1.93969723e-01  8.61753215e-02\n",
      "  1.07380792e+00 -3.77126553e-04  4.97871776e-02  5.62265735e-01\n",
      "  1.29076919e-02  7.30779547e-02 -1.38238689e-01  3.38103351e-02\n",
      "  8.61088394e-02 -1.07184095e-01  4.97432181e-01  6.36332250e-03\n",
      "  4.45967175e-02  2.13522647e-01  2.31342645e-02  2.01760333e-02\n",
      " -2.85042589e-03  2.69877714e-02 -9.56042798e-01  1.12969408e-01\n",
      " -1.42288167e+00  2.42155934e-02  2.41024667e-02  3.97075769e-01\n",
      "  9.46060389e-01 -2.61123383e-01  8.10767105e-02  1.38554874e-01\n",
      "  6.36476456e-02 -5.19244750e-01 -7.65173825e-02  3.25753598e-02\n",
      "  1.13774610e-01 -1.66556714e-01  4.89279920e-02  3.24447545e-01\n",
      "  4.50823494e-02 -2.35537862e-01 -1.22867687e-01 -2.76961984e-01\n",
      "  3.82009829e-01  8.13850032e-02 -1.02829786e-01  2.16798372e-01\n",
      " -9.20159681e-03 -1.07408804e-01  2.69877714e-02 -6.65647960e-01\n",
      "  3.58406601e-01 -5.20878453e-01  7.28125139e-01  1.75998463e-01\n",
      " -1.50080829e-01  1.00034667e+00 -9.71640992e-02  1.07069600e-02\n",
      "  4.71917626e-02  3.37437434e-02 -2.87871066e-01 -2.41104926e-02\n",
      " -4.16341495e-02 -1.03989286e-01 -2.77174152e-02 -6.02264852e-02\n",
      "  8.99175453e-01 -5.05296247e-02 -1.07654838e-01 -7.93688018e-02\n",
      " -2.90399042e-01 -3.94533554e-02 -5.08377034e-02 -1.79983344e-01\n",
      " -4.95692278e-02 -3.44006183e-02  4.48875859e-02  2.69877714e-02\n",
      " -1.44137195e-02  3.22685442e-02  8.11704837e-02  9.55828956e-01\n",
      "  4.24412085e-01 -3.74855755e-01  1.00354014e+00  6.14146999e-02\n",
      " -6.36346852e-02  8.71096592e-03  1.77062793e-02  2.11172192e-01\n",
      " -7.02678569e-02 -2.76615376e-02 -1.16239177e-01 -6.58650420e-02\n",
      " -3.12917615e-02  9.87234864e-01 -2.36110436e-02  4.18020964e-02\n",
      " -3.29589250e-02 -6.32369634e-02  2.63596081e-02 -1.81191341e-02\n",
      " -8.64806194e-02  9.11938771e-03  2.20397677e-03  3.17884295e-02\n",
      "  2.69877714e-02  3.30585646e-01 -1.24230266e-01  2.41292110e-01\n",
      "  9.94570057e-01  6.29529474e-01 -4.01077755e-01  1.00408727e+00\n",
      "  8.26300512e-02  2.35567435e-03 -6.80936099e-02  1.10156493e-02\n",
      "  3.93755765e-01  2.58748933e-02  3.22187975e-03 -5.75380840e-02\n",
      "  1.14268407e-02  1.69164501e-03  1.00070345e+00  2.00422911e-02\n",
      "  6.47815416e-02  6.34722068e-03  1.45272261e-01  4.01051064e-02\n",
      "  2.30504082e-02  3.40182465e-02  3.40977943e-02  1.87003504e-02\n",
      " -2.54907174e-02  2.69877714e-02  4.69584088e-01 -1.14520557e-01\n",
      "  2.89351903e-01  1.00183162e+00  7.59777137e-01 -3.22306391e-01\n",
      "  1.00416828e+00  5.98168891e-02  1.08418630e-01 -5.09684499e-02\n",
      "  1.02183468e-02  4.52838909e-01  2.39849266e-02  5.63509570e-03\n",
      "  2.04136644e-02  4.10378354e-02 -1.31841651e-03  1.00328725e+00\n",
      " -1.40103790e-02  4.72683065e-02  3.65053324e-02  2.82715257e-01\n",
      " -1.03103014e-01 -3.17996903e-02  1.36110554e-01 -6.08060954e-02\n",
      "  9.92867954e-03 -4.55975908e-02  2.69877714e-02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent (5005/9999): loss=0.45280724654666415, gradient=0.007614524824757153\n",
      "Gradient Descent (5010/9999): loss=0.4528051083029994, gradient=0.007613238725609973\n",
      "Gradient Descent (5015/9999): loss=0.4528029709177873, gradient=0.007611953384300624\n",
      "Gradient Descent (5020/9999): loss=0.45280083439042373, gradient=0.007610668799947289\n",
      "Gradient Descent (5025/9999): loss=0.4527986987203048, gradient=0.007609384971680395\n",
      "Gradient Descent (5030/9999): loss=0.45279656390682704, gradient=0.007608101898642618\n",
      "Gradient Descent (5035/9999): loss=0.4527944299493887, gradient=0.007606819579988887\n",
      "Gradient Descent (5040/9999): loss=0.4527922968473868, gradient=0.007605538014886453\n",
      "Gradient Descent (5045/9999): loss=0.45279016460022103, gradient=0.007604257202514794\n",
      "Gradient Descent (5050/9999): loss=0.45278803320729055, gradient=0.007602977142065709\n",
      "Gradient Descent (5055/9999): loss=0.4527859026679954, gradient=0.007601697832743261\n",
      "Gradient Descent (5060/9999): loss=0.45278377298173617, gradient=0.007600419273763765\n",
      "Gradient Descent (5065/9999): loss=0.4527816441479142, gradient=0.00759914146435584\n",
      "Gradient Descent (5070/9999): loss=0.45277951616593115, gradient=0.007597864403760343\n",
      "Gradient Descent (5075/9999): loss=0.45277738903518994, gradient=0.0075965880912303866\n",
      "Gradient Descent (5080/9999): loss=0.4527752627550929, gradient=0.007595312526031317\n",
      "Gradient Descent (5085/9999): loss=0.4527731373250439, gradient=0.007594037707440696\n",
      "Gradient Descent (5090/9999): loss=0.4527710127444473, gradient=0.0075927636347482876\n",
      "Gradient Descent (5095/9999): loss=0.4527688890127076, gradient=0.007591490307256048\n",
      "Gradient Descent (5100/9999): loss=0.45276676612922995, gradient=0.007590217724278061\n",
      "Gradient Descent (5105/9999): loss=0.4527646440934204, gradient=0.00758894588514054\n",
      "Gradient Descent (5110/9999): loss=0.4527625229046848, gradient=0.0075876747891818344\n",
      "Gradient Descent (5115/9999): loss=0.4527604025624302, gradient=0.007586404435752334\n",
      "Gradient Descent (5120/9999): loss=0.45275828306606375, gradient=0.007585134824214478\n",
      "Gradient Descent (5125/9999): loss=0.45275616441499344, gradient=0.00758386595394271\n",
      "Gradient Descent (5130/9999): loss=0.45275404660862717, gradient=0.007582597824323427\n",
      "Gradient Descent (5135/9999): loss=0.4527519296463739, gradient=0.007581330434754967\n",
      "Gradient Descent (5140/9999): loss=0.45274981352764293, gradient=0.007580063784647551\n",
      "Gradient Descent (5145/9999): loss=0.45274769825184397, gradient=0.007578797873423209\n",
      "Gradient Descent (5150/9999): loss=0.4527455838183867, gradient=0.007577532700515809\n",
      "Gradient Descent (5155/9999): loss=0.4527434702266823, gradient=0.007576268265370933\n",
      "Gradient Descent (5160/9999): loss=0.4527413574761412, gradient=0.0075750045674458704\n",
      "Gradient Descent (5165/9999): loss=0.45273924556617495, gradient=0.007573741606209557\n",
      "Gradient Descent (5170/9999): loss=0.45273713449619596, gradient=0.0075724793811424935\n",
      "Gradient Descent (5175/9999): loss=0.45273502426561557, gradient=0.0075712178917367295\n",
      "Gradient Descent (5180/9999): loss=0.4527329148738469, gradient=0.007569957137495788\n",
      "Gradient Descent (5185/9999): loss=0.45273080632030294, gradient=0.007568697117934573\n",
      "Gradient Descent (5190/9999): loss=0.45272869860439713, gradient=0.007567437832579367\n",
      "Gradient Descent (5195/9999): loss=0.45272659172554314, gradient=0.007566179280967705\n",
      "Gradient Descent (5200/9999): loss=0.4527244856831552, gradient=0.007564921462648374\n",
      "Gradient Descent (5205/9999): loss=0.4527223804766477, gradient=0.007563664377181254\n",
      "Gradient Descent (5210/9999): loss=0.45272027610543547, gradient=0.0075624080241373315\n",
      "Gradient Descent (5215/9999): loss=0.45271817256893404, gradient=0.007561152403098589\n",
      "Gradient Descent (5220/9999): loss=0.45271606986655855, gradient=0.007559897513657956\n",
      "Gradient Descent (5225/9999): loss=0.45271396799772506, gradient=0.007558643355419172\n",
      "Gradient Descent (5230/9999): loss=0.45271186696184956, gradient=0.0075573899279967665\n",
      "Gradient Descent (5235/9999): loss=0.4527097667583488, gradient=0.007556137231015957\n",
      "Gradient Descent (5240/9999): loss=0.4527076673866392, gradient=0.007554885264112606\n",
      "Gradient Descent (5245/9999): loss=0.45270556884613833, gradient=0.007553634026933037\n",
      "Gradient Descent (5250/9999): loss=0.4527034711362631, gradient=0.0075523835191340614\n",
      "Gradient Descent (5255/9999): loss=0.45270137425643114, gradient=0.007551133740382829\n",
      "Gradient Descent (5260/9999): loss=0.4526992782060605, gradient=0.007549884690356766\n",
      "Gradient Descent (5265/9999): loss=0.4526971829845695, gradient=0.007548636368743469\n",
      "Gradient Descent (5270/9999): loss=0.45269508859137636, gradient=0.007547388775240618\n",
      "Gradient Descent (5275/9999): loss=0.4526929950259, gradient=0.007546141909555876\n",
      "Gradient Descent (5280/9999): loss=0.4526909022875589, gradient=0.0075448957714068406\n",
      "Gradient Descent (5285/9999): loss=0.45268881037577274, gradient=0.007543650360520851\n",
      "Gradient Descent (5290/9999): loss=0.4526867192899603, gradient=0.0075424056766350105\n",
      "Gradient Descent (5295/9999): loss=0.45268462902954176, gradient=0.007541161719495981\n",
      "Gradient Descent (5300/9999): loss=0.4526825395939365, gradient=0.007539918488859974\n",
      "Gradient Descent (5305/9999): loss=0.4526804509825649, gradient=0.007538675984492559\n",
      "Gradient Descent (5310/9999): loss=0.45267836319484683, gradient=0.007537434206168638\n",
      "Gradient Descent (5315/9999): loss=0.4526762762302028, gradient=0.007536193153672306\n",
      "Gradient Descent (5320/9999): loss=0.45267419008805376, gradient=0.007534952826796739\n",
      "Gradient Descent (5325/9999): loss=0.45267210476782, gradient=0.007533713225344099\n",
      "Gradient Descent (5330/9999): loss=0.4526700202689227, gradient=0.007532474349125454\n",
      "Gradient Descent (5335/9999): loss=0.45266793659078297, gradient=0.007531236197960613\n",
      "Gradient Descent (5340/9999): loss=0.45266585373282225, gradient=0.007529998771678051\n",
      "Gradient Descent (5345/9999): loss=0.45266377169446165, gradient=0.0075287620701147955\n",
      "Gradient Descent (5350/9999): loss=0.45266169047512306, gradient=0.0075275260931163355\n",
      "Gradient Descent (5355/9999): loss=0.4526596100742278, gradient=0.007526290840536435\n",
      "Gradient Descent (5360/9999): loss=0.4526575304911982, gradient=0.007525056312237122\n",
      "Gradient Descent (5365/9999): loss=0.4526554517254561, gradient=0.007523822508088479\n",
      "Gradient Descent (5370/9999): loss=0.4526533737764235, gradient=0.007522589427968613\n",
      "Gradient Descent (5375/9999): loss=0.4526512966435229, gradient=0.007521357071763474\n",
      "Gradient Descent (5380/9999): loss=0.45264922032617627, gradient=0.007520125439366749\n",
      "Gradient Descent (5385/9999): loss=0.45264714482380647, gradient=0.007518894530679797\n",
      "Gradient Descent (5390/9999): loss=0.4526450701358359, gradient=0.0075176643456114295\n",
      "Gradient Descent (5395/9999): loss=0.4526429962616871, gradient=0.0075164348840779304\n",
      "Gradient Descent (5400/9999): loss=0.45264092320078303, gradient=0.0075152061460027775\n",
      "Gradient Descent (5405/9999): loss=0.45263885095254636, gradient=0.007513978131316661\n",
      "Gradient Descent (5410/9999): loss=0.4526367795164002, gradient=0.007512750839957285\n",
      "Gradient Descent (5415/9999): loss=0.45263470889176743, gradient=0.007511524271869264\n",
      "Gradient Descent (5420/9999): loss=0.4526326390780713, gradient=0.0075102984270040115\n",
      "Gradient Descent (5425/9999): loss=0.4526305700747346, gradient=0.007509073305319585\n",
      "Gradient Descent (5430/9999): loss=0.452628501881181, gradient=0.00750784890678061\n",
      "Gradient Descent (5435/9999): loss=0.45262643449683354, gradient=0.0075066252313581265\n",
      "Gradient Descent (5440/9999): loss=0.4526243679211154, gradient=0.0075054022790294805\n",
      "Gradient Descent (5445/9999): loss=0.45262230215345006, gradient=0.0075041800497781625\n",
      "Gradient Descent (5450/9999): loss=0.4526202371932611, gradient=0.00750295854359374\n",
      "Gradient Descent (5455/9999): loss=0.45261817303997176, gradient=0.007501737760471706\n",
      "Gradient Descent (5460/9999): loss=0.4526161096930057, gradient=0.007500517700413333\n",
      "Gradient Descent (5465/9999): loss=0.45261404715178627, gradient=0.007499298363425592\n",
      "Gradient Descent (5470/9999): loss=0.4526119854157372, gradient=0.007498079749520968\n",
      "Gradient Descent (5475/9999): loss=0.4526099244842821, gradient=0.00749686185871742\n",
      "Gradient Descent (5480/9999): loss=0.45260786435684436, gradient=0.007495644691038169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent (5485/9999): loss=0.4526058050328477, gradient=0.0074944282465115985\n",
      "Gradient Descent (5490/9999): loss=0.45260374651171587, gradient=0.007493212525171203\n",
      "Gradient Descent (5495/9999): loss=0.4526016887928724, gradient=0.007491997527055304\n",
      "Gradient Descent (5500/9999): loss=0.4525996318757408, gradient=0.007490783252207128\n",
      "Gradient Descent (5505/9999): loss=0.452597575759745, gradient=0.007489569700674484\n",
      "Gradient Descent (5510/9999): loss=0.45259552044430856, gradient=0.007488356872509756\n",
      "Gradient Descent (5515/9999): loss=0.4525934659288553, gradient=0.007487144767769764\n",
      "Gradient Descent (5520/9999): loss=0.4525914122128086, gradient=0.0074859333865155975\n",
      "Gradient Descent (5525/9999): loss=0.4525893592955921, gradient=0.0074847227288125494\n",
      "Gradient Descent (5530/9999): loss=0.4525873071766296, gradient=0.007483512794729921\n",
      "Gradient Descent (5535/9999): loss=0.4525852558553445, gradient=0.007482303584340958\n",
      "Gradient Descent (5540/9999): loss=0.45258320533116064, gradient=0.0074810950977227016\n",
      "Gradient Descent (5545/9999): loss=0.4525811556035015, gradient=0.00747988733495584\n",
      "Gradient Descent (5550/9999): loss=0.4525791066717906, gradient=0.007478680296124666\n",
      "Gradient Descent (5555/9999): loss=0.4525770585354516, gradient=0.007477473981316846\n",
      "Gradient Descent (5560/9999): loss=0.4525750111939077, gradient=0.007476268390623344\n",
      "Gradient Descent (5565/9999): loss=0.4525729646465829, gradient=0.00747506352413837\n",
      "Gradient Descent (5570/9999): loss=0.4525709188929001, gradient=0.007473859381959114\n",
      "Gradient Descent (5575/9999): loss=0.4525688739322832, gradient=0.007472655964185777\n",
      "Gradient Descent (5580/9999): loss=0.4525668297641551, gradient=0.007471453270921299\n",
      "Gradient Descent (5585/9999): loss=0.4525647863879395, gradient=0.007470251302271389\n",
      "Gradient Descent (5590/9999): loss=0.45256274380305966, gradient=0.007469050058344271\n",
      "Gradient Descent (5595/9999): loss=0.45256070200893866, gradient=0.007467849539250659\n",
      "Gradient Descent (5600/9999): loss=0.4525586610049999, gradient=0.0074666497451036035\n",
      "Gradient Descent (5605/9999): loss=0.4525566207906665, gradient=0.007465450676018368\n",
      "Gradient Descent (5610/9999): loss=0.45255458136536153, gradient=0.007464252332112291\n",
      "Gradient Descent (5615/9999): loss=0.4525525427285082, gradient=0.00746305471350475\n",
      "Gradient Descent (5620/9999): loss=0.4525505048795294, gradient=0.007461857820316935\n",
      "Gradient Descent (5625/9999): loss=0.45254846781784835, gradient=0.007460661652671822\n",
      "Gradient Descent (5630/9999): loss=0.4525464315428878, gradient=0.007459466210694014\n",
      "Gradient Descent (5635/9999): loss=0.45254439605407065, gradient=0.0074582714945096405\n",
      "Gradient Descent (5640/9999): loss=0.45254236135082, gradient=0.007457077504246251\n",
      "Gradient Descent (5645/9999): loss=0.45254032743255834, gradient=0.007455884240032647\n",
      "Gradient Descent (5650/9999): loss=0.45253829429870857, gradient=0.0074546917019988725\n",
      "Gradient Descent (5655/9999): loss=0.45253626194869323, gradient=0.007453499890276011\n",
      "Gradient Descent (5660/9999): loss=0.4525342303819351, gradient=0.007452308804996138\n",
      "Gradient Descent (5665/9999): loss=0.4525321995978566, gradient=0.007451118446292147\n",
      "Gradient Descent (5670/9999): loss=0.4525301695958804, gradient=0.007449928814297719\n",
      "Gradient Descent (5675/9999): loss=0.45252814037542877, gradient=0.007448739909147125\n",
      "Gradient Descent (5680/9999): loss=0.4525261119359245, gradient=0.00744755173097525\n",
      "Gradient Descent (5685/9999): loss=0.4525240842767897, gradient=0.007446364279917301\n",
      "Gradient Descent (5690/9999): loss=0.4525220573974466, gradient=0.00744517755610889\n",
      "Gradient Descent (5695/9999): loss=0.45252003129731744, gradient=0.007443991559685814\n",
      "Gradient Descent (5700/9999): loss=0.4525180059758246, gradient=0.00744280629078401\n",
      "Gradient Descent (5705/9999): loss=0.45251598143239014, gradient=0.007441621749539387\n",
      "Gradient Descent (5710/9999): loss=0.45251395766643604, gradient=0.007440437936087818\n",
      "Gradient Descent (5715/9999): loss=0.4525119346773843, gradient=0.007439254850564957\n",
      "Gradient Descent (5720/9999): loss=0.45250991246465705, gradient=0.007438072493106197\n",
      "Gradient Descent (5725/9999): loss=0.45250789102767597, gradient=0.007436890863846512\n",
      "Gradient Descent (5730/9999): loss=0.45250587036586326, gradient=0.0074357099629204434\n",
      "Gradient Descent (5735/9999): loss=0.45250385047864045, gradient=0.007434529790461945\n",
      "Gradient Descent (5740/9999): loss=0.4525018313654295, gradient=0.007433350346604271\n",
      "Gradient Descent (5745/9999): loss=0.4524998130256518, gradient=0.0074321716314799825\n",
      "Gradient Descent (5750/9999): loss=0.45249779545872915, gradient=0.007430993645220694\n",
      "Gradient Descent (5755/9999): loss=0.45249577866408314, gradient=0.007429816387957163\n",
      "Gradient Descent (5760/9999): loss=0.4524937626411354, gradient=0.007428639859819067\n",
      "Gradient Descent (5765/9999): loss=0.4524917473893075, gradient=0.007427464060934979\n",
      "Gradient Descent (5770/9999): loss=0.4524897329080205, gradient=0.007426288991432248\n",
      "Gradient Descent (5775/9999): loss=0.4524877191966963, gradient=0.007425114651436951\n",
      "Gradient Descent (5780/9999): loss=0.4524857062547559, gradient=0.007423941041073727\n",
      "Gradient Descent (5785/9999): loss=0.45248369408162087, gradient=0.0074227681604658305\n",
      "Gradient Descent (5790/9999): loss=0.4524816826767123, gradient=0.007421596009734881\n",
      "Gradient Descent (5795/9999): loss=0.4524796720394513, gradient=0.00742042458900096\n",
      "Gradient Descent (5800/9999): loss=0.4524776621692593, gradient=0.007419253898382343\n",
      "Gradient Descent (5805/9999): loss=0.4524756530655574, gradient=0.00741808393799557\n",
      "Gradient Descent (5810/9999): loss=0.4524736447277667, gradient=0.007416914707955303\n",
      "Gradient Descent (5815/9999): loss=0.4524716371553084, gradient=0.007415746208374272\n",
      "Gradient Descent (5820/9999): loss=0.45246963034760335, gradient=0.007414578439363147\n",
      "Gradient Descent (5825/9999): loss=0.4524676243040724, gradient=0.007413411401030526\n",
      "Gradient Descent (5830/9999): loss=0.45246561902413696, gradient=0.007412245093482855\n",
      "Gradient Descent (5835/9999): loss=0.4524636145072177, gradient=0.007411079516824294\n",
      "Gradient Descent (5840/9999): loss=0.4524616107527356, gradient=0.007409914671156716\n",
      "Gradient Descent (5845/9999): loss=0.45245960776011174, gradient=0.007408750556579605\n",
      "Gradient Descent (5850/9999): loss=0.4524576055287665, gradient=0.00740758717319002\n",
      "Gradient Descent (5855/9999): loss=0.45245560405812146, gradient=0.007406424521082465\n",
      "Gradient Descent (5860/9999): loss=0.4524536033475969, gradient=0.0074052626003488714\n",
      "Gradient Descent (5865/9999): loss=0.4524516033966137, gradient=0.007404101411078513\n",
      "Gradient Descent (5870/9999): loss=0.45244960420459296, gradient=0.007402940953357963\n",
      "Gradient Descent (5875/9999): loss=0.4524476057709553, gradient=0.007401781227271031\n",
      "Gradient Descent (5880/9999): loss=0.4524456080951214, gradient=0.00740062223289866\n",
      "Gradient Descent (5885/9999): loss=0.4524436111765123, gradient=0.007399463970318909\n",
      "Gradient Descent (5890/9999): loss=0.45244161501454816, gradient=0.007398306439606892\n",
      "Gradient Descent (5895/9999): loss=0.45243961960865053, gradient=0.007397149640834686\n",
      "Gradient Descent (5900/9999): loss=0.45243762495823986, gradient=0.007395993574071332\n",
      "Gradient Descent (5905/9999): loss=0.4524356310627365, gradient=0.007394838239382705\n",
      "Gradient Descent (5910/9999): loss=0.45243363792156194, gradient=0.007393683636831521\n",
      "Gradient Descent (5915/9999): loss=0.4524316455341365, gradient=0.007392529766477275\n",
      "Gradient Descent (5920/9999): loss=0.452429653899881, gradient=0.007391376628376177\n",
      "Gradient Descent (5925/9999): loss=0.4524276630182166, gradient=0.0073902242225810585\n",
      "Gradient Descent (5930/9999): loss=0.4524256728885635, gradient=0.007389072549141466\n",
      "Gradient Descent (5935/9999): loss=0.4524236835103429, gradient=0.007387921608103407\n",
      "Gradient Descent (5940/9999): loss=0.45242169488297573, gradient=0.007386771399509462\n",
      "Gradient Descent (5945/9999): loss=0.45241970700588263, gradient=0.007385621923398696\n",
      "Gradient Descent (5950/9999): loss=0.45241771987848467, gradient=0.0073844731798065975\n",
      "Gradient Descent (5955/9999): loss=0.4524157335002026, gradient=0.007383325168765015\n",
      "Gradient Descent (5960/9999): loss=0.45241374787045757, gradient=0.007382177890302215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent (5965/9999): loss=0.4524117629886705, gradient=0.007381031344442694\n",
      "Gradient Descent (5970/9999): loss=0.4524097788542624, gradient=0.0073798855312072476\n",
      "Gradient Descent (5975/9999): loss=0.4524077954666542, gradient=0.007378740450612902\n",
      "Gradient Descent (5980/9999): loss=0.45240581282526715, gradient=0.007377596102672855\n",
      "Gradient Descent (5985/9999): loss=0.4524038309295225, gradient=0.007376452487396494\n",
      "Gradient Descent (5990/9999): loss=0.45240184977884135, gradient=0.007375309604789276\n",
      "Gradient Descent (5995/9999): loss=0.4523998693726449, gradient=0.007374167454852794\n",
      "Gradient Descent (6000/9999): loss=0.4523978897103545, gradient=0.007373026037584635\n",
      "Gradient Descent (6000/9999): loss=0.4523978897103545, gradient=0.007373026037584635, w=[-1.09575648e+00  1.02110655e+00 -7.92463088e-01 -1.48318048e-01\n",
      "  5.01599234e-01 -5.48722336e-02  1.41232817e+00 -1.41995018e-01\n",
      "  8.02553738e-02 -4.48333268e-01  1.90976175e-01  8.26265047e-02\n",
      "  1.07349880e+00 -4.48044082e-04  4.75464481e-02  5.62552228e-01\n",
      "  1.28377940e-02  7.03812395e-02 -1.37869920e-01  3.25553656e-02\n",
      "  8.80047420e-02 -1.07704509e-01  4.99354599e-01  5.91578332e-03\n",
      "  4.20086803e-02  2.07120623e-01  2.29061015e-02  1.91514275e-02\n",
      " -4.07462780e-03  2.71079142e-02 -9.77905231e-01  1.21341298e-01\n",
      " -1.43875059e+00  1.24826428e-02  2.18220413e-02  3.91226719e-01\n",
      "  9.45753925e-01 -2.66543821e-01  8.82843875e-02  1.36936840e-01\n",
      "  6.57390583e-02 -5.23641714e-01 -7.63968523e-02  2.97136833e-02\n",
      "  1.11231041e-01 -1.66867910e-01  4.56499028e-02  3.15681968e-01\n",
      "  4.39056137e-02 -2.41974589e-01 -1.22805144e-01 -2.79859355e-01\n",
      "  3.83471588e-01  7.78608257e-02 -9.89479491e-02  2.18910194e-01\n",
      " -1.04856599e-02 -1.08100181e-01  2.71079142e-02 -6.74403800e-01\n",
      "  3.71403025e-01 -5.25432361e-01  7.22359883e-01  1.60026396e-01\n",
      " -1.59064437e-01  1.00010032e+00 -9.19587104e-02  1.59693858e-02\n",
      "  4.76880221e-02  3.54900497e-02 -2.80882036e-01 -2.38878967e-02\n",
      " -4.12239445e-02 -1.01363972e-01 -2.75953468e-02 -5.97624886e-02\n",
      "  8.96526975e-01 -5.01119499e-02 -1.06768177e-01 -7.90615640e-02\n",
      " -2.85454303e-01 -3.82921501e-02 -5.04641651e-02 -1.73016699e-01\n",
      " -4.89783607e-02 -3.42749118e-02  5.21637689e-02  2.71079142e-02\n",
      " -9.30104744e-03  3.04697117e-02  7.59937101e-02  9.54644374e-01\n",
      "  4.06757843e-01 -3.79118817e-01  1.00350526e+00  7.04887529e-02\n",
      " -6.39968767e-02  9.68732954e-03  1.82845383e-02  2.10167358e-01\n",
      " -7.08738063e-02 -2.77994997e-02 -1.13483843e-01 -6.59182955e-02\n",
      " -3.13449924e-02  9.86698004e-01 -2.33273471e-02  4.64467476e-02\n",
      " -3.25503444e-02 -6.00351658e-02  2.52720833e-02 -1.83866545e-02\n",
      " -8.33686895e-02  8.17384800e-03  2.33898816e-03  3.48292788e-02\n",
      "  2.71079142e-02  3.38821204e-01 -1.34193510e-01  2.32093623e-01\n",
      "  9.94304619e-01  6.15452782e-01 -3.98733715e-01  1.00409126e+00\n",
      "  8.60108587e-02 -4.36091914e-05 -6.77969167e-02  1.04547614e-02\n",
      "  3.83206186e-01  2.57155978e-02  4.41503687e-03 -5.93222537e-02\n",
      "  1.13400162e-02  3.23425395e-03  1.00058660e+00  2.05726065e-02\n",
      "  6.42332247e-02  6.79973020e-03  1.39314530e-01  3.93028579e-02\n",
      "  2.44966608e-02  3.38060582e-02  3.36348721e-02  1.93170526e-02\n",
      " -3.01571208e-02  2.71079142e-02  4.76171375e-01 -1.26211549e-01\n",
      "  2.77564162e-01  1.00176466e+00  7.49578954e-01 -3.14591401e-01\n",
      "  1.00417789e+00  5.53029953e-02  1.05885121e-01 -4.89212427e-02\n",
      "  8.69022408e-03  4.37189697e-01  2.45213118e-02  7.55446773e-03\n",
      "  1.34836119e-02  4.12620283e-02  7.72665440e-04  1.00326303e+00\n",
      " -1.36101141e-02  3.89262632e-02  3.69782204e-02  2.69499142e-01\n",
      " -1.03159945e-01 -2.97086646e-02  1.34282077e-01 -6.21583768e-02\n",
      "  1.03121071e-02 -5.56806631e-02  2.71079142e-02]\n",
      "Gradient Descent (6005/9999): loss=0.4523959107913916, gradient=0.007371885352978446\n",
      "Gradient Descent (6010/9999): loss=0.4523939326151775, gradient=0.007370745401023846\n",
      "Gradient Descent (6015/9999): loss=0.45239195518113373, gradient=0.00736960618170641\n",
      "Gradient Descent (6020/9999): loss=0.4523899784886818, gradient=0.007368467695007664\n",
      "Gradient Descent (6025/9999): loss=0.45238800253724365, gradient=0.007367329940905002\n",
      "Gradient Descent (6030/9999): loss=0.45238602732624067, gradient=0.007366192919371724\n",
      "Gradient Descent (6035/9999): loss=0.4523840528550945, gradient=0.007365056630376946\n",
      "Gradient Descent (6040/9999): loss=0.4523820791232273, gradient=0.007363921073885638\n",
      "Gradient Descent (6045/9999): loss=0.4523801061300613, gradient=0.007362786249858544\n",
      "Gradient Descent (6050/9999): loss=0.4523781338750178, gradient=0.007361652158252219\n",
      "Gradient Descent (6055/9999): loss=0.45237616235751954, gradient=0.0073605187990189525\n",
      "Gradient Descent (6060/9999): loss=0.45237419157698855, gradient=0.007359386172106766\n",
      "Gradient Descent (6065/9999): loss=0.45237222153284695, gradient=0.0073582542774594095\n",
      "Gradient Descent (6070/9999): loss=0.4523702522245172, gradient=0.0073571231150163325\n",
      "Gradient Descent (6075/9999): loss=0.4523682836514221, gradient=0.007355992684712665\n",
      "Gradient Descent (6080/9999): loss=0.45236631581298414, gradient=0.007354862986479165\n",
      "Gradient Descent (6085/9999): loss=0.4523643487086261, gradient=0.007353734020242313\n",
      "Gradient Descent (6090/9999): loss=0.4523623823377705, gradient=0.007352605785924168\n",
      "Gradient Descent (6095/9999): loss=0.4523604166998408, gradient=0.007351478283442393\n",
      "Gradient Descent (6100/9999): loss=0.45235845179425965, gradient=0.007350351512710304\n",
      "Gradient Descent (6105/9999): loss=0.45235648762045066, gradient=0.007349225473636783\n",
      "Gradient Descent (6110/9999): loss=0.45235452417783695, gradient=0.007348100166126311\n",
      "Gradient Descent (6115/9999): loss=0.45235256146584213, gradient=0.00734697559007894\n",
      "Gradient Descent (6120/9999): loss=0.45235059948388967, gradient=0.007345851745390266\n",
      "Gradient Descent (6125/9999): loss=0.45234863823140337, gradient=0.00734472863195146\n",
      "Gradient Descent (6130/9999): loss=0.45234667770780707, gradient=0.007343606249649232\n",
      "Gradient Descent (6135/9999): loss=0.45234471791252484, gradient=0.007342484598365839\n",
      "Gradient Descent (6140/9999): loss=0.4523427588449809, gradient=0.007341363677979064\n",
      "Gradient Descent (6145/9999): loss=0.4523408005045997, gradient=0.007340243488362229\n",
      "Gradient Descent (6150/9999): loss=0.4523388428908054, gradient=0.007339124029384165\n",
      "Gradient Descent (6155/9999): loss=0.45233688600302296, gradient=0.007338005300909246\n",
      "Gradient Descent (6160/9999): loss=0.4523349298406769, gradient=0.007336887302797348\n",
      "Gradient Descent (6165/9999): loss=0.45233297440319264, gradient=0.007335770034903858\n",
      "Gradient Descent (6170/9999): loss=0.4523310196899949, gradient=0.0073346534970797055\n",
      "Gradient Descent (6175/9999): loss=0.4523290657005092, gradient=0.007333537689171294\n",
      "Gradient Descent (6180/9999): loss=0.4523271124341611, gradient=0.007332422611020584\n",
      "Gradient Descent (6185/9999): loss=0.4523251598903761, gradient=0.007331308262465008\n",
      "Gradient Descent (6190/9999): loss=0.4523232080685801, gradient=0.007330194643337562\n",
      "Gradient Descent (6195/9999): loss=0.45232125696819947, gradient=0.007329081753466705\n",
      "Gradient Descent (6200/9999): loss=0.45231930658866004, gradient=0.007327969592676491\n",
      "Gradient Descent (6205/9999): loss=0.45231735692938835, gradient=0.007326858160786445\n",
      "Gradient Descent (6210/9999): loss=0.4523154079898113, gradient=0.007325747457611604\n",
      "Gradient Descent (6215/9999): loss=0.4523134597693553, gradient=0.007324637482962609\n",
      "Gradient Descent (6220/9999): loss=0.4523115122674476, gradient=0.007323528236645595\n",
      "Gradient Descent (6225/9999): loss=0.4523095654835155, gradient=0.007322419718462242\n",
      "Gradient Descent (6230/9999): loss=0.4523076194169865, gradient=0.007321311928209809\n",
      "Gradient Descent (6235/9999): loss=0.45230567406728817, gradient=0.007320204865681103\n",
      "Gradient Descent (6240/9999): loss=0.4523037294338484, gradient=0.007319098530664485\n",
      "Gradient Descent (6245/9999): loss=0.4523017855160955, gradient=0.007317992922943928\n",
      "Gradient Descent (6250/9999): loss=0.45229984231345755, gradient=0.00731688804229894\n",
      "Gradient Descent (6255/9999): loss=0.4522978998253633, gradient=0.0073157838885046875\n",
      "Gradient Descent (6260/9999): loss=0.45229595805124134, gradient=0.007314680461331917\n",
      "Gradient Descent (6265/9999): loss=0.4522940169905212, gradient=0.007313577760546962\n",
      "Gradient Descent (6270/9999): loss=0.45229207664263155, gradient=0.007312475785911848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent (6275/9999): loss=0.4522901370070023, gradient=0.007311374537184187\n",
      "Gradient Descent (6280/9999): loss=0.4522881980830633, gradient=0.007310274014117276\n",
      "Gradient Descent (6285/9999): loss=0.4522862598702443, gradient=0.0073091742164600685\n",
      "Gradient Descent (6290/9999): loss=0.45228432236797556, gradient=0.007308075143957209\n",
      "Gradient Descent (6295/9999): loss=0.4522823855756879, gradient=0.007306976796349002\n",
      "Gradient Descent (6300/9999): loss=0.452280449492812, gradient=0.007305879173371502\n",
      "Gradient Descent (6305/9999): loss=0.4522785141187788, gradient=0.007304782274756482\n",
      "Gradient Descent (6310/9999): loss=0.4522765794530197, gradient=0.007303686100231424\n",
      "Gradient Descent (6315/9999): loss=0.4522746454949665, gradient=0.007302590649519576\n",
      "Gradient Descent (6320/9999): loss=0.4522727122440508, gradient=0.007301495922339983\n",
      "Gradient Descent (6325/9999): loss=0.45227077969970486, gradient=0.0073004019184074575\n",
      "Gradient Descent (6330/9999): loss=0.4522688478613611, gradient=0.0072993086374326084\n",
      "Gradient Descent (6335/9999): loss=0.45226691672845226, gradient=0.007298216079121859\n",
      "Gradient Descent (6340/9999): loss=0.4522649863004112, gradient=0.007297124243177493\n",
      "Gradient Descent (6345/9999): loss=0.4522630565766714, gradient=0.007296033129297678\n",
      "Gradient Descent (6350/9999): loss=0.45226112755666636, gradient=0.007294942737176399\n",
      "Gradient Descent (6355/9999): loss=0.45225919923983, gradient=0.007293853066503555\n",
      "Gradient Descent (6360/9999): loss=0.4522572716255965, gradient=0.007292764116964971\n",
      "Gradient Descent (6365/9999): loss=0.4522553447134003, gradient=0.00729167588824239\n",
      "Gradient Descent (6370/9999): loss=0.4522534185026763, gradient=0.007290588380013547\n",
      "Gradient Descent (6375/9999): loss=0.4522514929928593, gradient=0.007289501591952097\n",
      "Gradient Descent (6380/9999): loss=0.45224956818338524, gradient=0.007288415523727716\n",
      "Gradient Descent (6385/9999): loss=0.45224764407368956, gradient=0.0072873301750060885\n",
      "Gradient Descent (6390/9999): loss=0.4522457206632082, gradient=0.007286245545448962\n",
      "Gradient Descent (6395/9999): loss=0.4522437979513777, gradient=0.007285161634714084\n",
      "Gradient Descent (6400/9999): loss=0.4522418759376347, gradient=0.0072840784424553304\n",
      "Gradient Descent (6405/9999): loss=0.4522399546214161, gradient=0.0072829959683226655\n",
      "Gradient Descent (6410/9999): loss=0.4522380340021597, gradient=0.007281914211962146\n",
      "Gradient Descent (6415/9999): loss=0.4522361140793027, gradient=0.007280833173016045\n",
      "Gradient Descent (6420/9999): loss=0.45223419485228333, gradient=0.007279752851122702\n",
      "Gradient Descent (6425/9999): loss=0.45223227632054025, gradient=0.00727867324591676\n",
      "Gradient Descent (6430/9999): loss=0.4522303584835118, gradient=0.007277594357028974\n",
      "Gradient Descent (6435/9999): loss=0.4522284413406371, gradient=0.007276516184086413\n",
      "Gradient Descent (6440/9999): loss=0.4522265248913558, gradient=0.007275438726712352\n",
      "Gradient Descent (6445/9999): loss=0.4522246091351073, gradient=0.0072743619845263875\n",
      "Gradient Descent (6450/9999): loss=0.4522226940713322, gradient=0.007273285957144399\n",
      "Gradient Descent (6455/9999): loss=0.4522207796994706, gradient=0.00727221064417862\n",
      "Gradient Descent (6460/9999): loss=0.45221886601896344, gradient=0.007271136045237614\n",
      "Gradient Descent (6465/9999): loss=0.4522169530292521, gradient=0.007270062159926349\n",
      "Gradient Descent (6470/9999): loss=0.4522150407297781, gradient=0.007268988987846201\n",
      "Gradient Descent (6475/9999): loss=0.45221312911998307, gradient=0.007267916528594959\n",
      "Gradient Descent (6480/9999): loss=0.4522112181993099, gradient=0.007266844781766864\n",
      "Gradient Descent (6485/9999): loss=0.45220930796720094, gradient=0.0072657737469526565\n",
      "Gradient Descent (6490/9999): loss=0.45220739842309926, gradient=0.007264703423739563\n",
      "Gradient Descent (6495/9999): loss=0.45220548956644846, gradient=0.007263633811711369\n",
      "Gradient Descent (6500/9999): loss=0.4522035813966923, gradient=0.007262564910448368\n",
      "Gradient Descent (6505/9999): loss=0.45220167391327515, gradient=0.0072614967195275045\n",
      "Gradient Descent (6510/9999): loss=0.4521997671156417, gradient=0.007260429238522264\n",
      "Gradient Descent (6515/9999): loss=0.4521978610032366, gradient=0.007259362467002795\n",
      "Gradient Descent (6520/9999): loss=0.4521959555755055, gradient=0.007258296404535903\n",
      "Gradient Descent (6525/9999): loss=0.4521940508318943, gradient=0.007257231050685071\n",
      "Gradient Descent (6530/9999): loss=0.452192146771849, gradient=0.007256166405010527\n",
      "Gradient Descent (6535/9999): loss=0.4521902433948167, gradient=0.007255102467069174\n",
      "Gradient Descent (6540/9999): loss=0.4521883407002437, gradient=0.0072540392364147215\n",
      "Gradient Descent (6545/9999): loss=0.45218643868757796, gradient=0.007252976712597663\n",
      "Gradient Descent (6550/9999): loss=0.45218453735626735, gradient=0.007251914895165307\n",
      "Gradient Descent (6555/9999): loss=0.4521826367057597, gradient=0.007250853783661796\n",
      "Gradient Descent (6560/9999): loss=0.4521807367355041, gradient=0.007249793377628127\n",
      "Gradient Descent (6565/9999): loss=0.4521788374449495, gradient=0.007248733676602234\n",
      "Gradient Descent (6570/9999): loss=0.45217693883354537, gradient=0.007247674680118916\n",
      "Gradient Descent (6575/9999): loss=0.45217504090074156, gradient=0.007246616387709995\n",
      "Gradient Descent (6580/9999): loss=0.4521731436459889, gradient=0.007245558798904199\n",
      "Gradient Descent (6585/9999): loss=0.45217124706873785, gradient=0.007244501913227266\n",
      "Gradient Descent (6590/9999): loss=0.45216935116843954, gradient=0.007243445730201996\n",
      "Gradient Descent (6595/9999): loss=0.452167455944546, gradient=0.0072423902493482135\n",
      "Gradient Descent (6600/9999): loss=0.45216556139650865, gradient=0.007241335470182849\n",
      "Gradient Descent (6605/9999): loss=0.4521636675237809, gradient=0.007240281392219883\n",
      "Gradient Descent (6610/9999): loss=0.4521617743258151, gradient=0.007239228014970509\n",
      "Gradient Descent (6615/9999): loss=0.45215988180206484, gradient=0.0072381753379430255\n",
      "Gradient Descent (6620/9999): loss=0.45215798995198414, gradient=0.007237123360642937\n",
      "Gradient Descent (6625/9999): loss=0.45215609877502727, gradient=0.007236072082572956\n",
      "Gradient Descent (6630/9999): loss=0.4521542082706487, gradient=0.00723502150323303\n",
      "Gradient Descent (6635/9999): loss=0.452152318438304, gradient=0.0072339716221203835\n",
      "Gradient Descent (6640/9999): loss=0.4521504292774487, gradient=0.00723292243872953\n",
      "Gradient Descent (6645/9999): loss=0.452148540787539, gradient=0.007231873952552313\n",
      "Gradient Descent (6650/9999): loss=0.45214665296803136, gradient=0.007230826163077899\n",
      "Gradient Descent (6655/9999): loss=0.4521447658183831, gradient=0.007229779069792825\n",
      "Gradient Descent (6660/9999): loss=0.4521428793380514, gradient=0.0072287326721810615\n",
      "Gradient Descent (6665/9999): loss=0.4521409935264942, gradient=0.007227686969723965\n",
      "Gradient Descent (6670/9999): loss=0.45213910838317045, gradient=0.00722664196190037\n",
      "Gradient Descent (6675/9999): loss=0.45213722390753847, gradient=0.007225597648186578\n",
      "Gradient Descent (6680/9999): loss=0.4521353400990581, gradient=0.007224554028056384\n",
      "Gradient Descent (6685/9999): loss=0.4521334569571887, gradient=0.007223511100981134\n",
      "Gradient Descent (6690/9999): loss=0.45213157448139124, gradient=0.007222468866429709\n",
      "Gradient Descent (6695/9999): loss=0.45212969267112607, gradient=0.007221427323868591\n",
      "Gradient Descent (6700/9999): loss=0.4521278115258545, gradient=0.007220386472761846\n",
      "Gradient Descent (6705/9999): loss=0.45212593104503856, gradient=0.0072193463125712\n",
      "Gradient Descent (6710/9999): loss=0.45212405122814014, gradient=0.007218306842756011\n",
      "Gradient Descent (6715/9999): loss=0.4521221720746224, gradient=0.007217268062773336\n",
      "Gradient Descent (6720/9999): loss=0.45212029358394806, gradient=0.007216229972077941\n",
      "Gradient Descent (6725/9999): loss=0.4521184157555814, gradient=0.007215192570122345\n",
      "Gradient Descent (6730/9999): loss=0.4521165385889861, gradient=0.007214155856356799\n",
      "Gradient Descent (6735/9999): loss=0.4521146620836273, gradient=0.007213119830229361\n",
      "Gradient Descent (6740/9999): loss=0.45211278623897, gradient=0.007212084491185892\n",
      "Gradient Descent (6745/9999): loss=0.45211091105447976, gradient=0.007211049838670075\n",
      "Gradient Descent (6750/9999): loss=0.45210903652962287, gradient=0.007210015872123488\n",
      "Gradient Descent (6755/9999): loss=0.45210716266386597, gradient=0.007208982590985582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent (6760/9999): loss=0.45210528945667633, gradient=0.007207949994693721\n",
      "Gradient Descent (6765/9999): loss=0.4521034169075218, gradient=0.0072069180826832184\n",
      "Gradient Descent (6770/9999): loss=0.4521015450158701, gradient=0.007205886854387314\n",
      "Gradient Descent (6775/9999): loss=0.4520996737811903, gradient=0.007204856309237278\n",
      "Gradient Descent (6780/9999): loss=0.4520978032029514, gradient=0.0072038264466623425\n",
      "Gradient Descent (6785/9999): loss=0.45209593328062325, gradient=0.007202797266089835\n",
      "Gradient Descent (6790/9999): loss=0.4520940640136761, gradient=0.007201768766945083\n",
      "Gradient Descent (6795/9999): loss=0.4520921954015805, gradient=0.007200740948651589\n",
      "Gradient Descent (6800/9999): loss=0.45209032744380795, gradient=0.007199713810630829\n",
      "Gradient Descent (6805/9999): loss=0.45208846013983006, gradient=0.007198687352302525\n",
      "Gradient Descent (6810/9999): loss=0.452086593489119, gradient=0.0071976615730845045\n",
      "Gradient Descent (6815/9999): loss=0.4520847274911478, gradient=0.007196636472392795\n",
      "Gradient Descent (6820/9999): loss=0.4520828621453897, gradient=0.007195612049641616\n",
      "Gradient Descent (6825/9999): loss=0.4520809974513185, gradient=0.00719458830424341\n",
      "Gradient Descent (6830/9999): loss=0.4520791334084087, gradient=0.007193565235608901\n",
      "Gradient Descent (6835/9999): loss=0.45207727001613496, gradient=0.007192542843147034\n",
      "Gradient Descent (6840/9999): loss=0.45207540727397294, gradient=0.00719152112626511\n",
      "Gradient Descent (6845/9999): loss=0.4520735451813985, gradient=0.007190500084368704\n",
      "Gradient Descent (6850/9999): loss=0.45207168373788786, gradient=0.007189479716861774\n",
      "Gradient Descent (6855/9999): loss=0.4520698229429184, gradient=0.007188460023146615\n",
      "Gradient Descent (6860/9999): loss=0.45206796279596745, gradient=0.007187441002623897\n",
      "Gradient Descent (6865/9999): loss=0.45206610329651303, gradient=0.007186422654692775\n",
      "Gradient Descent (6870/9999): loss=0.45206424444403404, gradient=0.007185404978750729\n",
      "Gradient Descent (6875/9999): loss=0.45206238623800915, gradient=0.007184387974193804\n",
      "Gradient Descent (6880/9999): loss=0.45206052867791835, gradient=0.007183371640416456\n",
      "Gradient Descent (6885/9999): loss=0.4520586717632419, gradient=0.0071823559768116624\n",
      "Gradient Descent (6890/9999): loss=0.4520568154934603, gradient=0.007181340982770924\n",
      "Gradient Descent (6895/9999): loss=0.45205495986805483, gradient=0.007180326657684277\n",
      "Gradient Descent (6900/9999): loss=0.45205310488650763, gradient=0.007179313000940346\n",
      "Gradient Descent (6905/9999): loss=0.4520512505483008, gradient=0.007178300011926309\n",
      "Gradient Descent (6910/9999): loss=0.4520493968529173, gradient=0.007177287690027999\n",
      "Gradient Descent (6915/9999): loss=0.4520475437998404, gradient=0.007176276034629838\n",
      "Gradient Descent (6920/9999): loss=0.4520456913885547, gradient=0.0071752650451149275\n",
      "Gradient Descent (6925/9999): loss=0.4520438396185442, gradient=0.007174254720865015\n",
      "Gradient Descent (6930/9999): loss=0.45204198848929406, gradient=0.007173245061260586\n",
      "Gradient Descent (6935/9999): loss=0.4520401380002902, gradient=0.00717223606568078\n",
      "Gradient Descent (6940/9999): loss=0.45203828815101854, gradient=0.007171227733503532\n",
      "Gradient Descent (6945/9999): loss=0.4520364389409659, gradient=0.007170220064105513\n",
      "Gradient Descent (6950/9999): loss=0.4520345903696196, gradient=0.00716921305686214\n",
      "Gradient Descent (6955/9999): loss=0.4520327424364675, gradient=0.007168206711147687\n",
      "Gradient Descent (6960/9999): loss=0.45203089514099787, gradient=0.007167201026335179\n",
      "Gradient Descent (6965/9999): loss=0.4520290484827, gradient=0.007166196001796523\n",
      "Gradient Descent (6970/9999): loss=0.4520272024610629, gradient=0.007165191636902468\n",
      "Gradient Descent (6975/9999): loss=0.4520253570755772, gradient=0.007164187931022632\n",
      "Gradient Descent (6980/9999): loss=0.452023512325733, gradient=0.007163184883525567\n",
      "Gradient Descent (6985/9999): loss=0.45202166821102197, gradient=0.007162182493778685\n",
      "Gradient Descent (6990/9999): loss=0.45201982473093555, gradient=0.007161180761148376\n",
      "Gradient Descent (6995/9999): loss=0.4520179818849661, gradient=0.007160179684999958\n",
      "Gradient Descent (7000/9999): loss=0.45201613967260657, gradient=0.007159179264697766\n",
      "Gradient Descent (7000/9999): loss=0.45201613967260657, gradient=0.007159179264697766, w=[-1.10081689e+00  1.03977050e+00 -8.00458751e-01 -1.43986492e-01\n",
      "  5.15137845e-01 -4.65138954e-02  1.42497063e+00 -1.42935499e-01\n",
      "  7.92413664e-02 -4.56509167e-01  1.87882869e-01  7.90040462e-02\n",
      "  1.07273217e+00 -5.33807305e-04  4.54614052e-02  5.63528345e-01\n",
      "  1.27731342e-02  6.78801187e-02 -1.37728855e-01  3.13889060e-02\n",
      "  9.02026834e-02 -1.08141681e-01  5.01043861e-01  5.46594939e-03\n",
      "  3.95977848e-02  2.01226529e-01  2.26700761e-02  1.82035639e-02\n",
      " -5.31607666e-03  2.72569062e-02 -9.99657891e-01  1.28602844e-01\n",
      " -1.45416884e+00  1.33636852e-03  2.01093016e-02  3.85314291e-01\n",
      "  9.45402892e-01 -2.72271517e-01  9.45339425e-02  1.35274383e-01\n",
      "  6.78048689e-02 -5.27796744e-01 -7.62544470e-02  2.70615715e-02\n",
      "  1.08427798e-01 -1.67138527e-01  4.26292767e-02  3.07227605e-01\n",
      "  4.28255426e-02 -2.48203333e-01 -1.22706745e-01 -2.82653843e-01\n",
      "  3.84706462e-01  7.45477163e-02 -9.52430122e-02  2.20561793e-01\n",
      " -1.17367878e-02 -1.09153303e-01  2.72569062e-02 -6.83057243e-01\n",
      "  3.83716538e-01 -5.29756421e-01  7.16753897e-01  1.44628967e-01\n",
      " -1.67932924e-01  9.99847941e-01 -8.73238458e-02  2.03453960e-02\n",
      "  4.81868786e-02  3.72364469e-02 -2.73567881e-01 -2.36613648e-02\n",
      " -4.08078177e-02 -9.91262965e-02 -2.74733589e-02 -5.92900964e-02\n",
      "  8.93929954e-01 -4.96909652e-02 -1.05905515e-01 -7.87407559e-02\n",
      " -2.80183382e-01 -3.71523191e-02 -5.00807704e-02 -1.66269324e-01\n",
      " -4.83910929e-02 -3.41438903e-02  5.90115756e-02  2.72569062e-02\n",
      " -4.24486727e-03  2.92875217e-02  7.08940990e-02  9.53484283e-01\n",
      "  3.89440969e-01 -3.83194960e-01  1.00346952e+00  7.90869485e-02\n",
      " -6.44577671e-02  1.06502961e-02  1.88821508e-02  2.09325223e-01\n",
      " -7.14806201e-02 -2.79352842e-02 -1.10723560e-01 -6.59746950e-02\n",
      " -3.14020991e-02  9.86166884e-01 -2.30520160e-02  5.10202020e-02\n",
      " -3.21388076e-02 -5.64612693e-02  2.41784190e-02 -1.86411649e-02\n",
      " -8.02498414e-02  7.22241799e-03  2.48849038e-03  3.77315106e-02\n",
      "  2.72569062e-02  3.46915469e-01 -1.42746924e-01  2.22930583e-01\n",
      "  9.94042819e-01  6.01529406e-01 -3.96198572e-01  1.00409483e+00\n",
      "  8.91845930e-02 -2.03984527e-03 -6.74302304e-02  9.92960704e-03\n",
      "  3.72727968e-01  2.55660215e-02  5.49856673e-03 -6.08301033e-02\n",
      "  1.12555421e-02  4.63225916e-03  1.00047008e+00  2.10314944e-02\n",
      "  6.37376802e-02  7.24969502e-03  1.33640543e-01  3.85222622e-02\n",
      "  2.58114523e-02  3.37399885e-02  3.31949579e-02  1.98817132e-02\n",
      " -3.46831888e-02  2.72569062e-02  4.82605170e-01 -1.36189312e-01\n",
      "  2.65798784e-01  1.00169803e+00  7.39440931e-01 -3.06714137e-01\n",
      "  1.00418710e+00  5.08158420e-02  1.03983870e-01 -4.68710620e-02\n",
      "  7.21228621e-03  4.21572918e-01  2.50392840e-02  9.33958602e-03\n",
      "  6.91087430e-03  4.14576090e-02  2.69161342e-03  1.00323852e+00\n",
      " -1.32734657e-02  3.07734176e-02  3.74454686e-02  2.56461942e-01\n",
      " -1.03077175e-01 -2.77484139e-02  1.32657541e-01 -6.31841883e-02\n",
      "  1.06934099e-02 -6.54635390e-02  2.72569062e-02]\n",
      "Gradient Descent (7005/9999): loss=0.4520142980933502, gradient=0.007158179499605045\n",
      "Gradient Descent (7010/9999): loss=0.4520124571466912, gradient=0.007157180389084142\n",
      "Gradient Descent (7015/9999): loss=0.45201061683212407, gradient=0.007156181932496378\n",
      "Gradient Descent (7020/9999): loss=0.4520087771491438, gradient=0.007155184129202164\n",
      "Gradient Descent (7025/9999): loss=0.4520069380972461, gradient=0.007154186978560921\n",
      "Gradient Descent (7030/9999): loss=0.4520050996759271, gradient=0.00715319047993119\n",
      "Gradient Descent (7035/9999): loss=0.4520032618846841, gradient=0.007152194632670635\n",
      "Gradient Descent (7040/9999): loss=0.45200142472301413, gradient=0.007151199436136019\n",
      "Gradient Descent (7045/9999): loss=0.45199958819041514, gradient=0.007150204889683221\n",
      "Gradient Descent (7050/9999): loss=0.45199775228638556, gradient=0.007149210992667317\n",
      "Gradient Descent (7055/9999): loss=0.45199591701042474, gradient=0.00714821774444255\n",
      "Gradient Descent (7060/9999): loss=0.45199408236203215, gradient=0.007147225144362337\n",
      "Gradient Descent (7065/9999): loss=0.451992248340708, gradient=0.0071462331917793295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent (7070/9999): loss=0.4519904149459532, gradient=0.007145241886045389\n",
      "Gradient Descent (7075/9999): loss=0.4519885821772692, gradient=0.007144251226511599\n",
      "Gradient Descent (7080/9999): loss=0.45198675003415756, gradient=0.007143261212528367\n",
      "Gradient Descent (7085/9999): loss=0.4519849185161211, gradient=0.007142271843445318\n",
      "Gradient Descent (7090/9999): loss=0.451983087622663, gradient=0.0071412831186114046\n",
      "Gradient Descent (7095/9999): loss=0.4519812573532865, gradient=0.007140295037374882\n",
      "Gradient Descent (7100/9999): loss=0.4519794277074963, gradient=0.0071393075990833265\n",
      "Gradient Descent (7105/9999): loss=0.45197759868479703, gradient=0.007138320803083682\n",
      "Gradient Descent (7110/9999): loss=0.451975770284694, gradient=0.007137334648722212\n",
      "Gradient Descent (7115/9999): loss=0.4519739425066932, gradient=0.007136349135344604\n",
      "Gradient Descent (7120/9999): loss=0.4519721153503013, gradient=0.007135364262295894\n",
      "Gradient Descent (7125/9999): loss=0.45197028881502527, gradient=0.007134380028920557\n",
      "Gradient Descent (7130/9999): loss=0.4519684629003727, gradient=0.007133396434562491\n",
      "Gradient Descent (7135/9999): loss=0.45196663760585215, gradient=0.00713241347856501\n",
      "Gradient Descent (7140/9999): loss=0.4519648129309722, gradient=0.007131431160270898\n",
      "Gradient Descent (7145/9999): loss=0.4519629888752425, gradient=0.007130449479022421\n",
      "Gradient Descent (7150/9999): loss=0.4519611654381729, gradient=0.007129468434161309\n",
      "Gradient Descent (7155/9999): loss=0.45195934261927423, gradient=0.007128488025028792\n",
      "Gradient Descent (7160/9999): loss=0.45195752041805715, gradient=0.007127508250965663\n",
      "Gradient Descent (7165/9999): loss=0.4519556988340338, gradient=0.007126529111312185\n",
      "Gradient Descent (7170/9999): loss=0.4519538778667166, gradient=0.007125550605408187\n",
      "Gradient Descent (7175/9999): loss=0.451952057515618, gradient=0.007124572732593054\n",
      "Gradient Descent (7180/9999): loss=0.45195023778025206, gradient=0.0071235954922057925\n",
      "Gradient Descent (7185/9999): loss=0.4519484186601322, gradient=0.007122618883584915\n",
      "Gradient Descent (7190/9999): loss=0.4519466001547737, gradient=0.007121642906068615\n",
      "Gradient Descent (7195/9999): loss=0.4519447822636916, gradient=0.007120667558994633\n",
      "Gradient Descent (7200/9999): loss=0.4519429649864013, gradient=0.007119692841700409\n",
      "Gradient Descent (7205/9999): loss=0.4519411483224196, gradient=0.007118718753522986\n",
      "Gradient Descent (7210/9999): loss=0.4519393322712636, gradient=0.007117745293799062\n",
      "Gradient Descent (7215/9999): loss=0.45193751683245054, gradient=0.007116772461865041\n",
      "Gradient Descent (7220/9999): loss=0.45193570200549854, gradient=0.0071158002570569905\n",
      "Gradient Descent (7225/9999): loss=0.4519338877899266, gradient=0.0071148286787106555\n",
      "Gradient Descent (7230/9999): loss=0.45193207418525405, gradient=0.007113857726161555\n",
      "Gradient Descent (7235/9999): loss=0.4519302611910004, gradient=0.007112887398744884\n",
      "Gradient Descent (7240/9999): loss=0.45192844880668664, gradient=0.007111917695795575\n",
      "Gradient Descent (7245/9999): loss=0.45192663703183333, gradient=0.00711094861664836\n",
      "Gradient Descent (7250/9999): loss=0.45192482586596244, gradient=0.007109980160637687\n",
      "Gradient Descent (7255/9999): loss=0.45192301530859613, gradient=0.007109012327097802\n",
      "Gradient Descent (7260/9999): loss=0.45192120535925717, gradient=0.007108045115362723\n",
      "Gradient Descent (7265/9999): loss=0.4519193960174693, gradient=0.007107078524766316\n",
      "Gradient Descent (7270/9999): loss=0.4519175872827558, gradient=0.007106112554642223\n",
      "Gradient Descent (7275/9999): loss=0.45191577915464176, gradient=0.007105147204323924\n",
      "Gradient Descent (7280/9999): loss=0.4519139716326524, gradient=0.007104182473144729\n",
      "Gradient Descent (7285/9999): loss=0.45191216471631324, gradient=0.0071032183604378225\n",
      "Gradient Descent (7290/9999): loss=0.4519103584051507, gradient=0.00710225486553624\n",
      "Gradient Descent (7295/9999): loss=0.45190855269869146, gradient=0.007101291987772903\n",
      "Gradient Descent (7300/9999): loss=0.45190674759646343, gradient=0.0071003297264805865\n",
      "Gradient Descent (7305/9999): loss=0.4519049430979943, gradient=0.007099368080992019\n",
      "Gradient Descent (7310/9999): loss=0.451903139202813, gradient=0.0070984070506398015\n",
      "Gradient Descent (7315/9999): loss=0.4519013359104486, gradient=0.007097446634756459\n",
      "Gradient Descent (7320/9999): loss=0.4518995332204311, gradient=0.007096486832674466\n",
      "Gradient Descent (7325/9999): loss=0.451897731132291, gradient=0.007095527643726249\n",
      "Gradient Descent (7330/9999): loss=0.45189592964555897, gradient=0.007094569067244155\n",
      "Gradient Descent (7335/9999): loss=0.451894128759767, gradient=0.0070936111025605335\n",
      "Gradient Descent (7340/9999): loss=0.4518923284744469, gradient=0.0070926537490076935\n",
      "Gradient Descent (7345/9999): loss=0.45189052878913183, gradient=0.00709169700591795\n",
      "Gradient Descent (7350/9999): loss=0.45188872970335464, gradient=0.007090740872623602\n",
      "Gradient Descent (7355/9999): loss=0.4518869312166498, gradient=0.007089785348456954\n",
      "Gradient Descent (7360/9999): loss=0.45188513332855174, gradient=0.007088830432750346\n",
      "Gradient Descent (7365/9999): loss=0.45188333603859504, gradient=0.00708787612483618\n",
      "Gradient Descent (7370/9999): loss=0.451881539346316, gradient=0.007086922424046813\n",
      "Gradient Descent (7375/9999): loss=0.4518797432512505, gradient=0.00708596932971474\n",
      "Gradient Descent (7380/9999): loss=0.4518779477529357, gradient=0.00708501684117248\n",
      "Gradient Descent (7385/9999): loss=0.4518761528509089, gradient=0.007084064957752631\n",
      "Gradient Descent (7390/9999): loss=0.45187435854470803, gradient=0.007083113678787871\n",
      "Gradient Descent (7395/9999): loss=0.451872564833872, gradient=0.007082163003610972\n",
      "Gradient Descent (7400/9999): loss=0.4518707717179396, gradient=0.007081212931554816\n",
      "Gradient Descent (7405/9999): loss=0.4518689791964511, gradient=0.007080263461952403\n",
      "Gradient Descent (7410/9999): loss=0.4518671872689465, gradient=0.007079314594136803\n",
      "Gradient Descent (7415/9999): loss=0.4518653959349666, gradient=0.007078366327441289\n",
      "Gradient Descent (7420/9999): loss=0.4518636051940534, gradient=0.007077418661199233\n",
      "Gradient Descent (7425/9999): loss=0.45186181504574885, gradient=0.007076471594744161\n",
      "Gradient Descent (7430/9999): loss=0.4518600254895955, gradient=0.0070755251274097685\n",
      "Gradient Descent (7435/9999): loss=0.45185823652513685, gradient=0.007074579258529925\n",
      "Gradient Descent (7440/9999): loss=0.45185644815191656, gradient=0.007073633987438654\n",
      "Gradient Descent (7445/9999): loss=0.45185466036947913, gradient=0.007072689313470195\n",
      "Gradient Descent (7450/9999): loss=0.4518528731773697, gradient=0.007071745235958945\n",
      "Gradient Descent (7455/9999): loss=0.4518510865751337, gradient=0.007070801754239532\n",
      "Gradient Descent (7460/9999): loss=0.45184930056231754, gradient=0.007069858867646804\n",
      "Gradient Descent (7465/9999): loss=0.4518475151384679, gradient=0.007068916575515807\n",
      "Gradient Descent (7470/9999): loss=0.451845730303132, gradient=0.007067974877181838\n",
      "Gradient Descent (7475/9999): loss=0.45184394605585804, gradient=0.007067033771980416\n",
      "Gradient Descent (7480/9999): loss=0.45184216239619435, gradient=0.007066093259247324\n",
      "Gradient Descent (7485/9999): loss=0.4518403793236899, gradient=0.00706515333831859\n",
      "Gradient Descent (7490/9999): loss=0.4518385968378947, gradient=0.007064214008530505\n",
      "Gradient Descent (7495/9999): loss=0.4518368149383589, gradient=0.007063275269219642\n",
      "Gradient Descent (7500/9999): loss=0.4518350336246333, gradient=0.007062337119722825\n",
      "Gradient Descent (7505/9999): loss=0.45183325289626924, gradient=0.007061399559377209\n",
      "Gradient Descent (7510/9999): loss=0.45183147275281865, gradient=0.007060462587520216\n",
      "Gradient Descent (7515/9999): loss=0.4518296931938346, gradient=0.007059526203489573\n",
      "Gradient Descent (7520/9999): loss=0.4518279142188696, gradient=0.0070585904066233305\n",
      "Gradient Descent (7525/9999): loss=0.45182613582747777, gradient=0.007057655196259865\n",
      "Gradient Descent (7530/9999): loss=0.4518243580192132, gradient=0.007056720571737848\n",
      "Gradient Descent (7535/9999): loss=0.4518225807936308, gradient=0.007055786532396312\n",
      "Gradient Descent (7540/9999): loss=0.45182080415028636, gradient=0.007054853077574614\n",
      "Gradient Descent (7545/9999): loss=0.45181902808873553, gradient=0.00705392020661248\n",
      "Gradient Descent (7550/9999): loss=0.4518172526085353, gradient=0.007052987918849993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent (7555/9999): loss=0.4518154777092422, gradient=0.0070520562136275645\n",
      "Gradient Descent (7560/9999): loss=0.45181370339041477, gradient=0.007051125090286013\n",
      "Gradient Descent (7565/9999): loss=0.45181192965161077, gradient=0.007050194548166519\n",
      "Gradient Descent (7570/9999): loss=0.45181015649238954, gradient=0.007049264586610668\n",
      "Gradient Descent (7575/9999): loss=0.4518083839123103, gradient=0.0070483352049604065\n",
      "Gradient Descent (7580/9999): loss=0.451806611910933, gradient=0.007047406402558098\n",
      "Gradient Descent (7585/9999): loss=0.4518048404878187, gradient=0.007046478178746498\n",
      "Gradient Descent (7590/9999): loss=0.4518030696425283, gradient=0.0070455505328688006\n",
      "Gradient Descent (7595/9999): loss=0.4518012993746237, gradient=0.007044623464268591\n",
      "Gradient Descent (7600/9999): loss=0.45179952968366716, gradient=0.007043696972289906\n",
      "Gradient Descent (7605/9999): loss=0.4517977605692215, gradient=0.007042771056277179\n",
      "Gradient Descent (7610/9999): loss=0.45179599203085063, gradient=0.007041845715575303\n",
      "Gradient Descent (7615/9999): loss=0.4517942240681182, gradient=0.007040920949529631\n",
      "Gradient Descent (7620/9999): loss=0.4517924566805889, gradient=0.007039996757485939\n",
      "Gradient Descent (7625/9999): loss=0.4517906898678282, gradient=0.007039073138790463\n",
      "Gradient Descent (7630/9999): loss=0.4517889236294017, gradient=0.007038150092789946\n",
      "Gradient Descent (7635/9999): loss=0.45178715796487573, gradient=0.007037227618831529\n",
      "Gradient Descent (7640/9999): loss=0.45178539287381714, gradient=0.007036305716262888\n",
      "Gradient Descent (7645/9999): loss=0.45178362835579333, gradient=0.007035384384432126\n",
      "Gradient Descent (7650/9999): loss=0.45178186441037277, gradient=0.007034463622687905\n",
      "Gradient Descent (7655/9999): loss=0.4517801010371236, gradient=0.007033543430379326\n",
      "Gradient Descent (7660/9999): loss=0.4517783382356153, gradient=0.007032623806855992\n",
      "Gradient Descent (7665/9999): loss=0.45177657600541743, gradient=0.007031704751468027\n",
      "Gradient Descent (7670/9999): loss=0.45177481434610056, gradient=0.007030786263566058\n",
      "Gradient Descent (7675/9999): loss=0.45177305325723527, gradient=0.007029868342501243\n",
      "Gradient Descent (7680/9999): loss=0.451771292738393, gradient=0.007028950987625229\n",
      "Gradient Descent (7685/9999): loss=0.4517695327891461, gradient=0.007028034198290217\n",
      "Gradient Descent (7690/9999): loss=0.4517677734090667, gradient=0.007027117973848927\n",
      "Gradient Descent (7695/9999): loss=0.4517660145977284, gradient=0.007026202313654658\n",
      "Gradient Descent (7700/9999): loss=0.45176425635470424, gradient=0.007025287217061151\n",
      "Gradient Descent (7705/9999): loss=0.4517624986795689, gradient=0.007024372683422819\n",
      "Gradient Descent (7710/9999): loss=0.45176074157189733, gradient=0.007023458712094536\n",
      "Gradient Descent (7715/9999): loss=0.45175898503126466, gradient=0.007022545302431759\n",
      "Gradient Descent (7720/9999): loss=0.4517572290572469, gradient=0.00702163245379056\n",
      "Gradient Descent (7725/9999): loss=0.4517554736494204, gradient=0.00702072016552748\n",
      "Gradient Descent (7730/9999): loss=0.45175371880736254, gradient=0.007019808436999733\n",
      "Gradient Descent (7735/9999): loss=0.4517519645306505, gradient=0.007018897267565044\n",
      "Gradient Descent (7740/9999): loss=0.4517502108188625, gradient=0.007017986656581755\n",
      "Gradient Descent (7745/9999): loss=0.4517484576715777, gradient=0.007017076603408771\n",
      "Gradient Descent (7750/9999): loss=0.4517467050883748, gradient=0.0070161671074056\n",
      "Gradient Descent (7755/9999): loss=0.4517449530688341, gradient=0.0070152581679323745\n",
      "Gradient Descent (7760/9999): loss=0.45174320161253567, gradient=0.007014349784349777\n",
      "Gradient Descent (7765/9999): loss=0.45174145071906086, gradient=0.00701344195601913\n",
      "Gradient Descent (7770/9999): loss=0.4517397003879905, gradient=0.007012534682302369\n",
      "Gradient Descent (7775/9999): loss=0.45173795061890726, gradient=0.007011627962562006\n",
      "Gradient Descent (7780/9999): loss=0.4517362014113935, gradient=0.007010721796161232\n",
      "Gradient Descent (7785/9999): loss=0.4517344527650323, gradient=0.007009816182463805\n",
      "Gradient Descent (7790/9999): loss=0.45173270467940735, gradient=0.007008911120834138\n",
      "Gradient Descent (7795/9999): loss=0.451730957154103, gradient=0.007008006610637286\n",
      "Gradient Descent (7800/9999): loss=0.45172921018870416, gradient=0.00700710265123892\n",
      "Gradient Descent (7805/9999): loss=0.4517274637827961, gradient=0.007006199242005351\n",
      "Gradient Descent (7810/9999): loss=0.4517257179359645, gradient=0.0070052963823035485\n",
      "Gradient Descent (7815/9999): loss=0.4517239726477962, gradient=0.007004394071501107\n",
      "Gradient Descent (7820/9999): loss=0.45172222791787797, gradient=0.0070034923089663135\n",
      "Gradient Descent (7825/9999): loss=0.45172048374579715, gradient=0.007002591094068072\n",
      "Gradient Descent (7830/9999): loss=0.45171874013114227, gradient=0.007001690426175951\n",
      "Gradient Descent (7835/9999): loss=0.45171699707350155, gradient=0.0070007903046602\n",
      "Gradient Descent (7840/9999): loss=0.4517152545724646, gradient=0.0069998907288917386\n",
      "Gradient Descent (7845/9999): loss=0.4517135126276207, gradient=0.006998991698242132\n",
      "Gradient Descent (7850/9999): loss=0.4517117712385605, gradient=0.006998093212083626\n",
      "Gradient Descent (7855/9999): loss=0.4517100304048744, gradient=0.0069971952697891636\n",
      "Gradient Descent (7860/9999): loss=0.45170829012615415, gradient=0.0069962978707323615\n",
      "Gradient Descent (7865/9999): loss=0.45170655040199137, gradient=0.006995401014287508\n",
      "Gradient Descent (7870/9999): loss=0.4517048112319787, gradient=0.006994504699829592\n",
      "Gradient Descent (7875/9999): loss=0.45170307261570886, gradient=0.006993608926734293\n",
      "Gradient Descent (7880/9999): loss=0.45170133455277567, gradient=0.006992713694377999\n",
      "Gradient Descent (7885/9999): loss=0.4516995970427728, gradient=0.006991819002137763\n",
      "Gradient Descent (7890/9999): loss=0.4516978600852953, gradient=0.006990924849391365\n",
      "Gradient Descent (7895/9999): loss=0.4516961236799381, gradient=0.006990031235517277\n",
      "Gradient Descent (7900/9999): loss=0.4516943878262964, gradient=0.006989138159894697\n",
      "Gradient Descent (7905/9999): loss=0.4516926525239671, gradient=0.006988245621903528\n",
      "Gradient Descent (7910/9999): loss=0.4516909177725466, gradient=0.006987353620924379\n",
      "Gradient Descent (7915/9999): loss=0.451689183571632, gradient=0.006986462156338572\n",
      "Gradient Descent (7920/9999): loss=0.4516874499208217, gradient=0.006985571227528162\n",
      "Gradient Descent (7925/9999): loss=0.45168571681971337, gradient=0.006984680833875912\n",
      "Gradient Descent (7930/9999): loss=0.45168398426790596, gradient=0.006983790974765342\n",
      "Gradient Descent (7935/9999): loss=0.4516822522649995, gradient=0.006982901649580673\n",
      "Gradient Descent (7940/9999): loss=0.451680520810593, gradient=0.006982012857706882\n",
      "Gradient Descent (7945/9999): loss=0.4516787899042875, gradient=0.0069811245985296295\n",
      "Gradient Descent (7950/9999): loss=0.45167705954568405, gradient=0.006980236871435376\n",
      "Gradient Descent (7955/9999): loss=0.4516753297343836, gradient=0.00697934967581128\n",
      "Gradient Descent (7960/9999): loss=0.45167360046998867, gradient=0.006978463011045275\n",
      "Gradient Descent (7965/9999): loss=0.4516717123945447, gradient=0.006977576876525988\n",
      "Gradient Descent (7970/9999): loss=0.4516695125011093, gradient=0.0069766912716428525\n",
      "Gradient Descent (7975/9999): loss=0.4516673131902026, gradient=0.006975806195786028\n",
      "Gradient Descent (7980/9999): loss=0.4516651144614055, gradient=0.006974921648346407\n",
      "Gradient Descent (7985/9999): loss=0.45166291631429917, gradient=0.006974037628715676\n",
      "Gradient Descent (7990/9999): loss=0.4516607187484666, gradient=0.006973154136286246\n",
      "Gradient Descent (7995/9999): loss=0.4516585217634904, gradient=0.006972271170451295\n",
      "Gradient Descent (8000/9999): loss=0.4516563253589537, gradient=0.006971388730604792\n",
      "Gradient Descent (8000/9999): loss=0.4516563253589537, gradient=0.006971388730604792, w=[-1.10570858e+00  1.05800706e+00 -8.07682216e-01 -1.39565102e-01\n",
      "  5.28531869e-01 -3.83746288e-02  1.43727187e+00 -1.43780460e-01\n",
      "  7.82650582e-02 -4.64085218e-01  1.84704740e-01  7.53272194e-02\n",
      "  1.07160870e+00 -6.32295943e-04  4.35198425e-02  5.65108713e-01\n",
      "  1.27143598e-02  6.55581121e-02 -1.37731913e-01  3.03011451e-02\n",
      "  9.26067600e-02 -1.08525256e-01  5.02521327e-01  5.01573838e-03\n",
      "  3.73512133e-02  1.95802285e-01  2.24301942e-02  1.73264228e-02\n",
      " -6.56745557e-03  2.74326456e-02 -1.02127875e+00  1.34779986e-01\n",
      " -1.46916299e+00 -9.26161119e-03  1.89318979e-02  3.79345271e-01\n",
      "  9.45028659e-01 -2.78259374e-01  1.00001998e-01  1.33585976e-01\n",
      "  6.98523304e-02 -5.31727221e-01 -7.60930769e-02  2.46003930e-02\n",
      "  1.05416372e-01 -1.67371436e-01  3.98406412e-02  2.99084344e-01\n",
      "  4.18296040e-02 -2.54269033e-01 -1.22583107e-01 -2.85386345e-01\n",
      "  3.85743312e-01  7.14255131e-02 -9.17218046e-02  2.21815833e-01\n",
      " -1.29530101e-02 -1.10505807e-01  2.74326456e-02 -6.91608982e-01\n",
      "  3.95221731e-01 -5.33861373e-01  7.11299212e-01  1.29790418e-01\n",
      " -1.76700637e-01  9.99591905e-01 -8.31939177e-02  2.39640854e-02\n",
      "  4.86880190e-02  3.89829817e-02 -2.65998072e-01 -2.34332399e-02\n",
      " -4.03878060e-02 -9.72264355e-02 -2.73518161e-02 -5.88121486e-02\n",
      "  8.91382627e-01 -4.92668853e-02 -1.05059464e-01 -7.84093132e-02\n",
      " -2.74668204e-01 -3.60332555e-02 -4.96887275e-02 -1.59761685e-01\n",
      " -4.78088702e-02 -3.40092597e-02  6.55053577e-02  2.74326456e-02\n",
      "  7.46760249e-04  2.84693343e-02  6.58690097e-02  9.52347558e-01\n",
      "  3.72457632e-01 -3.87111225e-01  1.00343318e+00  8.72708647e-02\n",
      " -6.50215662e-02  1.15959426e-02  1.94945648e-02  2.08600628e-01\n",
      " -7.20895365e-02 -2.80688972e-02 -1.07978656e-01 -6.60341242e-02\n",
      " -3.14627837e-02  9.85641303e-01 -2.27834153e-02  5.55598290e-02\n",
      " -3.17242829e-02 -5.25959610e-02  2.30829434e-02 -1.88818907e-02\n",
      " -7.71692691e-02  6.26723054e-03  2.64943399e-03  4.05522561e-02\n",
      "  2.74326456e-02  3.54858628e-01 -1.50189317e-01  2.13802274e-01\n",
      "  9.93784530e-01  5.87759723e-01 -3.93504378e-01  1.00409805e+00\n",
      "  9.22014414e-02 -3.71264544e-03 -6.70028372e-02  9.43234677e-03\n",
      "  3.62299504e-01  2.54226559e-02  6.48032237e-03 -6.21220427e-02\n",
      "  1.11723404e-02  5.89549621e-03  1.00035390e+00  2.14267106e-02\n",
      "  6.33413346e-02  7.69834345e-03  1.28195039e-01  3.77616878e-02\n",
      "  2.70040694e-02  3.37591619e-02  3.27742737e-02  2.03967418e-02\n",
      " -3.90300658e-02  2.74326456e-02  4.88875656e-01 -1.44751399e-01\n",
      "  2.54055487e-01  1.00163174e+00  7.29364084e-01 -2.98706086e-01\n",
      "  1.00419599e+00  4.63955545e-02  1.02604465e-01 -4.48351453e-02\n",
      "  5.77418163e-03  4.05978724e-01  2.55390813e-02  1.10022545e-02\n",
      "  6.27973441e-04  4.16268487e-02  4.45474555e-03  1.00321374e+00\n",
      " -1.29907212e-02  2.28552349e-02  3.79088289e-02  2.43571258e-01\n",
      " -1.02874674e-01 -2.59069839e-02  1.31167281e-01 -6.39286948e-02\n",
      "  1.10668259e-02 -7.49201186e-02  2.74326456e-02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent (8005/9999): loss=0.4516541295344407, gradient=0.00697050681614141\n",
      "Gradient Descent (8010/9999): loss=0.45165193428953576, gradient=0.006969625426456655\n",
      "Gradient Descent (8015/9999): loss=0.45164973962382376, gradient=0.006968744560946766\n",
      "Gradient Descent (8020/9999): loss=0.45164754553689035, gradient=0.006967864219008754\n",
      "Gradient Descent (8025/9999): loss=0.4516453520283215, gradient=0.0069669844000404095\n",
      "Gradient Descent (8030/9999): loss=0.4516431590977037, gradient=0.0069661051034402895\n",
      "Gradient Descent (8035/9999): loss=0.45164096674462445, gradient=0.006965226328607741\n",
      "Gradient Descent (8040/9999): loss=0.4516387749686709, gradient=0.006964348074942902\n",
      "Gradient Descent (8045/9999): loss=0.45163658376943167, gradient=0.006963470341846641\n",
      "Gradient Descent (8050/9999): loss=0.45163439314649556, gradient=0.006962593128720665\n",
      "Gradient Descent (8055/9999): loss=0.4516322030994515, gradient=0.006961716434967448\n",
      "Gradient Descent (8060/9999): loss=0.45163001362788946, gradient=0.006960840259990225\n",
      "Gradient Descent (8065/9999): loss=0.45162782473139973, gradient=0.006959964603193053\n",
      "Gradient Descent (8070/9999): loss=0.45162563640957315, gradient=0.006959089463980787\n",
      "Gradient Descent (8075/9999): loss=0.4516234486620011, gradient=0.006958214841759007\n",
      "Gradient Descent (8080/9999): loss=0.4516212614882758, gradient=0.006957340735934182\n",
      "Gradient Descent (8085/9999): loss=0.45161907488798964, gradient=0.006956467145913483\n",
      "Gradient Descent (8090/9999): loss=0.4516168888607353, gradient=0.0069555940711049655\n",
      "Gradient Descent (8095/9999): loss=0.4516147034061063, gradient=0.006954721510917424\n",
      "Gradient Descent (8100/9999): loss=0.451612518523697, gradient=0.006953849464760474\n",
      "Gradient Descent (8105/9999): loss=0.45161033421310165, gradient=0.006952977932044519\n",
      "Gradient Descent (8110/9999): loss=0.4516081504739156, gradient=0.006952106912180789\n",
      "Gradient Descent (8115/9999): loss=0.4516059673057341, gradient=0.0069512364045813055\n",
      "Gradient Descent (8120/9999): loss=0.4516037847081539, gradient=0.0069503664086589\n",
      "Gradient Descent (8125/9999): loss=0.4516016026807708, gradient=0.006949496923827203\n",
      "Gradient Descent (8130/9999): loss=0.45159942122318264, gradient=0.006948627949500664\n",
      "Gradient Descent (8135/9999): loss=0.45159724033498666, gradient=0.006947759485094541\n",
      "Gradient Descent (8140/9999): loss=0.4515950600157814, gradient=0.0069468915300248905\n",
      "Gradient Descent (8145/9999): loss=0.45159288026516553, gradient=0.006946024083708617\n",
      "Gradient Descent (8150/9999): loss=0.4515907010827381, gradient=0.0069451571455633855\n",
      "Gradient Descent (8155/9999): loss=0.4515885224680989, gradient=0.0069442907150077254\n",
      "Gradient Descent (8160/9999): loss=0.4515863444208481, gradient=0.006943424791460959\n",
      "Gradient Descent (8165/9999): loss=0.4515841669405872, gradient=0.006942559374343232\n",
      "Gradient Descent (8170/9999): loss=0.45158199002691624, gradient=0.006941694463075502\n",
      "Gradient Descent (8175/9999): loss=0.4515798136794382, gradient=0.006940830057079554\n",
      "Gradient Descent (8180/9999): loss=0.45157763789775474, gradient=0.0069399661557780065\n",
      "Gradient Descent (8185/9999): loss=0.4515754626814688, gradient=0.006939102758594263\n",
      "Gradient Descent (8190/9999): loss=0.4515732880301835, gradient=0.006938239864952593\n",
      "Gradient Descent (8195/9999): loss=0.45157111394350335, gradient=0.0069373774742780515\n",
      "Gradient Descent (8200/9999): loss=0.45156894042103224, gradient=0.006936515585996555\n",
      "Gradient Descent (8205/9999): loss=0.4515667674623749, gradient=0.0069356541995348134\n",
      "Gradient Descent (8210/9999): loss=0.45156459506713675, gradient=0.006934793314320382\n",
      "Gradient Descent (8215/9999): loss=0.4515624232349239, gradient=0.006933932929781632\n",
      "Gradient Descent (8220/9999): loss=0.45156025196534255, gradient=0.006933073045347777\n",
      "Gradient Descent (8225/9999): loss=0.4515580812579994, gradient=0.006932213660448862\n",
      "Gradient Descent (8230/9999): loss=0.4515559111125023, gradient=0.0069313547745157465\n",
      "Gradient Descent (8235/9999): loss=0.45155374152845856, gradient=0.006930496386980098\n",
      "Gradient Descent (8240/9999): loss=0.45155157250547684, gradient=0.006929638497274492\n",
      "Gradient Descent (8245/9999): loss=0.45154940404316596, gradient=0.00692878110483226\n",
      "Gradient Descent (8250/9999): loss=0.45154723614113546, gradient=0.006927924209087587\n",
      "Gradient Descent (8255/9999): loss=0.45154506879899486, gradient=0.006927067809475523\n",
      "Gradient Descent (8260/9999): loss=0.4515429020163547, gradient=0.006926211905431914\n",
      "Gradient Descent (8265/9999): loss=0.4515407357928259, gradient=0.006925356496393451\n",
      "Gradient Descent (8270/9999): loss=0.45153857012801973, gradient=0.006924501581797662\n",
      "Gradient Descent (8275/9999): loss=0.45153640502154807, gradient=0.006923647161082932\n",
      "Gradient Descent (8280/9999): loss=0.4515342404730231, gradient=0.006922793233688427\n",
      "Gradient Descent (8285/9999): loss=0.45153207648205784, gradient=0.006921939799054218\n",
      "Gradient Descent (8290/9999): loss=0.45152991304826573, gradient=0.006921086856621161\n",
      "Gradient Descent (8295/9999): loss=0.4515277501712602, gradient=0.006920234405830979\n",
      "Gradient Descent (8300/9999): loss=0.4515255878506556, gradient=0.006919382446126205\n",
      "Gradient Descent (8305/9999): loss=0.4515234260860671, gradient=0.0069185309769502325\n",
      "Gradient Descent (8310/9999): loss=0.45152126487710964, gradient=0.006917679997747298\n",
      "Gradient Descent (8315/9999): loss=0.45151910422339897, gradient=0.006916829507962454\n",
      "Gradient Descent (8320/9999): loss=0.4515169441245513, gradient=0.006915979507041605\n",
      "Gradient Descent (8325/9999): loss=0.4515147845801837, gradient=0.00691512999443152\n",
      "Gradient Descent (8330/9999): loss=0.451512625589913, gradient=0.006914280969579743\n",
      "Gradient Descent (8335/9999): loss=0.4515104671533569, gradient=0.006913432431934742\n",
      "Gradient Descent (8340/9999): loss=0.451508309270134, gradient=0.006912584380945741\n",
      "Gradient Descent (8345/9999): loss=0.4515061519398625, gradient=0.006911736816062844\n",
      "Gradient Descent (8350/9999): loss=0.4515039951621617, gradient=0.006910889736737035\n",
      "Gradient Descent (8355/9999): loss=0.45150183893665125, gradient=0.006910043142420046\n",
      "Gradient Descent (8360/9999): loss=0.4514996832629511, gradient=0.006909197032564557\n",
      "Gradient Descent (8365/9999): loss=0.4514975281406821, gradient=0.006908351406623993\n",
      "Gradient Descent (8370/9999): loss=0.4514953735694649, gradient=0.00690750626405268\n",
      "Gradient Descent (8375/9999): loss=0.45149321954892135, gradient=0.006906661604305769\n",
      "Gradient Descent (8380/9999): loss=0.45149106607867306, gradient=0.006905817426839218\n",
      "Gradient Descent (8385/9999): loss=0.45148891315834305, gradient=0.006904973731109919\n",
      "Gradient Descent (8390/9999): loss=0.45148676078755373, gradient=0.006904130516575478\n",
      "Gradient Descent (8395/9999): loss=0.45148460896592885, gradient=0.006903287782694451\n",
      "Gradient Descent (8400/9999): loss=0.4514824576930922, gradient=0.006902445528926168\n",
      "Gradient Descent (8405/9999): loss=0.45148030696866803, gradient=0.006901603754730832\n",
      "Gradient Descent (8410/9999): loss=0.4514781567922812, gradient=0.006900762459569468\n",
      "Gradient Descent (8415/9999): loss=0.4514760071635572, gradient=0.006899921642903961\n",
      "Gradient Descent (8420/9999): loss=0.45147385808212137, gradient=0.006899081304197032\n",
      "Gradient Descent (8425/9999): loss=0.45147170954760035, gradient=0.006898241442912219\n",
      "Gradient Descent (8430/9999): loss=0.45146956155962054, gradient=0.006897402058513945\n",
      "Gradient Descent (8435/9999): loss=0.45146741411780933, gradient=0.006896563150467405\n",
      "Gradient Descent (8440/9999): loss=0.4514652672217942, gradient=0.00689572471823871\n",
      "Gradient Descent (8445/9999): loss=0.4514631208712031, gradient=0.006894886761294766\n",
      "Gradient Descent (8450/9999): loss=0.4514609750656649, gradient=0.006894049279103322\n",
      "Gradient Descent (8455/9999): loss=0.4514588298048081, gradient=0.006893212271132962\n",
      "Gradient Descent (8460/9999): loss=0.45145668508826275, gradient=0.006892375736853139\n",
      "Gradient Descent (8465/9999): loss=0.45145454091565856, gradient=0.006891539675734102\n",
      "Gradient Descent (8470/9999): loss=0.45145239728662545, gradient=0.006890704087246965\n",
      "Gradient Descent (8475/9999): loss=0.45145025420079476, gradient=0.006889868970863683\n",
      "Gradient Descent (8480/9999): loss=0.4514481116577981, gradient=0.006889034326057041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent (8485/9999): loss=0.451445969657266, gradient=0.006888200152300647\n",
      "Gradient Descent (8490/9999): loss=0.45144382819883205, gradient=0.006887366449068953\n",
      "Gradient Descent (8495/9999): loss=0.45144168728212797, gradient=0.006886533215837257\n",
      "Gradient Descent (8500/9999): loss=0.45143954690678734, gradient=0.006885700452081696\n",
      "Gradient Descent (8505/9999): loss=0.45143740707244356, gradient=0.006884868157279233\n",
      "Gradient Descent (8510/9999): loss=0.45143526777873055, gradient=0.0068840363309076455\n",
      "Gradient Descent (8515/9999): loss=0.451433129025283, gradient=0.0068832049724455785\n",
      "Gradient Descent (8520/9999): loss=0.4514309908117355, gradient=0.006882374081372497\n",
      "Gradient Descent (8525/9999): loss=0.4514288531377237, gradient=0.0068815436571687165\n",
      "Gradient Descent (8530/9999): loss=0.45142671600288325, gradient=0.006880713699315328\n",
      "Gradient Descent (8535/9999): loss=0.45142457940685055, gradient=0.006879884207294342\n",
      "Gradient Descent (8540/9999): loss=0.45142244334926224, gradient=0.0068790551805885235\n",
      "Gradient Descent (8545/9999): loss=0.4514203078297553, gradient=0.006878226618681497\n",
      "Gradient Descent (8550/9999): loss=0.45141817284796754, gradient=0.0068773985210577445\n",
      "Gradient Descent (8555/9999): loss=0.45141603840353695, gradient=0.00687657088720253\n",
      "Gradient Descent (8560/9999): loss=0.4514139044961019, gradient=0.0068757437166019845\n",
      "Gradient Descent (8565/9999): loss=0.45141177112530145, gradient=0.006874917008743023\n",
      "Gradient Descent (8570/9999): loss=0.45140963829077474, gradient=0.0068740907631134685\n",
      "Gradient Descent (8575/9999): loss=0.45140750599216173, gradient=0.006873264979201869\n",
      "Gradient Descent (8580/9999): loss=0.45140537422910276, gradient=0.006872439656497681\n",
      "Gradient Descent (8585/9999): loss=0.451403243001238, gradient=0.006871614794491145\n",
      "Gradient Descent (8590/9999): loss=0.45140111230820945, gradient=0.006870790392673336\n",
      "Gradient Descent (8595/9999): loss=0.4513989821496579, gradient=0.006869966450536156\n",
      "Gradient Descent (8600/9999): loss=0.45139685252522566, gradient=0.00686914296757235\n",
      "Gradient Descent (8605/9999): loss=0.45139472343455495, gradient=0.006868319943275434\n",
      "Gradient Descent (8610/9999): loss=0.4513925948772887, gradient=0.006867497377139792\n",
      "Gradient Descent (8615/9999): loss=0.4513904668530704, gradient=0.006866675268660613\n",
      "Gradient Descent (8620/9999): loss=0.45138833936154354, gradient=0.006865853617333928\n",
      "Gradient Descent (8625/9999): loss=0.4513862124023524, gradient=0.006865032422656531\n",
      "Gradient Descent (8630/9999): loss=0.45138408597514135, gradient=0.006864211684126102\n",
      "Gradient Descent (8635/9999): loss=0.4513819600795555, gradient=0.0068633914012410984\n",
      "Gradient Descent (8640/9999): loss=0.4513798347152405, gradient=0.006862571573500806\n",
      "Gradient Descent (8645/9999): loss=0.45137770988184195, gradient=0.006861752200405328\n",
      "Gradient Descent (8650/9999): loss=0.45137558557900626, gradient=0.006860933281455591\n",
      "Gradient Descent (8655/9999): loss=0.4513734618063801, gradient=0.0068601148161532965\n",
      "Gradient Descent (8660/9999): loss=0.45137133856361056, gradient=0.006859296804001025\n",
      "Gradient Descent (8665/9999): loss=0.45136921585034545, gradient=0.006858479244502141\n",
      "Gradient Descent (8670/9999): loss=0.45136709366623246, gradient=0.006857662137160765\n",
      "Gradient Descent (8675/9999): loss=0.4513649720109203, gradient=0.006856845481481942\n",
      "Gradient Descent (8680/9999): loss=0.4513628508840574, gradient=0.006856029276971402\n",
      "Gradient Descent (8685/9999): loss=0.45136073028529344, gradient=0.006855213523135793\n",
      "Gradient Descent (8690/9999): loss=0.451358610214278, gradient=0.006854398219482511\n",
      "Gradient Descent (8695/9999): loss=0.451356490670661, gradient=0.006853583365519767\n",
      "Gradient Descent (8700/9999): loss=0.45135437165409287, gradient=0.006852768960756587\n",
      "Gradient Descent (8705/9999): loss=0.4513522531642249, gradient=0.006851955004702792\n",
      "Gradient Descent (8710/9999): loss=0.45135013520070827, gradient=0.006851141496869027\n",
      "Gradient Descent (8715/9999): loss=0.4513480177631948, gradient=0.006850328436766722\n",
      "Gradient Descent (8720/9999): loss=0.4513459008513366, gradient=0.006849515823908107\n",
      "Gradient Descent (8725/9999): loss=0.45134378446478624, gradient=0.0068487036578062455\n",
      "Gradient Descent (8730/9999): loss=0.4513416686031968, gradient=0.006847891937974942\n",
      "Gradient Descent (8735/9999): loss=0.4513395532662217, gradient=0.006847080663928859\n",
      "Gradient Descent (8740/9999): loss=0.4513374384535146, gradient=0.006846269835183422\n",
      "Gradient Descent (8745/9999): loss=0.45133532416473016, gradient=0.00684545945125487\n",
      "Gradient Descent (8750/9999): loss=0.45133321039952273, gradient=0.006844649511660228\n",
      "Gradient Descent (8755/9999): loss=0.4513310971575474, gradient=0.006843840015917311\n",
      "Gradient Descent (8760/9999): loss=0.4513289844384595, gradient=0.006843030963544768\n",
      "Gradient Descent (8765/9999): loss=0.4513268722419154, gradient=0.006842222354061985\n",
      "Gradient Descent (8770/9999): loss=0.4513247605675707, gradient=0.006841414186989148\n",
      "Gradient Descent (8775/9999): loss=0.4513226494150828, gradient=0.006840606461847281\n",
      "Gradient Descent (8780/9999): loss=0.4513205387841084, gradient=0.006839799178158163\n",
      "Gradient Descent (8785/9999): loss=0.45131842867430505, gradient=0.0068389923354443246\n",
      "Gradient Descent (8790/9999): loss=0.4513163190853307, gradient=0.006838185933229183\n",
      "Gradient Descent (8795/9999): loss=0.4513142100168437, gradient=0.0068373799710368345\n",
      "Gradient Descent (8800/9999): loss=0.4513121014685026, gradient=0.006836574448392212\n",
      "Gradient Descent (8805/9999): loss=0.45130999343996675, gradient=0.006835769364821055\n",
      "Gradient Descent (8810/9999): loss=0.45130788593089544, gradient=0.0068349647198498245\n",
      "Gradient Descent (8815/9999): loss=0.45130577894094887, gradient=0.0068341605130058025\n",
      "Gradient Descent (8820/9999): loss=0.4513036724697873, gradient=0.006833356743817039\n",
      "Gradient Descent (8825/9999): loss=0.45130156651707115, gradient=0.006832553411812379\n",
      "Gradient Descent (8830/9999): loss=0.4512994610824618, gradient=0.006831750516521444\n",
      "Gradient Descent (8835/9999): loss=0.45129735616562056, gradient=0.006830948057474591\n",
      "Gradient Descent (8840/9999): loss=0.45129525176620955, gradient=0.006830146034202994\n",
      "Gradient Descent (8845/9999): loss=0.4512931478838911, gradient=0.006829344446238601\n",
      "Gradient Descent (8850/9999): loss=0.45129104451832763, gradient=0.006828543293114105\n",
      "Gradient Descent (8855/9999): loss=0.4512889416691824, gradient=0.006827742574362983\n",
      "Gradient Descent (8860/9999): loss=0.45128683933611885, gradient=0.006826942289519501\n",
      "Gradient Descent (8865/9999): loss=0.45128473751880105, gradient=0.006826142438118661\n",
      "Gradient Descent (8870/9999): loss=0.451282636216893, gradient=0.006825343019696275\n",
      "Gradient Descent (8875/9999): loss=0.4512805354300595, gradient=0.006824544033788862\n",
      "Gradient Descent (8880/9999): loss=0.45127843515796556, gradient=0.0068237454799337655\n",
      "Gradient Descent (8885/9999): loss=0.4512763354002765, gradient=0.0068229473576690725\n",
      "Gradient Descent (8890/9999): loss=0.45127423615665835, gradient=0.006822149666533609\n",
      "Gradient Descent (8895/9999): loss=0.4512721374267773, gradient=0.006821352406066996\n",
      "Gradient Descent (8900/9999): loss=0.4512700392102997, gradient=0.006820555575809584\n",
      "Gradient Descent (8905/9999): loss=0.45126794150689264, gradient=0.006819759175302503\n",
      "Gradient Descent (8910/9999): loss=0.45126584431622363, gradient=0.006818963204087629\n",
      "Gradient Descent (8915/9999): loss=0.45126374763796034, gradient=0.006818167661707633\n",
      "Gradient Descent (8920/9999): loss=0.4512616514717707, gradient=0.006817372547705888\n",
      "Gradient Descent (8925/9999): loss=0.45125955581732363, gradient=0.006816577861626531\n",
      "Gradient Descent (8930/9999): loss=0.45125746067428785, gradient=0.006815783603014479\n",
      "Gradient Descent (8935/9999): loss=0.4512553660423325, gradient=0.006814989771415382\n",
      "Gradient Descent (8940/9999): loss=0.45125327192112746, gradient=0.0068141963663756085\n",
      "Gradient Descent (8945/9999): loss=0.45125117831034256, gradient=0.006813403387442369\n",
      "Gradient Descent (8950/9999): loss=0.45124908520964835, gradient=0.006812610834163494\n",
      "Gradient Descent (8955/9999): loss=0.45124699261871537, gradient=0.0068118187060876665\n",
      "Gradient Descent (8960/9999): loss=0.4512449005372154, gradient=0.006811027002764248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent (8965/9999): loss=0.4512428089648195, gradient=0.00681023572374338\n",
      "Gradient Descent (8970/9999): loss=0.4512407179011996, gradient=0.006809444868575931\n",
      "Gradient Descent (8975/9999): loss=0.4512386273460282, gradient=0.0068086544368135115\n",
      "Gradient Descent (8980/9999): loss=0.45123653729897767, gradient=0.006807864428008464\n",
      "Gradient Descent (8985/9999): loss=0.4512344477597216, gradient=0.006807074841713884\n",
      "Gradient Descent (8990/9999): loss=0.4512323587279328, gradient=0.0068062856774835714\n",
      "Gradient Descent (8995/9999): loss=0.45123027020328554, gradient=0.006805496934872119\n",
      "Gradient Descent (9000/9999): loss=0.45122818218545385, gradient=0.0068047086134348095\n",
      "Gradient Descent (9000/9999): loss=0.45122818218545385, gradient=0.0068047086134348095, w=[-1.11043876e+00  1.07585268e+00 -8.14136253e-01 -1.35081005e-01\n",
      "  5.41731542e-01 -3.04740529e-02  1.44926507e+00 -1.44539749e-01\n",
      "  7.73357283e-02 -4.71124190e-01  1.81463507e-01  7.16124590e-02\n",
      "  1.07021855e+00 -7.40559534e-04  4.17105016e-02  5.67207684e-01\n",
      "  1.26617995e-02  6.34006517e-02 -1.37805848e-01  2.92838924e-02\n",
      "  9.51457590e-02 -1.08877645e-01  5.03813372e-01  4.56726195e-03\n",
      "  3.52568248e-02  1.90811913e-01  2.21890613e-02  1.65146660e-02\n",
      " -7.81914443e-03  2.76332663e-02 -1.04276032e+00  1.39982641e-01\n",
      " -1.48375990e+00 -1.93578678e-02  1.82594486e-02  3.73329110e-01\n",
      "  9.44649327e-01 -2.84468720e-01  1.04838834e-01  1.31885227e-01\n",
      "  7.18870645e-02 -5.35446556e-01 -7.59142328e-02  2.23139582e-02\n",
      "  1.02248870e-01 -1.67569998e-01  3.72622291e-02  2.91254680e-01\n",
      "  4.09074491e-02 -2.60209531e-01 -1.22442420e-01 -2.88086263e-01\n",
      "  3.86607567e-01  6.84786223e-02 -8.83942786e-02  2.22726802e-01\n",
      " -1.41325949e-02 -1.12107800e-01  2.76332663e-02 -7.00064939e-01\n",
      "  4.05881883e-01 -5.37756941e-01  7.05985864e-01  1.15494680e-01\n",
      " -1.85376188e-01  9.99334293e-01 -7.95177126e-02  2.69463733e-02\n",
      "  4.91990390e-02  4.07291326e-02 -2.58229377e-01 -2.32052178e-02\n",
      " -3.99655643e-02 -9.56016757e-02 -2.72308070e-02 -5.83304543e-02\n",
      "  8.88883543e-01 -4.88398042e-02 -1.04231595e-01 -7.80696629e-02\n",
      " -2.68977994e-01 -3.49345740e-02 -4.92894224e-02 -1.53514947e-01\n",
      " -4.72326919e-02 -3.38723577e-02  7.17038651e-02  2.76332663e-02\n",
      "  5.66440918e-03  2.78082067e-02  6.09165563e-02  9.51232694e-01\n",
      "  3.55803206e-01 -3.90887520e-01  1.00339649e+00  9.50877676e-02\n",
      " -6.56826598e-02  1.25193427e-02  2.01173927e-02  2.07957474e-01\n",
      " -7.27008314e-02 -2.82003000e-02 -1.05245920e-01 -6.60966069e-02\n",
      " -3.15259223e-02  9.85121057e-01 -2.25202162e-02  6.00882521e-02\n",
      " -3.13069549e-02 -4.85104486e-02  2.19884026e-02 -1.91075858e-02\n",
      " -7.41673317e-02  5.30994210e-03  2.82017483e-03  4.33401007e-02\n",
      "  2.76332663e-02  3.62640910e-01 -1.56811406e-01  2.04708498e-01\n",
      "  9.93529524e-01  5.74143479e-01 -3.90676053e-01  1.00410096e+00\n",
      "  9.51008797e-02 -5.12362224e-03 -6.65157362e-02  8.95582718e-03\n",
      "  3.51903938e-01  2.52829565e-02  7.36794671e-03 -6.32322333e-02\n",
      "  1.10899796e-02  7.03382497e-03  1.00023806e+00  2.17652632e-02\n",
      "  6.30783480e-02  8.14640464e-03  1.22928165e-01  3.70192685e-02\n",
      "  2.80831123e-02  3.38120019e-02  3.23699151e-02  2.08647774e-02\n",
      " -4.31596190e-02  2.76332663e-02  4.94973760e-01 -1.52210326e-01\n",
      "  2.42334357e-01  1.00156578e+00  7.19349030e-01 -2.90592484e-01\n",
      "  1.00420455e+00  4.20755676e-02  1.01657834e-01 -4.28292091e-02\n",
      "  4.36662737e-03  3.90399622e-01  2.60217001e-02  1.25527999e-02\n",
      " -5.40879773e-03  4.17719871e-02  6.07772412e-03  1.00318873e+00\n",
      " -1.27540150e-02  1.52097408e-02  3.83694361e-02  2.30797491e-01\n",
      " -1.02570793e-01 -2.41738288e-02  1.29754090e-01 -6.44313496e-02\n",
      "  1.14290079e-02 -8.40196187e-02  2.76332663e-02]\n",
      "Gradient Descent (9005/9999): loss=0.4512260946741121, gradient=0.006803920712727634\n",
      "Gradient Descent (9010/9999): loss=0.45122400766893533, gradient=0.0068031332323073715\n",
      "Gradient Descent (9015/9999): loss=0.45122192116959875, gradient=0.006802346171731511\n",
      "Gradient Descent (9020/9999): loss=0.4512198351757782, gradient=0.00680155953055825\n",
      "Gradient Descent (9025/9999): loss=0.45121774968714956, gradient=0.006800773308346539\n",
      "Gradient Descent (9030/9999): loss=0.45121566470338903, gradient=0.006799987504656023\n",
      "Gradient Descent (9035/9999): loss=0.4512135802241736, gradient=0.006799202119047099\n",
      "Gradient Descent (9040/9999): loss=0.4512114962491804, gradient=0.006798417151080881\n",
      "Gradient Descent (9045/9999): loss=0.4512094127780868, gradient=0.006797632600319181\n",
      "Gradient Descent (9050/9999): loss=0.4512073298105707, gradient=0.006796848466324565\n",
      "Gradient Descent (9055/9999): loss=0.45120524734631, gradient=0.0067960647486602944\n",
      "Gradient Descent (9060/9999): loss=0.4512031653849839, gradient=0.006795281446890337\n",
      "Gradient Descent (9065/9999): loss=0.45120108392627084, gradient=0.006794498560579423\n",
      "Gradient Descent (9070/9999): loss=0.45119900296985027, gradient=0.006793716089292954\n",
      "Gradient Descent (9075/9999): loss=0.45119692251540167, gradient=0.0067929340325970505\n",
      "Gradient Descent (9080/9999): loss=0.4511948425626053, gradient=0.006792152390058556\n",
      "Gradient Descent (9085/9999): loss=0.4511927631111412, gradient=0.006791371161245006\n",
      "Gradient Descent (9090/9999): loss=0.4511906841606905, gradient=0.006790590345724678\n",
      "Gradient Descent (9095/9999): loss=0.45118860571093417, gradient=0.006789809943066518\n",
      "Gradient Descent (9100/9999): loss=0.45118652776155366, gradient=0.006789029952840215\n",
      "Gradient Descent (9105/9999): loss=0.45118445031223053, gradient=0.006788250374616107\n",
      "Gradient Descent (9110/9999): loss=0.45118237336264716, gradient=0.006787471207965287\n",
      "Gradient Descent (9115/9999): loss=0.45118029691248585, gradient=0.006786692452459543\n",
      "Gradient Descent (9120/9999): loss=0.4511782209614297, gradient=0.006785914107671343\n",
      "Gradient Descent (9125/9999): loss=0.451176145509162, gradient=0.006785136173173852\n",
      "Gradient Descent (9130/9999): loss=0.4511740705553662, gradient=0.006784358648540958\n",
      "Gradient Descent (9135/9999): loss=0.4511719960997261, gradient=0.0067835815333472235\n",
      "Gradient Descent (9140/9999): loss=0.4511699221419261, gradient=0.0067828048271679\n",
      "Gradient Descent (9145/9999): loss=0.4511678486816508, gradient=0.006782028529578957\n",
      "Gradient Descent (9150/9999): loss=0.451165775718585, gradient=0.006781252640157034\n",
      "Gradient Descent (9155/9999): loss=0.45116370325241434, gradient=0.006780477158479473\n",
      "Gradient Descent (9160/9999): loss=0.4511616312828244, gradient=0.006779702084124293\n",
      "Gradient Descent (9165/9999): loss=0.45115955980950107, gradient=0.006778927416670219\n",
      "Gradient Descent (9170/9999): loss=0.45115748883213086, gradient=0.006778153155696625\n",
      "Gradient Descent (9175/9999): loss=0.45115541835040035, gradient=0.0067773793007836\n",
      "Gradient Descent (9180/9999): loss=0.4511533483639969, gradient=0.0067766058515119415\n",
      "Gradient Descent (9185/9999): loss=0.45115127887260753, gradient=0.006775832807463062\n",
      "Gradient Descent (9190/9999): loss=0.4511492098759203, gradient=0.006775060168219088\n",
      "Gradient Descent (9195/9999): loss=0.45114714137362316, gradient=0.006774287933362856\n",
      "Gradient Descent (9200/9999): loss=0.45114507336540444, gradient=0.006773516102477816\n",
      "Gradient Descent (9205/9999): loss=0.45114300585095324, gradient=0.006772744675148143\n",
      "Gradient Descent (9210/9999): loss=0.45114093882995837, gradient=0.006771973650958673\n",
      "Gradient Descent (9215/9999): loss=0.4511388723021095, gradient=0.006771203029494885\n",
      "Gradient Descent (9220/9999): loss=0.4511368062670962, gradient=0.006770432810342998\n",
      "Gradient Descent (9225/9999): loss=0.4511347407246089, gradient=0.006769662993089839\n",
      "Gradient Descent (9230/9999): loss=0.45113267567433796, gradient=0.00676889357732293\n",
      "Gradient Descent (9235/9999): loss=0.45113061111597424, gradient=0.006768124562630433\n",
      "Gradient Descent (9240/9999): loss=0.45112854704920896, gradient=0.00676735594860125\n",
      "Gradient Descent (9245/9999): loss=0.4511264834737333, gradient=0.006766587734824842\n",
      "Gradient Descent (9250/9999): loss=0.45112442038923956, gradient=0.006765819920891414\n",
      "Gradient Descent (9255/9999): loss=0.45112235779541954, gradient=0.00676505250639179\n",
      "Gradient Descent (9260/9999): loss=0.451120295691966, gradient=0.006764285490917495\n",
      "Gradient Descent (9265/9999): loss=0.45111823407857154, gradient=0.0067635188740606655\n",
      "Gradient Descent (9270/9999): loss=0.4511161729549298, gradient=0.006762752655414123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent (9275/9999): loss=0.45111411232073373, gradient=0.0067619868345713575\n",
      "Gradient Descent (9280/9999): loss=0.4511120521756775, gradient=0.006761221411126471\n",
      "Gradient Descent (9285/9999): loss=0.45110999251945527, gradient=0.006760456384674254\n",
      "Gradient Descent (9290/9999): loss=0.4511079333517613, gradient=0.006759691754810136\n",
      "Gradient Descent (9295/9999): loss=0.4511058746722908, gradient=0.00675892752113021\n",
      "Gradient Descent (9300/9999): loss=0.45110381648073866, gradient=0.006758163683231198\n",
      "Gradient Descent (9305/9999): loss=0.4511017587768006, gradient=0.006757400240710492\n",
      "Gradient Descent (9310/9999): loss=0.4510997015601722, gradient=0.006756637193166108\n",
      "Gradient Descent (9315/9999): loss=0.4510976448305499, gradient=0.006755874540196709\n",
      "Gradient Descent (9320/9999): loss=0.4510955885876298, gradient=0.006755112281401619\n",
      "Gradient Descent (9325/9999): loss=0.45109353283110926, gradient=0.006754350416380782\n",
      "Gradient Descent (9330/9999): loss=0.451091477560685, gradient=0.006753588944734827\n",
      "Gradient Descent (9335/9999): loss=0.4510894227760546, gradient=0.006752827866064971\n",
      "Gradient Descent (9340/9999): loss=0.45108736847691583, gradient=0.006752067179973093\n",
      "Gradient Descent (9345/9999): loss=0.4510853146629668, gradient=0.006751306886061696\n",
      "Gradient Descent (9350/9999): loss=0.4510832613339059, gradient=0.006750546983933917\n",
      "Gradient Descent (9355/9999): loss=0.45108120848943223, gradient=0.006749787473193569\n",
      "Gradient Descent (9360/9999): loss=0.4510791561292444, gradient=0.006749028353445022\n",
      "Gradient Descent (9365/9999): loss=0.45107710425304215, gradient=0.006748269624293364\n",
      "Gradient Descent (9370/9999): loss=0.45107505286052507, gradient=0.006747511285344233\n",
      "Gradient Descent (9375/9999): loss=0.4510730019513932, gradient=0.006746753336203952\n",
      "Gradient Descent (9380/9999): loss=0.45107095152534715, gradient=0.006745995776479442\n",
      "Gradient Descent (9385/9999): loss=0.4510689015820872, gradient=0.006745238605778263\n",
      "Gradient Descent (9390/9999): loss=0.45106685212131475, gradient=0.00674448182370858\n",
      "Gradient Descent (9395/9999): loss=0.4510648031427308, gradient=0.0067437254298792085\n",
      "Gradient Descent (9400/9999): loss=0.45106275464603734, gradient=0.006742969423899578\n",
      "Gradient Descent (9405/9999): loss=0.45106070663093606, gradient=0.006742213805379702\n",
      "Gradient Descent (9410/9999): loss=0.4510586590971293, gradient=0.006741458573930255\n",
      "Gradient Descent (9415/9999): loss=0.45105661204431974, gradient=0.006740703729162538\n",
      "Gradient Descent (9420/9999): loss=0.4510545654722105, gradient=0.006739949270688426\n",
      "Gradient Descent (9425/9999): loss=0.4510525193805043, gradient=0.006739195198120432\n",
      "Gradient Descent (9430/9999): loss=0.45105047376890506, gradient=0.006738441511071678\n",
      "Gradient Descent (9435/9999): loss=0.4510484286371165, gradient=0.0067376882091559065\n",
      "Gradient Descent (9440/9999): loss=0.4510463839848429, gradient=0.006736935291987455\n",
      "Gradient Descent (9445/9999): loss=0.45104433981178865, gradient=0.006736182759181288\n",
      "Gradient Descent (9450/9999): loss=0.4510422961176585, gradient=0.006735430610352965\n",
      "Gradient Descent (9455/9999): loss=0.45104025290215777, gradient=0.006734678845118649\n",
      "Gradient Descent (9460/9999): loss=0.4510382101649918, gradient=0.006733927463095133\n",
      "Gradient Descent (9465/9999): loss=0.45103616790586604, gradient=0.0067331764638997845\n",
      "Gradient Descent (9470/9999): loss=0.45103412612448696, gradient=0.006732425847150583\n",
      "Gradient Descent (9475/9999): loss=0.4510320848205605, gradient=0.006731675612466132\n",
      "Gradient Descent (9480/9999): loss=0.45103004399379387, gradient=0.006730925759465596\n",
      "Gradient Descent (9485/9999): loss=0.45102800364389334, gradient=0.006730176287768767\n",
      "Gradient Descent (9490/9999): loss=0.4510259637705669, gradient=0.00672942719699603\n",
      "Gradient Descent (9495/9999): loss=0.4510239243735216, gradient=0.0067286784867683575\n",
      "Gradient Descent (9500/9999): loss=0.4510218854524655, gradient=0.006727930156707312\n",
      "Gradient Descent (9505/9999): loss=0.4510198470071069, gradient=0.006727182206435057\n",
      "Gradient Descent (9510/9999): loss=0.45101780903715416, gradient=0.006726434635574379\n",
      "Gradient Descent (9515/9999): loss=0.45101577154231604, gradient=0.006725687443748596\n",
      "Gradient Descent (9520/9999): loss=0.4510137345223019, gradient=0.006724940630581647\n",
      "Gradient Descent (9525/9999): loss=0.451011697976821, gradient=0.00672419419569808\n",
      "Gradient Descent (9530/9999): loss=0.4510096619055829, gradient=0.006723448138723001\n",
      "Gradient Descent (9535/9999): loss=0.45100762630829794, gradient=0.0067227024592821085\n",
      "Gradient Descent (9540/9999): loss=0.4510055911846761, gradient=0.0067219571570016744\n",
      "Gradient Descent (9545/9999): loss=0.4510035565344283, gradient=0.006721212231508579\n",
      "Gradient Descent (9550/9999): loss=0.45100152235726537, gradient=0.006720467682430258\n",
      "Gradient Descent (9555/9999): loss=0.4509994886528985, gradient=0.0067197235093947785\n",
      "Gradient Descent (9560/9999): loss=0.4509974554210391, gradient=0.006718979712030713\n",
      "Gradient Descent (9565/9999): loss=0.45099542266139936, gradient=0.006718236289967249\n",
      "Gradient Descent (9570/9999): loss=0.45099339037369096, gradient=0.006717493242834162\n",
      "Gradient Descent (9575/9999): loss=0.45099135855762656, gradient=0.006716750570261807\n",
      "Gradient Descent (9580/9999): loss=0.45098932721291907, gradient=0.006716008271881069\n",
      "Gradient Descent (9585/9999): loss=0.45098729633928103, gradient=0.006715266347323462\n",
      "Gradient Descent (9590/9999): loss=0.45098526593642607, gradient=0.0067145247962210495\n",
      "Gradient Descent (9595/9999): loss=0.45098323600406764, gradient=0.006713783618206429\n",
      "Gradient Descent (9600/9999): loss=0.45098120654192003, gradient=0.006713042812912817\n",
      "Gradient Descent (9605/9999): loss=0.4509791775496971, gradient=0.0067123023799740035\n",
      "Gradient Descent (9610/9999): loss=0.4509771490271131, gradient=0.006711562319024298\n",
      "Gradient Descent (9615/9999): loss=0.45097512097388326, gradient=0.006710822629698606\n",
      "Gradient Descent (9620/9999): loss=0.45097309338972263, gradient=0.006710083311632405\n",
      "Gradient Descent (9625/9999): loss=0.45097106627434624, gradient=0.006709344364461704\n",
      "Gradient Descent (9630/9999): loss=0.45096903962747037, gradient=0.006708605787823123\n",
      "Gradient Descent (9635/9999): loss=0.4509670134488103, gradient=0.006707867581353777\n",
      "Gradient Descent (9640/9999): loss=0.45096498773808263, gradient=0.00670712974469141\n",
      "Gradient Descent (9645/9999): loss=0.45096296249500395, gradient=0.006706392277474277\n",
      "Gradient Descent (9650/9999): loss=0.45096093771929086, gradient=0.006705655179341207\n",
      "Gradient Descent (9655/9999): loss=0.45095891341066063, gradient=0.00670491844993158\n",
      "Gradient Descent (9660/9999): loss=0.45095688956883073, gradient=0.006704182088885346\n",
      "Gradient Descent (9665/9999): loss=0.450954866193519, gradient=0.0067034460958429925\n",
      "Gradient Descent (9670/9999): loss=0.4509528432844428, gradient=0.006702710470445552\n",
      "Gradient Descent (9675/9999): loss=0.45095082084132115, gradient=0.006701975212334615\n",
      "Gradient Descent (9680/9999): loss=0.4509487988638721, gradient=0.0067012403211523675\n",
      "Gradient Descent (9685/9999): loss=0.4509467773518147, gradient=0.006700505796541473\n",
      "Gradient Descent (9690/9999): loss=0.45094475630486824, gradient=0.0066997716381451645\n",
      "Gradient Descent (9695/9999): loss=0.450942735722752, gradient=0.00669903784560726\n",
      "Gradient Descent (9700/9999): loss=0.4509407156051858, gradient=0.006698304418572075\n",
      "Gradient Descent (9705/9999): loss=0.45093869595188907, gradient=0.0066975713566845025\n",
      "Gradient Descent (9710/9999): loss=0.45093667676258287, gradient=0.006696838659589933\n",
      "Gradient Descent (9715/9999): loss=0.4509346580369873, gradient=0.006696106326934357\n",
      "Gradient Descent (9720/9999): loss=0.4509326397748233, gradient=0.0066953743583642806\n",
      "Gradient Descent (9725/9999): loss=0.4509306219758121, gradient=0.006694642753526742\n",
      "Gradient Descent (9730/9999): loss=0.450928604639675, gradient=0.006693911512069309\n",
      "Gradient Descent (9735/9999): loss=0.4509265877661338, gradient=0.006693180633640121\n",
      "Gradient Descent (9740/9999): loss=0.45092457135491026, gradient=0.006692450117887834\n",
      "Gradient Descent (9745/9999): loss=0.45092255540572684, gradient=0.006691719964461615\n",
      "Gradient Descent (9750/9999): loss=0.45092053991830605, gradient=0.006690990173011226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent (9755/9999): loss=0.4509185248923707, gradient=0.0066902607431869\n",
      "Gradient Descent (9760/9999): loss=0.4509165103276438, gradient=0.006689531674639422\n",
      "Gradient Descent (9765/9999): loss=0.4509144962238489, gradient=0.006688802967020121\n",
      "Gradient Descent (9770/9999): loss=0.45091248258070915, gradient=0.006688074619980851\n",
      "Gradient Descent (9775/9999): loss=0.45091046939794915, gradient=0.0066873466331739745\n",
      "Gradient Descent (9780/9999): loss=0.4509084566752929, gradient=0.006686619006252423\n",
      "Gradient Descent (9785/9999): loss=0.45090644441246464, gradient=0.006685891738869597\n",
      "Gradient Descent (9790/9999): loss=0.4509044326091893, gradient=0.006685164830679478\n",
      "Gradient Descent (9795/9999): loss=0.4509024212651921, gradient=0.006684438281336537\n",
      "Gradient Descent (9800/9999): loss=0.4509004103801979, gradient=0.006683712090495754\n",
      "Gradient Descent (9805/9999): loss=0.4508983999539326, gradient=0.006682986257812691\n",
      "Gradient Descent (9810/9999): loss=0.4508963899861223, gradient=0.006682260782943359\n",
      "Gradient Descent (9815/9999): loss=0.45089438047649255, gradient=0.00668153566554435\n",
      "Gradient Descent (9820/9999): loss=0.4508923714247702, gradient=0.006680810905272719\n",
      "Gradient Descent (9825/9999): loss=0.45089036283068185, gradient=0.006680086501786092\n",
      "Gradient Descent (9830/9999): loss=0.4508883546939543, gradient=0.006679362454742578\n",
      "Gradient Descent (9835/9999): loss=0.4508863470143148, gradient=0.006678638763800805\n",
      "Gradient Descent (9840/9999): loss=0.45088433979149134, gradient=0.00667791542861991\n",
      "Gradient Descent (9845/9999): loss=0.45088233302521097, gradient=0.0066771924488595695\n",
      "Gradient Descent (9850/9999): loss=0.45088032671520206, gradient=0.0066764698241799335\n",
      "Gradient Descent (9855/9999): loss=0.4508783208611929, gradient=0.006675747554241715\n",
      "Gradient Descent (9860/9999): loss=0.4508763154629122, gradient=0.006675025638706076\n",
      "Gradient Descent (9865/9999): loss=0.45087431052008875, gradient=0.006674304077234736\n",
      "Gradient Descent (9870/9999): loss=0.45087230603245165, gradient=0.006673582869489883\n",
      "Gradient Descent (9875/9999): loss=0.4508703019997301, gradient=0.006672862015134258\n",
      "Gradient Descent (9880/9999): loss=0.4508682984216541, gradient=0.006672141513831069\n",
      "Gradient Descent (9885/9999): loss=0.4508662952979533, gradient=0.006671421365244037\n",
      "Gradient Descent (9890/9999): loss=0.4508642926283583, gradient=0.0066707015690374094\n",
      "Gradient Descent (9895/9999): loss=0.45086229041259906, gradient=0.006669982124875873\n",
      "Gradient Descent (9900/9999): loss=0.4508602886504066, gradient=0.0066692630324247165\n",
      "Gradient Descent (9905/9999): loss=0.4508582873415119, gradient=0.006668544291349662\n",
      "Gradient Descent (9910/9999): loss=0.4508562864856461, gradient=0.006667825901316905\n",
      "Gradient Descent (9915/9999): loss=0.45085428608254097, gradient=0.006667107861993234\n",
      "Gradient Descent (9920/9999): loss=0.4508522861319279, gradient=0.006666390173045837\n",
      "Gradient Descent (9925/9999): loss=0.45085028663353927, gradient=0.006665672834142466\n",
      "Gradient Descent (9930/9999): loss=0.45084828758710743, gradient=0.006664955844951321\n",
      "Gradient Descent (9935/9999): loss=0.4508462889923646, gradient=0.0066642392051411295\n",
      "Gradient Descent (9940/9999): loss=0.45084429084904404, gradient=0.006663522914381108\n",
      "Gradient Descent (9945/9999): loss=0.4508422931568786, gradient=0.0066628069723409615\n",
      "Gradient Descent (9950/9999): loss=0.4508402959156019, gradient=0.0066620913786908575\n",
      "Gradient Descent (9955/9999): loss=0.4508382991249472, gradient=0.006661376133101524\n",
      "Gradient Descent (9960/9999): loss=0.45083630278464887, gradient=0.006660661235244099\n",
      "Gradient Descent (9965/9999): loss=0.45083430689444054, gradient=0.006659946684790267\n",
      "Gradient Descent (9970/9999): loss=0.45083231145405717, gradient=0.006659232481412155\n",
      "Gradient Descent (9975/9999): loss=0.45083031646323307, gradient=0.00665851862478244\n",
      "Gradient Descent (9980/9999): loss=0.4508283219217032, gradient=0.00665780511457421\n",
      "Gradient Descent (9985/9999): loss=0.45082632782920296, gradient=0.006657091950461101\n",
      "Gradient Descent (9990/9999): loss=0.45082433418546736, gradient=0.006656379132117167\n",
      "Gradient Descent (9995/9999): loss=0.4508223409902328, gradient=0.006655666659217015\n",
      "Training loss: 0.450820746756788\n",
      "Training accuracy: 0.776504\n"
     ]
    }
   ],
   "source": [
    "max_iters = 10000\n",
    "w_initial = np.ones(tx.shape[1])\n",
    "\n",
    "# Run gradient descent \n",
    "w, loss = logistic_regression_mean(y, tx, BEST, max_iters, 0.01, \n",
    "                                    threshold=1e-7, verbose=True)\n",
    "# w, loss = logistic_regression_mean(y, tx, BEST3, max_iters, 0.01, \n",
    "#                                      threshold=1e-7, verbose=True)\n",
    "print(f'Training loss: {loss}')\n",
    "\n",
    "acc = eval_model(y, tx, w, thresh=0.5)\n",
    "print(f'Training accuracy: {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST=w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238, 29)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load test data\n",
    "y_test, x_test, ids_test = load_csv_data(path.join(DATA_PATH, 'test.csv'))\n",
    "fx_test = feature_transform(x_test)\n",
    "fx_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx_test=np.hstack((fx_test,fx_test**2,fx_test**3,fx_test**4,fx_test**5,fx_test**6))\n",
    "\n",
    "# # Standardise to mean and s.d. of training data\n",
    "fx_test = standardise_to_fixed(fx_test, mu_x, sigma_x)\n",
    "\n",
    "# # Add offset term\n",
    "tx_test = np.c_[np.ones(fx_test.shape[0]), fx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on test set\n",
    "y_pred = predict_labels(w, tx_test, thresh=0.5)\n",
    "create_csv_submission(ids_test, y_pred, '../data/final_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
