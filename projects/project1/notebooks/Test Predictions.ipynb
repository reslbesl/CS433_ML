{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from scripts.proj1_helpers import *\n",
    "from scripts.implementations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "y, tx, ids = load_csv_data('../data/train.csv')\n",
    "\n",
    "# Normalise data\n",
    "tx, mean_tx, std_tx = standardise(tx)\n",
    "\n",
    "# Check shape of data\n",
    "print('Shape: y: {}, x:{}\\n'.format(y.shape, tx.shape))\n",
    "\n",
    "# Check that data is normalised\n",
    "print(np.mean(tx, axis=0), np.std(tx, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test, tx_test, ids_test = load_csv_data('../data/test.csv')\n",
    "\n",
    "# Don't forget to standardise to same mean and std\n",
    "tx_test = standardise_to_fixed(tx_test, mean_tx, std_tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Random baseline guessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get baseline frequency of the two classes in training data\n",
    "prior_probs = [sum(y == 1)/len(y), sum(y == -1)/len(y)]\n",
    "\n",
    "y_test_pred = np.random.choice([1., -1.], size=len(y_test), p=prior_probs)\n",
    "\n",
    "# Save in submission file\n",
    "create_csv_submission(ids_test, y_test_pred, '../data/random_basline_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy on the test after submission to the AICrowd platform for the random guess model is 55%. This is thus our baseline. Anything that goes below that is probably overfitting or a model that diverged. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Least-squares gradient descent with shuffle split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.4636087303159818, gradient=0.7850384279684134\n",
      "Gradient Descent(1/49): loss=0.44856737946369746, gradient=0.42135566888668025\n",
      "Gradient Descent(2/49): loss=0.43907337842098365, gradient=0.3239920655829415\n",
      "Gradient Descent(3/49): loss=0.4326351490635258, gradient=0.2651820191539501\n",
      "Gradient Descent(4/49): loss=0.428106852004358, gradient=0.2214672862075326\n",
      "Gradient Descent(5/49): loss=0.4248031527475808, gradient=0.18834801945977317\n",
      "Gradient Descent(6/49): loss=0.42229957092091547, gradient=0.163237290452147\n",
      "Gradient Descent(7/49): loss=0.4203298629959851, gradient=0.14417116938505284\n",
      "Gradient Descent(8/49): loss=0.41872496575343865, gradient=0.12962655796411296\n",
      "Gradient Descent(9/49): loss=0.41737599713032336, gradient=0.11843430296549144\n",
      "Gradient Descent(10/49): loss=0.4162116878207794, gradient=0.10971152110227202\n",
      "Gradient Descent(11/49): loss=0.4151845274903265, gradient=0.10280296649612077\n",
      "Gradient Descent(12/49): loss=0.41426219842172696, gradient=0.09722964761304473\n",
      "Gradient Descent(13/49): loss=0.41342223384139487, gradient=0.09264521540094509\n",
      "Gradient Descent(14/49): loss=0.41264865465695477, gradient=0.08880066617567395\n",
      "Gradient Descent(15/49): loss=0.4119298283583921, gradient=0.0855170738422077\n",
      "Gradient Descent(16/49): loss=0.4112570888844293, gradient=0.08266532276197713\n",
      "Gradient Descent(17/49): loss=0.4106238345719097, gradient=0.08015147389834694\n",
      "Gradient Descent(18/49): loss=0.4100249295348622, gradient=0.07790640642645445\n",
      "Gradient Descent(19/49): loss=0.4094562998316255, gradient=0.07587857387384252\n",
      "Gradient Descent(20/49): loss=0.40891465626613277, gradient=0.07402896711245346\n",
      "Gradient Descent(21/49): loss=0.40839730066024266, gradient=0.07232761366348618\n",
      "Gradient Descent(22/49): loss=0.4079019879703608, gradient=0.0707511357866349\n",
      "Gradient Descent(23/49): loss=0.4074268263583879, gradient=0.06928103515036463\n",
      "Gradient Descent(24/49): loss=0.4069702034841894, gradient=0.06790247625038727\n",
      "Gradient Descent(25/49): loss=0.4065307312191823, gradient=0.06660341351437192\n",
      "Gradient Descent(26/49): loss=0.40610720351963087, gradient=0.06537395684438747\n",
      "Gradient Descent(27/49): loss=0.40569856385697695, gradient=0.06420590408085539\n",
      "Gradient Descent(28/49): loss=0.4053038796999838, gradient=0.06309239159205617\n",
      "Gradient Descent(29/49): loss=0.40492232227930003, gradient=0.062027629476321816\n",
      "Gradient Descent(30/49): loss=0.40455315036533734, gradient=0.061006698163295875\n",
      "Gradient Descent(31/49): loss=0.40419569713539744, gradient=0.06002539017153296\n",
      "Gradient Descent(32/49): loss=0.4038493594474053, gradient=0.05908008552811008\n",
      "Gradient Descent(33/49): loss=0.4035135890090211, gradient=0.05816765261668575\n",
      "Gradient Descent(34/49): loss=0.4031878850543101, gradient=0.05728536848083837\n",
      "Gradient Descent(35/49): loss=0.4028717882302577, gradient=0.056430854193080915\n",
      "Gradient Descent(36/49): loss=0.4025648754620841, gradient=0.055602022021981445\n",
      "Gradient Descent(37/49): loss=0.4022667556162253, gradient=0.054797031934362694\n",
      "Gradient Descent(38/49): loss=0.4019770658177126, gradient=0.054014255553539434\n",
      "Gradient Descent(39/49): loss=0.401695468307667, gradient=0.053252246123673526\n",
      "Gradient Descent(40/49): loss=0.40142164774906525, gradient=0.05250971334951232\n",
      "Gradient Descent(41/49): loss=0.4011553089064789, gradient=0.051785502221005274\n",
      "Gradient Descent(42/49): loss=0.40089617463928456, gradient=0.0510785751151651\n",
      "Gradient Descent(43/49): loss=0.4006439841588139, gradient=0.05038799660824354\n",
      "Gradient Descent(44/49): loss=0.4003984915086744, gradient=0.049712920540650476\n",
      "Gradient Descent(45/49): loss=0.40015946423453147, gradient=0.04905257896283635\n",
      "Gradient Descent(46/49): loss=0.3999266822153373, gradient=0.04840627265823954\n",
      "Gradient Descent(47/49): loss=0.3996999366326532, gradient=0.04777336299354179\n",
      "Gradient Descent(48/49): loss=0.3994790290584944, gradient=0.04715326488994375\n",
      "Gradient Descent(49/49): loss=0.3992637706452601, gradient=0.04654544074434378\n",
      "Accuracy of predictions using least-squares gradient descent 0.7090533333333333 \n",
      "\n",
      "Gradient Descent(0/49): loss=0.39841366914764487, gradient=0.04685309782829711\n",
      "Gradient Descent(1/49): loss=0.3982040862395503, gradient=0.04595684506258133\n",
      "Gradient Descent(2/49): loss=0.3980005612704857, gradient=0.04527448571860474\n",
      "Gradient Descent(3/49): loss=0.39780265232871004, gradient=0.044639529620662935\n",
      "Gradient Descent(4/49): loss=0.3976100330342729, gradient=0.04403478541486137\n",
      "Gradient Descent(5/49): loss=0.39742243805178906, gradient=0.043453594654831985\n",
      "Gradient Descent(6/49): loss=0.3972396439083205, gradient=0.042891605369704454\n",
      "Gradient Descent(7/49): loss=0.3970614571772959, gradient=0.042345753348732876\n",
      "Gradient Descent(8/49): loss=0.3968877065935793, gradient=0.04181382832836944\n",
      "Gradient Descent(9/49): loss=0.39671823767452535, gradient=0.041294202340442045\n",
      "Gradient Descent(10/49): loss=0.3965529089837292, gradient=0.04078565085584065\n",
      "Gradient Descent(11/49): loss=0.396391589487982, gradient=0.04028723323102461\n",
      "Gradient Descent(12/49): loss=0.3962341566525904, gradient=0.03979821173228508\n",
      "Gradient Descent(13/49): loss=0.3960804950430562, gradient=0.039317995930023567\n",
      "Gradient Descent(14/49): loss=0.39593049527942786, gradient=0.038846103968985246\n",
      "Gradient Descent(15/49): loss=0.3957840532401106, gradient=0.03838213520423163\n",
      "Gradient Descent(16/49): loss=0.3956410694448189, gradient=0.037925750593075375\n",
      "Gradient Descent(17/49): loss=0.39550144856807923, gradient=0.03747665845218371\n",
      "Gradient Descent(18/49): loss=0.39536509904921435, gradient=0.03703460397744769\n",
      "Gradient Descent(19/49): loss=0.3952319327745898, gradient=0.03659936143894319\n",
      "Gradient Descent(20/49): loss=0.39510186481467346, gradient=0.03617072830290644\n",
      "Gradient Descent(21/49): loss=0.3949748132031778, gradient=0.03574852075929296\n",
      "Gradient Descent(22/49): loss=0.39485069874888634, gradient=0.03533257028658302\n",
      "Gradient Descent(23/49): loss=0.39472944487315387, gradient=0.03492272099024867\n",
      "Gradient Descent(24/49): loss=0.3946109774677946, gradient=0.034518827523916884\n",
      "Gradient Descent(25/49): loss=0.39449522476933524, gradient=0.034120753453237396\n",
      "Gradient Descent(26/49): loss=0.3943821172465532, gradient=0.033728369958751614\n",
      "Gradient Descent(27/49): loss=0.3942715874989123, gradient=0.03334155480016712\n",
      "Gradient Descent(28/49): loss=0.394163570164037, gradient=0.032960191483484017\n",
      "Gradient Descent(29/49): loss=0.3940580018327691, gradient=0.03258416858642476\n",
      "Gradient Descent(30/49): loss=0.3939548209706476, gradient=0.032213379208060076\n",
      "Gradient Descent(31/49): loss=0.393853967844902, gradient=0.031847720516338246\n",
      "Gradient Descent(32/49): loss=0.39375538445621155, gradient=0.03148709337315351\n",
      "Gradient Descent(33/49): loss=0.39365901447464285, gradient=0.03113140202109993\n",
      "Gradient Descent(34/49): loss=0.39356480317927456, gradient=0.03078055381951988\n",
      "Gradient Descent(35/49): loss=0.39347269740111435, gradient=0.030434459020117925\n",
      "Gradient Descent(36/49): loss=0.3933826454689729, gradient=0.030093030574487645\n",
      "Gradient Descent(37/49): loss=0.3932945971580224, gradient=0.029756183967496814\n",
      "Gradient Descent(38/49): loss=0.39320850364080506, gradient=0.02942383707174316\n",
      "Gradient Descent(39/49): loss=0.3931243174404902, gradient=0.029095910019265363\n",
      "Gradient Descent(40/49): loss=0.39304199238621496, gradient=0.028772325087480247\n",
      "Gradient Descent(41/49): loss=0.3929614835703535, gradient=0.02845300659691219\n",
      "Gradient Descent(42/49): loss=0.3928827473075923, gradient=0.028137880818774413\n",
      "Gradient Descent(43/49): loss=0.3928057410956903, gradient=0.02782687589083485\n",
      "Gradient Descent(44/49): loss=0.39273042357782717, gradient=0.027519921740304275\n",
      "Gradient Descent(45/49): loss=0.3926567545064457, gradient=0.02721695001272241\n",
      "Gradient Descent(46/49): loss=0.39258469470850643, gradient=0.026917894006011942\n",
      "Gradient Descent(47/49): loss=0.39251420605207743, gradient=0.026622688609020492\n",
      "Gradient Descent(48/49): loss=0.3924452514141952, gradient=0.026331270243991272\n",
      "Gradient Descent(49/49): loss=0.39237779464992795, gradient=0.026043576812504365\n",
      "Accuracy of predictions using least-squares gradient descent 0.7138933333333334 \n",
      "\n",
      "Gradient Descent(0/49): loss=0.39273557121740305, gradient=0.027383863418902032\n",
      "Gradient Descent(1/49): loss=0.3926669722710351, gradient=0.02634328618943132\n",
      "Gradient Descent(2/49): loss=0.39260108777629443, gradient=0.02578858825981866\n",
      "Gradient Descent(3/49): loss=0.3925375100058061, gradient=0.02532118642746312\n",
      "Gradient Descent(4/49): loss=0.3924759627436921, gradient=0.024905061103115377\n",
      "Gradient Descent(5/49): loss=0.39241623727465647, gradient=0.024527310040505214\n",
      "Gradient Descent(6/49): loss=0.3923581724988073, gradient=0.02417907548482221\n",
      "Gradient Descent(7/49): loss=0.3923016420364696, gradient=0.023853807588077904\n",
      "Gradient Descent(8/49): loss=0.3922465450322257, gradient=0.02354662722566724\n",
      "Gradient Descent(9/49): loss=0.39219279952313213, gradient=0.02325388800459144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(10/49): loss=0.3921403376329163, gradient=0.0229728546973048\n",
      "Gradient Descent(11/49): loss=0.39208910207371, gradient=0.02270146646570106\n",
      "Gradient Descent(12/49): loss=0.39203904358779296, gradient=0.022438162604586195\n",
      "Gradient Descent(13/49): loss=0.39199011906774084, gradient=0.02218175426612966\n",
      "Gradient Descent(14/49): loss=0.39194229016810234, gradient=0.02193132993247839\n",
      "Gradient Descent(15/49): loss=0.39189552227461916, gradient=0.02168618563760748\n",
      "Gradient Descent(16/49): loss=0.39184978373460055, gradient=0.02144577334208858\n",
      "Gradient Descent(17/49): loss=0.3918050452788412, gradient=0.021209662635221685\n",
      "Gradient Descent(18/49): loss=0.39176127958464035, gradient=0.020977512237130347\n",
      "Gradient Descent(19/49): loss=0.3917184609432256, gradient=0.020749048721963103\n",
      "Gradient Descent(20/49): loss=0.3916765650047805, gradient=0.02052405057518716\n",
      "Gradient Descent(21/49): loss=0.39163556858143445, gradient=0.020302336202183465\n",
      "Gradient Descent(22/49): loss=0.39159544949375175, gradient=0.0200837548729203\n",
      "Gradient Descent(23/49): loss=0.3915561864500335, gradient=0.019868179855615446\n",
      "Gradient Descent(24/49): loss=0.3915177589504963, gradient=0.019655503188176944\n",
      "Gradient Descent(25/49): loss=0.3914801472104186, gradient=0.019445631679551757\n",
      "Gradient Descent(26/49): loss=0.3914433320978249, gradient=0.019238483838228394\n",
      "Gradient Descent(27/49): loss=0.39140729508237815, gradient=0.019033987502416524\n",
      "Gradient Descent(28/49): loss=0.3913720181929682, gradient=0.01883207800339124\n",
      "Gradient Descent(29/49): loss=0.3913374839820837, gradient=0.01863269673560013\n",
      "Gradient Descent(30/49): loss=0.3913036754955169, gradient=0.0184357900383675\n",
      "Gradient Descent(31/49): loss=0.3912705762462852, gradient=0.018241308317266367\n",
      "Gradient Descent(32/49): loss=0.3912381701919112, gradient=0.018049205350586156\n",
      "Gradient Descent(33/49): loss=0.3912064417144036, gradient=0.01785943773932399\n",
      "Gradient Descent(34/49): loss=0.3911753756024161, gradient=0.017671964468912324\n",
      "Gradient Descent(35/49): loss=0.3911449570351922, gradient=0.017486746558275434\n",
      "Gradient Descent(36/49): loss=0.39111517156797404, gradient=0.017303746777408424\n",
      "Gradient Descent(37/49): loss=0.3910860051186298, gradient=0.017122929418926427\n",
      "Gradient Descent(38/49): loss=0.3910574439553035, gradient=0.016944260112284972\n",
      "Gradient Descent(39/49): loss=0.39102947468493, gradient=0.016767705671863527\n",
      "Gradient Descent(40/49): loss=0.39100208424248806, gradient=0.016593233972024727\n",
      "Gradient Descent(41/49): loss=0.3909752598808959, gradient=0.016420813843740466\n",
      "Gradient Descent(42/49): loss=0.3909489891614594, gradient=0.016250414988526227\n",
      "Gradient Descent(43/49): loss=0.3909232599448151, gradient=0.016082007906317284\n",
      "Gradient Descent(44/49): loss=0.3908980603823054, gradient=0.015915563834619288\n",
      "Gradient Descent(45/49): loss=0.3908733789077469, gradient=0.015751054696809406\n",
      "Gradient Descent(46/49): loss=0.39084920422954994, gradient=0.015588453057895526\n",
      "Gradient Descent(47/49): loss=0.3908255253231608, gradient=0.015427732086377668\n",
      "Gradient Descent(48/49): loss=0.39080233142379855, gradient=0.015268865521125377\n",
      "Gradient Descent(49/49): loss=0.39077961201946243, gradient=0.015111827642392465\n",
      "Accuracy of predictions using least-squares gradient descent 0.71636 \n",
      "\n",
      "Gradient Descent(0/49): loss=0.39168724632799196, gradient=0.017697865584650793\n",
      "Gradient Descent(1/49): loss=0.39166176618259185, gradient=0.01613984799062985\n",
      "Gradient Descent(2/49): loss=0.3916379896810399, gradient=0.015535993170194549\n",
      "Gradient Descent(3/49): loss=0.3916154690093972, gradient=0.01509832310659917\n",
      "Gradient Descent(4/49): loss=0.39159393362838574, gradient=0.014749750877115388\n",
      "Gradient Descent(5/49): loss=0.39157319938933893, gradient=0.01446250313255198\n",
      "Gradient Descent(6/49): loss=0.3915531382686066, gradient=0.014218534707067543\n",
      "Gradient Descent(7/49): loss=0.3915336599002437, gradient=0.014005346560851603\n",
      "Gradient Descent(8/49): loss=0.3915146993986728, gradient=0.013814254433844703\n",
      "Gradient Descent(9/49): loss=0.39149620924248557, gradient=0.013639220581307485\n",
      "Gradient Descent(10/49): loss=0.39147815383548934, gradient=0.013476037115036461\n",
      "Gradient Descent(11/49): loss=0.39146050584252107, gradient=0.013321759237351776\n",
      "Gradient Descent(12/49): loss=0.39144324370702677, gradient=0.013174314390457355\n",
      "Gradient Descent(13/49): loss=0.3914263499591162, gradient=0.013032233615023104\n",
      "Gradient Descent(14/49): loss=0.39140981005487097, gradient=0.012894467083954956\n",
      "Gradient Descent(15/49): loss=0.39139361157447367, gradient=0.012760257335522822\n",
      "Gradient Descent(16/49): loss=0.3913777436639697, gradient=0.0126290519721803\n",
      "Gradient Descent(17/49): loss=0.39136219664338645, gradient=0.012500443346466246\n",
      "Gradient Descent(18/49): loss=0.39134696172912853, gradient=0.012374126721906109\n",
      "Gradient Descent(19/49): loss=0.39133203083537804, gradient=0.012249871109393547\n",
      "Gradient Descent(20/49): loss=0.3913173964304974, gradient=0.012127498826436364\n",
      "Gradient Descent(21/49): loss=0.3913030514320126, gradient=0.012006871081609069\n",
      "Gradient Descent(22/49): loss=0.3912889891288689, gradient=0.011887877738937224\n",
      "Gradient Descent(23/49): loss=0.39127520312313696, gradient=0.011770429996325604\n",
      "Gradient Descent(24/49): loss=0.39126168728571736, gradient=0.011654455106604077\n",
      "Gradient Descent(25/49): loss=0.3912484357222078, gradient=0.011539892538935698\n",
      "Gradient Descent(26/49): loss=0.3912354427462389, gradient=0.011426691162516414\n",
      "Gradient Descent(27/49): loss=0.3912227028583386, gradient=0.011314807160939824\n",
      "Gradient Descent(28/49): loss=0.3912102107289472, gradient=0.011204202472720243\n",
      "Gradient Descent(29/49): loss=0.3911979611845736, gradient=0.011094843613730917\n",
      "Gradient Descent(30/49): loss=0.3911859491963593, gradient=0.01098670077918878\n",
      "Gradient Descent(31/49): loss=0.391174169870507, gradient=0.010879747152045982\n",
      "Gradient Descent(32/49): loss=0.3911626184401674, gradient=0.01077395836516433\n",
      "Gradient Descent(33/49): loss=0.391151290258483, gradient=0.010669312079116327\n",
      "Gradient Descent(34/49): loss=0.3911401807925511, gradient=0.010565787647736335\n",
      "Gradient Descent(35/49): loss=0.39112928561813265, gradient=0.010463365850878197\n",
      "Gradient Descent(36/49): loss=0.3911186004149668, gradient=0.010362028679117686\n",
      "Gradient Descent(37/49): loss=0.39110812096258557, gradient=0.01026175915895634\n",
      "Gradient Descent(38/49): loss=0.3910978431365381, gradient=0.010162541209880164\n",
      "Gradient Descent(39/49): loss=0.3910877629049641, gradient=0.010064359526669769\n",
      "Gradient Descent(40/49): loss=0.3910778763254558, gradient=0.009967199481889073\n",
      "Gradient Descent(41/49): loss=0.3910681795421685, gradient=0.009871047044610762\n",
      "Gradient Descent(42/49): loss=0.3910586687831418, gradient=0.009775888712297173\n",
      "Gradient Descent(43/49): loss=0.391049340357805, gradient=0.009681711453408135\n",
      "Gradient Descent(44/49): loss=0.3910401906546373, gradient=0.009588502658805344\n",
      "Gradient Descent(45/49): loss=0.39103121613897107, gradient=0.009496250100408502\n",
      "Gradient Descent(46/49): loss=0.39102241335091265, gradient=0.009404941895863921\n",
      "Gradient Descent(47/49): loss=0.3910137789033724, gradient=0.00931456647821542\n",
      "Gradient Descent(48/49): loss=0.391005309480193, gradient=0.009225112569761038\n",
      "Gradient Descent(49/49): loss=0.3909970018343595, gradient=0.009136569159421928\n",
      "Accuracy of predictions using least-squares gradient descent 0.7190666666666666 \n",
      "\n",
      "Gradient Descent(0/49): loss=0.3902366686546658, gradient=0.01152741525563716\n",
      "Gradient Descent(1/49): loss=0.39022539248381516, gradient=0.010754911327877207\n",
      "Gradient Descent(2/49): loss=0.3902150898741568, gradient=0.010251446576923076\n",
      "Gradient Descent(3/49): loss=0.3902055054771596, gradient=0.009869001986268972\n",
      "Gradient Descent(4/49): loss=0.3901964698363732, gradient=0.009568879403372182\n",
      "Gradient Descent(5/49): loss=0.3901878671345962, gradient=0.009327138328561772\n",
      "Gradient Descent(6/49): loss=0.3901796169890096, gradient=0.009127047758591038\n",
      "Gradient Descent(7/49): loss=0.390171662721807, gradient=0.00895686463897273\n",
      "Gradient Descent(8/49): loss=0.3901639636676731, gradient=0.008808359928584981\n",
      "Gradient Descent(9/49): loss=0.39015649008757136, gradient=0.008675760541664244\n",
      "Gradient Descent(10/49): loss=0.39014921977948896, gradient=0.008555000689719625\n",
      "Gradient Descent(11/49): loss=0.39014213580133233, gradient=0.008443199518897588\n",
      "Gradient Descent(12/49): loss=0.39013522492762553, gradient=0.008338299699563798\n",
      "Gradient Descent(13/49): loss=0.3901284765938224, gradient=0.008238818361882328\n",
      "Gradient Descent(14/49): loss=0.390121882167079, gradient=0.008143675562335424\n",
      "Gradient Descent(15/49): loss=0.39011543443732827, gradient=0.008052075927231788\n",
      "Gradient Descent(16/49): loss=0.3901091272582656, gradient=0.007963426681044323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(17/49): loss=0.39010295529124167, gradient=0.007877280576943044\n",
      "Gradient Descent(18/49): loss=0.39009691382045275, gradient=0.007793295909653595\n",
      "Gradient Descent(19/49): loss=0.39009099861799945, gradient=0.007711208291203996\n",
      "Gradient Descent(20/49): loss=0.3900852058441769, gradient=0.007630810567350824\n",
      "Gradient Descent(21/49): loss=0.3900795319729104, gradient=0.007551938401720225\n",
      "Gradient Descent(22/49): loss=0.39007397373532643, gradient=0.007474459832794413\n",
      "Gradient Descent(23/49): loss=0.3900685280765424, gradient=0.007398267636524544\n",
      "Gradient Descent(24/49): loss=0.39006319212219526, gradient=0.007323273686185222\n",
      "Gradient Descent(25/49): loss=0.3900579631522196, gradient=0.007249404746061085\n",
      "Gradient Descent(26/49): loss=0.3900528385800815, gradient=0.007176599303582832\n",
      "Gradient Descent(27/49): loss=0.3900478159361572, gradient=0.007104805160387658\n",
      "Gradient Descent(28/49): loss=0.39004289285429455, gradient=0.007033977583143347\n",
      "Gradient Descent(29/49): loss=0.3900380670608397, gradient=0.006964077871074112\n",
      "Gradient Descent(30/49): loss=0.39003333636559245, gradient=0.006895072236555028\n",
      "Gradient Descent(31/49): loss=0.3900286986542829, gradient=0.0068269309230564925\n",
      "Gradient Descent(32/49): loss=0.39002415188225686, gradient=0.00675962750462949\n",
      "Gradient Descent(33/49): loss=0.3900196940691326, gradient=0.00669313832543821\n",
      "Gradient Descent(34/49): loss=0.39001532329423877, gradient=0.006627442048219369\n",
      "Gradient Descent(35/49): loss=0.3900110376926894, gradient=0.0065625192881267145\n",
      "Gradient Descent(36/49): loss=0.3900068354519817, gradient=0.006498352314005283\n",
      "Gradient Descent(37/49): loss=0.3900027148090223, gradient=0.006434924803286096\n",
      "Gradient Descent(38/49): loss=0.3899986740475082, gradient=0.006372221639801411\n",
      "Gradient Descent(39/49): loss=0.38999471149560755, gradient=0.0063102287461658645\n",
      "Gradient Descent(40/49): loss=0.3899908255238877, gradient=0.006248932944155187\n",
      "Gradient Descent(41/49): loss=0.38998701454345286, gradient=0.006188321837883744\n",
      "Gradient Descent(42/49): loss=0.3899832770042619, gradient=0.006128383715640533\n",
      "Gradient Descent(43/49): loss=0.38997961139359644, gradient=0.006069107467067177\n",
      "Gradient Descent(44/49): loss=0.3899760162346597, gradient=0.006010482513006128\n",
      "Gradient Descent(45/49): loss=0.38997249008528756, gradient=0.005952498745855912\n",
      "Gradient Descent(46/49): loss=0.3899690315367588, gradient=0.00589514647867414\n",
      "Gradient Descent(47/49): loss=0.38996563921268507, gradient=0.005838416401590377\n",
      "Gradient Descent(48/49): loss=0.38996231176798163, gradient=0.005782299544348416\n",
      "Gradient Descent(49/49): loss=0.38995904788789865, gradient=0.005726787244006028\n",
      "Accuracy of predictions using least-squares gradient descent 0.7170133333333333 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters for gradient descent\n",
    "max_iters = 50\n",
    "gamma = .1\n",
    "\n",
    "num_samples, num_dim = tx.shape\n",
    "# Initial weights vector to train a linear model\n",
    "initial_w = np.zeros(num_dim)\n",
    "\n",
    "res = {\n",
    "    'weights': {},\n",
    "    'accuracy': {},\n",
    "    'loss': {}\n",
    "}\n",
    "n_iter = 0\n",
    "\n",
    "for train_data, eval_data in train_eval_split(y, tx, train_size=.7, num_splits=5):\n",
    "    # Get training data\n",
    "    y_train, tx_train = train_data\n",
    "    \n",
    "    # Run gradient descent under MSE loss to find optimal weights\n",
    "    final_w, final_loss = least_squares_GD(y_train, tx_train, initial_w, max_iters, gamma)\n",
    "    \n",
    "    # Get validation set\n",
    "    y_eval, tx_eval = eval_data\n",
    "    \n",
    "    # Get predictions from current model\n",
    "    y_pred = predict_labels(final_w, tx_eval)\n",
    "    \n",
    "    acc = get_accuracy(y_pred, y_eval)\n",
    "    \n",
    "    print('Accuracy of predictions using least-squares gradient descent', acc, '\\n')\n",
    "    \n",
    "    res['weights'][n_iter] = w\n",
    "    res['loss'][n_iter] = loss\n",
    "    res['accuracy'][n_iter] = acc\n",
    "    \n",
    "    n_iter += 1\n",
    "\n",
    "# Select model with highest accuracy on validation set\n",
    "iter_max_acc = max(res['accuracy'], key=res['accuracy'].get)\n",
    "w_max_acc = res['weights'][iter_max_acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from current model\n",
    "y_test_pred = predict_labels(w_max_acc, tx_test)\n",
    "\n",
    "# Save in submission file\n",
    "create_csv_submission(ids_test, y_test_pred, '../data/test_gd_submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
