{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from scripts.proj1_helpers import *\n",
    "from scripts.implementations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "y, tx, ids = load_csv_data('../data/train.csv')\n",
    "\n",
    "# Normalise data\n",
    "tx, mean_tx, std_tx = standardise(tx)\n",
    "\n",
    "# Check shape of data\n",
    "print('Shape: y: {}, x:{}\\n'.format(y.shape, tx.shape))\n",
    "\n",
    "# Check that data is normalised\n",
    "print(np.mean(tx, axis=0), np.std(tx, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test, tx_test, ids_test = load_csv_data('../data/test.csv')\n",
    "\n",
    "# Don't forget to standardise to same mean and std\n",
    "tx_test = standardise_to_fixed(tx_test, mean_tx, std_tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Least-squares gradient descent with shuffle split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=447.78343902876844\n",
      "Gradient Descent(1/99): loss=34659.25048998579\n",
      "Gradient Descent(2/99): loss=2692563.018168814\n",
      "Gradient Descent(3/99): loss=209181694.48257086\n",
      "Gradient Descent(4/99): loss=16251055788.886072\n",
      "Gradient Descent(5/99): loss=1262523547308.5322\n",
      "Gradient Descent(6/99): loss=98083824722823.39\n",
      "Gradient Descent(7/99): loss=7620005735948605.0\n",
      "Gradient Descent(8/99): loss=5.91988409709504e+17\n",
      "Gradient Descent(9/99): loss=4.599081541068685e+19\n",
      "Gradient Descent(10/99): loss=3.572967084233661e+21\n",
      "Gradient Descent(11/99): loss=2.775792007821361e+23\n",
      "Gradient Descent(12/99): loss=2.1564769809060515e+25\n",
      "Gradient Descent(13/99): loss=1.6753391306244195e+27\n",
      "Gradient Descent(14/99): loss=1.3015493452761717e+29\n",
      "Gradient Descent(15/99): loss=1.011156885924012e+31\n",
      "Gradient Descent(16/99): loss=7.855547326441059e+32\n",
      "Gradient Descent(17/99): loss=6.102873318373776e+34\n",
      "Gradient Descent(18/99): loss=4.741243505052176e+36\n",
      "Gradient Descent(19/99): loss=3.6834108790233604e+38\n",
      "Gradient Descent(20/99): loss=2.86159436638308e+40\n",
      "Gradient Descent(21/99): loss=2.2231357257343455e+42\n",
      "Gradient Descent(22/99): loss=1.7271254490493203e+44\n",
      "Gradient Descent(23/99): loss=1.3417814676017969e+46\n",
      "Gradient Descent(24/99): loss=1.0424127024418639e+48\n",
      "Gradient Descent(25/99): loss=8.098369730462218e+49\n",
      "Gradient Descent(26/99): loss=6.291518909702057e+51\n",
      "Gradient Descent(27/99): loss=4.887799829914595e+53\n",
      "Gradient Descent(28/99): loss=3.7972685960574224e+55\n",
      "Gradient Descent(29/99): loss=2.950048957069489e+57\n",
      "Gradient Descent(30/99): loss=2.2918549554652505e+59\n",
      "Gradient Descent(31/99): loss=1.780512531598272e+61\n",
      "Gradient Descent(32/99): loss=1.3832572029127056e+63\n",
      "Gradient Descent(33/99): loss=1.0746346658354114e+65\n",
      "Gradient Descent(34/99): loss=8.34869800485019e+66\n",
      "Gradient Descent(35/99): loss=6.485995714832488e+68\n",
      "Gradient Descent(36/99): loss=5.038886349510569e+70\n",
      "Gradient Descent(37/99): loss=3.914645762904186e+72\n",
      "Gradient Descent(38/99): loss=3.0412377628863144e+74\n",
      "Gradient Descent(39/99): loss=2.3626983616376952e+76\n",
      "Gradient Descent(40/99): loss=1.8355498594057645e+78\n",
      "Gradient Descent(41/99): loss=1.4260149924635937e+80\n",
      "Gradient Descent(42/99): loss=1.1078526406192209e+82\n",
      "Gradient Descent(43/99): loss=8.606764163163842e+83\n",
      "Gradient Descent(44/99): loss=6.686483982104037e+85\n",
      "Gradient Descent(45/99): loss=5.194643096447831e+87\n",
      "Gradient Descent(46/99): loss=4.035651169089035e+89\n",
      "Gradient Descent(47/99): loss=3.1352453010114455e+91\n",
      "Gradient Descent(48/99): loss=2.4357316045562103e+93\n",
      "Gradient Descent(49/99): loss=1.892288443114817e+95\n",
      "Gradient Descent(50/99): loss=1.4700944657645428e+97\n",
      "Gradient Descent(51/99): loss=1.1420974144481376e+99\n",
      "Gradient Descent(52/99): loss=8.872807390719365e+100\n",
      "Gradient Descent(53/99): loss=6.893169531501513e+102\n",
      "Gradient Descent(54/99): loss=5.355214431874108e+104\n",
      "Gradient Descent(55/99): loss=4.160396966924167e+106\n",
      "Gradient Descent(56/99): loss=3.2321587011287e+108\n",
      "Gradient Descent(57/99): loss=2.511022374147496e+110\n",
      "Gradient Descent(58/99): loss=1.9507808701557538e+112\n",
      "Gradient Descent(59/99): loss=1.515536477311411e+114\n",
      "Gradient Descent(60/99): loss=1.1774007266526621e+116\n",
      "Gradient Descent(61/99): loss=9.147074266278765e+117\n",
      "Gradient Descent(62/99): loss=7.106243926881985e+119\n",
      "Gradient Descent(63/99): loss=5.520749179277331e+121\n",
      "Gradient Descent(64/99): loss=4.2889987754564136e+123\n",
      "Gradient Descent(65/99): loss=3.332067786183049e+125\n",
      "Gradient Descent(66/99): loss=2.5886404526980566e+127\n",
      "Gradient Descent(67/99): loss=2.011081353486199e+129\n",
      "Gradient Descent(68/99): loss=1.5623831444511764e+131\n",
      "Gradient Descent(69/99): loss=1.2137952976559712e+133\n",
      "Gradient Descent(70/99): loss=9.429818990586128e+134\n",
      "Gradient Descent(71/99): loss=7.325904653522729e+136\n",
      "Gradient Descent(72/99): loss=5.6914007624201114e+138\n",
      "Gradient Descent(73/99): loss=4.421575787626493e+140\n",
      "Gradient Descent(74/99): loss=3.4350651556316673e+142\n",
      "Gradient Descent(75/99): loss=2.6686577795308915e+144\n",
      "Gradient Descent(76/99): loss=2.0732457818376196e+146\n",
      "Gradient Descent(77/99): loss=1.6106778864179062e+148\n",
      "Gradient Descent(78/99): loss=1.2513148593005561e+150\n",
      "Gradient Descent(79/99): loss=9.721303621971486e+151\n",
      "Gradient Descent(80/99): loss=7.552355301157426e+153\n",
      "Gradient Descent(81/99): loss=5.867327347538745e+155\n",
      "Gradient Descent(82/99): loss=4.5582508807418643e+157\n",
      "Gradient Descent(83/99): loss=3.5412462712685935e+159\n",
      "Gradient Descent(84/99): loss=2.7511485176815756e+161\n",
      "Gradient Descent(85/99): loss=2.1373317715150333e+163\n",
      "Gradient Descent(86/99): loss=1.66046546457523e+165\n",
      "Gradient Descent(87/99): loss=1.2899941861120867e+167\n",
      "Gradient Descent(88/99): loss=1.0021798319236192e+169\n",
      "Gradient Descent(89/99): loss=7.785805752671711e+170\n",
      "Gradient Descent(90/99): loss=6.048691989937804e+172\n",
      "Gradient Descent(91/99): loss=4.699150730363756e+174\n",
      "Gradient Descent(92/99): loss=3.650709545702172e+176\n",
      "Gradient Descent(93/99): loss=2.8361891226352085e+178\n",
      "Gradient Descent(94/99): loss=2.2033987197979585e+180\n",
      "Gradient Descent(95/99): loss=1.7117920239029493e+182\n",
      "Gradient Descent(96/99): loss=1.3298691275296988e+184\n",
      "Gradient Descent(97/99): loss=1.0331581592045429e+186\n",
      "Gradient Descent(98/99): loss=8.026472378629501e+187\n",
      "Gradient Descent(99/99): loss=6.235662785115548e+189\n",
      "Accuracy of predictions using least-squares gradient descent 0.62884\n",
      "Gradient Descent(0/99): loss=447.914929979647\n",
      "Gradient Descent(1/99): loss=34673.32104232946\n",
      "Gradient Descent(2/99): loss=2693999.4424944413\n",
      "Gradient Descent(3/99): loss=209319985.85344493\n",
      "Gradient Descent(4/99): loss=16263873819.66972\n",
      "Gradient Descent(5/99): loss=1263680537903.4656\n",
      "Gradient Descent(6/99): loss=98186232851484.8\n",
      "Gradient Descent(7/99): loss=7628934712853833.0\n",
      "Gradient Descent(8/99): loss=5.927576928327615e+17\n",
      "Gradient Descent(9/99): loss=4.605645422819228e+19\n",
      "Gradient Descent(10/99): loss=3.5785228968290005e+21\n",
      "Gradient Descent(11/99): loss=2.780462877077188e+23\n",
      "Gradient Descent(12/99): loss=2.160381261680615e+25\n",
      "Gradient Descent(13/99): loss=1.6785864088668911e+27\n",
      "Gradient Descent(14/99): loss=1.3042384610578968e+29\n",
      "Gradient Descent(15/99): loss=1.0133752747652438e+31\n",
      "Gradient Descent(16/99): loss=7.873785953778431e+32\n",
      "Gradient Descent(17/99): loss=6.117822961516439e+34\n",
      "Gradient Descent(18/99): loss=4.753463963609202e+36\n",
      "Gradient Descent(19/99): loss=3.693375861881828e+38\n",
      "Gradient Descent(20/99): loss=2.8697020449848867e+40\n",
      "Gradient Descent(21/99): loss=2.2297188628926857e+42\n",
      "Gradient Descent(22/99): loss=1.7324607675657267e+44\n",
      "Gradient Descent(23/99): loss=1.3460980938469443e+46\n",
      "Gradient Descent(24/99): loss=1.0458996314267965e+48\n",
      "Gradient Descent(25/99): loss=8.126495713937932e+49\n",
      "Gradient Descent(26/99): loss=6.314174955637014e+51\n",
      "Gradient Descent(27/99): loss=4.906026751729412e+53\n",
      "Gradient Descent(28/99): loss=3.8119150415996814e+55\n",
      "Gradient Descent(29/99): loss=2.9618053507864968e+57\n",
      "Gradient Descent(30/99): loss=2.301281859699104e+59\n",
      "Gradient Descent(31/99): loss=1.7880642279121532e+61\n",
      "Gradient Descent(32/99): loss=1.3893012147400054e+63\n",
      "Gradient Descent(33/99): loss=1.0794678597937277e+65\n",
      "Gradient Descent(34/99): loss=8.38731621310574e+66\n",
      "Gradient Descent(35/99): loss=6.516828881970572e+68\n",
      "Gradient Descent(36/99): loss=5.0634860541593816e+70\n",
      "Gradient Descent(37/99): loss=3.9342587453230435e+72\n",
      "Gradient Descent(38/99): loss=3.0568647192058995e+74\n",
      "Gradient Descent(39/99): loss=2.3751416763409904e+76\n",
      "Gradient Descent(40/99): loss=1.8454522855552822e+78\n",
      "Gradient Descent(41/99): loss=1.4338909430901216e+80\n",
      "Gradient Descent(42/99): loss=1.1141134630079248e+82\n",
      "Gradient Descent(43/99): loss=8.656507766068557e+83\n",
      "Gradient Descent(44/99): loss=6.725987001511848e+85\n",
      "Gradient Descent(45/99): loss=5.225999024899045e+87\n",
      "Gradient Descent(46/99): loss=4.0605290795398767e+89\n",
      "Gradient Descent(47/99): loss=3.1549750252981596e+91\n",
      "Gradient Descent(48/99): loss=2.4513720294260367e+93\n",
      "Gradient Descent(49/99): loss=1.9046822172814012e+95\n",
      "Gradient Descent(50/99): loss=1.479911782169352e+97\n",
      "Gradient Descent(51/99): loss=1.1498710194972618e+99\n",
      "Gradient Descent(52/99): loss=8.934339042435955e+100\n",
      "Gradient Descent(53/99): loss=6.941858066837343e+102\n",
      "Gradient Descent(54/99): loss=5.39372786181801e+104\n",
      "Gradient Descent(55/99): loss=4.1908520697551486e+106\n",
      "Gradient Descent(56/99): loss=3.2562341891404097e+108\n",
      "Gradient Descent(57/99): loss=2.530049001502074e+110\n",
      "Gradient Descent(58/99): loss=1.9658131381795495e+112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(59/99): loss=1.5274096635855817e+114\n",
      "Gradient Descent(60/99): loss=1.1867762174868195e+116\n",
      "Gradient Descent(61/99): loss=9.221087334788969e+117\n",
      "Gradient Descent(62/99): loss=7.164657530453889e+119\n",
      "Gradient Descent(63/99): loss=5.566839968538691e+121\n",
      "Gradient Descent(64/99): loss=4.3253577862718605e+123\n",
      "Gradient Descent(65/99): loss=3.3607432735620363e+125\n",
      "Gradient Descent(66/99): loss=2.611251116992e+127\n",
      "Gradient Descent(67/99): loss=2.0289060606420338e+129\n",
      "Gradient Descent(68/99): loss=1.5764319931251474e+131\n",
      "Gradient Descent(69/99): loss=1.2248658906179852e+133\n",
      "Gradient Descent(70/99): loss=9.517038835434712e+134\n",
      "Gradient Descent(71/99): loss=7.394607759832043e+136\n",
      "Gradient Descent(72/99): loss=5.7455081215154855e+138\n",
      "Gradient Descent(73/99): loss=4.464180474009327e+140\n",
      "Gradient Descent(74/99): loss=3.4686065850115974e+142\n",
      "Gradient Descent(75/99): loss=2.6950594205660418e+144\n",
      "Gradient Descent(76/99): loss=2.0940239552585207e+146\n",
      "Gradient Descent(77/99): loss=1.6270276980667134e+148\n",
      "Gradient Descent(78/99): loss=1.2641780547106751e+150\n",
      "Gradient Descent(79/99): loss=9.822488922044958e+151\n",
      "Gradient Descent(80/99): loss=7.63193825934407e+153\n",
      "Gradient Descent(81/99): loss=5.929910642476327e+155\n",
      "Gradient Descent(82/99): loss=4.607458686487848e+157\n",
      "Gradient Descent(83/99): loss=3.579931777661213e+159\n",
      "Gradient Descent(84/99): loss=2.781557558029412e+161\n",
      "Gradient Descent(85/99): loss=2.1612318136646838e+163\n",
      "Gradient Descent(86/99): loss=1.6792472760137292e+165\n",
      "Gradient Descent(87/99): loss=1.3047519457054614e+167\n",
      "Gradient Descent(88/99): loss=1.0137742452459805e+169\n",
      "Gradient Descent(89/99): loss=7.876885899322224e+170\n",
      "Gradient Descent(90/99): loss=6.1202315763986067e+172\n",
      "Gradient Descent(91/99): loss=4.7553354241134585e+174\n",
      "Gradient Descent(92/99): loss=3.6948299608517236e+176\n",
      "Gradient Descent(93/99): loss=2.8708318598057035e+178\n",
      "Gradient Descent(94/99): loss=2.230596713407519e+180\n",
      "Gradient Descent(95/99): loss=1.73314284529403e+182\n",
      "Gradient Descent(96/99): loss=1.3466280588234302e+184\n",
      "Gradient Descent(97/99): loss=1.0463114068955449e+186\n",
      "Gradient Descent(98/99): loss=8.129695152470343e+187\n",
      "Gradient Descent(99/99): loss=6.316660875197524e+189\n",
      "Accuracy of predictions using least-squares gradient descent 0.62936\n",
      "Gradient Descent(0/99): loss=447.2235848312737\n",
      "Gradient Descent(1/99): loss=34647.7837324889\n",
      "Gradient Descent(2/99): loss=2694230.1306058452\n",
      "Gradient Descent(3/99): loss=209510166.39909616\n",
      "Gradient Descent(4/99): loss=16292045714.665215\n",
      "Gradient Descent(5/99): loss=1266911094744.5295\n",
      "Gradient Descent(6/99): loss=98518243205756.38\n",
      "Gradient Descent(7/99): loss=7661030268512546.0\n",
      "Gradient Descent(8/99): loss=5.957412847130074e+17\n",
      "Gradient Descent(9/99): loss=4.632636419284276e+19\n",
      "Gradient Descent(10/99): loss=3.6024564259664686e+21\n",
      "Gradient Descent(11/99): loss=2.8013621459618216e+23\n",
      "Gradient Descent(12/99): loss=2.1784107689026326e+25\n",
      "Gradient Descent(13/99): loss=1.693987864050922e+27\n",
      "Gradient Descent(14/99): loss=1.317288238066023e+29\n",
      "Gradient Descent(15/99): loss=1.0243569856501222e+31\n",
      "Gradient Descent(16/99): loss=7.965661604864453e+32\n",
      "Gradient Descent(17/99): loss=6.194301956455331e+34\n",
      "Gradient Descent(18/99): loss=4.816847442315985e+36\n",
      "Gradient Descent(19/99): loss=3.745703623370548e+38\n",
      "Gradient Descent(20/99): loss=2.912754826087121e+40\n",
      "Gradient Descent(21/99): loss=2.265032562629553e+42\n",
      "Gradient Descent(22/99): loss=1.7613471837120331e+44\n",
      "Gradient Descent(23/99): loss=1.3696685658102307e+46\n",
      "Gradient Descent(24/99): loss=1.0650892666231895e+48\n",
      "Gradient Descent(25/99): loss=8.282406227267573e+49\n",
      "Gradient Descent(26/99): loss=6.440610666462471e+51\n",
      "Gradient Descent(27/99): loss=5.008383387473048e+53\n",
      "Gradient Descent(28/99): loss=3.894646867343309e+55\n",
      "Gradient Descent(29/99): loss=3.028576897536625e+57\n",
      "Gradient Descent(30/99): loss=2.35509876420433e+59\n",
      "Gradient Descent(31/99): loss=1.831384962907235e+61\n",
      "Gradient Descent(32/99): loss=1.4241317321126656e+63\n",
      "Gradient Descent(33/99): loss=1.1074412160677782e+65\n",
      "Gradient Descent(34/99): loss=8.611745805469161e+66\n",
      "Gradient Descent(35/99): loss=6.696713535852068e+68\n",
      "Gradient Descent(36/99): loss=5.20753552116967e+70\n",
      "Gradient Descent(37/99): loss=4.049512654089281e+72\n",
      "Gradient Descent(38/99): loss=3.1490044895451293e+74\n",
      "Gradient Descent(39/99): loss=2.4487463362194485e+76\n",
      "Gradient Descent(40/99): loss=1.9042077072472873e+78\n",
      "Gradient Descent(41/99): loss=1.480760558457058e+80\n",
      "Gradient Descent(42/99): loss=1.15147723808542e+82\n",
      "Gradient Descent(43/99): loss=8.954181162215667e+83\n",
      "Gradient Descent(44/99): loss=6.963000017185774e+85\n",
      "Gradient Descent(45/99): loss=5.414606691666734e+87\n",
      "Gradient Descent(46/99): loss=4.210536486152641e+89\n",
      "Gradient Descent(47/99): loss=3.274220734907215e+91\n",
      "Gradient Descent(48/99): loss=2.5461176874142796e+93\n",
      "Gradient Descent(49/99): loss=1.979926157405984e+95\n",
      "Gradient Descent(50/99): loss=1.5396411596203459e+97\n",
      "Gradient Descent(51/99): loss=1.1972642977265403e+99\n",
      "Gradient Descent(52/99): loss=9.310233034846218e+100\n",
      "Gradient Descent(53/99): loss=7.239875049121417e+102\n",
      "Gradient Descent(54/99): loss=5.62991178960927e+104\n",
      "Gradient Descent(55/99): loss=4.377963230543317e+106\n",
      "Gradient Descent(56/99): loss=3.4044160484652217e+108\n",
      "Gradient Descent(57/99): loss=2.6473608892346054e+110\n",
      "Gradient Descent(58/99): loss=2.058655457522167e+112\n",
      "Gradient Descent(59/99): loss=1.600863074626402e+114\n",
      "Gradient Descent(60/99): loss=1.2448720228235284e+116\n",
      "Gradient Descent(61/99): loss=9.680442867172726e+117\n",
      "Gradient Descent(62/99): loss=7.527759672198798e+119\n",
      "Gradient Descent(63/99): loss=5.853778226877031e+121\n",
      "Gradient Descent(64/99): loss=4.5520474911030983e+123\n",
      "Gradient Descent(65/99): loss=3.5397884166706435e+125\n",
      "Gradient Descent(66/99): loss=2.752629901002882e+127\n",
      "Gradient Descent(67/99): loss=2.1405153302981285e+129\n",
      "Gradient Descent(68/99): loss=1.664519402906996e+131\n",
      "Gradient Descent(69/99): loss=1.2943728098728303e+133\n",
      "Gradient Descent(70/99): loss=1.0065373632846144e+135\n",
      "Gradient Descent(71/99): loss=7.827091669111019e+136\n",
      "Gradient Descent(72/99): loss=6.086546434475845e+138\n",
      "Gradient Descent(73/99): loss=4.733053995678848e+140\n",
      "Gradient Descent(74/99): loss=3.680543698660014e+142\n",
      "Gradient Descent(75/99): loss=2.8620848040426627e+144\n",
      "Gradient Descent(76/99): loss=2.225630259060439e+146\n",
      "Gradient Descent(77/99): loss=1.7307069458769393e+148\n",
      "Gradient Descent(78/99): loss=1.3458419341275361e+150\n",
      "Gradient Descent(79/99): loss=1.0465610691464394e+152\n",
      "Gradient Descent(80/99): loss=8.138326230434901e+153\n",
      "Gradient Descent(81/99): loss=6.328570380226697e+155\n",
      "Gradient Descent(82/99): loss=4.9212579986907455e+157\n",
      "Gradient Descent(83/99): loss=3.826895939302183e+159\n",
      "Gradient Descent(84/99): loss=2.9758920451120104e+161\n",
      "Gradient Descent(85/99): loss=2.3141296770603463e+163\n",
      "Gradient Descent(86/99): loss=1.7995263541389217e+165\n",
      "Gradient Descent(87/99): loss=1.399357664067515e+167\n",
      "Gradient Descent(88/99): loss=1.0881762678721713e+169\n",
      "Gradient Descent(89/99): loss=8.461936646834893e+170\n",
      "Gradient Descent(90/99): loss=6.58021810704093e+172\n",
      "Gradient Descent(91/99): loss=5.116945699708705e+174\n",
      "Gradient Descent(92/99): loss=3.979067694694046e+176\n",
      "Gradient Descent(93/99): loss=3.094224689517259e+178\n",
      "Gradient Descent(94/99): loss=2.4061481643011744e+180\n",
      "Gradient Descent(95/99): loss=1.8710822805415376e+182\n",
      "Gradient Descent(96/99): loss=1.455001380421369e+184\n",
      "Gradient Descent(97/99): loss=1.1314462431953328e+186\n",
      "Gradient Descent(98/99): loss=8.798415028789201e+187\n",
      "Gradient Descent(99/99): loss=6.841872292598077e+189\n",
      "Accuracy of predictions using least-squares gradient descent 0.62806\n",
      "Gradient Descent(0/99): loss=447.78118256749315\n",
      "Gradient Descent(1/99): loss=34661.883183113874\n",
      "Gradient Descent(2/99): loss=2693128.051157446\n",
      "Gradient Descent(3/99): loss=209253683.30778918\n",
      "Gradient Descent(4/99): loss=16258831273.678703\n",
      "Gradient Descent(5/99): loss=1263297213724.4097\n",
      "Gradient Descent(6/99): loss=98157107567558.67\n",
      "Gradient Descent(7/99): loss=7626722881488713.0\n",
      "Gradient Descent(8/99): loss=5.925898119093297e+17\n",
      "Gradient Descent(9/99): loss=4.604371899116247e+19\n",
      "Gradient Descent(10/99): loss=3.5775573861224557e+21\n",
      "Gradient Descent(11/99): loss=2.779731336092958e+23\n",
      "Gradient Descent(12/99): loss=2.1598273533864977e+25\n",
      "Gradient Descent(13/99): loss=1.6781672875599565e+27\n",
      "Gradient Descent(14/99): loss=1.3039215567950876e+29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(15/99): loss=1.0131358410322848e+31\n",
      "Gradient Descent(16/99): loss=7.871978394981742e+32\n",
      "Gradient Descent(17/99): loss=6.11645954484449e+34\n",
      "Gradient Descent(18/99): loss=4.752436488846097e+36\n",
      "Gradient Descent(19/99): loss=3.692602299569379e+38\n",
      "Gradient Descent(20/99): loss=2.869120244907397e+40\n",
      "Gradient Descent(21/99): loss=2.229281767142244e+42\n",
      "Gradient Descent(22/99): loss=1.7321327700133527e+44\n",
      "Gradient Descent(23/99): loss=1.345852272770454e+46\n",
      "Gradient Descent(24/99): loss=1.0457156469058876e+48\n",
      "Gradient Descent(25/99): loss=8.125120686037636e+49\n",
      "Gradient Descent(26/99): loss=6.313148928966743e+51\n",
      "Gradient Descent(27/99): loss=4.905262449553882e+53\n",
      "Gradient Descent(28/99): loss=3.811346757337075e+55\n",
      "Gradient Descent(29/99): loss=2.9613836678575416e+57\n",
      "Gradient Descent(30/99): loss=2.300969653671894e+59\n",
      "Gradient Descent(31/99): loss=1.7878336416129872e+61\n",
      "Gradient Descent(32/99): loss=1.389131371194938e+63\n",
      "Gradient Descent(33/99): loss=1.0793431343516684e+65\n",
      "Gradient Descent(34/99): loss=8.386403372849957e+66\n",
      "Gradient Descent(35/99): loss=6.516163330616344e+68\n",
      "Gradient Descent(36/99): loss=5.063002894510141e+70\n",
      "Gradient Descent(37/99): loss=3.933909727120601e+72\n",
      "Gradient Descent(38/99): loss=3.0566140418198165e+74\n",
      "Gradient Descent(39/99): loss=2.3749628356338818e+76\n",
      "Gradient Descent(40/99): loss=1.8453257079470774e+78\n",
      "Gradient Descent(41/99): loss=1.433802212530838e+80\n",
      "Gradient Descent(42/99): loss=1.1140519940760953e+82\n",
      "Gradient Descent(43/99): loss=8.656088229311734e+83\n",
      "Gradient Descent(44/99): loss=6.72570614585793e+85\n",
      "Gradient Descent(45/99): loss=5.225815860708729e+87\n",
      "Gradient Descent(46/99): loss=4.060414002305665e+89\n",
      "Gradient Descent(47/99): loss=3.154906776199338e+91\n",
      "Gradient Descent(48/99): loss=2.4513354453157933e+93\n",
      "Gradient Descent(49/99): loss=1.904666569165814e+95\n",
      "Gradient Descent(50/99): loss=1.4799095515997541e+97\n",
      "Gradient Descent(51/99): loss=1.14987700019086e+99\n",
      "Gradient Descent(52/99): loss=8.934445447281833e+100\n",
      "Gradient Descent(53/99): loss=6.941987311443284e+102\n",
      "Gradient Descent(54/99): loss=5.393864467201101e+104\n",
      "Gradient Descent(55/99): loss=4.1909863249931405e+106\n",
      "Gradient Descent(56/99): loss=3.256360348519014e+108\n",
      "Gradient Descent(57/99): loss=2.5301639989064437e+110\n",
      "Gradient Descent(58/99): loss=1.9659156776902128e+112\n",
      "Gradient Descent(59/99): loss=1.527499582421763e+114\n",
      "Gradient Descent(60/99): loss=1.1868540450524501e+116\n",
      "Gradient Descent(61/99): loss=9.221753907284648e+117\n",
      "Gradient Descent(62/99): loss=7.165223515142647e+119\n",
      "Gradient Descent(63/99): loss=5.567317078522075e+121\n",
      "Gradient Descent(64/99): loss=4.3257575129791846e+123\n",
      "Gradient Descent(65/99): loss=3.361076403081987e+125\n",
      "Gradient Descent(66/99): loss=2.611527473155627e+127\n",
      "Gradient Descent(67/99): loss=2.029134397775926e+129\n",
      "Gradient Descent(68/99): loss=1.576619984495978e+131\n",
      "Gradient Descent(69/99): loss=1.2250201752218276e+133\n",
      "Gradient Descent(70/99): loss=9.518301457914472e+134\n",
      "Gradient Descent(71/99): loss=7.395638412839312e+136\n",
      "Gradient Descent(72/99): loss=5.746347473370379e+138\n",
      "Gradient Descent(73/99): loss=4.4648625908189696e+140\n",
      "Gradient Descent(74/99): loss=3.469159852806886e+142\n",
      "Gradient Descent(75/99): loss=2.6955073844992715e+144\n",
      "Gradient Descent(76/99): loss=2.094386066992988e+146\n",
      "Gradient Descent(77/99): loss=1.627319970569909e+148\n",
      "Gradient Descent(78/99): loss=1.2644136285807857e+150\n",
      "Gradient Descent(79/99): loss=9.824385204225883e+151\n",
      "Gradient Descent(80/99): loss=7.633462852606863e+153\n",
      "Gradient Descent(81/99): loss=5.931135018714837e+155\n",
      "Gradient Descent(82/99): loss=4.608440925105448e+157\n",
      "Gradient Descent(83/99): loss=3.5807189843384534e+159\n",
      "Gradient Descent(84/99): loss=2.782187870729488e+161\n",
      "Gradient Descent(85/99): loss=2.161736059682552e+163\n",
      "Gradient Descent(86/99): loss=1.6796503359446268e+165\n",
      "Gradient Descent(87/99): loss=1.305073872641591e+167\n",
      "Gradient Descent(88/99): loss=1.0140311805396316e+169\n",
      "Gradient Descent(89/99): loss=7.878935105989912e+170\n",
      "Gradient Descent(90/99): loss=6.121864849497527e+172\n",
      "Gradient Descent(91/99): loss=4.75663636409715e+174\n",
      "Gradient Descent(92/99): loss=3.695865566537005e+176\n",
      "Gradient Descent(93/99): loss=2.8716557752899927e+178\n",
      "Gradient Descent(94/99): loss=2.2312518524539105e+180\n",
      "Gradient Descent(95/99): loss=1.733663509365525e+182\n",
      "Gradient Descent(96/99): loss=1.3470416440887697e+184\n",
      "Gradient Descent(97/99): loss=1.046639778196302e+186\n",
      "Gradient Descent(98/99): loss=8.132301106725212e+187\n",
      "Gradient Descent(99/99): loss=6.318728054117594e+189\n",
      "Accuracy of predictions using least-squares gradient descent 0.63074\n",
      "Gradient Descent(0/99): loss=444.84101404711083\n",
      "Gradient Descent(1/99): loss=34327.48443715451\n",
      "Gradient Descent(2/99): loss=2658940.590531215\n",
      "Gradient Descent(3/99): loss=205961796.86898196\n",
      "Gradient Descent(4/99): loss=15953824749.02985\n",
      "Gradient Descent(5/99): loss=1235785124605.4563\n",
      "Gradient Descent(6/99): loss=95724059795584.92\n",
      "Gradient Descent(7/99): loss=7414796829406517.0\n",
      "Gradient Descent(8/99): loss=5.743510266779638e+17\n",
      "Gradient Descent(9/99): loss=4.4489297473094935e+19\n",
      "Gradient Descent(10/99): loss=3.446146167958932e+21\n",
      "Gradient Descent(11/99): loss=2.6693888385448637e+23\n",
      "Gradient Descent(12/99): loss=2.0677117057887944e+25\n",
      "Gradient Descent(13/99): loss=1.601651897438274e+27\n",
      "Gradient Descent(14/99): loss=1.2406414266485238e+29\n",
      "Gradient Descent(15/99): loss=9.610022951793058e+30\n",
      "Gradient Descent(16/99): loss=7.44393497994591e+32\n",
      "Gradient Descent(17/99): loss=5.766080712151048e+34\n",
      "Gradient Descent(18/99): loss=4.4664128406024385e+36\n",
      "Gradient Descent(19/99): loss=3.4596885924020094e+38\n",
      "Gradient Descent(20/99): loss=2.679878816303554e+40\n",
      "Gradient Descent(21/99): loss=2.0758372547878294e+42\n",
      "Gradient Descent(22/99): loss=1.607945957164113e+44\n",
      "Gradient Descent(23/99): loss=1.245516812648549e+46\n",
      "Gradient Descent(24/99): loss=9.647787748577017e+47\n",
      "Gradient Descent(25/99): loss=7.473187635553632e+49\n",
      "Gradient Descent(26/99): loss=5.7887398532818024e+51\n",
      "Gradient Descent(27/99): loss=4.483964637733953e+53\n",
      "Gradient Descent(28/99): loss=3.473284234918533e+55\n",
      "Gradient Descent(29/99): loss=2.6904100168439952e+57\n",
      "Gradient Descent(30/99): loss=2.083994735001675e+59\n",
      "Gradient Descent(31/99): loss=1.6142647508461718e+61\n",
      "Gradient Descent(32/99): loss=1.250411357599871e+63\n",
      "Gradient Descent(33/99): loss=9.685700950821026e+64\n",
      "Gradient Descent(34/99): loss=7.502555246203672e+66\n",
      "Gradient Descent(35/99): loss=5.811488038722329e+68\n",
      "Gradient Descent(36/99): loss=4.501585408691588e+70\n",
      "Gradient Descent(37/99): loss=3.486933304641188e+72\n",
      "Gradient Descent(38/99): loss=2.7009826021605966e+74\n",
      "Gradient Descent(39/99): loss=2.0921842719113683e+76\n",
      "Gradient Descent(40/99): loss=1.6206083756821812e+78\n",
      "Gradient Descent(41/99): loss=1.255325136792007e+80\n",
      "Gradient Descent(42/99): loss=9.723763141718509e+81\n",
      "Gradient Descent(43/99): loss=7.532038263638328e+83\n",
      "Gradient Descent(44/99): loss=5.834325618392756e+85\n",
      "Gradient Descent(45/99): loss=4.519275424523857e+87\n",
      "Gradient Descent(46/99): loss=3.50063601152444e+89\n",
      "Gradient Descent(47/99): loss=2.7115967348842953e+91\n",
      "Gradient Descent(48/99): loss=2.100405991491021e+93\n",
      "Gradient Descent(49/99): loss=1.626976929251842e+95\n",
      "Gradient Descent(50/99): loss=1.2602582258102844e+97\n",
      "Gradient Descent(51/99): loss=9.761974906754369e+98\n",
      "Gradient Descent(52/99): loss=7.56163714137482e+100\n",
      "Gradient Descent(53/99): loss=5.857252943587972e+102\n",
      "Gradient Descent(54/99): loss=4.537034957344286e+104\n",
      "Gradient Descent(55/99): loss=3.5143925663477247e+106\n",
      "Gradient Descent(56/99): loss=2.7222525782851065e+108\n",
      "Gradient Descent(57/99): loss=2.1086600202097933e+110\n",
      "Gradient Descent(58/99): loss=1.6333705095183456e+112\n",
      "Gradient Descent(59/99): loss=1.265210700537107e+114\n",
      "Gradient Descent(60/99): loss=9.800336833714637e+115\n",
      "Gradient Descent(61/99): loss=7.591352334713134e+117\n",
      "Gradient Descent(62/99): loss=5.880270366984036e+119\n",
      "Gradient Descent(63/99): loss=4.554864280336042e+121\n",
      "Gradient Descent(64/99): loss=3.528203180719088e+123\n",
      "Gradient Descent(65/99): loss=2.732950296274887e+125\n",
      "Gradient Descent(66/99): loss=2.1169464850340015e+127\n",
      "Gradient Descent(67/99): loss=1.639789214829896e+129\n",
      "Gradient Descent(68/99): loss=1.2701826371531098e+131\n",
      "Gradient Descent(69/99): loss=9.83884951269536e+132\n",
      "Gradient Descent(70/99): loss=7.621184300742147e+134\n",
      "Gradient Descent(71/99): loss=5.903378242642415e+136\n",
      "Gradient Descent(72/99): loss=4.572763667755707e+138\n",
      "Gradient Descent(73/99): loss=3.5420680670779538e+140\n",
      "Gradient Descent(74/99): loss=2.743690053409446e+142\n",
      "Gradient Descent(75/99): loss=2.1252655134287195e+144\n",
      "Gradient Descent(76/99): loss=1.646233143921165e+146\n",
      "Gradient Descent(77/99): loss=1.2751741121382657e+148\n",
      "Gradient Descent(78/99): loss=9.8775135361112e+149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(79/99): loss=7.651133498347021e+151\n",
      "Gradient Descent(80/99): loss=5.926576926016048e+153\n",
      "Gradient Descent(81/99): loss=4.590733394937428e+155\n",
      "Gradient Descent(82/99): loss=3.555987438698519e+157\n",
      "Gradient Descent(83/99): loss=2.7544720148912254e+159\n",
      "Gradient Descent(84/99): loss=2.1336172333600118e+161\n",
      "Gradient Descent(85/99): loss=1.65270239591474e+163\n",
      "Gradient Descent(86/99): loss=1.2801852022730738e+165\n",
      "Gradient Descent(87/99): loss=9.916329498704828e+166\n",
      "Gradient Descent(88/99): loss=7.681200388216165e+168\n",
      "Gradient Descent(89/99): loss=5.949866773954815e+170\n",
      "Gradient Descent(90/99): loss=4.608773738297512e+172\n",
      "Gradient Descent(91/99): loss=3.5699615096931715e+174\n",
      "Gradient Descent(92/99): loss=2.765296346571895e+176\n",
      "Gradient Descent(93/99): loss=2.142001773296742e+178\n",
      "Gradient Descent(94/99): loss=1.659197070322781e+180\n",
      "Gradient Descent(95/99): loss=1.2852159846397787e+182\n",
      "Gradient Descent(96/99): loss=9.955297997556518e+183\n",
      "Gradient Descent(97/99): loss=7.711385432848408e+185\n",
      "Gradient Descent(98/99): loss=5.9732481447107774e+187\n",
      "Gradient Descent(99/99): loss=4.626884975338489e+189\n",
      "Accuracy of predictions using least-squares gradient descent 0.62668\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters for gradient descent\n",
    "max_iters = 100\n",
    "gamma = .8\n",
    "\n",
    "num_samples, num_dim = tx.shape\n",
    "# Initial weights vector to train a linear model\n",
    "initial_w = np.random.randn(num_dim)\n",
    "\n",
    "res = {\n",
    "    'weights': {},\n",
    "    'accuracy': {},\n",
    "    'loss': {}\n",
    "}\n",
    "n_iter = 0\n",
    "\n",
    "for train_data, eval_data in train_eval_split(y, tx, train_size=.8, num_splits=5):\n",
    "    # Get training data\n",
    "    y_train, tx_train = train_data\n",
    "    \n",
    "    # Run gradient descent under MSE loss to find optimal weights\n",
    "    final_w, final_loss = least_squares_GD(y_train, tx_train, initial_w, max_iters, gamma)\n",
    "    \n",
    "    # Get validation set\n",
    "    y_eval, tx_eval = eval_data\n",
    "    \n",
    "    # Get predictions from current model\n",
    "    y_pred = predict_labels(final_w, tx_eval)\n",
    "    \n",
    "    acc = get_accuracy(y_pred, y_eval)\n",
    "    \n",
    "    print('Accuracy of predictions using least-squares gradient descent', acc)\n",
    "    \n",
    "    res['weights'][n_iter] = w\n",
    "    res['loss'][n_iter] = loss\n",
    "    res['accuracy'][n_iter] = acc\n",
    "    \n",
    "    n_iter += 1\n",
    "\n",
    "# Select model with highest accuracy on validation set\n",
    "iter_max_acc = max(res['accuracy'], key=res['accuracy'].get)\n",
    "w_max_acc = res['weights'][iter_max_acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from current model\n",
    "y_test_pred = predict_labels(w_max_acc, tx_test)\n",
    "\n",
    "# Save in submission file\n",
    "create_csv_submission(ids_test, y_test_pred, '../data/test_gd_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weights': {0: array([-6.51882623e+45,  6.61806111e+45,  9.54326342e+44, -2.12129413e+46,\n",
       "         -2.53441436e+46, -2.45930336e+46, -2.53424448e+46,  1.19929520e+46,\n",
       "         -1.01660450e+46, -2.40068198e+46, -1.54773217e+45, -1.48437282e+46,\n",
       "         -2.53444675e+46, -8.70184270e+45, -1.91063115e+44, -1.31385745e+44,\n",
       "         -7.49727759e+45, -4.09575238e+44,  3.34459667e+43, -1.25714311e+46,\n",
       "         -2.22472410e+44, -2.25745208e+46, -2.62799025e+46, -2.14062558e+46,\n",
       "         -2.04440605e+46, -2.04441732e+46, -2.54758582e+46, -2.53443595e+46,\n",
       "         -2.53442253e+46, -2.46622080e+46]),\n",
       "  1: array([-6.51882623e+45,  6.61806111e+45,  9.54326342e+44, -2.12129413e+46,\n",
       "         -2.53441436e+46, -2.45930336e+46, -2.53424448e+46,  1.19929520e+46,\n",
       "         -1.01660450e+46, -2.40068198e+46, -1.54773217e+45, -1.48437282e+46,\n",
       "         -2.53444675e+46, -8.70184270e+45, -1.91063115e+44, -1.31385745e+44,\n",
       "         -7.49727759e+45, -4.09575238e+44,  3.34459667e+43, -1.25714311e+46,\n",
       "         -2.22472410e+44, -2.25745208e+46, -2.62799025e+46, -2.14062558e+46,\n",
       "         -2.04440605e+46, -2.04441732e+46, -2.54758582e+46, -2.53443595e+46,\n",
       "         -2.53442253e+46, -2.46622080e+46]),\n",
       "  2: array([-6.51882623e+45,  6.61806111e+45,  9.54326342e+44, -2.12129413e+46,\n",
       "         -2.53441436e+46, -2.45930336e+46, -2.53424448e+46,  1.19929520e+46,\n",
       "         -1.01660450e+46, -2.40068198e+46, -1.54773217e+45, -1.48437282e+46,\n",
       "         -2.53444675e+46, -8.70184270e+45, -1.91063115e+44, -1.31385745e+44,\n",
       "         -7.49727759e+45, -4.09575238e+44,  3.34459667e+43, -1.25714311e+46,\n",
       "         -2.22472410e+44, -2.25745208e+46, -2.62799025e+46, -2.14062558e+46,\n",
       "         -2.04440605e+46, -2.04441732e+46, -2.54758582e+46, -2.53443595e+46,\n",
       "         -2.53442253e+46, -2.46622080e+46]),\n",
       "  3: array([-6.51882623e+45,  6.61806111e+45,  9.54326342e+44, -2.12129413e+46,\n",
       "         -2.53441436e+46, -2.45930336e+46, -2.53424448e+46,  1.19929520e+46,\n",
       "         -1.01660450e+46, -2.40068198e+46, -1.54773217e+45, -1.48437282e+46,\n",
       "         -2.53444675e+46, -8.70184270e+45, -1.91063115e+44, -1.31385745e+44,\n",
       "         -7.49727759e+45, -4.09575238e+44,  3.34459667e+43, -1.25714311e+46,\n",
       "         -2.22472410e+44, -2.25745208e+46, -2.62799025e+46, -2.14062558e+46,\n",
       "         -2.04440605e+46, -2.04441732e+46, -2.54758582e+46, -2.53443595e+46,\n",
       "         -2.53442253e+46, -2.46622080e+46]),\n",
       "  4: array([-6.51882623e+45,  6.61806111e+45,  9.54326342e+44, -2.12129413e+46,\n",
       "         -2.53441436e+46, -2.45930336e+46, -2.53424448e+46,  1.19929520e+46,\n",
       "         -1.01660450e+46, -2.40068198e+46, -1.54773217e+45, -1.48437282e+46,\n",
       "         -2.53444675e+46, -8.70184270e+45, -1.91063115e+44, -1.31385745e+44,\n",
       "         -7.49727759e+45, -4.09575238e+44,  3.34459667e+43, -1.25714311e+46,\n",
       "         -2.22472410e+44, -2.25745208e+46, -2.62799025e+46, -2.14062558e+46,\n",
       "         -2.04440605e+46, -2.04441732e+46, -2.54758582e+46, -2.53443595e+46,\n",
       "         -2.53442253e+46, -2.46622080e+46])},\n",
       " 'accuracy': {0: 0.62884, 1: 0.62936, 2: 0.62806, 3: 0.63074, 4: 0.62668},\n",
       " 'loss': {0: 5.79119569837263e+94,\n",
       "  1: 5.79119569837263e+94,\n",
       "  2: 5.79119569837263e+94,\n",
       "  3: 5.79119569837263e+94,\n",
       "  4: 5.79119569837263e+94}}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
