{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from scripts.proj1_helpers import *\n",
    "from scripts.implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, tx, ids = load_csv_data('../data/train.csv')\n",
    "tx, mean_tx, std_tx = standardise(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: y: (250000,), x:(250000, 30)\n",
      "\n",
      "[-2.50602916e-15  4.49575133e-15 -3.48448848e-15  7.18646387e-15\n",
      " -2.36304576e-14 -3.26035021e-15  1.26038877e-14  2.16223188e-14\n",
      "  6.40057962e-15  2.86143687e-15 -6.98486646e-15  3.63458152e-15\n",
      " -1.27422117e-14 -5.95722149e-15  1.35646161e-16  7.13136217e-17\n",
      "  2.58023760e-14 -1.06327391e-16 -1.87188487e-16  8.24115935e-15\n",
      "  1.41040513e-16 -8.99509711e-15 -6.01698247e-16 -4.92204144e-15\n",
      "  3.11615622e-15 -1.67606551e-15 -9.40773592e-15  1.79148900e-14\n",
      " -5.09692022e-15 -1.77122317e-15] [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Check shape of data\n",
    "print('Shape: y: {}, x:{}\\n'.format(y.shape, tx.shape))\n",
    "\n",
    "# Check that data is normalised\n",
    "print(np.mean(tx, axis=0), np.std(tx, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=2.1032328292825775\n",
      "Gradient Descent(1/49): loss=6.328695973678255\n",
      "Gradient Descent(2/49): loss=374.37971234251586\n",
      "Gradient Descent(3/49): loss=28988.45404960595\n",
      "Gradient Descent(4/49): loss=2251840.533409206\n",
      "Gradient Descent(5/49): loss=174930364.15147564\n",
      "Gradient Descent(6/49): loss=13589170021.248972\n",
      "Gradient Descent(7/49): loss=1055651736298.777\n",
      "Gradient Descent(8/49): loss=82006523331592.3\n",
      "Gradient Descent(9/49): loss=6370538348676969.0\n",
      "Gradient Descent(10/49): loss=4.9488451897739955e+17\n",
      "Gradient Descent(11/49): loss=3.844426855610303e+19\n",
      "Gradient Descent(12/49): loss=2.986478113859284e+21\n",
      "Gradient Descent(13/49): loss=2.319995114888081e+23\n",
      "Gradient Descent(14/49): loss=1.8022490465028724e+25\n",
      "Gradient Descent(15/49): loss=1.400046752157572e+27\n",
      "Gradient Descent(16/49): loss=1.0876026884466648e+29\n",
      "Gradient Descent(17/49): loss=8.448857912019872e+30\n",
      "Gradient Descent(18/49): loss=6.563352663227784e+32\n",
      "Gradient Descent(19/49): loss=5.098629735578119e+34\n",
      "Gradient Descent(20/49): loss=3.9607844518501164e+36\n",
      "Gradient Descent(21/49): loss=3.07686854853339e+38\n",
      "Gradient Descent(22/49): loss=2.39021339839178e+40\n",
      "Gradient Descent(23/49): loss=1.8567969348494798e+42\n",
      "Gradient Descent(24/49): loss=1.4424213585222874e+44\n",
      "Gradient Descent(25/49): loss=1.1205206861728947e+46\n",
      "Gradient Descent(26/49): loss=8.70457582122655e+47\n",
      "Gradient Descent(27/49): loss=6.762002804809491e+49\n",
      "Gradient Descent(28/49): loss=5.2529477451100104e+51\n",
      "Gradient Descent(29/49): loss=4.080663792867802e+53\n",
      "Gradient Descent(30/49): loss=3.1699947911957353e+55\n",
      "Gradient Descent(31/49): loss=2.462556948154268e+57\n",
      "Gradient Descent(32/49): loss=1.9129958004175443e+59\n",
      "Gradient Descent(33/49): loss=1.4860784986751453e+61\n",
      "Gradient Descent(34/49): loss=1.1544349986249487e+63\n",
      "Gradient Descent(35/49): loss=8.968033433215757e+64\n",
      "Gradient Descent(36/49): loss=6.966665403861698e+66\n",
      "Gradient Descent(37/49): loss=5.411936430745453e+68\n",
      "Gradient Descent(38/49): loss=4.2041714697816045e+70\n",
      "Gradient Descent(39/49): loss=3.265939645357386e+72\n",
      "Gradient Descent(40/49): loss=2.537090088685494e+74\n",
      "Gradient Descent(41/49): loss=1.9708956126168024e+76\n",
      "Gradient Descent(42/49): loss=1.5310569905086517e+78\n",
      "Gradient Descent(43/49): loss=1.1893757808274147e+80\n",
      "Gradient Descent(44/49): loss=9.23946500220637e+81\n",
      "Gradient Descent(45/49): loss=7.177522436820472e+83\n",
      "Gradient Descent(46/49): loss=5.5757371578072454e+85\n",
      "Gradient Descent(47/49): loss=4.3314172998564545e+87\n",
      "Gradient Descent(48/49): loss=3.3647884207070637e+89\n",
      "Gradient Descent(49/49): loss=2.6138790913771876e+91\n",
      "Accuracy of predictions using least-squares gradient descent 0.3746\n",
      "Gradient Descent(0/49): loss=8.36168418553999\n",
      "Gradient Descent(1/49): loss=500.55170733734155\n",
      "Gradient Descent(2/49): loss=38778.02954370062\n",
      "Gradient Descent(3/49): loss=3012319.0692389356\n",
      "Gradient Descent(4/49): loss=234006848.81242156\n",
      "Gradient Descent(5/49): loss=18178427012.725636\n",
      "Gradient Descent(6/49): loss=1412160419607.8892\n",
      "Gradient Descent(7/49): loss=109701298654200.44\n",
      "Gradient Descent(8/49): loss=8521960224437677.0\n",
      "Gradient Descent(9/49): loss=6.620140960757637e+17\n",
      "Gradient Descent(10/49): loss=5.142744766001638e+19\n",
      "Gradient Descent(11/49): loss=3.9950544686302767e+21\n",
      "Gradient Descent(12/49): loss=3.103490632635817e+23\n",
      "Gradient Descent(13/49): loss=2.4108943150806156e+25\n",
      "Gradient Descent(14/49): loss=1.8728625559122297e+27\n",
      "Gradient Descent(15/49): loss=1.4549016650781145e+29\n",
      "Gradient Descent(16/49): loss=1.1302158016694693e+31\n",
      "Gradient Descent(17/49): loss=8.779890689552357e+32\n",
      "Gradient Descent(18/49): loss=6.820509889051436e+34\n",
      "Gradient Descent(19/49): loss=5.298397985979989e+36\n",
      "Gradient Descent(20/49): loss=4.115971045346722e+38\n",
      "Gradient Descent(21/49): loss=3.1974226343435264e+40\n",
      "Gradient Descent(22/49): loss=2.483863805157339e+42\n",
      "Gradient Descent(23/49): loss=1.929547672648331e+44\n",
      "Gradient Descent(24/49): loss=1.4989365412435638e+46\n",
      "Gradient Descent(25/49): loss=1.1644235519672e+48\n",
      "Gradient Descent(26/49): loss=9.045627823917245e+49\n",
      "Gradient Descent(27/49): loss=7.026943296585873e+51\n",
      "Gradient Descent(28/49): loss=5.458762294295848e+53\n",
      "Gradient Descent(29/49): loss=4.240547351521096e+55\n",
      "Gradient Descent(30/49): loss=3.2941976351092005e+57\n",
      "Gradient Descent(31/49): loss=2.5590418310660977e+59\n",
      "Gradient Descent(32/49): loss=1.9879484531683186e+61\n",
      "Gradient Descent(33/49): loss=1.5443042018613415e+63\n",
      "Gradient Descent(34/49): loss=1.1996666533710719e+65\n",
      "Gradient Descent(35/49): loss=9.319407908596467e+66\n",
      "Gradient Descent(36/49): loss=7.239624734317407e+68\n",
      "Gradient Descent(37/49): loss=5.623980279411723e+70\n",
      "Gradient Descent(38/49): loss=4.368894154593829e+72\n",
      "Gradient Descent(39/49): loss=3.393901682749238e+74\n",
      "Gradient Descent(40/49): loss=2.6364952376006915e+76\n",
      "Gradient Descent(41/49): loss=2.0481168247220404e+78\n",
      "Gradient Descent(42/49): loss=1.5910449857390556e+80\n",
      "Gradient Descent(43/49): loss=1.2359764424028662e+82\n",
      "Gradient Descent(44/49): loss=9.601474376070114e+83\n",
      "Gradient Descent(45/49): loss=7.458743308659486e+85\n",
      "Gradient Descent(46/49): loss=5.794198845452979e+87\n",
      "Gradient Descent(47/49): loss=4.5011255745551243e+89\n",
      "Gradient Descent(48/49): loss=3.4966234294519054e+91\n",
      "Gradient Descent(49/49): loss=2.7162928927173086e+93\n",
      "Accuracy of predictions using least-squares gradient descent 0.62762\n",
      "Gradient Descent(0/49): loss=5.2785836074043075\n",
      "Gradient Descent(1/49): loss=29.320939713582803\n",
      "Gradient Descent(2/49): loss=2042.3855685297801\n",
      "Gradient Descent(3/49): loss=158492.174817376\n",
      "Gradient Descent(4/49): loss=12312070.527180038\n",
      "Gradient Descent(5/49): loss=956442317.8640625\n",
      "Gradient Descent(6/49): loss=74299607031.79489\n",
      "Gradient Descent(7/49): loss=5771839564130.792\n",
      "Gradient Descent(8/49): loss=448375614425494.44\n",
      "Gradient Descent(9/49): loss=3.4831302806966212e+16\n",
      "Gradient Descent(10/49): loss=2.70581096785351e+18\n",
      "Gradient Descent(11/49): loss=2.1019635798096432e+20\n",
      "Gradient Descent(12/49): loss=1.6328749285657285e+22\n",
      "Gradient Descent(13/49): loss=1.2684713274527714e+24\n",
      "Gradient Descent(14/49): loss=9.85390540586665e+25\n",
      "Gradient Descent(15/49): loss=7.654840093449602e+27\n",
      "Gradient Descent(16/49): loss=5.946533322858786e+29\n",
      "Gradient Descent(17/49): loss=4.619464042120132e+31\n",
      "Gradient Descent(18/49): loss=3.5885526705804937e+33\n",
      "Gradient Descent(19/49): loss=2.7877065720420738e+35\n",
      "Gradient Descent(20/49): loss=2.1655827976323013e+37\n",
      "Gradient Descent(21/49): loss=1.6822964441216624e+39\n",
      "Gradient Descent(22/49): loss=1.3068635976415447e+41\n",
      "Gradient Descent(23/49): loss=1.0152149276712597e+43\n",
      "Gradient Descent(24/49): loss=7.886525810547998e+44\n",
      "Gradient Descent(25/49): loss=6.126514461633309e+46\n",
      "Gradient Descent(26/49): loss=4.7592793519297204e+48\n",
      "Gradient Descent(27/49): loss=3.697165834105638e+50\n",
      "Gradient Descent(28/49): loss=2.8720808748777663e+52\n",
      "Gradient Descent(29/49): loss=2.2311275506617978e+54\n",
      "Gradient Descent(30/49): loss=1.7332137792024955e+56\n",
      "Gradient Descent(31/49): loss=1.346417869980743e+58\n",
      "Gradient Descent(32/49): loss=1.0459419965133398e+60\n",
      "Gradient Descent(33/49): loss=8.125223858518417e+61\n",
      "Gradient Descent(34/49): loss=6.31194301128676e+63\n",
      "Gradient Descent(35/49): loss=4.903326390935446e+65\n",
      "Gradient Descent(36/49): loss=3.8090663450306594e+67\n",
      "Gradient Descent(37/49): loss=2.9590088980548993e+69\n",
      "Gradient Descent(38/49): loss=2.298656118235098e+71\n",
      "Gradient Descent(39/49): loss=1.7856722071274877e+73\n",
      "Gradient Descent(40/49): loss=1.3871693142842803e+75\n",
      "Gradient Descent(41/49): loss=1.0775990681892036e+77\n",
      "Gradient Descent(42/49): loss=8.371146476530689e+78\n",
      "Gradient Descent(43/49): loss=6.502983846235913e+80\n",
      "Gradient Descent(44/49): loss=5.051733239045076e+82\n",
      "Gradient Descent(45/49): loss=3.9243537000702075e+84\n",
      "Gradient Descent(46/49): loss=3.048567933916849e+86\n",
      "Gradient Descent(47/49): loss=2.3682285436044708e+88\n",
      "Gradient Descent(48/49): loss=1.8397183714836968e+90\n",
      "Gradient Descent(49/49): loss=1.4291541648354852e+92\n",
      "Accuracy of predictions using least-squares gradient descent 0.62856\n",
      "Gradient Descent(0/49): loss=7385.213767162818\n",
      "Gradient Descent(1/49): loss=573615.6646945088\n",
      "Gradient Descent(2/49): loss=44560298.88575104\n",
      "Gradient Descent(3/49): loss=3461591550.922416\n",
      "Gradient Descent(4/49): loss=268907896866.92618\n",
      "Gradient Descent(5/49): loss=20889656086530.363\n",
      "Gradient Descent(6/49): loss=1622777674061488.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(7/49): loss=1.260627445719679e+17\n",
      "Gradient Descent(8/49): loss=9.792971534568327e+18\n",
      "Gradient Descent(9/49): loss=7.607504644016046e+20\n",
      "Gradient Descent(10/49): loss=5.909761577927081e+22\n",
      "Gradient Descent(11/49): loss=4.5908985327291005e+24\n",
      "Gradient Descent(12/49): loss=3.566362036758696e+26\n",
      "Gradient Descent(13/49): loss=2.7704681527066376e+28\n",
      "Gradient Descent(14/49): loss=2.1521914225336447e+30\n",
      "Gradient Descent(15/49): loss=1.6718935804052505e+32\n",
      "Gradient Descent(16/49): loss=1.298782308550251e+34\n",
      "Gradient Descent(17/49): loss=1.0089371146423308e+36\n",
      "Gradient Descent(18/49): loss=7.837757679645886e+37\n",
      "Gradient Descent(19/49): loss=6.088629762284533e+39\n",
      "Gradient Descent(20/49): loss=4.729849262685054e+41\n",
      "Gradient Descent(21/49): loss=3.674303566017462e+43\n",
      "Gradient Descent(22/49): loss=2.854320707798783e+45\n",
      "Gradient Descent(23/49): loss=2.217330864635008e+47\n",
      "Gradient Descent(24/49): loss=1.7224960565326766e+49\n",
      "Gradient Descent(25/49): loss=1.3380919880258913e+51\n",
      "Gradient Descent(26/49): loss=1.0394741756467652e+53\n",
      "Gradient Descent(27/49): loss=8.074979683800362e+54\n",
      "Gradient Descent(28/49): loss=6.272911672213301e+56\n",
      "Gradient Descent(29/49): loss=4.8730055539744195e+58\n",
      "Gradient Descent(30/49): loss=3.78551211461378e+60\n",
      "Gradient Descent(31/49): loss=2.940711191720276e+62\n",
      "Gradient Descent(32/49): loss=2.2844418539104703e+64\n",
      "Gradient Descent(33/49): loss=1.7746300958051844e+66\n",
      "Gradient Descent(34/49): loss=1.3785914364800192e+68\n",
      "Gradient Descent(35/49): loss=1.0709354886003798e+70\n",
      "Gradient Descent(36/49): loss=8.31938158322036e+71\n",
      "Gradient Descent(37/49): loss=6.462771162591777e+73\n",
      "Gradient Descent(38/49): loss=5.0204946944939145e+75\n",
      "Gradient Descent(39/49): loss=3.900086564001642e+77\n",
      "Gradient Descent(40/49): loss=3.029716418859666e+79\n",
      "Gradient Descent(41/49): loss=2.3535840623213084e+81\n",
      "Gradient Descent(42/49): loss=1.828342053378656e+83\n",
      "Gradient Descent(43/49): loss=1.420316664133044e+85\n",
      "Gradient Descent(44/49): loss=1.1033490274350972e+87\n",
      "Gradient Descent(45/49): loss=8.571180688674408e+88\n",
      "Gradient Descent(46/49): loss=6.6583770476226e+90\n",
      "Gradient Descent(47/49): loss=5.1724478247073155e+92\n",
      "Gradient Descent(48/49): loss=4.01812878843686e+94\n",
      "Gradient Descent(49/49): loss=3.1214155285130574e+96\n",
      "Accuracy of predictions using least-squares gradient descent 0.36948\n",
      "Gradient Descent(0/49): loss=2.5774589321599803\n",
      "Gradient Descent(1/49): loss=50.48265245445132\n",
      "Gradient Descent(2/49): loss=3821.787554620628\n",
      "Gradient Descent(3/49): loss=296811.34583010315\n",
      "Gradient Descent(4/49): loss=23057223.05469729\n",
      "Gradient Descent(5/49): loss=1791161398.1138985\n",
      "Gradient Descent(6/49): loss=139143349913.62738\n",
      "Gradient Descent(7/49): loss=10809116282128.57\n",
      "Gradient Descent(8/49): loss=839687953995701.0\n",
      "Gradient Descent(9/49): loss=6.522974142217947e+16\n",
      "Gradient Descent(10/49): loss=5.067262363068539e+18\n",
      "Gradient Descent(11/49): loss=3.936417237956403e+20\n",
      "Gradient Descent(12/49): loss=3.0579392897069437e+22\n",
      "Gradient Descent(13/49): loss=2.3755085231736875e+24\n",
      "Gradient Descent(14/49): loss=1.845373700735462e+26\n",
      "Gradient Descent(15/49): loss=1.4335474119101335e+28\n",
      "Gradient Descent(16/49): loss=1.1136271105279119e+30\n",
      "Gradient Descent(17/49): loss=8.651024242374323e+31\n",
      "Gradient Descent(18/49): loss=6.720402164659127e+33\n",
      "Gradient Descent(19/49): loss=5.220630989973938e+35\n",
      "Gradient Descent(20/49): loss=4.055559067105116e+37\n",
      "Gradient Descent(21/49): loss=3.1504926087221014e+39\n",
      "Gradient Descent(22/49): loss=2.4474070068710847e+41\n",
      "Gradient Descent(23/49): loss=1.9012268242429746e+43\n",
      "Gradient Descent(24/49): loss=1.4769359681789215e+45\n",
      "Gradient Descent(25/49): loss=1.1473327781229677e+47\n",
      "Gradient Descent(26/49): loss=8.912861032008528e+48\n",
      "Gradient Descent(27/49): loss=6.9238056552221755e+50\n",
      "Gradient Descent(28/49): loss=5.37864155843163e+52\n",
      "Gradient Descent(29/49): loss=4.178306910198703e+54\n",
      "Gradient Descent(30/49): loss=3.2458471988055262e+56\n",
      "Gradient Descent(31/49): loss=2.521481610716028e+58\n",
      "Gradient Descent(32/49): loss=1.9587704299570112e+60\n",
      "Gradient Descent(33/49): loss=1.521637747016697e+62\n",
      "Gradient Descent(34/49): loss=1.1820586005052483e+64\n",
      "Gradient Descent(35/49): loss=9.182622721918403e+65\n",
      "Gradient Descent(36/49): loss=7.133365470802473e+67\n",
      "Gradient Descent(37/49): loss=5.541434564068215e+69\n",
      "Gradient Descent(38/49): loss=4.3047699088934473e+71\n",
      "Gradient Descent(39/49): loss=3.3440878448107354e+73\n",
      "Gradient Descent(40/49): loss=2.597798198390932e+75\n",
      "Gradient Descent(41/49): loss=2.0180556829675863e+77\n",
      "Gradient Descent(42/49): loss=1.567692495160062e+79\n",
      "Gradient Descent(43/49): loss=1.2178354542562211e+81\n",
      "Gradient Descent(44/49): loss=9.460549171615617e+82\n",
      "Gradient Descent(45/49): loss=7.3492679422128915e+84\n",
      "Gradient Descent(46/49): loss=5.709154754830721e+86\n",
      "Gradient Descent(47/49): loss=4.435060508188777e+88\n",
      "Gradient Descent(48/49): loss=3.445301897737582e+90\n",
      "Gradient Descent(49/49): loss=2.676424627045638e+92\n",
      "Accuracy of predictions using least-squares gradient descent 0.62788\n"
     ]
    }
   ],
   "source": [
    "max_iters = 50\n",
    "gamma = .8\n",
    "num_samples, num_dim = tx.shape\n",
    "\n",
    "n_iter = 0\n",
    "res = {\n",
    "    'weights': {},\n",
    "    'accuracy': {},\n",
    "    'loss': {}\n",
    "}\n",
    "\n",
    "for train_data, eval_data in train_eval_split(y, tx, train_size=.8, num_splits=5):\n",
    "    # Get training data\n",
    "    y_train, tx_train = train_data\n",
    "    \n",
    "    # Initial weights vector to train a linear model\n",
    "    initial_w = np.random.randn(num_dim)\n",
    "    \n",
    "    # Run gradient descent under MSE loss to find optimal weights\n",
    "    final_w, final_loss = least_squares_GD(y, tx, initial_w, max_iters, gamma)\n",
    "    \n",
    "    # Get validation set\n",
    "    y_eval, tx_eval = eval_data\n",
    "    \n",
    "    # Get predictions from current model\n",
    "    y_pred = predict_labels(final_w, tx_eval)\n",
    "    \n",
    "    acc = get_accuracy(y_pred, y_eval)\n",
    "    \n",
    "    print('Accuracy of predictions using least-squares gradient descent', acc)\n",
    "    \n",
    "    res['weights'][n_iter] = w\n",
    "    res['loss'][n_iter] = loss\n",
    "    res['accuracy'][n_iter] = acc\n",
    "    \n",
    "    n_iter += 1\n",
    "\n",
    "# Select model with highest accuracy on validation set\n",
    "iter_max_acc = max(res['accuracy'], key=res['accuracy'].get)\n",
    "w_max_acc = res['weights'][iter_max_acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test, tx_test, ids_test = load_csv_data('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to standardise to same mean and std\n",
    "tx_test = standardise_to_fixed(tx_test, mean_tx, std_tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from current model\n",
    "y_test_pred = predict_labels(w_max_acc, tx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(ids_test, y_test_pred, '../data/test_gd_submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
