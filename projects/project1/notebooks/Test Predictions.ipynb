{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from scripts.proj1_helpers import *\n",
    "from scripts.implementations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "y, tx, ids = load_csv_data('../data/train.csv')\n",
    "\n",
    "# Normalise data\n",
    "tx, mean_tx, std_tx = standardise(tx)\n",
    "\n",
    "# Check shape of data\n",
    "print('Shape: y: {}, x:{}\\n'.format(y.shape, tx.shape))\n",
    "\n",
    "# Check that data is normalised\n",
    "print(np.mean(tx, axis=0), np.std(tx, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test, tx_test, ids_test = load_csv_data('../data/test.csv')\n",
    "\n",
    "# Don't forget to standardise to same mean and std\n",
    "tx_test = standardise_to_fixed(tx_test, mean_tx, std_tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Random baseline guessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get baseline frequency of the two classes in training data\n",
    "prior_probs = [sum(y == 1)/len(y), sum(y == -1)/len(y)]\n",
    "\n",
    "y_test_pred = np.random.choice([1., -1.], size=len(y_test), p=prior_probs)\n",
    "\n",
    "# Save in submission file\n",
    "create_csv_submission(ids_test, y_test_pred, '../data/random_basline_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Least-squares gradient descent with shuffle split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.46315167757106357, gradient=0.7903536762018937\n",
      "Gradient Descent(1/49): loss=0.44799835514714237, gradient=0.42266504946818695\n",
      "Gradient Descent(2/49): loss=0.43842275681264475, gradient=0.32533437472345905\n",
      "Gradient Descent(3/49): loss=0.4319295014182043, gradient=0.266329495696419\n",
      "Gradient Descent(4/49): loss=0.42736744255838277, gradient=0.22233015198607392\n",
      "Gradient Descent(5/49): loss=0.4240452959042287, gradient=0.18892053179245485\n",
      "Gradient Descent(6/49): loss=0.4215336433802593, gradient=0.16354779457452734\n",
      "Gradient Descent(7/49): loss=0.41956254803449466, gradient=0.14426433331226246\n",
      "Gradient Descent(8/49): loss=0.4179603418456687, gradient=0.12955227429501995\n",
      "Gradient Descent(9/49): loss=0.4166163823670975, gradient=0.1182400737429003\n",
      "Gradient Descent(10/49): loss=0.4154582529490452, gradient=0.10943802587252502\n",
      "Gradient Descent(11/49): loss=0.4144377218229711, gradient=0.10248233869518214\n",
      "Gradient Descent(12/49): loss=0.4135220360894587, gradient=0.09688563908819149\n",
      "Gradient Descent(13/49): loss=0.4126884808707179, gradient=0.09229438092374155\n",
      "Gradient Descent(14/49): loss=0.4119209480282409, gradient=0.08845389631265052\n",
      "Gradient Descent(15/49): loss=0.41120774964305434, gradient=0.08518107241999286\n",
      "Gradient Descent(16/49): loss=0.410540208228183, gradient=0.08234384591865627\n",
      "Gradient Descent(17/49): loss=0.4099117357182834, gradient=0.07984628627381986\n",
      "Gradient Descent(18/49): loss=0.40931722298365336, gradient=0.07761797912169728\n",
      "Gradient Descent(19/49): loss=0.40875262875186474, gradient=0.075606573763156\n",
      "Gradient Descent(20/49): loss=0.40821469812048994, gradient=0.07377258916363504\n",
      "Gradient Descent(21/49): loss=0.4077007664016006, gradient=0.0720858005059707\n",
      "Gradient Descent(22/49): loss=0.4072086199598668, gradient=0.07052271892477573\n",
      "Gradient Descent(23/49): loss=0.4067363956991773, gradient=0.06906482307443011\n",
      "Gradient Descent(24/49): loss=0.4062825071778809, gradient=0.06769730729838755\n",
      "Gradient Descent(25/49): loss=0.40584558937383874, gradient=0.06640818577473624\n",
      "Gradient Descent(26/49): loss=0.4054244567288643, gradient=0.06518764339153965\n",
      "Gradient Descent(27/49): loss=0.40501807080471713, gradient=0.06402755904802951\n",
      "Gradient Descent(28/49): loss=0.4046255150075852, gradient=0.06292115068331153\n",
      "Gradient Descent(29/49): loss=0.4042459745906365, gradient=0.06186270724246878\n",
      "Gradient Descent(30/49): loss=0.4038787206546786, gradient=0.06084738352001754\n",
      "Gradient Descent(31/49): loss=0.403523097218014, gradient=0.059871041083330856\n",
      "Gradient Descent(32/49): loss=0.40317851067147975, gradient=0.05893012342203758\n",
      "Gradient Descent(33/49): loss=0.4028444211079471, gradient=0.05802155685907218\n",
      "Gradient Descent(34/49): loss=0.40252033513992724, gradient=0.05714267110394657\n",
      "Gradient Descent(35/49): loss=0.4022057999094615, gradient=0.05629113496726981\n",
      "Gradient Descent(36/49): loss=0.40190039806120825, gradient=0.055464903912997035\n",
      "Gradient Descent(37/49): loss=0.4016037434995144, gradient=0.05466217695203408\n",
      "Gradient Descent(38/49): loss=0.40131547778794613, gradient=0.053881360979139196\n",
      "Gradient Descent(39/49): loss=0.4010352670785672, gradient=0.05312104109311687\n",
      "Gradient Descent(40/49): loss=0.4007627994805078, gradient=0.05237995576495838\n",
      "Gradient Descent(41/49): loss=0.400497782794713, gradient=0.051656975962111835\n",
      "Gradient Descent(42/49): loss=0.4002399425554139, gradient=0.05095108752180261\n",
      "Gradient Descent(43/49): loss=0.3999890203296799, gradient=0.050261376208067\n",
      "Gradient Descent(44/49): loss=0.3997447722350482, gradient=0.04958701499699102\n",
      "Gradient Descent(45/49): loss=0.3995069676421748, gradient=0.048927253220620644\n",
      "Gradient Descent(46/49): loss=0.39927538803506746, gradient=0.04828140726785322\n",
      "Gradient Descent(47/49): loss=0.399049826006019, gradient=0.04764885259464882\n",
      "Gradient Descent(48/49): loss=0.39883008436610284, gradient=0.04702901683921411\n",
      "Gradient Descent(49/49): loss=0.39861597535514304, gradient=0.04642137387276321\n",
      "Accuracy of predictions using least-squares gradient descent 0.7089\n",
      "Gradient Descent(0/49): loss=0.39893056823166495, gradient=0.045636387662456875\n",
      "Gradient Descent(1/49): loss=0.39872982580107647, gradient=0.044958851502465015\n",
      "Gradient Descent(2/49): loss=0.3985344431628882, gradient=0.044349332551662866\n",
      "Gradient Descent(3/49): loss=0.3983441533507724, gradient=0.04376464005069388\n",
      "Gradient Descent(4/49): loss=0.39815874083741254, gradient=0.043198095430255025\n",
      "Gradient Descent(5/49): loss=0.3979780204731952, gradient=0.04264645870466236\n",
      "Gradient Descent(6/49): loss=0.3978018278747867, gradient=0.042107631004660764\n",
      "Gradient Descent(7/49): loss=0.39763001354775507, gradient=0.04158018103268606\n",
      "Gradient Descent(8/49): loss=0.39746243913868035, gradient=0.04106310004344841\n",
      "Gradient Descent(9/49): loss=0.39729897499768385, gradient=0.04055565127150941\n",
      "Gradient Descent(10/49): loss=0.397139498558267, gradient=0.040057275464796274\n",
      "Gradient Descent(11/49): loss=0.39698389323175454, gradient=0.039567531172754564\n",
      "Gradient Descent(12/49): loss=0.3968320476293363, gradient=0.0390860567123134\n",
      "Gradient Descent(13/49): loss=0.3966838549954937, gradient=0.03861254571792458\n",
      "Gradient Descent(14/49): loss=0.3965392127800677, gradient=0.038146731248705455\n",
      "Gradient Descent(15/49): loss=0.3963980223030696, gradient=0.03768837531963639\n",
      "Gradient Descent(16/49): loss=0.3962601884829742, gradient=0.0372372618962503\n",
      "Gradient Descent(17/49): loss=0.39612561960962644, gradient=0.03679319212012845\n",
      "Gradient Descent(18/49): loss=0.3959942271494225, gradient=0.036355980985917344\n",
      "Gradient Descent(19/49): loss=0.3958659255745601, gradient=0.035925454974068204\n",
      "Gradient Descent(20/49): loss=0.39574063221079864, gradient=0.03550145032154637\n",
      "Gradient Descent(21/49): loss=0.39561826709988296, gradient=0.03508381172513413\n",
      "Gradient Descent(22/49): loss=0.3954987528739048, gradient=0.03467239134328895\n",
      "Gradient Descent(23/49): loss=0.39538201463961786, gradient=0.0342670480081002\n",
      "Gradient Descent(24/49): loss=0.39526797987123063, gradient=0.033867646588225137\n",
      "Gradient Descent(25/49): loss=0.3951565783105459, gradient=0.03347405746272613\n",
      "Gradient Descent(26/49): loss=0.39504774187355796, gradient=0.03308615607820942\n",
      "Gradient Descent(27/49): loss=0.39494140456279414, gradient=0.032703822569933504\n",
      "Gradient Descent(28/49): loss=0.3948375023848197, gradient=0.032326941433088376\n",
      "Gradient Descent(29/49): loss=0.3947359732724122, gradient=0.03195540123420945\n",
      "Gradient Descent(30/49): loss=0.394636757010998, gradient=0.031589094355288026\n",
      "Gradient Descent(31/49): loss=0.3945397951689885, gradient=0.031227916764948713\n",
      "Gradient Descent(32/49): loss=0.3944450310317103, gradient=0.03087176781235496\n",
      "Gradient Descent(33/49): loss=0.3943524095386524, gradient=0.0305205500404445\n",
      "Gradient Descent(34/49): loss=0.39426187722379674, gradient=0.030174169015783004\n",
      "Gradient Descent(35/49): loss=0.3941733821588057, gradient=0.029832533172831886\n",
      "Gradient Descent(36/49): loss=0.3940868738988829, gradient=0.029495553670839093\n",
      "Gradient Descent(37/49): loss=0.39400230343112824, gradient=0.029163144261845558\n",
      "Gradient Descent(38/49): loss=0.3939196231252278, gradient=0.028835221168555907\n",
      "Gradient Descent(39/49): loss=0.39383878668633593, gradient=0.028511702971005096\n",
      "Gradient Descent(40/49): loss=0.3937597491100202, gradient=0.0281925105011109\n",
      "Gradient Descent(41/49): loss=0.39368246663914136, gradient=0.027877566744327193\n",
      "Gradient Descent(42/49): loss=0.3936068967225648, gradient=0.02756679674771945\n",
      "Gradient Descent(43/49): loss=0.39353299797559566, gradient=0.027260127533867484\n",
      "Gradient Descent(44/49): loss=0.39346073014204574, gradient=0.026957488020076522\n",
      "Gradient Descent(45/49): loss=0.3933900540578425, gradient=0.026658808942438487\n",
      "Gradient Descent(46/49): loss=0.3933209316160994, gradient=0.02636402278433835\n",
      "Gradient Descent(47/49): loss=0.3932533257335724, gradient=0.026073063709043956\n",
      "Gradient Descent(48/49): loss=0.3931872003184308, gradient=0.025785867496059742\n",
      "Gradient Descent(49/49): loss=0.3931225202392764, gradient=0.02550237148095296\n",
      "Accuracy of predictions using least-squares gradient descent 0.7163\n",
      "Gradient Descent(0/49): loss=0.39347354796095735, gradient=0.027593613200494412\n",
      "Gradient Descent(1/49): loss=0.393407070584847, gradient=0.025938340096554092\n",
      "Gradient Descent(2/49): loss=0.393342907438532, gradient=0.02543202318270337\n",
      "Gradient Descent(3/49): loss=0.3932806421331151, gradient=0.025041906411459938\n",
      "Gradient Descent(4/49): loss=0.3932200645616118, gradient=0.024693991265177823\n",
      "Gradient Descent(5/49): loss=0.393161031319053, gradient=0.024373114369210778\n",
      "Gradient Descent(6/49): loss=0.393103438356314, gradient=0.02407119609805432\n",
      "Gradient Descent(7/49): loss=0.3930472069594373, gradient=0.023783061733792903\n",
      "Gradient Descent(8/49): loss=0.39299227505542383, gradient=0.023505315301416172\n",
      "Gradient Descent(9/49): loss=0.39293859168943684, gradient=0.023235702347081204\n",
      "Gradient Descent(10/49): loss=0.39288611348451224, gradient=0.022972707141436576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(11/49): loss=0.3928348023518485, gradient=0.02271529505993282\n",
      "Gradient Descent(12/49): loss=0.3927846239948488, gradient=0.02246274713783569\n",
      "Gradient Descent(13/49): loss=0.3927355469202502, gradient=0.02221455333747715\n",
      "Gradient Descent(14/49): loss=0.39268754177558207, gradient=0.021970343310591025\n",
      "Gradient Descent(15/49): loss=0.39264058089833487, gradient=0.021729841189881846\n",
      "Gradient Descent(16/49): loss=0.3925946380037551, gradient=0.021492835847690177\n",
      "Gradient Descent(17/49): loss=0.392549687964339, gradient=0.021259161162334703\n",
      "Gradient Descent(18/49): loss=0.39250570665068973, gradient=0.021028682798547013\n",
      "Gradient Descent(19/49): loss=0.39246267081396113, gradient=0.020801289256805407\n",
      "Gradient Descent(20/49): loss=0.3924205579968853, gradient=0.020576885741539604\n",
      "Gradient Descent(21/49): loss=0.39237934646475114, gradient=0.020355389906516926\n",
      "Gradient Descent(22/49): loss=0.392339015150534, gradient=0.020136728862037784\n",
      "Gradient Descent(23/49): loss=0.39229954361024133, gradient=0.019920837039037898\n",
      "Gradient Descent(24/49): loss=0.39226091198577123, gradient=0.01970765464163936\n",
      "Gradient Descent(25/49): loss=0.3922231009733869, gradient=0.019497126508668666\n",
      "Gradient Descent(26/49): loss=0.39218609179647873, gradient=0.01928920126303713\n",
      "Gradient Descent(27/49): loss=0.39214986618165365, gradient=0.01908383066646491\n",
      "Gradient Descent(28/49): loss=0.3921144063374542, gradient=0.018880969122701294\n",
      "Gradient Descent(29/49): loss=0.39207969493518563, gradient=0.01868057328964283\n",
      "Gradient Descent(30/49): loss=0.39204571509147124, gradient=0.01848260177242336\n",
      "Gradient Descent(31/49): loss=0.3920124503522298, gradient=0.018287014877539982\n",
      "Gradient Descent(32/49): loss=0.39197988467785677, gradient=0.018093774413591324\n",
      "Gradient Descent(33/49): loss=0.3919480024294203, gradient=0.01790284352806618\n",
      "Gradient Descent(34/49): loss=0.39191678835573673, gradient=0.017714186572337826\n",
      "Gradient Descent(35/49): loss=0.3918862275812085, gradient=0.017527768988971622\n",
      "Gradient Descent(36/49): loss=0.3918563055943308, gradient=0.0173435572168613\n",
      "Gradient Descent(37/49): loss=0.3918270082367945, gradient=0.0171615186107422\n",
      "Gradient Descent(38/49): loss=0.3917983216931167, gradient=0.0169816213723958\n",
      "Gradient Descent(39/49): loss=0.39177023248075393, gradient=0.01680383449143393\n",
      "Gradient Descent(40/49): loss=0.391742727440646, gradient=0.01662812769398907\n",
      "Gradient Descent(41/49): loss=0.39171579372815685, gradient=0.016454471397969\n",
      "Gradient Descent(42/49): loss=0.3916894188043813, gradient=0.016282836673797693\n",
      "Gradient Descent(43/49): loss=0.3916635904277862, gradient=0.016113195209761105\n",
      "Gradient Descent(44/49): loss=0.3916382966461644, gradient=0.01594551928124552\n",
      "Gradient Descent(45/49): loss=0.39161352578888114, gradient=0.015779781723274874\n",
      "Gradient Descent(46/49): loss=0.3915892664593905, gradient=0.01561595590586118\n",
      "Gradient Descent(47/49): loss=0.3915655075280089, gradient=0.015454015711765752\n",
      "Gradient Descent(48/49): loss=0.3915422381249323, gradient=0.015293935516330296\n",
      "Gradient Descent(49/49): loss=0.3915194476334774, gradient=0.01513569016909779\n",
      "Accuracy of predictions using least-squares gradient descent 0.71958\n",
      "Gradient Descent(0/49): loss=0.3909435036558465, gradient=0.019753684489036055\n",
      "Gradient Descent(1/49): loss=0.39091637262467016, gradient=0.016690346756797793\n",
      "Gradient Descent(2/49): loss=0.39089092463464503, gradient=0.016058690778005383\n",
      "Gradient Descent(3/49): loss=0.3908666389509331, gradient=0.015665717714617805\n",
      "Gradient Descent(4/49): loss=0.39084327663500873, gradient=0.015352890317270107\n",
      "Gradient Descent(5/49): loss=0.3908206842176347, gradient=0.015089756188356378\n",
      "Gradient Descent(6/49): loss=0.39079875754744425, gradient=0.014860296212641155\n",
      "Gradient Descent(7/49): loss=0.3907774237567111, gradient=0.014654241186079393\n",
      "Gradient Descent(8/49): loss=0.39075663024905827, gradient=0.014464843378148731\n",
      "Gradient Descent(9/49): loss=0.3907363377855117, gradient=0.014287616320893525\n",
      "Gradient Descent(10/49): loss=0.3907165160889433, gradient=0.014119537355358602\n",
      "Gradient Descent(11/49): loss=0.39069714101441466, gradient=0.013958538665534592\n",
      "Gradient Descent(12/49): loss=0.3906781927023882, gradient=0.013803181448319688\n",
      "Gradient Descent(13/49): loss=0.390659654355213, gradient=0.013652446218850077\n",
      "Gradient Descent(14/49): loss=0.39064151141338754, gradient=0.013505596826766384\n",
      "Gradient Descent(15/49): loss=0.39062375099146607, gradient=0.013362091426201471\n",
      "Gradient Descent(16/49): loss=0.3906063614849071, gradient=0.013221523531332045\n",
      "Gradient Descent(17/49): loss=0.3905893322911049, gradient=0.013083582498583813\n",
      "Gradient Descent(18/49): loss=0.39057265360786364, gradient=0.012948026669719675\n",
      "Gradient Descent(19/49): loss=0.3905563162852303, gradient=0.012814664854256535\n",
      "Gradient Descent(20/49): loss=0.39054031171468734, gradient=0.012683343369458392\n",
      "Gradient Descent(21/49): loss=0.39052463174492213, gradient=0.01255393683116186\n",
      "Gradient Descent(22/49): loss=0.39050926861680846, gradient=0.01242634151006761\n",
      "Gradient Descent(23/49): loss=0.3904942149124829, gradient=0.012300470467209102\n",
      "Gradient Descent(24/49): loss=0.390479463514917, gradient=0.01217624994083835\n",
      "Gradient Descent(25/49): loss=0.3904650075754175, gradient=0.012053616626088557\n",
      "Gradient Descent(26/49): loss=0.39045084048718054, gradient=0.01193251560053995\n",
      "Gradient Descent(27/49): loss=0.3904369558635357, gradient=0.01181289872351839\n",
      "Gradient Descent(28/49): loss=0.39042334751985863, gradient=0.011694723387451434\n",
      "Gradient Descent(29/49): loss=0.3904100094583811, gradient=0.011577951534157227\n",
      "Gradient Descent(30/49): loss=0.390396935855309, gradient=0.01146254887286883\n",
      "Gradient Descent(31/49): loss=0.39038412104980236, gradient=0.011348484253568863\n",
      "Gradient Descent(32/49): loss=0.39037155953445923, gradient=0.011235729161118632\n",
      "Gradient Descent(33/49): loss=0.390359245947029, gradient=0.01112425730421271\n",
      "Gradient Descent(34/49): loss=0.3903471750631371, gradient=0.011014044279414187\n",
      "Gradient Descent(35/49): loss=0.3903353417898442, gradient=0.010905067295092918\n",
      "Gradient Descent(36/49): loss=0.3903237411599015, gradient=0.010797304943494656\n",
      "Gradient Descent(37/49): loss=0.39031236832658783, gradient=0.010690737011723227\n",
      "Gradient Descent(38/49): loss=0.39030121855903677, gradient=0.010585344324360873\n",
      "Gradient Descent(39/49): loss=0.39029028723797804, gradient=0.010481108611943861\n",
      "Gradient Descent(40/49): loss=0.39027956985183293, gradient=0.010378012400658592\n",
      "Gradient Descent(41/49): loss=0.3902690619931105, gradient=0.010276038919529683\n",
      "Gradient Descent(42/49): loss=0.3902587593550667, gradient=0.010175172022075603\n",
      "Gradient Descent(43/49): loss=0.3902486577285857, gradient=0.010075396119969979\n",
      "Gradient Descent(44/49): loss=0.3902387529992596, gradient=0.009976696126697772\n",
      "Gradient Descent(45/49): loss=0.39022904114464063, gradient=0.00987905740954996\n",
      "Gradient Descent(46/49): loss=0.39021951823164314, gradient=0.009782465748596644\n",
      "Gradient Descent(47/49): loss=0.39021018041408145, gradient=0.00968690730151174\n",
      "Gradient Descent(48/49): loss=0.39020102393032785, gradient=0.009592368573313954\n",
      "Gradient Descent(49/49): loss=0.39019204510107697, gradient=0.009498836390248048\n",
      "Accuracy of predictions using least-squares gradient descent 0.71828\n",
      "Gradient Descent(0/49): loss=0.3904438024119053, gradient=0.015215536270955888\n",
      "Gradient Descent(1/49): loss=0.39042897069371013, gradient=0.012466115190289615\n",
      "Gradient Descent(2/49): loss=0.3904160873045519, gradient=0.011523267743234429\n",
      "Gradient Descent(3/49): loss=0.3904045309518722, gradient=0.010879549488318227\n",
      "Gradient Descent(4/49): loss=0.3903939426738556, gradient=0.01039040493839847\n",
      "Gradient Descent(5/49): loss=0.3903840829457884, gradient=0.010009271914787706\n",
      "Gradient Descent(6/49): loss=0.3903747879432705, gradient=0.009705705482012739\n",
      "Gradient Descent(7/49): loss=0.390365943892729, gradient=0.00945805645983132\n",
      "Gradient Descent(8/49): loss=0.39035747054227427, gradient=0.009250926010735131\n",
      "Gradient Descent(9/49): loss=0.39034931033606773, gradient=0.009073390818360706\n",
      "Gradient Descent(10/49): loss=0.39034142125559984, gradient=0.008917700241617\n",
      "Gradient Descent(11/49): loss=0.39033377204163044, gradient=0.008778337904627183\n",
      "Gradient Descent(12/49): loss=0.3903263389748014, gradient=0.008651356715313975\n",
      "Gradient Descent(13/49): loss=0.39031910368541445, gradient=0.008533912369498301\n",
      "Gradient Descent(14/49): loss=0.39031205164859195, gradient=0.008423937785398333\n",
      "Gradient Descent(15/49): loss=0.3903051711398434, gradient=0.008319916247459072\n",
      "Gradient Descent(16/49): loss=0.3902984525026384, gradient=0.008220723160519874\n",
      "Gradient Descent(17/49): loss=0.39029188762929656, gradient=0.008125515329469323\n",
      "Gradient Descent(18/49): loss=0.3902854695890438, gradient=0.00803365314386674\n",
      "Gradient Descent(19/49): loss=0.3902791923585278, gradient=0.007944645582539591\n",
      "Gradient Descent(20/49): loss=0.3902730506243344, gradient=0.007858111094595392\n",
      "Gradient Descent(21/49): loss=0.39026703963658715, gradient=0.007773749573774071\n",
      "Gradient Descent(22/49): loss=0.3902611550991397, gradient=0.007691322124203896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(23/49): loss=0.3902553930862487, gradient=0.007610636330499379\n",
      "Gradient Descent(24/49): loss=0.3902497499785942, gradient=0.007531535441436251\n",
      "Gradient Descent(25/49): loss=0.39024422241359563, gradient=0.007453890355390668\n",
      "Gradient Descent(26/49): loss=0.39023880724638593, gradient=0.0073775936263210905\n",
      "Gradient Descent(27/49): loss=0.39023350151882713, gradient=0.007302554938190746\n",
      "Gradient Descent(28/49): loss=0.390228302434649, gradient=0.007228697655246336\n",
      "Gradient Descent(29/49): loss=0.39022320733929705, gradient=0.007155956167176304\n",
      "Gradient Descent(30/49): loss=0.39021821370344706, gradient=0.007084273826665883\n",
      "Gradient Descent(31/49): loss=0.39021331910938956, gradient=0.0070136013323773855\n",
      "Gradient Descent(32/49): loss=0.3902085212396898, gradient=0.006943895449869995\n",
      "Gradient Descent(33/49): loss=0.3902038178676665, gradient=0.006875117991229215\n",
      "Gradient Descent(34/49): loss=0.39019920684933473, gradient=0.006807234994528555\n",
      "Gradient Descent(35/49): loss=0.39019468611653546, gradient=0.006740216059002539\n",
      "Gradient Descent(36/49): loss=0.3901902536710362, gradient=0.006674033802585317\n",
      "Gradient Descent(37/49): loss=0.3901859075794342, gradient=0.006608663416395512\n",
      "Gradient Descent(38/49): loss=0.3901816459687176, gradient=0.006544082296623621\n",
      "Gradient Descent(39/49): loss=0.39017746702238365, gradient=0.0064802697386677245\n",
      "Gradient Descent(40/49): loss=0.3901733689770151, gradient=0.0064172066816706565\n",
      "Gradient Descent(41/49): loss=0.39016935011924975, gradient=0.006354875494122562\n",
      "Gradient Descent(42/49): loss=0.39016540878307837, gradient=0.006293259793115636\n",
      "Gradient Descent(43/49): loss=0.39016154334742487, gradient=0.006232344291323198\n",
      "Gradient Descent(44/49): loss=0.39015775223396687, gradient=0.006172114666931483\n",
      "Gradient Descent(45/49): loss=0.3901540339051619, gradient=0.006112557452657116\n",
      "Gradient Descent(46/49): loss=0.39015038686245257, gradient=0.006053659940702632\n",
      "Gradient Descent(47/49): loss=0.3901468096446279, gradient=0.00599541010107\n",
      "Gradient Descent(48/49): loss=0.3901433008263168, gradient=0.005937796511111743\n",
      "Gradient Descent(49/49): loss=0.3901398590166014, gradient=0.005880808294565103\n",
      "Accuracy of predictions using least-squares gradient descent 0.7175\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters for gradient descent\n",
    "max_iters = 50\n",
    "gamma = .1\n",
    "\n",
    "num_samples, num_dim = tx.shape\n",
    "# Initial weights vector to train a linear model\n",
    "initial_w = np.zeros(num_dim)\n",
    "\n",
    "res = {\n",
    "    'weights': {},\n",
    "    'accuracy': {},\n",
    "    'loss': {}\n",
    "}\n",
    "n_iter = 0\n",
    "\n",
    "for train_data, eval_data in train_eval_split(y, tx, train_size=.8, num_splits=5):\n",
    "    # Get training data\n",
    "    y_train, tx_train = train_data\n",
    "    \n",
    "    # Run gradient descent under MSE loss to find optimal weights\n",
    "    final_w, final_loss = least_squares_GD(y_train, tx_train, initial_w, max_iters, gamma)\n",
    "    \n",
    "    # Get validation set\n",
    "    y_eval, tx_eval = eval_data\n",
    "    \n",
    "    # Get predictions from current model\n",
    "    y_pred = predict_labels(final_w, tx_eval)\n",
    "    \n",
    "    acc = get_accuracy(y_pred, y_eval)\n",
    "    \n",
    "    print('Accuracy of predictions using least-squares gradient descent', acc)\n",
    "    \n",
    "    res['weights'][n_iter] = w\n",
    "    res['loss'][n_iter] = loss\n",
    "    res['accuracy'][n_iter] = acc\n",
    "    \n",
    "    n_iter += 1\n",
    "\n",
    "# Select model with highest accuracy on validation set\n",
    "iter_max_acc = max(res['accuracy'], key=res['accuracy'].get)\n",
    "w_max_acc = res['weights'][iter_max_acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from current model\n",
    "y_test_pred = predict_labels(w_max_acc, tx_test)\n",
    "\n",
    "# Save in submission file\n",
    "create_csv_submission(ids_test, y_test_pred, '../data/test_gd_submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
