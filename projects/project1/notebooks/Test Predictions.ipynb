{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from scripts.proj1_helpers import *\n",
    "from scripts.implementations import *\n",
    "from scripts.plots import *\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: y: (250000,), x:(250000, 30)\n",
      "\n",
      "[-2.50602916e-15  4.49575133e-15 -3.48448848e-15  7.18646387e-15\n",
      " -2.36304576e-14 -3.26035021e-15  1.26038877e-14  2.16223188e-14\n",
      "  6.40057962e-15  2.86143687e-15 -6.98486646e-15  3.63458152e-15\n",
      " -1.27422117e-14 -5.95722149e-15  1.35646161e-16  7.13136217e-17\n",
      "  2.58023760e-14 -1.06327391e-16 -1.87188487e-16  8.24115935e-15\n",
      "  1.41040513e-16 -8.99509711e-15 -6.01698247e-16 -4.92204144e-15\n",
      "  3.11615622e-15 -1.67606551e-15 -9.40773592e-15  1.79148900e-14\n",
      " -5.09692022e-15 -1.77122317e-15] [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "y, tx, ids = load_csv_data('../data/train.csv')\n",
    "\n",
    "# Normalise data\n",
    "tx, mean_tx, std_tx = standardise(tx)\n",
    "\n",
    "# Check shape of data\n",
    "print('Shape: y: {}, x:{}\\n'.format(y.shape, tx.shape))\n",
    "\n",
    "num_samples, num_dim = tx.shape\n",
    "\n",
    "# Check that data is normalised\n",
    "print(np.mean(tx, axis=0), np.std(tx, axis=0))\n",
    "\n",
    "# Split into train and evaluation set\n",
    "(tx_train, y_train), (tx_eval, y_eval) = train_eval_split(y, tx, split_ratio=.7, seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test, tx_test, ids_test = load_csv_data('../data/test.csv')\n",
    "\n",
    "# Don't forget to standardise to same mean and std\n",
    "tx_test = standardise_to_fixed(tx_test, mean_tx, std_tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Random baseline guessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Baseline 0.5487733333333333\n",
      "F1 Baseline 0.3431288819875776\n"
     ]
    }
   ],
   "source": [
    "# Get baseline frequency of the two classes in training data\n",
    "prior_probs = [sum(y_train == 1)/len(y_train), sum(y_train == -1)/len(y_train)]\n",
    "\n",
    "# Test error on evaluation set\n",
    "y_eval_pred = np.random.choice([1., -1.], size=len(y_eval), p=prior_probs)\n",
    "\n",
    "acc_baseline = get_accuracy(y_eval_pred, y_eval)\n",
    "f1_baseline = get_f1_score(y_eval_pred, y_eval)\n",
    "\n",
    "print('Accuracy Baseline {}'.format(acc_baseline))\n",
    "print('F1 Baseline {}'.format(f1_baseline))\n",
    "\n",
    "# Save in submission file\n",
    "y_test_pred = np.random.choice([1., -1.], size=len(y_eval), p=prior_probs)\n",
    "create_csv_submission(ids_test, y_test_pred, '../data/random_baseline_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy on the test after submission to the AICrowd platform for the random guess model is 55%. This is thus our baseline. Anything that goes below that is probably overfitting or a model that diverged. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Least-squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Least-squares "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy LS on evaluation set:  0.7192133333333334\n",
      "F1 Score LS on evaluation set: 0.6679176850902784\n"
     ]
    }
   ],
   "source": [
    "# Get linear least-squares model\n",
    "w_ls, mse_ls = least_squares(y_train, tx_train)\n",
    "\n",
    "# Test error on evaluation set\n",
    "y_eval_pred = predict_labels(w_ls, tx_eval)\n",
    "\n",
    "acc_ls = get_accuracy(y_eval_pred, y_eval)\n",
    "f1_ls = get_f1_score(y_eval_pred, y_eval)\n",
    "\n",
    "print('Accuracy LS on evaluation set: ', acc_ls)\n",
    "print('F1 Score LS on evaluation set:', f1_ls)\n",
    "\n",
    "# Save current mode predictions on test set\n",
    "y_test_pred = predict_labels(w_ls, tx_test)\n",
    "create_csv_submission(ids_test, y_test_pred, '../data/least_squares_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Least-squares gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/999): loss=0.4632708999621822, gradient=0.7904131174682191\n",
      "Gradient Descent(1/999): loss=0.44826542306471534, gradient=0.42098180519313166\n",
      "Gradient Descent(2/999): loss=0.43881330725425555, gradient=0.3233296967125444\n",
      "Gradient Descent(3/999): loss=0.4324157757580377, gradient=0.26440473230784073\n",
      "Gradient Descent(4/999): loss=0.42792601927563717, gradient=0.22057970805408061\n",
      "Gradient Descent(5/999): loss=0.4246576177432147, gradient=0.18738412392577616\n",
      "Gradient Descent(6/999): loss=0.4221854023539241, gradient=0.1622418395708655\n",
      "Gradient Descent(7/999): loss=0.4202428682378767, gradient=0.1431888904535042\n",
      "Gradient Descent(8/999): loss=0.4186610098049385, gradient=0.12869565869111343\n",
      "Gradient Descent(9/999): loss=0.4173312028906516, gradient=0.11758315001644261\n",
      "Gradient Descent(10/999): loss=0.4161825415565521, gradient=0.10895802290524811\n",
      "Gradient Descent(11/999): loss=0.4151679185830494, gradient=0.10215580984853084\n",
      "Gradient Descent(12/999): loss=0.41425541799571197, gradient=0.09669040247029476\n",
      "Gradient Descent(13/999): loss=0.41342295025323567, gradient=0.09221053927168094\n",
      "Gradient Descent(14/999): loss=0.41265487785210286, gradient=0.08846418349898093\n",
      "Gradient Descent(15/999): loss=0.411939870542397, gradient=0.08527078977758576\n",
      "Gradient Descent(16/999): loss=0.4112695257882439, gradient=0.0825005953001778\n",
      "Gradient Descent(17/999): loss=0.41063746952933544, gradient=0.08005963408121491\n",
      "Gradient Descent(18/999): loss=0.4100387613240366, gradient=0.0778791253265642\n",
      "Gradient Descent(19/999): loss=0.4094694945006455, gradient=0.07590805951960936\n",
      "Gradient Descent(20/999): loss=0.4089265227720034, gradient=0.07410805319055082\n",
      "Gradient Descent(21/999): loss=0.40840726996677784, gradient=0.07244978261554415\n",
      "Gradient Descent(22/999): loss=0.4079095951868958, gradient=0.07091050419844097\n",
      "Gradient Descent(23/999): loss=0.40743169550216946, gradient=0.06947231899081222\n",
      "Gradient Descent(24/999): loss=0.4069720344819964, gradient=0.06812094663759143\n",
      "Gradient Descent(25/999): loss=0.40652928880900246, gradient=0.0668448492814699\n",
      "Gradient Descent(26/999): loss=0.4061023077607168, gradient=0.06563459744019555\n",
      "Gradient Descent(27/999): loss=0.4056900820011807, gradient=0.06448240468285002\n",
      "Gradient Descent(28/999): loss=0.4052917192167368, gradient=0.0633817813302612\n",
      "Gradient Descent(29/999): loss=0.40490642486040174, gradient=0.06232727310457311\n",
      "Gradient Descent(30/999): loss=0.4045334867641138, gradient=0.06131426120286019\n",
      "Gradient Descent(31/999): loss=0.40417226271836243, gradient=0.060338807388202076\n",
      "Gradient Descent(32/999): loss=0.40382217035605267, gradient=0.059397532525170445\n",
      "Gradient Descent(33/999): loss=0.403482678845446, gradient=0.05848752029528176\n",
      "Gradient Descent(34/999): loss=0.4031533020176431, gradient=0.057606240114388294\n",
      "Gradient Descent(35/999): loss=0.40283359264189444, gradient=0.056751484870994046\n",
      "Gradient Descent(36/999): loss=0.4025231376268511, gradient=0.055921320232953646\n",
      "Gradient Descent(37/999): loss=0.40222155397426984, gradient=0.055114043077086596\n",
      "Gradient Descent(38/999): loss=0.4019284853483335, gradient=0.05432814718073175\n",
      "Gradient Descent(39/999): loss=0.40164359915171816, gradient=0.053562294742879164\n",
      "Gradient Descent(40/999): loss=0.4013665840211724, gradient=0.05281529262066685\n",
      "Gradient Descent(41/999): loss=0.4010971476722066, gradient=0.052086072406098305\n",
      "Gradient Descent(42/999): loss=0.4008350150357428, gradient=0.051373673649450835\n",
      "Gradient Descent(43/999): loss=0.40057992664005093, gradient=0.05067722967536774\n",
      "Gradient Descent(44/999): loss=0.4003316371996743, gradient=0.049995955545839126\n",
      "Gradient Descent(45/999): loss=0.4000899143797359, gradient=0.04932913780901243\n",
      "Gradient Descent(46/999): loss=0.3998545377094666, gradient=0.048676125739664046\n",
      "Gradient Descent(47/999): loss=0.3996252976231646, gradient=0.04803632383039536\n",
      "Gradient Descent(48/999): loss=0.3994019946103979, gradient=0.04740918533525278\n",
      "Gradient Descent(49/999): loss=0.3991844384602, gradient=0.04679420670185253\n",
      "Gradient Descent(50/999): loss=0.3989724475864168, gradient=0.04619092275597108\n",
      "Gradient Descent(51/999): loss=0.398765848423369, gradient=0.045598902525268714\n",
      "Gradient Descent(52/999): loss=0.3985644748826436, gradient=0.04501774560743317\n",
      "Gradient Descent(53/999): loss=0.3983681678632102, gradient=0.044447079003318214\n",
      "Gradient Descent(54/999): loss=0.3981767748082012, gradient=0.04388655434827578\n",
      "Gradient Descent(55/999): loss=0.39799014930266563, gradient=0.0433358454853557\n",
      "Gradient Descent(56/999): loss=0.39780815070740944, gradient=0.042794646332732465\n",
      "Gradient Descent(57/999): loss=0.3976306438247151, gradient=0.042262669004968736\n",
      "Gradient Descent(58/999): loss=0.3974574985923217, gradient=0.04173964215379627\n",
      "Gradient Descent(59/999): loss=0.397288589802512, gradient=0.04122530949916787\n",
      "Gradient Descent(60/999): loss=0.39712379684359533, gradient=0.04071942852560962\n",
      "Gradient Descent(61/999): loss=0.39696300346140323, gradient=0.040221769322507275\n",
      "Gradient Descent(62/999): loss=0.39680609753873397, gradient=0.039732113549990906\n",
      "Gradient Descent(63/999): loss=0.3966529708909327, gradient=0.039250253514671644\n",
      "Gradient Descent(64/999): loss=0.3965035190760163, gradient=0.03877599134164583\n",
      "Gradient Descent(65/999): loss=0.3963576412179454, gradient=0.03830913823106576\n",
      "Gradient Descent(66/999): loss=0.39621523984181145, gradient=0.03784951378913098\n",
      "Gradient Descent(67/999): loss=0.3960762207198457, gradient=0.03739694542472841\n",
      "Gradient Descent(68/999): loss=0.3959404927272841, gradient=0.03695126780408669\n",
      "Gradient Descent(69/999): loss=0.3958079677072268, gradient=0.036512322356811414\n",
      "Gradient Descent(70/999): loss=0.3956785603437312, gradient=0.036079956827516795\n",
      "Gradient Descent(71/999): loss=0.3955521880424482, gradient=0.035654024867996764\n",
      "Gradient Descent(72/999): loss=0.3954287708181939, gradient=0.03523438566551398\n",
      "Gradient Descent(73/999): loss=0.39530823118890485, gradient=0.034820903603330323\n",
      "Gradient Descent(74/999): loss=0.3951904940754858, gradient=0.034413447950069134\n",
      "Gradient Descent(75/999): loss=0.3950754867071003, gradient=0.03401189257491696\n",
      "Gradient Descent(76/999): loss=0.3949631385315045, gradient=0.033616115686016634\n",
      "Gradient Descent(77/999): loss=0.39485338113006013, gradient=0.0332259995897254\n",
      "Gradient Descent(78/999): loss=0.39474614813709413, gradient=0.03284143046866548\n",
      "Gradient Descent(79/999): loss=0.3946413751633076, gradient=0.032462298176745986\n",
      "Gradient Descent(80/999): loss=0.3945389997229557, gradient=0.03208849604952776\n",
      "Gradient Descent(81/999): loss=0.3944389611645552, gradient=0.03171992072848854\n",
      "Gradient Descent(82/999): loss=0.39434120060488365, gradient=0.03135647199790144\n",
      "Gradient Descent(83/999): loss=0.3942456608660692, gradient=0.030998052633183528\n",
      "Gradient Descent(84/999): loss=0.39415228641557026, gradient=0.030644568259684386\n",
      "Gradient Descent(85/999): loss=0.39406102330887754, gradient=0.03029592722099908\n",
      "Gradient Descent(86/999): loss=0.39397181913476603, gradient=0.029952040455986203\n",
      "Gradient Descent(87/999): loss=0.3938846229629567, gradient=0.029612821383746584\n",
      "Gradient Descent(88/999): loss=0.39379938529404146, gradient=0.029278185795903917\n",
      "Gradient Descent(89/999): loss=0.39371605801154735, gradient=0.028948051755586117\n",
      "Gradient Descent(90/999): loss=0.39363459433601977, gradient=0.028622339502568137\n",
      "Gradient Descent(91/999): loss=0.3935549487810142, gradient=0.02830097136409085\n",
      "Gradient Descent(92/999): loss=0.393477077110894, gradient=0.027983871670911705\n",
      "Gradient Descent(93/999): loss=0.3934009363003383, gradient=0.027670966678188795\n",
      "Gradient Descent(94/999): loss=0.39332648449547275, gradient=0.027362184490837785\n",
      "Gradient Descent(95/999): loss=0.3932536809765368, gradient=0.027057454993026763\n",
      "Gradient Descent(96/999): loss=0.39318248612201306, gradient=0.02675670978151478\n",
      "Gradient Descent(97/999): loss=0.3931128613741441, gradient=0.026459882102554712\n",
      "Gradient Descent(98/999): loss=0.39304476920577, gradient=0.026166906792116095\n",
      "Gradient Descent(99/999): loss=0.39297817308842037, gradient=0.025877720219194943\n",
      "Gradient Descent(100/999): loss=0.39291303746160583, gradient=0.025592260232004338\n",
      "Gradient Descent(101/999): loss=0.3928493277032462, gradient=0.025310466106853127\n",
      "Gradient Descent(102/999): loss=0.3927870101011878, gradient=0.02503227849953531\n",
      "Gradient Descent(103/999): loss=0.39272605182575826, gradient=0.024757639399073553\n",
      "Gradient Descent(104/999): loss=0.39266642090331116, gradient=0.024486492083659692\n",
      "Gradient Descent(105/999): loss=0.3926080861907158, gradient=0.024218781078662835\n",
      "Gradient Descent(106/999): loss=0.3925510173507529, gradient=0.023954452116574123\n",
      "Gradient Descent(107/999): loss=0.3924951848283759, gradient=0.02369345209877066\n",
      "Gradient Descent(108/999): loss=0.39244055982779585, gradient=0.023435729058994468\n",
      "Gradient Descent(109/999): loss=0.3923871142903627, gradient=0.023181232128441382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(110/999): loss=0.3923348208732033, gradient=0.022929911502369383\n",
      "Gradient Descent(111/999): loss=0.39228365292858614, gradient=0.022681718408139564\n",
      "Gradient Descent(112/999): loss=0.39223358448398415, gradient=0.022436605074609938\n",
      "Gradient Descent(113/999): loss=0.3921845902228053, gradient=0.022194524702805515\n",
      "Gradient Descent(114/999): loss=0.39213664546576305, gradient=0.02195543143779676\n",
      "Gradient Descent(115/999): loss=0.39208972615286586, gradient=0.02171928034172256\n",
      "Gradient Descent(116/999): loss=0.39204380882599377, gradient=0.02148602736789228\n",
      "Gradient Descent(117/999): loss=0.39199887061204614, gradient=0.021255629335916657\n",
      "Gradient Descent(118/999): loss=0.3919548892066322, gradient=0.02102804390780937\n",
      "Gradient Descent(119/999): loss=0.3919118428582885, gradient=0.02080322956501232\n",
      "Gradient Descent(120/999): loss=0.39186971035319873, gradient=0.020581145586297363\n",
      "Gradient Descent(121/999): loss=0.39182847100040075, gradient=0.020361752026500982\n",
      "Gradient Descent(122/999): loss=0.3917881046174583, gradient=0.02014500969605059\n",
      "Gradient Descent(123/999): loss=0.39174859151658536, gradient=0.019930880141244542\n",
      "Gradient Descent(124/999): loss=0.3917099124911988, gradient=0.01971932562524934\n",
      "Gradient Descent(125/999): loss=0.3916720488028924, gradient=0.019510309109779838\n",
      "Gradient Descent(126/999): loss=0.3916349821688111, gradient=0.019303794237430425\n",
      "Gradient Descent(127/999): loss=0.3915986947494092, gradient=0.019099745314627315\n",
      "Gradient Descent(128/999): loss=0.3915631691365882, gradient=0.01889812729517311\n",
      "Gradient Descent(129/999): loss=0.39152838834218806, gradient=0.01869890576435639\n",
      "Gradient Descent(130/999): loss=0.39149433578682924, gradient=0.018502046923601783\n",
      "Gradient Descent(131/999): loss=0.39146099528908995, gradient=0.01830751757563521\n",
      "Gradient Descent(132/999): loss=0.391428351055004, gradient=0.018115285110142834\n",
      "Gradient Descent(133/999): loss=0.39139638766787355, gradient=0.017925317489901824\n",
      "Gradient Descent(134/999): loss=0.39136509007838105, gradient=0.017737583237361437\n",
      "Gradient Descent(135/999): loss=0.39133444359499353, gradient=0.017552051421657798\n",
      "Gradient Descent(136/999): loss=0.3913044338746473, gradient=0.017368691646040033\n",
      "Gradient Descent(137/999): loss=0.39127504691370457, gradient=0.017187474035695212\n",
      "Gradient Descent(138/999): loss=0.39124626903917226, gradient=0.017008369225951166\n",
      "Gradient Descent(139/999): loss=0.39121808690017523, gradient=0.016831348350845165\n",
      "Gradient Descent(140/999): loss=0.3911904874596732, gradient=0.016656383032040572\n",
      "Gradient Descent(141/999): loss=0.39116345798641816, gradient=0.016483445368080323\n",
      "Gradient Descent(142/999): loss=0.39113698604713687, gradient=0.016312507923960745\n",
      "Gradient Descent(143/999): loss=0.39111105949893893, gradient=0.01614354372101556\n",
      "Gradient Descent(144/999): loss=0.391085666481939, gradient=0.015976526227097377\n",
      "Gradient Descent(145/999): loss=0.39106079541208516, gradient=0.015811429347042855\n",
      "Gradient Descent(146/999): loss=0.3910364349741903, gradient=0.015648227413414127\n",
      "Gradient Descent(147/999): loss=0.39101257411515805, gradient=0.015486895177503469\n",
      "Gradient Descent(148/999): loss=0.3909892020373973, gradient=0.01532740780059174\n",
      "Gradient Descent(149/999): loss=0.39096630819241784, gradient=0.015169740845451779\n",
      "Gradient Descent(150/999): loss=0.39094388227460675, gradient=0.015013870268087084\n",
      "Gradient Descent(151/999): loss=0.39092191421517286, gradient=0.014859772409696846\n",
      "Gradient Descent(152/999): loss=0.3909003941762577, gradient=0.014707423988860224\n",
      "Gradient Descent(153/999): loss=0.39087931254521063, gradient=0.014556802093930146\n",
      "Gradient Descent(154/999): loss=0.3908586599290154, gradient=0.014407884175630922\n",
      "Gradient Descent(155/999): loss=0.39083842714887196, gradient=0.014260648039851389\n",
      "Gradient Descent(156/999): loss=0.39081860523492307, gradient=0.01411507184062669\n",
      "Gradient Descent(157/999): loss=0.3907991854211231, gradient=0.01397113407330307\n",
      "Gradient Descent(158/999): loss=0.39078015914024633, gradient=0.013828813567877895\n",
      "Gradient Descent(159/999): loss=0.3907615180190272, gradient=0.01368808948251035\n",
      "Gradient Descent(160/999): loss=0.39074325387343156, gradient=0.013548941297195326\n",
      "Gradient Descent(161/999): loss=0.39072535870405395, gradient=0.01341134880759739\n",
      "Gradient Descent(162/999): loss=0.3907078246916359, gradient=0.013275292119037456\n",
      "Gradient Descent(163/999): loss=0.390690644192705, gradient=0.013140751640626804\n",
      "Gradient Descent(164/999): loss=0.3906738097353271, gradient=0.013007708079546753\n",
      "Gradient Descent(165/999): loss=0.390657314014971, gradient=0.01287614243546513\n",
      "Gradient Descent(166/999): loss=0.3906411498904829, gradient=0.012746035995088065\n",
      "Gradient Descent(167/999): loss=0.3906253103801633, gradient=0.01261737032684174\n",
      "Gradient Descent(168/999): loss=0.3906097886579502, gradient=0.012490127275679901\n",
      "Gradient Descent(169/999): loss=0.39059457804969755, gradient=0.012364288958013395\n",
      "Gradient Descent(170/999): loss=0.3905796720295532, gradient=0.012239837756758258\n",
      "Gradient Descent(171/999): loss=0.39056506421642945, gradient=0.012116756316497973\n",
      "Gradient Descent(172/999): loss=0.3905507483705653, gradient=0.011995027538756954\n",
      "Gradient Descent(173/999): loss=0.3905367183901761, gradient=0.01187463457738194\n",
      "Gradient Descent(174/999): loss=0.3905229683081901, gradient=0.011755560834027465\n",
      "Gradient Descent(175/999): loss=0.39050949228906856, gradient=0.011637789953743398\n",
      "Gradient Descent(176/999): loss=0.3904962846257068, gradient=0.011521305820660334\n",
      "Gradient Descent(177/999): loss=0.3904833397364143, gradient=0.01140609255377079\n",
      "Gradient Descent(178/999): loss=0.3904706521619716, gradient=0.011292134502802824\n",
      "Gradient Descent(179/999): loss=0.3904582165627616, gradient=0.011179416244184812\n",
      "Gradient Descent(180/999): loss=0.3904460277159743, gradient=0.011067922577096103\n",
      "Gradient Descent(181/999): loss=0.39043408051288003, gradient=0.010957638519604683\n",
      "Gradient Descent(182/999): loss=0.3904223699561747, gradient=0.010848549304886126\n",
      "Gradient Descent(183/999): loss=0.3904108911573871, gradient=0.010740640377523297\n",
      "Gradient Descent(184/999): loss=0.3903996393343561, gradient=0.010633897389885032\n",
      "Gradient Descent(185/999): loss=0.3903886098087671, gradient=0.01052830619857998\n",
      "Gradient Descent(186/999): loss=0.39037779800375194, gradient=0.010423852860985093\n",
      "Gradient Descent(187/999): loss=0.3903671994415478, gradient=0.010320523631845649\n",
      "Gradient Descent(188/999): loss=0.39035680974121423, gradient=0.010218304959946087\n",
      "Gradient Descent(189/999): loss=0.3903466246164069, gradient=0.010117183484848441\n",
      "Gradient Descent(190/999): loss=0.390336639873207, gradient=0.010017146033697973\n",
      "Gradient Descent(191/999): loss=0.3903268514080023, gradient=0.0099181796180933\n",
      "Gradient Descent(192/999): loss=0.39031725520542243, gradient=0.00982027143101921\n",
      "Gradient Descent(193/999): loss=0.39030784733632345, gradient=0.009723408843842077\n",
      "Gradient Descent(194/999): loss=0.3902986239558235, gradient=0.009627579403364039\n",
      "Gradient Descent(195/999): loss=0.3902895813013837, gradient=0.009532770828936592\n",
      "Gradient Descent(196/999): loss=0.3902807156909386, gradient=0.009438971009630644\n",
      "Gradient Descent(197/999): loss=0.3902720235210712, gradient=0.009346168001462476\n",
      "Gradient Descent(198/999): loss=0.3902635012652309, gradient=0.009254350024673598\n",
      "Gradient Descent(199/999): loss=0.39025514547199774, gradient=0.00916350546106415\n",
      "Gradient Descent(200/999): loss=0.39024695276338606, gradient=0.009073622851377265\n",
      "Gradient Descent(201/999): loss=0.39023891983318926, gradient=0.008984690892733854\n",
      "Gradient Descent(202/999): loss=0.3902310434453661, gradient=0.008896698436117748\n",
      "Gradient Descent(203/999): loss=0.39022332043246505, gradient=0.00880963448390713\n",
      "Gradient Descent(204/999): loss=0.3902157476940854, gradient=0.008723488187454167\n",
      "Gradient Descent(205/999): loss=0.3902083221953773, gradient=0.00863824884470995\n",
      "Gradient Descent(206/999): loss=0.3902010409655751, gradient=0.008553905897894586\n",
      "Gradient Descent(207/999): loss=0.39019390109656876, gradient=0.008470448931210316\n",
      "Gradient Descent(208/999): loss=0.3901868997415067, gradient=0.008387867668598314\n",
      "Gradient Descent(209/999): loss=0.39018003411343294, gradient=0.008306151971536183\n",
      "Gradient Descent(210/999): loss=0.3901733014839575, gradient=0.008225291836877438\n",
      "Gradient Descent(211/999): loss=0.39016669918195596, gradient=0.008145277394729847\n",
      "Gradient Descent(212/999): loss=0.39016022459230204, gradient=0.008066098906373744\n",
      "Gradient Descent(213/999): loss=0.39015387515462935, gradient=0.007987746762217621\n",
      "Gradient Descent(214/999): loss=0.3901476483621218, gradient=0.00791021147979156\n",
      "Gradient Descent(215/999): loss=0.39014154176033317, gradient=0.007833483701777217\n",
      "Gradient Descent(216/999): loss=0.39013555294603375, gradient=0.007757554194073376\n",
      "Gradient Descent(217/999): loss=0.39012967956608513, gradient=0.007682413843896277\n",
      "Gradient Descent(218/999): loss=0.39012391931633966, gradient=0.007608053657914476\n",
      "Gradient Descent(219/999): loss=0.3901182699405675, gradient=0.007534464760417008\n",
      "Gradient Descent(220/999): loss=0.3901127292294067, gradient=0.0074616383915142555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(221/999): loss=0.39010729501934016, gradient=0.007389565905370962\n",
      "Gradient Descent(222/999): loss=0.3901019651916931, gradient=0.007318238768470674\n",
      "Gradient Descent(223/999): loss=0.39009673767165765, gradient=0.007247648557910911\n",
      "Gradient Descent(224/999): loss=0.39009161042733614, gradient=0.007177786959728106\n",
      "Gradient Descent(225/999): loss=0.39008658146881015, gradient=0.007108645767252681\n",
      "Gradient Descent(226/999): loss=0.3900816488472282, gradient=0.0070402168794922336\n",
      "Gradient Descent(227/999): loss=0.3900768106539153, gradient=0.006972492299543821\n",
      "Gradient Descent(228/999): loss=0.39007206501950503, gradient=0.006905464133033001\n",
      "Gradient Descent(229/999): loss=0.3900674101130872, gradient=0.0068391245865808675\n",
      "Gradient Descent(230/999): loss=0.39006284414138, gradient=0.0067734659662972655\n",
      "Gradient Descent(231/999): loss=0.39005836534791694, gradient=0.006708480676299856\n",
      "Gradient Descent(232/999): loss=0.3900539720122556, gradient=0.006644161217259245\n",
      "Gradient Descent(233/999): loss=0.39004966244920225, gradient=0.006580500184968861\n",
      "Gradient Descent(234/999): loss=0.3900454350080542, gradient=0.0065174902689390775\n",
      "Gradient Descent(235/999): loss=0.39004128807186184, gradient=0.00645512425101603\n",
      "Gradient Descent(236/999): loss=0.39003722005670427, gradient=0.006393395004023308\n",
      "Gradient Descent(237/999): loss=0.3900332294109828, gradient=0.006332295490427197\n",
      "Gradient Descent(238/999): loss=0.39002931461473156, gradient=0.00627181876102454\n",
      "Gradient Descent(239/999): loss=0.3900254741789411, gradient=0.00621195795365257\n",
      "Gradient Descent(240/999): loss=0.39002170664489916, gradient=0.0061527062919207905\n",
      "Gradient Descent(241/999): loss=0.39001801058354707, gradient=0.006094057083963953\n",
      "Gradient Descent(242/999): loss=0.3900143845948464, gradient=0.0060360037212163495\n",
      "Gradient Descent(243/999): loss=0.3900108273071664, gradient=0.005978539677206263\n",
      "Gradient Descent(244/999): loss=0.3900073373766794, gradient=0.005921658506370786\n",
      "Gradient Descent(245/999): loss=0.390003913486772, gradient=0.005865353842890369\n",
      "Gradient Descent(246/999): loss=0.39000055434747, gradient=0.005809619399542789\n",
      "Gradient Descent(247/999): loss=0.3899972586948758, gradient=0.00575444896657607\n",
      "Gradient Descent(248/999): loss=0.38999402529061705, gradient=0.00569983641060019\n",
      "Gradient Descent(249/999): loss=0.3899908529213094, gradient=0.00564577567349692\n",
      "Gradient Descent(250/999): loss=0.3899877403980306, gradient=0.005592260771347855\n",
      "Gradient Descent(251/999): loss=0.38998468655580515, gradient=0.005539285793379948\n",
      "Gradient Descent(252/999): loss=0.38998169025310214, gradient=0.0054868449009282125\n",
      "Gradient Descent(253/999): loss=0.38997875037134333, gradient=0.005434932326415775\n",
      "Gradient Descent(254/999): loss=0.38997586581442156, gradient=0.005383542372350479\n",
      "Gradient Descent(255/999): loss=0.3899730355082306, gradient=0.005332669410337366\n",
      "Gradient Descent(256/999): loss=0.3899702584002055, gradient=0.005282307880107931\n",
      "Gradient Descent(257/999): loss=0.38996753345887275, gradient=0.0052324522885647765\n",
      "Gradient Descent(258/999): loss=0.38996485967340927, gradient=0.0051830972088417\n",
      "Gradient Descent(259/999): loss=0.3899622360532133, gradient=0.005134237279379097\n",
      "Gradient Descent(260/999): loss=0.3899596616274831, gradient=0.0050858672030143146\n",
      "Gradient Descent(261/999): loss=0.3899571354448044, gradient=0.005037981746086352\n",
      "Gradient Descent(262/999): loss=0.389954656572749, gradient=0.004990575737555431\n",
      "Gradient Descent(263/999): loss=0.38995222409748026, gradient=0.004943644068136109\n",
      "Gradient Descent(264/999): loss=0.38994983712336695, gradient=0.004897181689444755\n",
      "Gradient Descent(265/999): loss=0.38994749477260815, gradient=0.0048511836131603725\n",
      "Gradient Descent(266/999): loss=0.3899451961848629, gradient=0.004805644910198718\n",
      "Gradient Descent(267/999): loss=0.38994294051689027, gradient=0.0047605607098998\n",
      "Gradient Descent(268/999): loss=0.38994072694219556, gradient=0.004715926199227966\n",
      "Gradient Descent(269/999): loss=0.38993855465068666, gradient=0.004671736621985032\n",
      "Gradient Descent(270/999): loss=0.38993642284833446, gradient=0.004627987278035294\n",
      "Gradient Descent(271/999): loss=0.3899343307568433, gradient=0.004584673522543287\n",
      "Gradient Descent(272/999): loss=0.3899322776133274, gradient=0.004541790765223333\n",
      "Gradient Descent(273/999): loss=0.38993026266999425, gradient=0.004499334469600586\n",
      "Gradient Descent(274/999): loss=0.38992828519383527, gradient=0.004457300152284249\n",
      "Gradient Descent(275/999): loss=0.3899263444663227, gradient=0.00441568338225154\n",
      "Gradient Descent(276/999): loss=0.3899244397831131, gradient=0.0043744797801432\n",
      "Gradient Descent(277/999): loss=0.38992257045375717, gradient=0.004333685017569927\n",
      "Gradient Descent(278/999): loss=0.38992073580141595, gradient=0.0042932948164293655\n",
      "Gradient Descent(279/999): loss=0.38991893516258286, gradient=0.004253304948233929\n",
      "Gradient Descent(280/999): loss=0.3899171678868128, gradient=0.00421371123344898\n",
      "Gradient Descent(281/999): loss=0.3899154333364537, gradient=0.0041745095408411365\n",
      "Gradient Descent(282/999): loss=0.38991373088638853, gradient=0.004135695786836469\n",
      "Gradient Descent(283/999): loss=0.3899120599237792, gradient=0.004097265934889185\n",
      "Gradient Descent(284/999): loss=0.38991041984781705, gradient=0.004059215994859115\n",
      "Gradient Descent(285/999): loss=0.3899088100694791, gradient=0.004021542022399539\n",
      "Gradient Descent(286/999): loss=0.38990723001128846, gradient=0.0039842401183537565\n",
      "Gradient Descent(287/999): loss=0.38990567910708124, gradient=0.003947306428161258\n",
      "Gradient Descent(288/999): loss=0.38990415680177654, gradient=0.003910737141272875\n",
      "Gradient Descent(289/999): loss=0.3899026625511529, gradient=0.0038745284905745176\n",
      "Gradient Descent(290/999): loss=0.3899011958216292, gradient=0.003838676751820022\n",
      "Gradient Descent(291/999): loss=0.3898997560900484, gradient=0.0038031782430722407\n",
      "Gradient Descent(292/999): loss=0.38989834284346886, gradient=0.003768029324153118\n",
      "Gradient Descent(293/999): loss=0.38989695557895776, gradient=0.0037332263961011368\n",
      "Gradient Descent(294/999): loss=0.3898955938033893, gradient=0.003698765900637966\n",
      "Gradient Descent(295/999): loss=0.3898942570332477, gradient=0.0036646443196422054\n",
      "Gradient Descent(296/999): loss=0.3898929447944343, gradient=0.0036308581746317126\n",
      "Gradient Descent(297/999): loss=0.3898916566220776, gradient=0.003597404026253097\n",
      "Gradient Descent(298/999): loss=0.38989039206034837, gradient=0.003564278473779285\n",
      "Gradient Descent(299/999): loss=0.3898891506622793, gradient=0.0035314781546141568\n",
      "Gradient Descent(300/999): loss=0.3898879319895857, gradient=0.0034989997438049625\n",
      "Gradient Descent(301/999): loss=0.3898867356124934, gradient=0.003466839953561526\n",
      "Gradient Descent(302/999): loss=0.3898855611095669, gradient=0.0034349955327828566\n",
      "Gradient Descent(303/999): loss=0.38988440806754365, gradient=0.0034034632665907102\n",
      "Gradient Descent(304/999): loss=0.3898832760811711, gradient=0.0033722399758698207\n",
      "Gradient Descent(305/999): loss=0.38988216475304616, gradient=0.0033413225168150884\n",
      "Gradient Descent(306/999): loss=0.38988107369345854, gradient=0.0033107077804854675\n",
      "Gradient Descent(307/999): loss=0.3898800025202383, gradient=0.00328039269236398\n",
      "Gradient Descent(308/999): loss=0.3898789508586049, gradient=0.003250374211924657\n",
      "Gradient Descent(309/999): loss=0.38987791834102087, gradient=0.003220649332205515\n",
      "Gradient Descent(310/999): loss=0.38987690460704727, gradient=0.003191215079387669\n",
      "Gradient Descent(311/999): loss=0.38987590930320254, gradient=0.00316206851238082\n",
      "Gradient Descent(312/999): loss=0.3898749320828256, gradient=0.0031332067224144684\n",
      "Gradient Descent(313/999): loss=0.3898739726059391, gradient=0.0031046268326351814\n",
      "Gradient Descent(314/999): loss=0.38987303053911726, gradient=0.003076325997709649\n",
      "Gradient Descent(315/999): loss=0.38987210555535756, gradient=0.003048301403433462\n",
      "Gradient Descent(316/999): loss=0.38987119733395115, gradient=0.0030205502663454547\n",
      "Gradient Descent(317/999): loss=0.3898703055603605, gradient=0.0029930698333474463\n",
      "Gradient Descent(318/999): loss=0.38986942992609624, gradient=0.0029658573813298025\n",
      "Gradient Descent(319/999): loss=0.38986857012859844, gradient=0.002938910216801999\n",
      "Gradient Descent(320/999): loss=0.3898677258711188, gradient=0.0029122256755284267\n",
      "Gradient Descent(321/999): loss=0.3898668968626065, gradient=0.002885801122169648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(322/999): loss=0.38986608281759627, gradient=0.002859633949928468\n",
      "Gradient Descent(323/999): loss=0.3898652834560979, gradient=0.0028337215802011163\n",
      "Gradient Descent(324/999): loss=0.3898644985034882, gradient=0.0028080614622333117\n",
      "Gradient Descent(325/999): loss=0.38986372769040606, gradient=0.002782651072781313\n",
      "Gradient Descent(326/999): loss=0.3898629707526488, gradient=0.0027574879157774724\n",
      "Gradient Descent(327/999): loss=0.3898622274310706, gradient=0.002732569522000712\n",
      "Gradient Descent(328/999): loss=0.3898614974714841, gradient=0.002707893448751452\n",
      "Gradient Descent(329/999): loss=0.3898607806245617, gradient=0.0026834572795311627\n",
      "Gradient Descent(330/999): loss=0.3898600766457413, gradient=0.002659258623726291\n",
      "Gradient Descent(331/999): loss=0.3898593852951331, gradient=0.0026352951162967758\n",
      "Gradient Descent(332/999): loss=0.38985870633742714, gradient=0.0026115644174686584\n",
      "Gradient Descent(333/999): loss=0.38985803954180503, gradient=0.0025880642124308873\n",
      "Gradient Descent(334/999): loss=0.3898573846818505, gradient=0.0025647922110367334\n",
      "Gradient Descent(335/999): loss=0.38985674153546535, gradient=0.0025417461475088225\n",
      "Gradient Descent(336/999): loss=0.38985610988478303, gradient=0.0025189237801484434\n",
      "Gradient Descent(337/999): loss=0.3898554895160871, gradient=0.0024963228910489664\n",
      "Gradient Descent(338/999): loss=0.389854880219731, gradient=0.002473941285812872\n",
      "Gradient Descent(339/999): loss=0.3898542817900576, gradient=0.002451776793272933\n",
      "Gradient Descent(340/999): loss=0.3898536940253215, gradient=0.0024298272652169637\n",
      "Gradient Descent(341/999): loss=0.389853116727613, gradient=0.002408090576116553\n",
      "Gradient Descent(342/999): loss=0.38985254970278443, gradient=0.0023865646228592914\n",
      "Gradient Descent(343/999): loss=0.38985199276037547, gradient=0.0023652473244845256\n",
      "Gradient Descent(344/999): loss=0.3898514457135423, gradient=0.002344136621923131\n",
      "Gradient Descent(345/999): loss=0.3898509083789877, gradient=0.0023232304777402128\n",
      "Gradient Descent(346/999): loss=0.38985038057689125, gradient=0.0023025268758816494\n",
      "Gradient Descent(347/999): loss=0.38984986213084244, gradient=0.002282023821423901\n",
      "Gradient Descent(348/999): loss=0.3898493528677746, gradient=0.0022617193403272275\n",
      "Gradient Descent(349/999): loss=0.3898488526178998, gradient=0.002241611479192153\n",
      "Gradient Descent(350/999): loss=0.3898483612146452, gradient=0.002221698305019107\n",
      "Gradient Descent(351/999): loss=0.38984787849459146, gradient=0.0022019779049713894\n",
      "Gradient Descent(352/999): loss=0.3898474042974115, gradient=0.0021824483861413355\n",
      "Gradient Descent(353/999): loss=0.3898469384658098, gradient=0.002163107875319248\n",
      "Gradient Descent(354/999): loss=0.38984648084546514, gradient=0.0021439545187657955\n",
      "Gradient Descent(355/999): loss=0.3898460312849724, gradient=0.002124986481987071\n",
      "Gradient Descent(356/999): loss=0.3898455896357859, gradient=0.0021062019495129256\n",
      "Gradient Descent(357/999): loss=0.3898451557521655, gradient=0.002087599124677753\n",
      "Gradient Descent(358/999): loss=0.3898447294911209, gradient=0.0020691762294048127\n",
      "Gradient Descent(359/999): loss=0.3898443107123596, gradient=0.00205093150399259\n",
      "Gradient Descent(360/999): loss=0.38984389927823493, gradient=0.002032863206904662\n",
      "Gradient Descent(361/999): loss=0.38984349505369476, gradient=0.0020149696145616474\n",
      "Gradient Descent(362/999): loss=0.3898430979062314, gradient=0.001997249021136551\n",
      "Gradient Descent(363/999): loss=0.3898427077058331, gradient=0.0019796997383520307\n",
      "Gradient Descent(364/999): loss=0.38984232432493593, gradient=0.001962320095280845\n",
      "Gradient Descent(365/999): loss=0.3898419476383763, gradient=0.0019451084381486093\n",
      "Gradient Descent(366/999): loss=0.3898415775233456, gradient=0.0019280631301391285\n",
      "Gradient Descent(367/999): loss=0.38984121385934395, gradient=0.0019111825512022384\n",
      "Gradient Descent(368/999): loss=0.3898408565281373, gradient=0.0018944650978642058\n",
      "Gradient Descent(369/999): loss=0.38984050541371223, gradient=0.0018779091830402952\n",
      "Gradient Descent(370/999): loss=0.38984016040223507, gradient=0.0018615132358500255\n",
      "Gradient Descent(371/999): loss=0.38983982138200923, gradient=0.0018452757014346041\n",
      "Gradient Descent(372/999): loss=0.38983948824343345, gradient=0.0018291950407766941\n",
      "Gradient Descent(373/999): loss=0.38983916087896386, gradient=0.0018132697305224823\n",
      "Gradient Descent(374/999): loss=0.3898388391830724, gradient=0.001797498262806074\n",
      "Gradient Descent(375/999): loss=0.38983852305220945, gradient=0.0017818791450759475\n",
      "Gradient Descent(376/999): loss=0.389838212384766, gradient=0.0017664108999237368\n",
      "Gradient Descent(377/999): loss=0.3898379070810358, gradient=0.001751092064915078\n",
      "Gradient Descent(378/999): loss=0.3898376070431797, gradient=0.0017359211924226107\n",
      "Gradient Descent(379/999): loss=0.3898373121751898, gradient=0.0017208968494610277\n",
      "Gradient Descent(380/999): loss=0.38983702238285417, gradient=0.0017060176175244034\n",
      "Gradient Descent(381/999): loss=0.3898367375737234, gradient=0.001691282092425095\n",
      "Gradient Descent(382/999): loss=0.3898364576570758, gradient=0.0016766888841349967\n",
      "Gradient Descent(383/999): loss=0.38983618254388547, gradient=0.0016622366166289563\n",
      "Gradient Descent(384/999): loss=0.3898359121467899, gradient=0.001647923927729445\n",
      "Gradient Descent(385/999): loss=0.3898356463800572, gradient=0.0016337494689538424\n",
      "Gradient Descent(386/999): loss=0.38983538515955657, gradient=0.0016197119053632414\n",
      "Gradient Descent(387/999): loss=0.38983512840272666, gradient=0.0016058099154132\n",
      "Gradient Descent(388/999): loss=0.3898348760285463, gradient=0.0015920421908063665\n",
      "Gradient Descent(389/999): loss=0.3898346279575054, gradient=0.0015784074363467309\n",
      "Gradient Descent(390/999): loss=0.3898343841115751, gradient=0.001564904369795988\n",
      "Gradient Descent(391/999): loss=0.38983414441418096, gradient=0.0015515317217314065\n",
      "Gradient Descent(392/999): loss=0.3898339087901748, gradient=0.00153828823540538\n",
      "Gradient Descent(393/999): loss=0.38983367716580736, gradient=0.0015251726666069118\n",
      "Gradient Descent(394/999): loss=0.3898334494687021, gradient=0.0015121837835246736\n",
      "Gradient Descent(395/999): loss=0.3898332256278292, gradient=0.0014993203666116414\n",
      "Gradient Descent(396/999): loss=0.38983300557347905, gradient=0.0014865812084515109\n",
      "Gradient Descent(397/999): loss=0.38983278923723924, gradient=0.0014739651136266086\n",
      "Gradient Descent(398/999): loss=0.38983257655196807, gradient=0.0014614708985875202\n",
      "Gradient Descent(399/999): loss=0.38983236745177097, gradient=0.0014490973915241223\n",
      "Gradient Descent(400/999): loss=0.3898321618719774, gradient=0.001436843432238344\n",
      "Gradient Descent(401/999): loss=0.3898319597491176, gradient=0.0014247078720182353\n",
      "Gradient Descent(402/999): loss=0.3898317610208991, gradient=0.0014126895735138906\n",
      "Gradient Descent(403/999): loss=0.38983156562618526, gradient=0.0014007874106143883\n",
      "Gradient Descent(404/999): loss=0.38983137350497316, gradient=0.0013890002683266704\n",
      "Gradient Descent(405/999): loss=0.389831184598372, gradient=0.0013773270426553892\n",
      "Gradient Descent(406/999): loss=0.3898309988485826, gradient=0.001365766640484694\n",
      "Gradient Descent(407/999): loss=0.3898308161988758, gradient=0.0013543179794609668\n",
      "Gradient Descent(408/999): loss=0.38983063659357314, gradient=0.0013429799878772442\n",
      "Gradient Descent(409/999): loss=0.3898304599780272, gradient=0.0013317516045588556\n",
      "Gradient Descent(410/999): loss=0.3898302862986011, gradient=0.0013206317787504119\n",
      "Gradient Descent(411/999): loss=0.3898301155026502, gradient=0.0013096194700042982\n",
      "Gradient Descent(412/999): loss=0.38982994753850325, gradient=0.0012987136480702263\n",
      "Gradient Descent(413/999): loss=0.38982978235544447, gradient=0.0012879132927862702\n",
      "Gradient Descent(414/999): loss=0.3898296199036942, gradient=0.0012772173939711372\n",
      "Gradient Descent(415/999): loss=0.3898294601343933, gradient=0.0012666249513176029\n",
      "Gradient Descent(416/999): loss=0.38982930299958374, gradient=0.0012561349742873438\n",
      "Gradient Descent(417/999): loss=0.38982914845219263, gradient=0.0012457464820069357\n",
      "Gradient Descent(418/999): loss=0.38982899644601643, gradient=0.0012354585031650095\n",
      "Gradient Descent(419/999): loss=0.3898288469357029, gradient=0.0012252700759106834\n",
      "Gradient Descent(420/999): loss=0.3898286998767361, gradient=0.0012151802477531287\n",
      "Gradient Descent(421/999): loss=0.38982855522542054, gradient=0.0012051880754623968\n",
      "Gradient Descent(422/999): loss=0.3898284129388654, gradient=0.001195292624971185\n",
      "Gradient Descent(423/999): loss=0.3898282729749702, gradient=0.0011854929712779782\n",
      "Gradient Descent(424/999): loss=0.38982813529240906, gradient=0.0011757881983511913\n",
      "Gradient Descent(425/999): loss=0.38982799985061656, gradient=0.0011661773990344138\n",
      "Gradient Descent(426/999): loss=0.38982786660977375, gradient=0.0011566596749528354\n",
      "Gradient Descent(427/999): loss=0.3898277355307936, gradient=0.0011472341364206081\n",
      "Gradient Descent(428/999): loss=0.3898276065753077, gradient=0.001137899902349345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(429/999): loss=0.38982747970565257, gradient=0.001128656100157891\n",
      "Gradient Descent(430/999): loss=0.38982735488485654, gradient=0.0011195018656825787\n",
      "Gradient Descent(431/999): loss=0.38982723207662645, gradient=0.0011104363430891624\n",
      "Gradient Descent(432/999): loss=0.38982711124533587, gradient=0.001101458684785336\n",
      "Gradient Descent(433/999): loss=0.3898269923560117, gradient=0.0010925680513343447\n",
      "Gradient Descent(434/999): loss=0.3898268753743216, gradient=0.0010837636113696716\n",
      "Gradient Descent(435/999): loss=0.38982676026656365, gradient=0.0010750445415106794\n",
      "Gradient Descent(436/999): loss=0.38982664699965314, gradient=0.0010664100262790597\n",
      "Gradient Descent(437/999): loss=0.38982653554111135, gradient=0.0010578592580165584\n",
      "Gradient Descent(438/999): loss=0.3898264258590542, gradient=0.0010493914368032117\n",
      "Gradient Descent(439/999): loss=0.38982631792218125, gradient=0.0010410057703769528\n",
      "Gradient Descent(440/999): loss=0.38982621169976517, gradient=0.0010327014740538043\n",
      "Gradient Descent(441/999): loss=0.3898261071616402, gradient=0.0010244777706491118\n",
      "Gradient Descent(442/999): loss=0.3898260042781928, gradient=0.0010163338903997352\n",
      "Gradient Descent(443/999): loss=0.3898259030203495, gradient=0.0010082690708870191\n",
      "Gradient Descent(444/999): loss=0.38982580335956896, gradient=0.0010002825569606453\n",
      "Gradient Descent(445/999): loss=0.38982570526783095, gradient=0.0009923736006634191\n",
      "Gradient Descent(446/999): loss=0.3898256087176264, gradient=0.0009845414611568575\n",
      "Gradient Descent(447/999): loss=0.38982551368194807, gradient=0.0009767854046476279\n",
      "Gradient Descent(448/999): loss=0.38982542013428184, gradient=0.0009691047043148827\n",
      "Gradient Descent(449/999): loss=0.3898253280485972, gradient=0.0009614986402382616\n",
      "Gradient Descent(450/999): loss=0.3898252373993373, gradient=0.0009539664993269775\n",
      "Gradient Descent(451/999): loss=0.38982514816141123, gradient=0.0009465075752493042\n",
      "Gradient Descent(452/999): loss=0.3898250603101855, gradient=0.000939121168363353\n",
      "Gradient Descent(453/999): loss=0.38982497382147413, gradient=0.0009318065856482442\n",
      "Gradient Descent(454/999): loss=0.38982488867153203, gradient=0.0009245631406361966\n",
      "Gradient Descent(455/999): loss=0.38982480483704574, gradient=0.0009173901533454509\n",
      "Gradient Descent(456/999): loss=0.38982472229512577, gradient=0.0009102869502138631\n",
      "Gradient Descent(457/999): loss=0.38982464102329795, gradient=0.0009032528640332226\n",
      "Gradient Descent(458/999): loss=0.3898245609994975, gradient=0.0008962872338844479\n",
      "Gradient Descent(459/999): loss=0.38982448220205956, gradient=0.0008893894050733543\n",
      "Gradient Descent(460/999): loss=0.38982440460971274, gradient=0.0008825587290673834\n",
      "Gradient Descent(461/999): loss=0.3898243282015713, gradient=0.0008757945634326437\n",
      "Gradient Descent(462/999): loss=0.389824252957129, gradient=0.0008690962717722037\n",
      "Gradient Descent(463/999): loss=0.38982417885625076, gradient=0.0008624632236645365\n",
      "Gradient Descent(464/999): loss=0.3898241058791661, gradient=0.0008558947946031197\n",
      "Gradient Descent(465/999): loss=0.38982403400646304, gradient=0.0008493903659364056\n",
      "Gradient Descent(466/999): loss=0.3898239632190818, gradient=0.0008429493248086824\n",
      "Gradient Descent(467/999): loss=0.3898238934983057, gradient=0.0008365710641014258\n",
      "Gradient Descent(468/999): loss=0.38982382482575817, gradient=0.000830254982375472\n",
      "Gradient Descent(469/999): loss=0.38982375718339446, gradient=0.0008240004838136214\n",
      "Gradient Descent(470/999): loss=0.38982369055349547, gradient=0.0008178069781643894\n",
      "Gradient Descent(471/999): loss=0.3898236249186632, gradient=0.0008116738806856589\n",
      "Gradient Descent(472/999): loss=0.38982356026181286, gradient=0.0008056006120896465\n",
      "Gradient Descent(473/999): loss=0.38982349656616794, gradient=0.0007995865984881037\n",
      "Gradient Descent(474/999): loss=0.3898234338152561, gradient=0.0007936312713382977\n",
      "Gradient Descent(475/999): loss=0.3898233719928997, gradient=0.0007877340673895491\n",
      "Gradient Descent(476/999): loss=0.3898233110832145, gradient=0.0007818944286303535\n",
      "Gradient Descent(477/999): loss=0.38982325107060206, gradient=0.0007761118022362553\n",
      "Gradient Descent(478/999): loss=0.389823191939744, gradient=0.0007703856405181234\n",
      "Gradient Descent(479/999): loss=0.38982313367559873, gradient=0.0007647154008710527\n",
      "Gradient Descent(480/999): loss=0.38982307626339463, gradient=0.0007591005457240854\n",
      "Gradient Descent(481/999): loss=0.38982301968862587, gradient=0.0007535405424900836\n",
      "Gradient Descent(482/999): loss=0.3898229639370472, gradient=0.0007480348635165737\n",
      "Gradient Descent(483/999): loss=0.38982290899466937, gradient=0.0007425829860369871\n",
      "Gradient Descent(484/999): loss=0.3898228548477542, gradient=0.0007371843921222809\n",
      "Gradient Descent(485/999): loss=0.3898228014828105, gradient=0.0007318385686335667\n",
      "Gradient Descent(486/999): loss=0.38982274888658885, gradient=0.0007265450071747171\n",
      "Gradient Descent(487/999): loss=0.3898226970460771, gradient=0.0007213032040460707\n",
      "Gradient Descent(488/999): loss=0.38982264594849614, gradient=0.0007161126601982071\n",
      "Gradient Descent(489/999): loss=0.38982259558129667, gradient=0.0007109728811865212\n",
      "Gradient Descent(490/999): loss=0.389822545932153, gradient=0.0007058833771263324\n",
      "Gradient Descent(491/999): loss=0.38982249698896093, gradient=0.0007008436626482259\n",
      "Gradient Descent(492/999): loss=0.3898224487398317, gradient=0.0006958532568543097\n",
      "Gradient Descent(493/999): loss=0.38982240117309014, gradient=0.0006909116832746295\n",
      "Gradient Descent(494/999): loss=0.38982235427726875, gradient=0.0006860184698243119\n",
      "Gradient Descent(495/999): loss=0.3898223080411055, gradient=0.0006811731487610993\n",
      "Gradient Descent(496/999): loss=0.3898222624535386, gradient=0.0006763752566433548\n",
      "Gradient Descent(497/999): loss=0.38982221750370444, gradient=0.000671624334288657\n",
      "Gradient Descent(498/999): loss=0.3898221731809322, gradient=0.0006669199267327462\n",
      "Gradient Descent(499/999): loss=0.3898221294747417, gradient=0.000662261583189018\n",
      "Gradient Descent(500/999): loss=0.38982208637483967, gradient=0.0006576488570086092\n",
      "Gradient Descent(501/999): loss=0.38982204387111524, gradient=0.0006530813056405358\n",
      "Gradient Descent(502/999): loss=0.3898220019536381, gradient=0.0006485584905927934\n",
      "Gradient Descent(503/999): loss=0.3898219606126538, gradient=0.0006440799773936805\n",
      "Gradient Descent(504/999): loss=0.38982191983858216, gradient=0.0006396453355534327\n",
      "Gradient Descent(505/999): loss=0.3898218796220124, gradient=0.0006352541385265085\n",
      "Gradient Descent(506/999): loss=0.38982183995370107, gradient=0.0006309059636743762\n",
      "Gradient Descent(507/999): loss=0.38982180082456813, gradient=0.0006266003922283607\n",
      "Gradient Descent(508/999): loss=0.3898217622256954, gradient=0.0006223370092533501\n",
      "Gradient Descent(509/999): loss=0.38982172414832217, gradient=0.0006181154036117014\n",
      "Gradient Descent(510/999): loss=0.38982168658384303, gradient=0.0006139351679275664\n",
      "Gradient Descent(511/999): loss=0.3898216495238045, gradient=0.0006097958985516829\n",
      "Gradient Descent(512/999): loss=0.38982161295990303, gradient=0.0006056971955265739\n",
      "Gradient Descent(513/999): loss=0.3898215768839818, gradient=0.0006016386625521594\n",
      "Gradient Descent(514/999): loss=0.38982154128802793, gradient=0.0005976199069517231\n",
      "Gradient Descent(515/999): loss=0.38982150616416955, gradient=0.0005936405396383368\n",
      "Gradient Descent(516/999): loss=0.38982147150467505, gradient=0.0005897001750815015\n",
      "Gradient Descent(517/999): loss=0.38982143730194757, gradient=0.0005857984312745349\n",
      "Gradient Descent(518/999): loss=0.38982140354852524, gradient=0.0005819349297019621\n",
      "Gradient Descent(519/999): loss=0.38982137023707675, gradient=0.0005781092953074806\n",
      "Gradient Descent(520/999): loss=0.38982133736040037, gradient=0.0005743211564622669\n",
      "Gradient Descent(521/999): loss=0.389821304911421, gradient=0.0005705701449336859\n",
      "Gradient Descent(522/999): loss=0.38982127288318746, gradient=0.0005668558958542201\n",
      "Gradient Descent(523/999): loss=0.3898212412688713, gradient=0.0005631780476909454\n",
      "Gradient Descent(524/999): loss=0.38982121006176335, gradient=0.0005595362422151943\n",
      "Gradient Descent(525/999): loss=0.3898211792552724, gradient=0.000555930124472795\n",
      "Gradient Descent(526/999): loss=0.3898211488429229, gradient=0.0005523593427543178\n",
      "Gradient Descent(527/999): loss=0.38982111881835246, gradient=0.00054882354856603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(528/999): loss=0.38982108917531, gradient=0.0005453223966008932\n",
      "Gradient Descent(529/999): loss=0.38982105990765414, gradient=0.0005418555447101441\n",
      "Gradient Descent(530/999): loss=0.38982103100935106, gradient=0.0005384226538749416\n",
      "Gradient Descent(531/999): loss=0.3898210024744714, gradient=0.0005350233881785119\n",
      "Gradient Descent(532/999): loss=0.38982097429719004, gradient=0.0005316574147786913\n",
      "Gradient Descent(533/999): loss=0.3898209464717836, gradient=0.000528324403880492\n",
      "Gradient Descent(534/999): loss=0.3898209189926279, gradient=0.0005250240287092513\n",
      "Gradient Descent(535/999): loss=0.38982089185419727, gradient=0.0005217559654840554\n",
      "Gradient Descent(536/999): loss=0.38982086505106167, gradient=0.0005185198933912531\n",
      "Gradient Descent(537/999): loss=0.38982083857788635, gradient=0.000515315494558541\n",
      "Gradient Descent(538/999): loss=0.38982081242942807, gradient=0.0005121424540292101\n",
      "Gradient Descent(539/999): loss=0.38982078660053576, gradient=0.0005090004597366613\n",
      "Gradient Descent(540/999): loss=0.3898207610861474, gradient=0.0005058892024792307\n",
      "Gradient Descent(541/999): loss=0.3898207358812884, gradient=0.0005028083758954037\n",
      "Gradient Descent(542/999): loss=0.3898207109810709, gradient=0.0004997576764391293\n",
      "Gradient Descent(543/999): loss=0.3898206863806914, gradient=0.0004967368033555357\n",
      "Gradient Descent(544/999): loss=0.3898206620754293, gradient=0.000493745458656872\n",
      "Gradient Descent(545/999): loss=0.38982063806064593, gradient=0.0004907833470987475\n",
      "Gradient Descent(546/999): loss=0.38982061433178306, gradient=0.0004878501761565341\n",
      "Gradient Descent(547/999): loss=0.3898205908843602, gradient=0.00048494565600222756\n",
      "Gradient Descent(548/999): loss=0.3898205677139749, gradient=0.00048206949948131954\n",
      "Gradient Descent(549/999): loss=0.38982054481630063, gradient=0.0004792214220901448\n",
      "Gradient Descent(550/999): loss=0.3898205221870846, gradient=0.0004764011419533217\n",
      "Gradient Descent(551/999): loss=0.3898204998221479, gradient=0.00047360837980146717\n",
      "Gradient Descent(552/999): loss=0.38982047771738326, gradient=0.00047084285894926633\n",
      "Gradient Descent(553/999): loss=0.3898204558687539, gradient=0.00046810430527360214\n",
      "Gradient Descent(554/999): loss=0.3898204342722923, gradient=0.0004653924471920488\n",
      "Gradient Descent(555/999): loss=0.38982041292409886, gradient=0.00046270701564153925\n",
      "Gradient Descent(556/999): loss=0.3898203918203413, gradient=0.00046004774405726144\n",
      "Gradient Descent(557/999): loss=0.38982037095725214, gradient=0.0004574143683517743\n",
      "Gradient Descent(558/999): loss=0.389820350331129, gradient=0.00045480662689440535\n",
      "Gradient Descent(559/999): loss=0.3898203299383322, gradient=0.0004522242604907497\n",
      "Gradient Descent(560/999): loss=0.38982030977528526, gradient=0.0004496670123624887\n",
      "Gradient Descent(561/999): loss=0.38982028983847106, gradient=0.0004471346281273113\n",
      "Gradient Descent(562/999): loss=0.3898202701244336, gradient=0.00044462685577919826\n",
      "Gradient Descent(563/999): loss=0.3898202506297757, gradient=0.00044214344566865285\n",
      "Gradient Descent(564/999): loss=0.3898202313511575, gradient=0.0004396841504834359\n",
      "Gradient Descent(565/999): loss=0.3898202122852959, gradient=0.00043724872522925545\n",
      "Gradient Descent(566/999): loss=0.3898201934289638, gradient=0.0004348369272107741\n",
      "Gradient Descent(567/999): loss=0.38982017477898884, gradient=0.00043244851601274376\n",
      "Gradient Descent(568/999): loss=0.38982015633225214, gradient=0.00043008325348131547\n",
      "Gradient Descent(569/999): loss=0.3898201380856882, gradient=0.0004277409037056567\n",
      "Gradient Descent(570/999): loss=0.3898201200362825, gradient=0.0004254212329995396\n",
      "Gradient Descent(571/999): loss=0.3898201021810721, gradient=0.0004231240098832925\n",
      "Gradient Descent(572/999): loss=0.3898200845171438, gradient=0.00042084900506580096\n",
      "Gradient Descent(573/999): loss=0.38982006704163363, gradient=0.00041859599142671277\n",
      "Gradient Descent(574/999): loss=0.38982004975172607, gradient=0.00041636474399892677\n",
      "Gradient Descent(575/999): loss=0.3898200326446527, gradient=0.0004141550399509548\n",
      "Gradient Descent(576/999): loss=0.3898200157176912, gradient=0.0004119666585697988\n",
      "Gradient Descent(577/999): loss=0.3898199989681658, gradient=0.00040979938124371647\n",
      "Gradient Descent(578/999): loss=0.38981998239344545, gradient=0.00040765299144522723\n",
      "Gradient Descent(579/999): loss=0.3898199659909431, gradient=0.00040552727471434815\n",
      "Gradient Descent(580/999): loss=0.3898199497581142, gradient=0.0004034220186418279\n",
      "Gradient Descent(581/999): loss=0.3898199336924581, gradient=0.00040133701285270136\n",
      "Gradient Descent(582/999): loss=0.38981991779151537, gradient=0.00039927204898972346\n",
      "Gradient Descent(583/999): loss=0.3898199020528674, gradient=0.0003972269206974006\n",
      "Gradient Descent(584/999): loss=0.38981988647413574, gradient=0.0003952014236055336\n",
      "Gradient Descent(585/999): loss=0.3898198710529823, gradient=0.0003931953553134967\n",
      "Gradient Descent(586/999): loss=0.3898198557871072, gradient=0.00039120851537423655\n",
      "Gradient Descent(587/999): loss=0.38981984067424896, gradient=0.00038924070527864414\n",
      "Gradient Descent(588/999): loss=0.38981982571218377, gradient=0.0003872917284398892\n",
      "Gradient Descent(589/999): loss=0.38981981089872464, gradient=0.0003853613901779934\n",
      "Gradient Descent(590/999): loss=0.38981979623172075, gradient=0.0003834494977045389\n",
      "Gradient Descent(591/999): loss=0.3898197817090566, gradient=0.00038155586010729225\n",
      "Gradient Descent(592/999): loss=0.3898197673286519, gradient=0.0003796802883352831\n",
      "Gradient Descent(593/999): loss=0.3898197530884609, gradient=0.0003778225951837098\n",
      "Gradient Descent(594/999): loss=0.3898197389864711, gradient=0.0003759825952790513\n",
      "Gradient Descent(595/999): loss=0.3898197250207033, gradient=0.0003741601050644158\n",
      "Gradient Descent(596/999): loss=0.389819711189211, gradient=0.0003723549427847749\n",
      "Gradient Descent(597/999): loss=0.38981969749007955, gradient=0.0003705669284724558\n",
      "Gradient Descent(598/999): loss=0.38981968392142563, gradient=0.0003687958839327551\n",
      "Gradient Descent(599/999): loss=0.3898196704813966, gradient=0.00036704163272957975\n",
      "Gradient Descent(600/999): loss=0.3898196571681706, gradient=0.00036530400017118495\n",
      "Gradient Descent(601/999): loss=0.3898196439799554, gradient=0.00036358281329606736\n",
      "Gradient Descent(602/999): loss=0.38981963091498756, gradient=0.0003618779008590175\n",
      "Gradient Descent(603/999): loss=0.38981961797153314, gradient=0.00036018909331702605\n",
      "Gradient Descent(604/999): loss=0.3898196051478855, gradient=0.00035851622281555764\n",
      "Gradient Descent(605/999): loss=0.38981959244236664, gradient=0.00035685912317483264\n",
      "Gradient Descent(606/999): loss=0.3898195798533253, gradient=0.0003552176298761188\n",
      "Gradient Descent(607/999): loss=0.3898195673791371, gradient=0.00035359158004817046\n",
      "Gradient Descent(608/999): loss=0.3898195550182039, gradient=0.0003519808124537971\n",
      "Gradient Descent(609/999): loss=0.3898195427689535, gradient=0.0003503851674764767\n",
      "Gradient Descent(610/999): loss=0.38981953062983904, gradient=0.00034880448710701437\n",
      "Gradient Descent(611/999): loss=0.38981951859933855, gradient=0.0003472386149304087\n",
      "Gradient Descent(612/999): loss=0.3898195066759549, gradient=0.0003456873961126283\n",
      "Gradient Descent(613/999): loss=0.38981949485821465, gradient=0.0003441506773877048\n",
      "Gradient Descent(614/999): loss=0.38981948314466786, gradient=0.000342628307044637\n",
      "Gradient Descent(615/999): loss=0.38981947153388863, gradient=0.0003411201349145567\n",
      "Gradient Descent(616/999): loss=0.38981946002447304, gradient=0.00033962601235800014\n",
      "Gradient Descent(617/999): loss=0.38981944861504014, gradient=0.0003381457922520694\n",
      "Gradient Descent(618/999): loss=0.3898194373042308, gradient=0.0003366793289778093\n",
      "Gradient Descent(619/999): loss=0.38981942609070785, gradient=0.00033522647840770147\n",
      "Gradient Descent(620/999): loss=0.38981941497315475, gradient=0.0003337870978930999\n",
      "Gradient Descent(621/999): loss=0.38981940395027664, gradient=0.00033236104625180993\n",
      "Gradient Descent(622/999): loss=0.38981939302079893, gradient=0.0003309481837558378\n",
      "Gradient Descent(623/999): loss=0.38981938218346723, gradient=0.00032954837211891244\n",
      "Gradient Descent(624/999): loss=0.389819371437047, gradient=0.0003281614744844732\n",
      "Gradient Descent(625/999): loss=0.38981936078032303, gradient=0.00032678735541342584\n",
      "Gradient Descent(626/999): loss=0.38981935021209996, gradient=0.0003254258808721023\n",
      "Gradient Descent(627/999): loss=0.38981933973120025, gradient=0.0003240769182202438\n",
      "Gradient Descent(628/999): loss=0.3898193293364657, gradient=0.00032274033619913284\n",
      "Gradient Descent(629/999): loss=0.38981931902675593, gradient=0.00032141600491964763\n",
      "Gradient Descent(630/999): loss=0.3898193088009487, gradient=0.0003201037958504376\n",
      "Gradient Descent(631/999): loss=0.38981929865793874, gradient=0.0003188035818063795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(632/999): loss=0.3898192885966393, gradient=0.00031751523693664845\n",
      "Gradient Descent(633/999): loss=0.38981927861597904, gradient=0.00031623863671338974\n",
      "Gradient Descent(634/999): loss=0.3898192687149043, gradient=0.00031497365791990516\n",
      "Gradient Descent(635/999): loss=0.389819258892378, gradient=0.00031372017863941416\n",
      "Gradient Descent(636/999): loss=0.3898192491473785, gradient=0.00031247807824350227\n",
      "Gradient Descent(637/999): loss=0.3898192394788999, gradient=0.00031124723738089\n",
      "Gradient Descent(638/999): loss=0.3898192298859529, gradient=0.0003100275379659805\n",
      "Gradient Descent(639/999): loss=0.3898192203675622, gradient=0.00030881886316784605\n",
      "Gradient Descent(640/999): loss=0.3898192109227688, gradient=0.00030762109739891\n",
      "Gradient Descent(641/999): loss=0.3898192015506274, gradient=0.00030643412630389065\n",
      "Gradient Descent(642/999): loss=0.38981919225020817, gradient=0.0003052578367488399\n",
      "Gradient Descent(643/999): loss=0.38981918302059454, gradient=0.0003040921168100996\n",
      "Gradient Descent(644/999): loss=0.3898191738608851, gradient=0.0003029368557633423\n",
      "Gradient Descent(645/999): loss=0.38981916477019135, gradient=0.0003017919440728412\n",
      "Gradient Descent(646/999): loss=0.38981915574763887, gradient=0.0003006572733805601\n",
      "Gradient Descent(647/999): loss=0.3898191467923661, gradient=0.0002995327364955315\n",
      "Gradient Descent(648/999): loss=0.38981913790352507, gradient=0.00029841822738308166\n",
      "Gradient Descent(649/999): loss=0.3898191290802807, gradient=0.00029731364115427193\n",
      "Gradient Descent(650/999): loss=0.3898191203218102, gradient=0.00029621887405534216\n",
      "Gradient Descent(651/999): loss=0.3898191116273035, gradient=0.00029513382345723653\n",
      "Gradient Descent(652/999): loss=0.3898191029959626, gradient=0.00029405838784511397\n",
      "Gradient Descent(653/999): loss=0.38981909442700186, gradient=0.00029299246680798317\n",
      "Gradient Descent(654/999): loss=0.3898190859196468, gradient=0.00029193596102848753\n",
      "Gradient Descent(655/999): loss=0.3898190774731357, gradient=0.00029088877227248364\n",
      "Gradient Descent(656/999): loss=0.389819069086717, gradient=0.0002898508033789971\n",
      "Gradient Descent(657/999): loss=0.3898190607596511, gradient=0.0002888219582500362\n",
      "Gradient Descent(658/999): loss=0.38981905249120974, gradient=0.00028780214184041944\n",
      "Gradient Descent(659/999): loss=0.38981904428067465, gradient=0.000286791260147901\n",
      "Gradient Descent(660/999): loss=0.38981903612733926, gradient=0.000285789220203108\n",
      "Gradient Descent(661/999): loss=0.3898190280305066, gradient=0.00028479593005970185\n",
      "Gradient Descent(662/999): loss=0.38981901998949037, gradient=0.0002838112987844982\n",
      "Gradient Descent(663/999): loss=0.3898190120036147, gradient=0.00028283523644766196\n",
      "Gradient Descent(664/999): loss=0.3898190040722134, gradient=0.0002818676541131472\n",
      "Gradient Descent(665/999): loss=0.3898189961946302, gradient=0.0002809084638286836\n",
      "Gradient Descent(666/999): loss=0.38981898837021817, gradient=0.0002799575786165426\n",
      "Gradient Descent(667/999): loss=0.38981898059834086, gradient=0.000279014912463828\n",
      "Gradient Descent(668/999): loss=0.38981897287837, gradient=0.0002780803803129608\n",
      "Gradient Descent(669/999): loss=0.389818965209687, gradient=0.00027715389805228904\n",
      "Gradient Descent(670/999): loss=0.3898189575916823, gradient=0.00027623538250666236\n",
      "Gradient Descent(671/999): loss=0.38981895002375555, gradient=0.00027532475142831385\n",
      "Gradient Descent(672/999): loss=0.38981894250531457, gradient=0.0002744219234873253\n",
      "Gradient Descent(673/999): loss=0.38981893503577575, gradient=0.0002735268182626848\n",
      "Gradient Descent(674/999): loss=0.38981892761456466, gradient=0.0002726393562329745\n",
      "Gradient Descent(675/999): loss=0.38981892024111436, gradient=0.0002717594587674248\n",
      "Gradient Descent(676/999): loss=0.3898189129148665, gradient=0.0002708870481168255\n",
      "Gradient Descent(677/999): loss=0.38981890563527083, gradient=0.0002700220474046398\n",
      "Gradient Descent(678/999): loss=0.3898188984017845, gradient=0.0002691643806179976\n",
      "Gradient Descent(679/999): loss=0.3898188912138732, gradient=0.00026831397259904547\n",
      "Gradient Descent(680/999): loss=0.38981888407100984, gradient=0.00026747074903593805\n",
      "Gradient Descent(681/999): loss=0.3898188769726745, gradient=0.00026663463645437295\n",
      "Gradient Descent(682/999): loss=0.3898188699183554, gradient=0.0002658055622087562\n",
      "Gradient Descent(683/999): loss=0.3898188629075474, gradient=0.00026498345447365434\n",
      "Gradient Descent(684/999): loss=0.3898188559397533, gradient=0.00026416824223528956\n",
      "Gradient Descent(685/999): loss=0.3898188490144819, gradient=0.0002633598552831395\n",
      "Gradient Descent(686/999): loss=0.3898188421312495, gradient=0.00026255822420129065\n",
      "Gradient Descent(687/999): loss=0.3898188352895794, gradient=0.0002617632803603315\n",
      "Gradient Descent(688/999): loss=0.38981882848900146, gradient=0.00026097495590897846\n",
      "Gradient Descent(689/999): loss=0.3898188217290519, gradient=0.0002601931837657479\n",
      "Gradient Descent(690/999): loss=0.3898188150092735, gradient=0.000259417897610942\n",
      "Gradient Descent(691/999): loss=0.38981880832921556, gradient=0.0002586490318784339\n",
      "Gradient Descent(692/999): loss=0.3898188016884336, gradient=0.00025788652174764675\n",
      "Gradient Descent(693/999): loss=0.3898187950864896, gradient=0.0002571303031356035\n",
      "Gradient Descent(694/999): loss=0.38981878852295093, gradient=0.0002563803126888894\n",
      "Gradient Descent(695/999): loss=0.3898187819973915, gradient=0.00025563648777589634\n",
      "Gradient Descent(696/999): loss=0.389818775509391, gradient=0.0002548987664789486\n",
      "Gradient Descent(697/999): loss=0.38981876905853496, gradient=0.0002541670875866352\n",
      "Gradient Descent(698/999): loss=0.3898187626444143, gradient=0.00025344139058598746\n",
      "Gradient Descent(699/999): loss=0.3898187562666258, gradient=0.00025272161565495906\n",
      "Gradient Descent(700/999): loss=0.38981874992477195, gradient=0.0002520077036548632\n",
      "Gradient Descent(701/999): loss=0.38981874361846014, gradient=0.0002512995961227819\n",
      "Gradient Descent(702/999): loss=0.38981873734730343, gradient=0.0002505972352641978\n",
      "Gradient Descent(703/999): loss=0.38981873111092036, gradient=0.00024990056394557763\n",
      "Gradient Descent(704/999): loss=0.3898187249089341, gradient=0.000249209525687048\n",
      "Gradient Descent(705/999): loss=0.3898187187409735, gradient=0.0002485240646551404\n",
      "Gradient Descent(706/999): loss=0.3898187126066719, gradient=0.0002478441256555809\n",
      "Gradient Descent(707/999): loss=0.3898187065056676, gradient=0.00024716965412618805\n",
      "Gradient Descent(708/999): loss=0.3898187004376041, gradient=0.0002465005961296801\n",
      "Gradient Descent(709/999): loss=0.3898186944021297, gradient=0.0002458368983467137\n",
      "Gradient Descent(710/999): loss=0.38981868839889705, gradient=0.00024517850806900015\n",
      "Gradient Descent(711/999): loss=0.38981868242756357, gradient=0.0002445253731922632\n",
      "Gradient Descent(712/999): loss=0.38981867648779134, gradient=0.0002438774422094723\n",
      "Gradient Descent(713/999): loss=0.3898186705792466, gradient=0.00024323466420405888\n",
      "Gradient Descent(714/999): loss=0.3898186647016005, gradient=0.0002425969888431481\n",
      "Gradient Descent(715/999): loss=0.38981865885452827, gradient=0.00024196436637090638\n",
      "Gradient Descent(716/999): loss=0.3898186530377094, gradient=0.00024133674760199776\n",
      "Gradient Descent(717/999): loss=0.3898186472508273, gradient=0.00024071408391492525\n",
      "Gradient Descent(718/999): loss=0.3898186414935701, gradient=0.00024009632724564158\n",
      "Gradient Descent(719/999): loss=0.38981863576563003, gradient=0.000239483430081071\n",
      "Gradient Descent(720/999): loss=0.38981863006670237, gradient=0.00023887534545271897\n",
      "Gradient Descent(721/999): loss=0.3898186243964873, gradient=0.00023827202693042164\n",
      "Gradient Descent(722/999): loss=0.38981861875468915, gradient=0.00023767342861601178\n",
      "Gradient Descent(723/999): loss=0.3898186131410145, gradient=0.00023707950513718302\n",
      "Gradient Descent(724/999): loss=0.38981860755517544, gradient=0.0002364902116414026\n",
      "Gradient Descent(725/999): loss=0.3898186019968873, gradient=0.00023590550378962548\n",
      "Gradient Descent(726/999): loss=0.38981859646586836, gradient=0.00023532533775057394\n",
      "Gradient Descent(727/999): loss=0.38981859096184107, gradient=0.0002347496701944515\n",
      "Gradient Descent(728/999): loss=0.3898185854845313, gradient=0.00023417845828729104\n",
      "Gradient Descent(729/999): loss=0.38981858003366887, gradient=0.00023361165968491845\n",
      "Gradient Descent(730/999): loss=0.3898185746089862, gradient=0.00023304923252734383\n",
      "Gradient Descent(731/999): loss=0.3898185692102198, gradient=0.00023249113543285648\n",
      "Gradient Descent(732/999): loss=0.389818563837109, gradient=0.0002319373274924161\n",
      "Gradient Descent(733/999): loss=0.3898185584893967, gradient=0.00023138776826404835\n",
      "Gradient Descent(734/999): loss=0.38981855316682934, gradient=0.00023084241776722352\n",
      "Gradient Descent(735/999): loss=0.3898185478691558, gradient=0.0002303012364774041\n",
      "Gradient Descent(736/999): loss=0.38981854259612847, gradient=0.00022976418532044863\n",
      "Gradient Descent(737/999): loss=0.38981853734750327, gradient=0.0002292312256674346\n",
      "Gradient Descent(738/999): loss=0.38981853212303824, gradient=0.000228702319329124\n",
      "Gradient Descent(739/999): loss=0.38981852692249513, gradient=0.00022817742855068602\n",
      "Gradient Descent(740/999): loss=0.38981852174563864, gradient=0.00022765651600661343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(741/999): loss=0.3898185165922361, gradient=0.00022713954479535478\n",
      "Gradient Descent(742/999): loss=0.3898185114620576, gradient=0.0002266264784342257\n",
      "Gradient Descent(743/999): loss=0.3898185063548762, gradient=0.00022611728085441537\n",
      "Gradient Descent(744/999): loss=0.38981850127046785, gradient=0.00022561191639588882\n",
      "Gradient Descent(745/999): loss=0.3898184962086114, gradient=0.0002251103498024242\n",
      "Gradient Descent(746/999): loss=0.389818491169088, gradient=0.0002246125462166828\n",
      "Gradient Descent(747/999): loss=0.38981848615168163, gradient=0.00022411847117538588\n",
      "Gradient Descent(748/999): loss=0.38981848115617884, gradient=0.0002236280906044536\n",
      "Gradient Descent(749/999): loss=0.3898184761823687, gradient=0.00022314137081427807\n",
      "Gradient Descent(750/999): loss=0.38981847123004304, gradient=0.00022265827849496543\n",
      "Gradient Descent(751/999): loss=0.3898184662989962, gradient=0.00022217878071175825\n",
      "Gradient Descent(752/999): loss=0.38981846138902476, gradient=0.00022170284490028152\n",
      "Gradient Descent(753/999): loss=0.38981845649992747, gradient=0.00022123043886212475\n",
      "Gradient Descent(754/999): loss=0.38981845163150675, gradient=0.00022076153076031995\n",
      "Gradient Descent(755/999): loss=0.38981844678356553, gradient=0.00022029608911473708\n",
      "Gradient Descent(756/999): loss=0.38981844195591053, gradient=0.0002198340827978452\n",
      "Gradient Descent(757/999): loss=0.3898184371483504, gradient=0.00021937548103028728\n",
      "Gradient Descent(758/999): loss=0.3898184323606955, gradient=0.00021892025337649036\n",
      "Gradient Descent(759/999): loss=0.3898184275927591, gradient=0.00021846836974057646\n",
      "Gradient Descent(760/999): loss=0.38981842284435614, gradient=0.00021801980036193906\n",
      "Gradient Descent(761/999): loss=0.38981841811530427, gradient=0.0002175745158112612\n",
      "Gradient Descent(762/999): loss=0.3898184134054228, gradient=0.00021713248698625518\n",
      "Gradient Descent(763/999): loss=0.3898184087145335, gradient=0.00021669368510767692\n",
      "Gradient Descent(764/999): loss=0.3898184040424597, gradient=0.00021625808171519555\n",
      "Gradient Descent(765/999): loss=0.38981839938902746, gradient=0.00021582564866360902\n",
      "Gradient Descent(766/999): loss=0.38981839475406393, gradient=0.00021539635811868317\n",
      "Gradient Descent(767/999): loss=0.3898183901373998, gradient=0.00021497018255335704\n",
      "Gradient Descent(768/999): loss=0.3898183855388657, gradient=0.00021454709474395781\n",
      "Gradient Descent(769/999): loss=0.38981838095829613, gradient=0.00021412706776629422\n",
      "Gradient Descent(770/999): loss=0.3898183763955261, gradient=0.00021371007499200622\n",
      "Gradient Descent(771/999): loss=0.3898183718503929, gradient=0.0002132960900847576\n",
      "Gradient Descent(772/999): loss=0.3898183673227359, gradient=0.0002128850869966053\n",
      "Gradient Descent(773/999): loss=0.38981836281239635, gradient=0.0002124770399644063\n",
      "Gradient Descent(774/999): loss=0.3898183583192168, gradient=0.00021207192350622132\n",
      "Gradient Descent(775/999): loss=0.3898183538430419, gradient=0.00021166971241774505\n",
      "Gradient Descent(776/999): loss=0.3898183493837183, gradient=0.00021127038176886536\n",
      "Gradient Descent(777/999): loss=0.38981834494109374, gradient=0.00021087390690012958\n",
      "Gradient Descent(778/999): loss=0.3898183405150181, gradient=0.0002104802634194391\n",
      "Gradient Descent(779/999): loss=0.389818336105343, gradient=0.00021008942719858093\n",
      "Gradient Descent(780/999): loss=0.38981833171192165, gradient=0.00020970137436996224\n",
      "Gradient Descent(781/999): loss=0.3898183273346085, gradient=0.00020931608132326255\n",
      "Gradient Descent(782/999): loss=0.3898183229732601, gradient=0.00020893352470231812\n",
      "Gradient Descent(783/999): loss=0.3898183186277343, gradient=0.00020855368140169038\n",
      "Gradient Descent(784/999): loss=0.38981831429789077, gradient=0.00020817652856366332\n",
      "Gradient Descent(785/999): loss=0.3898183099835903, gradient=0.0002078020435751534\n",
      "Gradient Descent(786/999): loss=0.3898183056846958, gradient=0.00020743020406442309\n",
      "Gradient Descent(787/999): loss=0.3898183014010712, gradient=0.0002070609878981913\n",
      "Gradient Descent(788/999): loss=0.38981829713258187, gradient=0.00020669437317857116\n",
      "Gradient Descent(789/999): loss=0.38981829287909514, gradient=0.000206330338240055\n",
      "Gradient Descent(790/999): loss=0.38981828864047935, gradient=0.00020596886164666234\n",
      "Gradient Descent(791/999): loss=0.3898182844166044, gradient=0.00020560992218889785\n",
      "Gradient Descent(792/999): loss=0.38981828020734155, gradient=0.00020525349888103741\n",
      "Gradient Descent(793/999): loss=0.38981827601256375, gradient=0.00020489957095819848\n",
      "Gradient Descent(794/999): loss=0.38981827183214424, gradient=0.0002045481178735457\n",
      "Gradient Descent(795/999): loss=0.38981826766595934, gradient=0.0002041991192956182\n",
      "Gradient Descent(796/999): loss=0.38981826351388543, gradient=0.00020385255510546805\n",
      "Gradient Descent(797/999): loss=0.3898182593758005, gradient=0.00020350840539411062\n",
      "Gradient Descent(798/999): loss=0.3898182552515837, gradient=0.0002031666504597589\n",
      "Gradient Descent(799/999): loss=0.3898182511411159, gradient=0.0002028272708052184\n",
      "Gradient Descent(800/999): loss=0.38981824704427903, gradient=0.00020249024713543922\n",
      "Gradient Descent(801/999): loss=0.3898182429609559, gradient=0.00020215556035473904\n",
      "Gradient Descent(802/999): loss=0.389818238891031, gradient=0.0002018231915644216\n",
      "Gradient Descent(803/999): loss=0.3898182348343899, gradient=0.00020149312206025838\n",
      "Gradient Descent(804/999): loss=0.3898182307909193, gradient=0.00020116533333001913\n",
      "Gradient Descent(805/999): loss=0.3898182267605071, gradient=0.00020083980705108706\n",
      "Gradient Descent(806/999): loss=0.3898182227430426, gradient=0.0002005165250880173\n",
      "Gradient Descent(807/999): loss=0.38981821873841593, gradient=0.0002001954694901705\n",
      "Gradient Descent(808/999): loss=0.38981821474651823, gradient=0.00019987662248942083\n",
      "Gradient Descent(809/999): loss=0.38981821076724227, gradient=0.00019955996649785983\n",
      "Gradient Descent(810/999): loss=0.38981820680048157, gradient=0.0001992454841054179\n",
      "Gradient Descent(811/999): loss=0.38981820284613083, gradient=0.0001989331580778493\n",
      "Gradient Descent(812/999): loss=0.3898181989040859, gradient=0.00019862297135423947\n",
      "Gradient Descent(813/999): loss=0.3898181949742435, gradient=0.00019831490704502606\n",
      "Gradient Descent(814/999): loss=0.38981819105650156, gradient=0.00019800894842979618\n",
      "Gradient Descent(815/999): loss=0.3898181871507591, gradient=0.00019770507895511315\n",
      "Gradient Descent(816/999): loss=0.38981818325691575, gradient=0.00019740328223248382\n",
      "Gradient Descent(817/999): loss=0.38981817937487306, gradient=0.00019710354203620567\n",
      "Gradient Descent(818/999): loss=0.3898181755045324, gradient=0.00019680584230150495\n",
      "Gradient Descent(819/999): loss=0.38981817164579713, gradient=0.0001965101671222455\n",
      "Gradient Descent(820/999): loss=0.3898181677985709, gradient=0.00019621650074919225\n",
      "Gradient Descent(821/999): loss=0.38981816396275887, gradient=0.00019592482758791494\n",
      "Gradient Descent(822/999): loss=0.38981816013826665, gradient=0.00019563513219693007\n",
      "Gradient Descent(823/999): loss=0.3898181563250011, gradient=0.00019534739928569875\n",
      "Gradient Descent(824/999): loss=0.3898181525228699, gradient=0.00019506161371286146\n",
      "Gradient Descent(825/999): loss=0.3898181487317817, gradient=0.000194777760484264\n",
      "Gradient Descent(826/999): loss=0.389818144951646, gradient=0.00019449582475126545\n",
      "Gradient Descent(827/999): loss=0.3898181411823732, gradient=0.00019421579180878756\n",
      "Gradient Descent(828/999): loss=0.38981813742387456, gradient=0.0001939376470935513\n",
      "Gradient Descent(829/999): loss=0.3898181336760624, gradient=0.00019366137618248433\n",
      "Gradient Descent(830/999): loss=0.3898181299388498, gradient=0.00019338696479075774\n",
      "Gradient Descent(831/999): loss=0.3898181262121503, gradient=0.00019311439877023887\n",
      "Gradient Descent(832/999): loss=0.38981812249587866, gradient=0.00019284366410772917\n",
      "Gradient Descent(833/999): loss=0.3898181187899509, gradient=0.0001925747469233051\n",
      "Gradient Descent(834/999): loss=0.389818115094283, gradient=0.0001923076334686795\n",
      "Gradient Descent(835/999): loss=0.3898181114087918, gradient=0.00019204231012564187\n",
      "Gradient Descent(836/999): loss=0.38981810773339615, gradient=0.0001917787634043659\n",
      "Gradient Descent(837/999): loss=0.38981810406801415, gradient=0.00019151697994191817\n",
      "Gradient Descent(838/999): loss=0.38981810041256565, gradient=0.00019125694650060183\n",
      "Gradient Descent(839/999): loss=0.38981809676697043, gradient=0.00019099864996653787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(840/999): loss=0.3898180931311505, gradient=0.00019074207734811582\n",
      "Gradient Descent(841/999): loss=0.389818089505027, gradient=0.00019048721577443888\n",
      "Gradient Descent(842/999): loss=0.38981808588852274, gradient=0.00019023405249394626\n",
      "Gradient Descent(843/999): loss=0.38981808228156095, gradient=0.00018998257487294078\n",
      "Gradient Descent(844/999): loss=0.38981807868406565, gradient=0.00018973277039408955\n",
      "Gradient Descent(845/999): loss=0.3898180750959618, gradient=0.00018948462665513278\n",
      "Gradient Descent(846/999): loss=0.38981807151717474, gradient=0.00018923813136735083\n",
      "Gradient Descent(847/999): loss=0.38981806794763063, gradient=0.0001889932723543479\n",
      "Gradient Descent(848/999): loss=0.38981806438725625, gradient=0.00018875003755059052\n",
      "Gradient Descent(849/999): loss=0.3898180608359794, gradient=0.00018850841500009922\n",
      "Gradient Descent(850/999): loss=0.3898180572937281, gradient=0.00018826839285516762\n",
      "Gradient Descent(851/999): loss=0.38981805376043127, gradient=0.00018802995937501217\n",
      "Gradient Descent(852/999): loss=0.3898180502360186, gradient=0.0001877931029245494\n",
      "Gradient Descent(853/999): loss=0.38981804672042025, gradient=0.00018755781197309313\n",
      "Gradient Descent(854/999): loss=0.3898180432135669, gradient=0.00018732407509313204\n",
      "Gradient Descent(855/999): loss=0.3898180397153906, gradient=0.00018709188095910768\n",
      "Gradient Descent(856/999): loss=0.3898180362258228, gradient=0.00018686121834616523\n",
      "Gradient Descent(857/999): loss=0.3898180327447966, gradient=0.00018663207612905241\n",
      "Gradient Descent(858/999): loss=0.3898180292722454, gradient=0.00018640444328082013\n",
      "Gradient Descent(859/999): loss=0.3898180258081032, gradient=0.0001861783088717859\n",
      "Gradient Descent(860/999): loss=0.3898180223523047, gradient=0.00018595366206829546\n",
      "Gradient Descent(861/999): loss=0.3898180189047845, gradient=0.00018573049213165718\n",
      "Gradient Descent(862/999): loss=0.3898180154654791, gradient=0.00018550878841698299\n",
      "Gradient Descent(863/999): loss=0.38981801203432476, gradient=0.00018528854037219234\n",
      "Gradient Descent(864/999): loss=0.38981800861125804, gradient=0.00018506973753676278\n",
      "Gradient Descent(865/999): loss=0.3898180051962168, gradient=0.00018485236954081133\n",
      "Gradient Descent(866/999): loss=0.3898180017891389, gradient=0.0001846364261040109\n",
      "Gradient Descent(867/999): loss=0.38981799838996317, gradient=0.0001844218970345195\n",
      "Gradient Descent(868/999): loss=0.3898179949986288, gradient=0.00018420877222797167\n",
      "Gradient Descent(869/999): loss=0.38981799161507547, gradient=0.00018399704166651573\n",
      "Gradient Descent(870/999): loss=0.38981798823924346, gradient=0.000183786695417762\n",
      "Gradient Descent(871/999): loss=0.3898179848710736, gradient=0.00018357772363379184\n",
      "Gradient Descent(872/999): loss=0.3898179815105071, gradient=0.00018337011655028958\n",
      "Gradient Descent(873/999): loss=0.38981797815748626, gradient=0.00018316386448552634\n",
      "Gradient Descent(874/999): loss=0.38981797481195296, gradient=0.0001829589578393769\n",
      "Gradient Descent(875/999): loss=0.38981797147385033, gradient=0.0001827553870924992\n",
      "Gradient Descent(876/999): loss=0.38981796814312175, gradient=0.00018255314280535848\n",
      "Gradient Descent(877/999): loss=0.3898179648197113, gradient=0.00018235221561728075\n",
      "Gradient Descent(878/999): loss=0.3898179615035631, gradient=0.00018215259624571785\n",
      "Gradient Descent(879/999): loss=0.38981795819462234, gradient=0.00018195427548522172\n",
      "Gradient Descent(880/999): loss=0.3898179548928341, gradient=0.00018175724420661533\n",
      "Gradient Descent(881/999): loss=0.3898179515981442, gradient=0.00018156149335626516\n",
      "Gradient Descent(882/999): loss=0.38981794831049954, gradient=0.00018136701395501988\n",
      "Gradient Descent(883/999): loss=0.3898179450298465, gradient=0.0001811737970976143\n",
      "Gradient Descent(884/999): loss=0.38981794175613205, gradient=0.00018098183395169747\n",
      "Gradient Descent(885/999): loss=0.38981793848930457, gradient=0.00018079111575709393\n",
      "Gradient Descent(886/999): loss=0.38981793522931185, gradient=0.0001806016338250131\n",
      "Gradient Descent(887/999): loss=0.3898179319761026, gradient=0.0001804133795372144\n",
      "Gradient Descent(888/999): loss=0.38981792872962573, gradient=0.00018022634434536078\n",
      "Gradient Descent(889/999): loss=0.38981792548983124, gradient=0.0001800405197700851\n",
      "Gradient Descent(890/999): loss=0.3898179222566686, gradient=0.00017985589740037868\n",
      "Gradient Descent(891/999): loss=0.38981791903008856, gradient=0.00017967246889281102\n",
      "Gradient Descent(892/999): loss=0.38981791581004166, gradient=0.00017949022597080872\n",
      "Gradient Descent(893/999): loss=0.38981791259647885, gradient=0.00017930916042386357\n",
      "Gradient Descent(894/999): loss=0.38981790938935273, gradient=0.00017912926410697914\n",
      "Gradient Descent(895/999): loss=0.3898179061886146, gradient=0.000178950528939801\n",
      "Gradient Descent(896/999): loss=0.38981790299421726, gradient=0.00017877294690605045\n",
      "Gradient Descent(897/999): loss=0.3898178998061134, gradient=0.00017859651005283273\n",
      "Gradient Descent(898/999): loss=0.38981789662425653, gradient=0.00017842121048984678\n",
      "Gradient Descent(899/999): loss=0.3898178934486002, gradient=0.00017824704038890742\n",
      "Gradient Descent(900/999): loss=0.3898178902790986, gradient=0.00017807399198317895\n",
      "Gradient Descent(901/999): loss=0.38981788711570586, gradient=0.0001779020575665231\n",
      "Gradient Descent(902/999): loss=0.3898178839583773, gradient=0.0001777312294929435\n",
      "Gradient Descent(903/999): loss=0.3898178808070679, gradient=0.00017756150017589817\n",
      "Gradient Descent(904/999): loss=0.3898178776617333, gradient=0.00017739286208769142\n",
      "Gradient Descent(905/999): loss=0.38981787452232947, gradient=0.0001772253077589359\n",
      "Gradient Descent(906/999): loss=0.389817871388813, gradient=0.00017705882977779773\n",
      "Gradient Descent(907/999): loss=0.38981786826114045, gradient=0.0001768934207896061\n",
      "Gradient Descent(908/999): loss=0.38981786513926875, gradient=0.00017672907349610084\n",
      "Gradient Descent(909/999): loss=0.38981786202315555, gradient=0.00017656578065495504\n",
      "Gradient Descent(910/999): loss=0.3898178589127588, gradient=0.0001764035350791694\n",
      "Gradient Descent(911/999): loss=0.38981785580803613, gradient=0.00017624232963651298\n",
      "Gradient Descent(912/999): loss=0.38981785270894675, gradient=0.00017608215724900108\n",
      "Gradient Descent(913/999): loss=0.389817849615449, gradient=0.00017592301089230721\n",
      "Gradient Descent(914/999): loss=0.38981784652750223, gradient=0.00017576488359528016\n",
      "Gradient Descent(915/999): loss=0.389817843445066, gradient=0.00017560776843930992\n",
      "Gradient Descent(916/999): loss=0.3898178403681, gradient=0.0001754516585580234\n",
      "Gradient Descent(917/999): loss=0.389817837296565, gradient=0.00017529654713645558\n",
      "Gradient Descent(918/999): loss=0.3898178342304208, gradient=0.00017514242741079272\n",
      "Gradient Descent(919/999): loss=0.3898178311696286, gradient=0.00017498929266783285\n",
      "Gradient Descent(920/999): loss=0.38981782811414967, gradient=0.0001748371362443827\n",
      "Gradient Descent(921/999): loss=0.38981782506394547, gradient=0.0001746859515268282\n",
      "Gradient Descent(922/999): loss=0.3898178220189777, gradient=0.00017453573195067273\n",
      "Gradient Descent(923/999): loss=0.3898178189792084, gradient=0.00017438647100008735\n",
      "Gradient Descent(924/999): loss=0.3898178159446003, gradient=0.00017423816220734113\n",
      "Gradient Descent(925/999): loss=0.389817812915116, gradient=0.00017409079915245445\n",
      "Gradient Descent(926/999): loss=0.3898178098907185, gradient=0.0001739443754626262\n",
      "Gradient Descent(927/999): loss=0.38981780687137135, gradient=0.0001737988848119073\n",
      "Gradient Descent(928/999): loss=0.38981780385703796, gradient=0.00017365432092064566\n",
      "Gradient Descent(929/999): loss=0.3898178008476824, gradient=0.00017351067755513188\n",
      "Gradient Descent(930/999): loss=0.38981779784326875, gradient=0.00017336794852713415\n",
      "Gradient Descent(931/999): loss=0.38981779484376194, gradient=0.00017322612769340217\n",
      "Gradient Descent(932/999): loss=0.3898177918491266, gradient=0.00017308520895540174\n",
      "Gradient Descent(933/999): loss=0.3898177888593275, gradient=0.00017294518625875186\n",
      "Gradient Descent(934/999): loss=0.3898177858743305, gradient=0.0001728060535928679\n",
      "Gradient Descent(935/999): loss=0.389817782894101, gradient=0.0001726678049905874\n",
      "Gradient Descent(936/999): loss=0.38981777991860517, gradient=0.00017253043452775327\n",
      "Gradient Descent(937/999): loss=0.38981777694780906, gradient=0.00017239393632276844\n",
      "Gradient Descent(938/999): loss=0.389817773981679, gradient=0.0001722583045362421\n",
      "Gradient Descent(939/999): loss=0.38981777102018217, gradient=0.00017212353337067322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(940/999): loss=0.3898177680632852, gradient=0.00017198961706994513\n",
      "Gradient Descent(941/999): loss=0.38981776511095545, gradient=0.00017185654991901635\n",
      "Gradient Descent(942/999): loss=0.3898177621631608, gradient=0.00017172432624358797\n",
      "Gradient Descent(943/999): loss=0.3898177592198687, gradient=0.00017159294040966486\n",
      "Gradient Descent(944/999): loss=0.38981775628104737, gradient=0.00017146238682321524\n",
      "Gradient Descent(945/999): loss=0.38981775334666524, gradient=0.00017133265992984233\n",
      "Gradient Descent(946/999): loss=0.38981775041669076, gradient=0.0001712037542144532\n",
      "Gradient Descent(947/999): loss=0.38981774749109266, gradient=0.0001710756642008298\n",
      "Gradient Descent(948/999): loss=0.3898177445698403, gradient=0.00017094838445134316\n",
      "Gradient Descent(949/999): loss=0.38981774165290256, gradient=0.0001708219095665793\n",
      "Gradient Descent(950/999): loss=0.3898177387402495, gradient=0.0001706962341851198\n",
      "Gradient Descent(951/999): loss=0.3898177358318507, gradient=0.0001705713529829936\n",
      "Gradient Descent(952/999): loss=0.38981773292767613, gradient=0.00017044726067358558\n",
      "Gradient Descent(953/999): loss=0.3898177300276963, gradient=0.00017032395200714803\n",
      "Gradient Descent(954/999): loss=0.3898177271318815, gradient=0.00017020142177058388\n",
      "Gradient Descent(955/999): loss=0.38981772424020256, gradient=0.00017007966478705416\n",
      "Gradient Descent(956/999): loss=0.38981772135263065, gradient=0.00016995867591572945\n",
      "Gradient Descent(957/999): loss=0.38981771846913665, gradient=0.00016983845005144976\n",
      "Gradient Descent(958/999): loss=0.38981771558969214, gradient=0.0001697189821244673\n",
      "Gradient Descent(959/999): loss=0.3898177127142688, gradient=0.0001696002671000257\n",
      "Gradient Descent(960/999): loss=0.3898177098428384, gradient=0.00016948229997827815\n",
      "Gradient Descent(961/999): loss=0.38981770697537343, gradient=0.00016936507579374608\n",
      "Gradient Descent(962/999): loss=0.3898177041118458, gradient=0.00016924858961522244\n",
      "Gradient Descent(963/999): loss=0.3898177012522284, gradient=0.00016913283654540432\n",
      "Gradient Descent(964/999): loss=0.38981769839649355, gradient=0.00016901781172064026\n",
      "Gradient Descent(965/999): loss=0.3898176955446147, gradient=0.0001689035103106017\n",
      "Gradient Descent(966/999): loss=0.38981769269656447, gradient=0.00016878992751809577\n",
      "Gradient Descent(967/999): loss=0.3898176898523169, gradient=0.000168677058578683\n",
      "Gradient Descent(968/999): loss=0.3898176870118448, gradient=0.00016856489876049125\n",
      "Gradient Descent(969/999): loss=0.38981768417512275, gradient=0.00016845344336397118\n",
      "Gradient Descent(970/999): loss=0.38981768134212447, gradient=0.00016834268772152806\n",
      "Gradient Descent(971/999): loss=0.389817678512824, gradient=0.00016823262719737792\n",
      "Gradient Descent(972/999): loss=0.38981767568719605, gradient=0.0001681232571871733\n",
      "Gradient Descent(973/999): loss=0.38981767286521485, gradient=0.00016801457311788902\n",
      "Gradient Descent(974/999): loss=0.3898176700468556, gradient=0.00016790657044745365\n",
      "Gradient Descent(975/999): loss=0.38981766723209293, gradient=0.0001677992446645675\n",
      "Gradient Descent(976/999): loss=0.38981766442090254, gradient=0.00016769259128844723\n",
      "Gradient Descent(977/999): loss=0.3898176616132592, gradient=0.0001675866058685331\n",
      "Gradient Descent(978/999): loss=0.3898176588091389, gradient=0.00016748128398435044\n",
      "Gradient Descent(979/999): loss=0.38981765600851714, gradient=0.00016737662124516214\n",
      "Gradient Descent(980/999): loss=0.38981765321137, gradient=0.00016727261328980527\n",
      "Gradient Descent(981/999): loss=0.3898176504176738, gradient=0.00016716925578650124\n",
      "Gradient Descent(982/999): loss=0.3898176476274045, gradient=0.00016706654443246195\n",
      "Gradient Descent(983/999): loss=0.38981764484053893, gradient=0.0001669644749538621\n",
      "Gradient Descent(984/999): loss=0.3898176420570536, gradient=0.0001668630431054977\n",
      "Gradient Descent(985/999): loss=0.3898176392769253, gradient=0.00016676224467056762\n",
      "Gradient Descent(986/999): loss=0.38981763650013135, gradient=0.00016666207546051277\n",
      "Gradient Descent(987/999): loss=0.38981763372664874, gradient=0.00016656253131474663\n",
      "Gradient Descent(988/999): loss=0.38981763095645483, gradient=0.00016646360810049313\n",
      "Gradient Descent(989/999): loss=0.38981762818952714, gradient=0.0001663653017124968\n",
      "Gradient Descent(990/999): loss=0.3898176254258437, gradient=0.00016626760807289553\n",
      "Gradient Descent(991/999): loss=0.3898176226653822, gradient=0.00016617052313099812\n",
      "Gradient Descent(992/999): loss=0.3898176199081207, gradient=0.0001660740428630127\n",
      "Gradient Descent(993/999): loss=0.38981761715403723, gradient=0.00016597816327190918\n",
      "Gradient Descent(994/999): loss=0.3898176144031111, gradient=0.00016588288038722323\n",
      "Gradient Descent(995/999): loss=0.3898176116553196, gradient=0.00016578819026481025\n",
      "Gradient Descent(996/999): loss=0.38981760891064243, gradient=0.00016569408898670204\n",
      "Gradient Descent(997/999): loss=0.38981760616905803, gradient=0.00016560057266084722\n",
      "Gradient Descent(998/999): loss=0.3898176034305456, gradient=0.000165507637421001\n",
      "Gradient Descent(999/999): loss=0.3898176006950845, gradient=0.0001654152794264717\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters for gradient descent\n",
    "max_iters = 1000\n",
    "gamma = .1\n",
    "\n",
    "# Initial weights vector to train a linear model\n",
    "initial_w = np.zeros(num_dim)\n",
    "\n",
    "# Run gradient descent under MSE loss to find optimal weights\n",
    "w_GD, mse_GD = least_squares_GD(y_train, tx_train, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on evaluation set:  0.7182533333333333\n",
      "F1 Score on evaluation set: 0.6675163244433955\n"
     ]
    }
   ],
   "source": [
    "# Test error on evaluation set\n",
    "y_eval_pred = predict_labels(w_GD, tx_eval)\n",
    "\n",
    "acc_GD = get_accuracy(y_eval_pred, y_eval)\n",
    "f1_GD = get_f1_score(y_eval_pred, y_eval)\n",
    "\n",
    "print('Accuracy on evaluation set: ', acc_GD)\n",
    "print('F1 Score on evaluation set:', f1_GD)\n",
    "\n",
    "# Save current model predictions on test set\n",
    "y_test_pred = predict_labels(w_GD, tx_test)\n",
    "create_csv_submission(ids_test, y_test_pred, '../data/ls_gd_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy on evaluation set very similar so GD seems to find a good solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Least-squares SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic GD(0/999): loss=0.6689394498538589, gradient=5.658154857955742\n",
      "Stochastic GD(1/999): loss=0.49576949037927287, gradient=3.3990562973214513\n",
      "Stochastic GD(2/999): loss=0.5739300421126895, gradient=2.041934871421115\n",
      "Stochastic GD(3/999): loss=0.5176405587078589, gradient=1.2266634190234653\n",
      "Stochastic GD(4/999): loss=0.5480865888940792, gradient=0.7369006546830341\n",
      "Stochastic GD(5/999): loss=0.5285807229645564, gradient=0.4426826189245766\n",
      "Stochastic GD(6/999): loss=0.53985981688647, gradient=0.265935306004328\n",
      "Stochastic GD(7/999): loss=0.5329257132425718, gradient=0.15975686407436065\n",
      "Stochastic GD(8/999): loss=0.5370341325279868, gradient=0.09597167071324571\n",
      "Stochastic GD(9/999): loss=0.5345454351564917, gradient=0.05765362028641643\n",
      "Stochastic GD(10/999): loss=0.5360330424037608, gradient=0.03463459484895323\n",
      "Stochastic GD(11/999): loss=0.5351366976018693, gradient=0.020806241731079176\n",
      "Stochastic GD(12/999): loss=0.5356741947549325, gradient=0.01249905468390935\n",
      "Stochastic GD(13/999): loss=0.5353509511661634, gradient=0.007508629862645932\n",
      "Stochastic GD(14/999): loss=0.5355450089329022, gradient=0.004510702916342292\n",
      "Stochastic GD(15/999): loss=0.5354283859249699, gradient=0.0027097408144621644\n",
      "Stochastic GD(16/999): loss=0.5354984291027202, gradient=0.0016278383697062725\n",
      "Stochastic GD(17/999): loss=0.5354563457237939, gradient=0.0009779008175793654\n",
      "Stochastic GD(18/999): loss=0.5354816245757702, gradient=0.0005874600493622971\n",
      "Stochastic GD(19/999): loss=0.5354664378911931, gradient=0.00035290829437242924\n",
      "Stochastic GD(20/999): loss=0.5354755607977445, gradient=0.0002120046535459282\n",
      "Stochastic GD(21/999): loss=0.5354700802403626, gradient=0.00012735878935557274\n",
      "Stochastic GD(22/999): loss=0.5354733725711769, gradient=7.650898673756081e-05\n",
      "Stochastic GD(23/999): loss=0.5354713947370197, gradient=4.5961688872838393e-05\n",
      "Stochastic GD(24/999): loss=0.5354725828881137, gradient=2.7610832845395623e-05\n",
      "Stochastic GD(25/999): loss=0.5354718691214851, gradient=1.6586816306588e-05\n",
      "Stochastic GD(26/999): loss=0.5354722979060498, gradient=9.964294692093949e-06\n",
      "Stochastic GD(27/999): loss=0.5354720403195804, gradient=5.985908740019014e-06\n",
      "Stochastic GD(28/999): loss=0.5354721950609189, gradient=3.5959497931630143e-06\n",
      "Stochastic GD(29/999): loss=0.5354721021022251, gradient=2.160215847357493e-06\n",
      "Stochastic GD(30/999): loss=0.5354721579458321, gradient=1.2977190403612374e-06\n",
      "Stochastic GD(31/999): loss=0.5354721243985732, gradient=7.795863133097898e-07\n",
      "Stochastic GD(32/999): loss=0.5354721445516119, gradient=4.6832542337157623e-07\n",
      "Stochastic GD(33/999): loss=0.5354721324449592, gradient=2.8133985573385575e-07\n",
      "Stochastic GD(34/999): loss=0.5354721397178593, gradient=1.6901092885519107e-07\n",
      "Stochastic GD(35/999): loss=0.5354721353487676, gradient=1.0153092114491522e-07\n",
      "Stochastic GD(36/999): loss=0.5354721379734371, gradient=6.099326167029349e-08\n",
      "Stochastic GD(37/999): loss=0.535472136396704, gradient=3.6640836164508844e-08\n",
      "Stochastic GD(38/999): loss=0.5354721373439042, gradient=2.201146362044458e-08\n",
      "Stochastic GD(39/999): loss=0.5354721367748871, gradient=1.322307608042311e-08\n",
      "Stochastic GD(40/999): loss=0.535472137116716, gradient=7.94357529837644e-09\n",
      "Stochastic GD(41/999): loss=0.5354721369113672, gradient=4.771991160584603e-09\n",
      "Stochastic GD(42/999): loss=0.5354721370347277, gradient=2.866709496808248e-09\n",
      "Stochastic GD(43/999): loss=0.5354721369606206, gradient=1.722137895157127e-09\n",
      "Stochastic GD(44/999): loss=0.5354721370051394, gradient=1.0345519147544682e-09\n",
      "Stochastic GD(45/999): loss=0.5354721369783952, gradient=6.214937610480248e-10\n",
      "Stochastic GD(46/999): loss=0.5354721369944616, gradient=3.7335332140091723e-10\n",
      "Stochastic GD(47/999): loss=0.5354721369848099, gradient=2.24288392647759e-10\n",
      "Stochastic GD(48/999): loss=0.5354721369906079, gradient=1.3473988056318863e-10\n",
      "Stochastic GD(49/999): loss=0.5354721369871248, gradient=8.094242717830027e-11\n",
      "Stochastic GD(50/999): loss=0.5354721369892174, gradient=4.862312335766598e-11\n",
      "Stochastic GD(51/999): loss=0.5354721369879605, gradient=2.920792144539665e-11\n",
      "Stochastic GD(52/999): loss=0.5354721369887155, gradient=1.7545105943950632e-11\n",
      "Stochastic GD(53/999): loss=0.5354721369882618, gradient=1.0540883556730813e-11\n",
      "Stochastic GD(54/999): loss=0.5354721369885344, gradient=6.3333246733587636e-12\n",
      "Stochastic GD(55/999): loss=0.5354721369883707, gradient=3.804266437399392e-12\n",
      "Stochastic GD(56/999): loss=0.5354721369884686, gradient=2.284067497751683e-12\n",
      "Stochastic GD(57/999): loss=0.5354721369884099, gradient=1.3706917712030171e-12\n",
      "Stochastic GD(58/999): loss=0.5354721369884452, gradient=8.235457892058459e-13\n",
      "Stochastic GD(59/999): loss=0.535472136988424, gradient=4.950069274555351e-13\n",
      "Stochastic GD(60/999): loss=0.5354721369884367, gradient=2.958734299892856e-13\n",
      "Stochastic GD(61/999): loss=0.5354721369884293, gradient=1.7589078640552011e-13\n",
      "Stochastic GD(62/999): loss=0.5354721369884335, gradient=1.0427810908327264e-13\n",
      "Stochastic GD(63/999): loss=0.535472136988431, gradient=6.156177524193203e-14\n",
      "Stochastic GD(64/999): loss=0.5354721369884325, gradient=3.6434520041143453e-14\n",
      "Stochastic GD(65/999): loss=0.5354721369884315, gradient=2.1358166920670303e-14\n",
      "Stochastic GD(66/999): loss=0.5354721369884322, gradient=1.319180898041401e-14\n",
      "Stochastic GD(67/999): loss=0.5354721369884318, gradient=8.794539320276006e-15\n",
      "Stochastic GD(68/999): loss=0.535472136988432, gradient=5.653632420177432e-15\n",
      "Stochastic GD(69/999): loss=0.535472136988432, gradient=2.5127255200788586e-15\n",
      "Stochastic GD(70/999): loss=0.535472136988432, gradient=6.281813800197147e-16\n",
      "Stochastic GD(71/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(72/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(73/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(74/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(75/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(76/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(77/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(78/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(79/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(80/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(81/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(82/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(83/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(84/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(85/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(86/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(87/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(88/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(89/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(90/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(91/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(92/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(93/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(94/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(95/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(96/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(97/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(98/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(99/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(100/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(101/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(102/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(103/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(104/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(105/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(106/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(107/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(108/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(109/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(110/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(111/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(112/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(113/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(114/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(115/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(116/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(117/999): loss=0.535472136988432, gradient=0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic GD(118/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(119/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(120/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(121/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(122/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(123/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(124/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(125/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(126/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(127/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(128/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(129/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(130/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(131/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(132/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(133/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(134/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(135/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(136/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(137/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(138/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(139/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(140/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(141/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(142/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(143/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(144/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(145/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(146/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(147/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(148/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(149/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(150/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(151/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(152/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(153/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(154/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(155/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(156/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(157/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(158/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(159/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(160/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(161/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(162/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(163/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(164/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(165/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(166/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(167/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(168/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(169/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(170/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(171/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(172/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(173/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(174/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(175/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(176/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(177/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(178/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(179/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(180/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(181/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(182/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(183/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(184/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(185/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(186/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(187/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(188/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(189/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(190/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(191/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(192/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(193/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(194/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(195/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(196/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(197/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(198/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(199/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(200/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(201/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(202/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(203/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(204/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(205/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(206/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(207/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(208/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(209/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(210/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(211/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(212/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(213/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(214/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(215/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(216/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(217/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(218/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(219/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(220/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(221/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(222/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(223/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(224/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(225/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(226/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(227/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(228/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(229/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(230/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(231/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(232/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(233/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(234/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(235/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(236/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(237/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(238/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(239/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(240/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(241/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(242/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(243/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(244/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(245/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(246/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(247/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(248/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(249/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(250/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(251/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(252/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(253/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(254/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(255/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(256/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(257/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(258/999): loss=0.535472136988432, gradient=0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic GD(259/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(260/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(261/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(262/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(263/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(264/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(265/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(266/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(267/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(268/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(269/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(270/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(271/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(272/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(273/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(274/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(275/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(276/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(277/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(278/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(279/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(280/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(281/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(282/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(283/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(284/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(285/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(286/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(287/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(288/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(289/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(290/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(291/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(292/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(293/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(294/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(295/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(296/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(297/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(298/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(299/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(300/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(301/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(302/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(303/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(304/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(305/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(306/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(307/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(308/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(309/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(310/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(311/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(312/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(313/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(314/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(315/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(316/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(317/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(318/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(319/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(320/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(321/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(322/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(323/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(324/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(325/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(326/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(327/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(328/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(329/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(330/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(331/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(332/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(333/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(334/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(335/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(336/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(337/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(338/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(339/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(340/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(341/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(342/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(343/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(344/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(345/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(346/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(347/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(348/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(349/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(350/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(351/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(352/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(353/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(354/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(355/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(356/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(357/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(358/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(359/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(360/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(361/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(362/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(363/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(364/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(365/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(366/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(367/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(368/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(369/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(370/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(371/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(372/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(373/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(374/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(375/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(376/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(377/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(378/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(379/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(380/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(381/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(382/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(383/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(384/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(385/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(386/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(387/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(388/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(389/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(390/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(391/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(392/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(393/999): loss=0.535472136988432, gradient=0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic GD(394/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(395/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(396/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(397/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(398/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(399/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(400/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(401/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(402/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(403/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(404/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(405/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(406/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(407/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(408/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(409/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(410/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(411/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(412/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(413/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(414/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(415/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(416/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(417/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(418/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(419/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(420/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(421/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(422/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(423/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(424/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(425/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(426/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(427/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(428/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(429/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(430/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(431/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(432/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(433/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(434/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(435/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(436/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(437/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(438/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(439/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(440/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(441/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(442/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(443/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(444/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(445/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(446/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(447/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(448/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(449/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(450/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(451/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(452/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(453/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(454/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(455/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(456/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(457/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(458/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(459/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(460/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(461/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(462/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(463/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(464/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(465/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(466/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(467/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(468/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(469/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(470/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(471/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(472/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(473/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(474/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(475/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(476/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(477/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(478/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(479/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(480/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(481/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(482/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(483/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(484/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(485/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(486/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(487/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(488/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(489/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(490/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(491/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(492/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(493/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(494/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(495/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(496/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(497/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(498/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(499/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(500/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(501/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(502/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(503/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(504/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(505/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(506/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(507/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(508/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(509/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(510/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(511/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(512/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(513/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(514/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(515/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(516/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(517/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(518/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(519/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(520/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(521/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(522/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(523/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(524/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(525/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(526/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(527/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(528/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(529/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(530/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(531/999): loss=0.535472136988432, gradient=0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic GD(532/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(533/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(534/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(535/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(536/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(537/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(538/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(539/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(540/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(541/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(542/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(543/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(544/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(545/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(546/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(547/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(548/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(549/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(550/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(551/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(552/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(553/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(554/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(555/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(556/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(557/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(558/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(559/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(560/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(561/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(562/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(563/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(564/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(565/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(566/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(567/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(568/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(569/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(570/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(571/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(572/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(573/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(574/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(575/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(576/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(577/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(578/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(579/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(580/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(581/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(582/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(583/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(584/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(585/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(586/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(587/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(588/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(589/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(590/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(591/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(592/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(593/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(594/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(595/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(596/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(597/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(598/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(599/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(600/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(601/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(602/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(603/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(604/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(605/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(606/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(607/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(608/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(609/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(610/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(611/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(612/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(613/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(614/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(615/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(616/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(617/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(618/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(619/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(620/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(621/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(622/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(623/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(624/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(625/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(626/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(627/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(628/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(629/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(630/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(631/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(632/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(633/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(634/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(635/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(636/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(637/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(638/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(639/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(640/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(641/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(642/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(643/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(644/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(645/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(646/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(647/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(648/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(649/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(650/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(651/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(652/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(653/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(654/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(655/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(656/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(657/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(658/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(659/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(660/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(661/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(662/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(663/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(664/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(665/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(666/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(667/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(668/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(669/999): loss=0.535472136988432, gradient=0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic GD(670/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(671/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(672/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(673/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(674/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(675/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(676/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(677/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(678/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(679/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(680/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(681/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(682/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(683/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(684/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(685/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(686/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(687/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(688/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(689/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(690/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(691/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(692/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(693/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(694/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(695/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(696/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(697/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(698/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(699/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(700/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(701/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(702/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(703/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(704/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(705/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(706/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(707/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(708/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(709/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(710/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(711/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(712/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(713/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(714/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(715/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(716/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(717/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(718/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(719/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(720/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(721/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(722/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(723/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(724/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(725/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(726/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(727/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(728/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(729/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(730/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(731/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(732/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(733/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(734/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(735/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(736/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(737/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(738/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(739/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(740/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(741/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(742/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(743/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(744/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(745/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(746/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(747/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(748/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(749/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(750/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(751/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(752/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(753/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(754/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(755/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(756/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(757/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(758/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(759/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(760/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(761/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(762/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(763/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(764/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(765/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(766/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(767/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(768/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(769/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(770/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(771/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(772/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(773/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(774/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(775/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(776/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(777/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(778/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(779/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(780/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(781/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(782/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(783/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(784/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(785/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(786/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(787/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(788/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(789/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(790/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(791/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(792/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(793/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(794/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(795/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(796/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(797/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(798/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(799/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(800/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(801/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(802/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(803/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(804/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(805/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(806/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(807/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(808/999): loss=0.535472136988432, gradient=0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic GD(809/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(810/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(811/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(812/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(813/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(814/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(815/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(816/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(817/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(818/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(819/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(820/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(821/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(822/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(823/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(824/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(825/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(826/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(827/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(828/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(829/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(830/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(831/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(832/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(833/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(834/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(835/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(836/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(837/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(838/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(839/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(840/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(841/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(842/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(843/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(844/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(845/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(846/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(847/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(848/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(849/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(850/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(851/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(852/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(853/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(854/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(855/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(856/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(857/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(858/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(859/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(860/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(861/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(862/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(863/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(864/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(865/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(866/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(867/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(868/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(869/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(870/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(871/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(872/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(873/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(874/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(875/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(876/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(877/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(878/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(879/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(880/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(881/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(882/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(883/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(884/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(885/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(886/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(887/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(888/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(889/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(890/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(891/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(892/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(893/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(894/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(895/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(896/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(897/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(898/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(899/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(900/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(901/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(902/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(903/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(904/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(905/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(906/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(907/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(908/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(909/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(910/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(911/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(912/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(913/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(914/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(915/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(916/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(917/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(918/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(919/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(920/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(921/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(922/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(923/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(924/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(925/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(926/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(927/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(928/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(929/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(930/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(931/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(932/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(933/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(934/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(935/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(936/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(937/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(938/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(939/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(940/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(941/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(942/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(943/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(944/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(945/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(946/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(947/999): loss=0.535472136988432, gradient=0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic GD(948/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(949/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(950/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(951/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(952/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(953/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(954/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(955/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(956/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(957/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(958/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(959/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(960/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(961/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(962/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(963/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(964/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(965/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(966/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(967/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(968/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(969/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(970/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(971/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(972/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(973/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(974/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(975/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(976/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(977/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(978/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(979/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(980/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(981/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(982/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(983/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(984/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(985/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(986/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(987/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(988/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(989/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(990/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(991/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(992/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(993/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(994/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(995/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(996/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(997/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(998/999): loss=0.535472136988432, gradient=0.0\n",
      "Stochastic GD(999/999): loss=0.535472136988432, gradient=0.0\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters for gradient descent\n",
    "max_iters = 1000\n",
    "gamma = .05\n",
    "\n",
    "# Initial weights vector to train a linear model\n",
    "initial_w = np.zeros(num_dim)\n",
    "\n",
    "# Run gradient descent under MSE loss to find optimal weights\n",
    "w_SGD, mse_SGD = least_squares_SGD(y_train, tx_train, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on evaluation set:  0.62456\n",
      "F1 Score on evaluation set: 0.4112407477104504\n"
     ]
    }
   ],
   "source": [
    "# Test error on evaluation set\n",
    "y_eval_pred = predict_labels(w_SGD, tx_eval)\n",
    "\n",
    "acc_SGD = get_accuracy(y_eval_pred, y_eval)\n",
    "f1_SGD = get_f1_score(y_eval_pred, y_eval)\n",
    "\n",
    "print('Accuracy on evaluation set: ', acc_SGD)\n",
    "print('F1 Score on evaluation set:', f1_SGD)\n",
    "\n",
    "# Save current model predictions on test set\n",
    "y_test_pred = predict_labels(w_SGD, tx_test)\n",
    "create_csv_submission(ids_test, y_test_pred, '../data/ls_sgd_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does not yield very satisfactor result. Could run some hyperparameter optimisation to get better results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Hyperparameter optimisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum test error 0.3894215801774716 with lambda 5.6234132519034905e-08\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k_fold = 5\n",
    "lambdas = np.logspace(-9, -2, 25)\n",
    "\n",
    "mse_train = np.empty((len(lambdas), k_fold), float)\n",
    "mse_test = np.empty((len(lambdas), k_fold), float)\n",
    "\n",
    "for l, lambda_ in enumerate(lambdas):\n",
    "    k = 0\n",
    "    for train_split, test_split in k_fold_iter(y_train, tx_train, k_fold):\n",
    "        # Train\n",
    "        w, mse_tr = ridge_regression(train_split[1], train_split[0], lambda_)\n",
    "        mse_train[l, k] = mse_tr\n",
    "\n",
    "        # Test\n",
    "        mse_te = compute_loss_mse(test_split[1], test_split[0], w)\n",
    "        mse_test[l, k] = mse_te\n",
    "        \n",
    "        k += 1\n",
    "\n",
    "fig = cross_validation_visualization(lambdas, mse_train, mse_test)\n",
    "\n",
    "avg_mse_test = np.mean(mse_test, axis=1)\n",
    "lambda_opt = lambdas[np.argmin(avg_mse_test)]\n",
    "\n",
    "print('Minimum test error {} with lambda {}'.format(np.min(avg_mse_test), lambda_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on evaluation set:  0.71908\n",
      "F1 Score on evaluation set: 0.6677704716400965\n"
     ]
    }
   ],
   "source": [
    "# Find optimal ridge regression model under lambda_opt\n",
    "w_ridge, mse_ridge = ridge_regression(y_train, tx_train, lambda_opt)\n",
    "\n",
    "# Test error on evaluation set\n",
    "y_eval_pred = predict_labels(w_ridge, tx_eval)\n",
    "\n",
    "acc_ridge = get_accuracy(y_eval_pred, y_eval)\n",
    "f1_ridge = get_f1_score(y_eval_pred, y_eval)\n",
    "\n",
    "print('Accuracy on evaluation set: ', acc_ridge)\n",
    "print('F1 Score on evaluation set:', f1_ridge)\n",
    "\n",
    "# Save current model predictions on test set\n",
    "y_test_pred = predict_labels(w_ridge, tx_test)\n",
    "create_csv_submission(ids_test, y_test_pred, '../data/ridge_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Lasso regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Hyperparameter optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=0.4627782705549722, gradient=0.7974960150381895\n",
      "Gradient Descent(1/99): loss=0.4476418201429259, gradient=0.42336927082495096\n",
      "Gradient Descent(2/99): loss=0.4381324589846833, gradient=0.32440122047931885\n",
      "Gradient Descent(3/99): loss=0.4317037774796922, gradient=0.26507794159929615\n",
      "Gradient Descent(4/99): loss=0.42719549163612036, gradient=0.22104900921204754\n",
      "Gradient Descent(5/99): loss=0.42391495773059823, gradient=0.18773693969660876\n",
      "Gradient Descent(6/99): loss=0.42143380660252533, gradient=0.16253369354295727\n",
      "Gradient Descent(7/99): loss=0.41948389649777623, gradient=0.14345533315799514\n",
      "Gradient Descent(8/99): loss=0.41789541062078456, gradient=0.1289577533806581\n",
      "Gradient Descent(9/99): loss=0.4165593432208432, gradient=0.11785179426692226\n",
      "Gradient Descent(10/99): loss=0.4154046245547404, gradient=0.10923747948869804\n",
      "Gradient Descent(11/99): loss=0.41438408967187157, gradient=0.10244640663647865\n",
      "Gradient Descent(12/99): loss=0.4134658133208543, gradient=0.09699040955214712\n",
      "Gradient Descent(13/99): loss=0.41262771621540634, gradient=0.09251736095925152\n",
      "Gradient Descent(14/99): loss=0.41185417675305913, gradient=0.08877504205758646\n",
      "Gradient Descent(15/99): loss=0.4111338800230914, gradient=0.08558307503845847\n",
      "Gradient Descent(16/99): loss=0.4104584358591149, gradient=0.08281201453267266\n",
      "Gradient Descent(17/99): loss=0.4098214789833546, gradient=0.08036825318792809\n",
      "Gradient Descent(18/99): loss=0.4092180743159669, gradient=0.07818335617267848\n",
      "Gradient Descent(19/99): loss=0.40864431757766584, gradient=0.07620662526399401\n",
      "Gradient Descent(20/99): loss=0.4080970624213793, gradient=0.0743999468614133\n",
      "Gradient Descent(21/99): loss=0.4075737306473026, gradient=0.07273422700806904\n",
      "Gradient Descent(22/99): loss=0.4070721777810267, gradient=0.07118691579419731\n",
      "Gradient Descent(23/99): loss=0.4065905961213652, gradient=0.06974027664884747\n",
      "Gradient Descent(24/99): loss=0.406127443563001, gradient=0.06838016498394131\n",
      "Gradient Descent(25/99): loss=0.40568139044806617, gradient=0.06709515624880194\n",
      "Gradient Descent(26/99): loss=0.4052512792454255, gradient=0.06587591480631111\n",
      "Gradient Descent(27/99): loss=0.40483609350199745, gradient=0.06471473181747933\n",
      "Gradient Descent(28/99): loss=0.40443493361013083, gradient=0.06360518117774044\n",
      "Gradient Descent(29/99): loss=0.4040469976580287, gradient=0.0625418603927231\n",
      "Gradient Descent(30/99): loss=0.4036715661298961, gradient=0.061520192050723906\n",
      "Gradient Descent(31/99): loss=0.4033079895551482, gradient=0.06053627059333746\n",
      "Gradient Descent(32/99): loss=0.40295567844782104, gradient=0.05958674207512474\n",
      "Gradient Descent(33/99): loss=0.4026140950428978, gradient=0.058668708989349894\n",
      "Gradient Descent(34/99): loss=0.40228274645687084, gradient=0.05777965415504602\n",
      "Gradient Descent(35/99): loss=0.40196117898743655, gradient=0.05691737930303966\n",
      "Gradient Descent(36/99): loss=0.4016489733318087, gradient=0.05607995512317676\n",
      "Gradient Descent(37/99): loss=0.4013457405513843, gradient=0.055265680339519896\n",
      "Gradient Descent(38/99): loss=0.40105111864695997, gradient=0.05447304796300786\n",
      "Gradient Descent(39/99): loss=0.4007647696365476, gradient=0.05370071729843045\n",
      "Gradient Descent(40/99): loss=0.40048637704933626, gradient=0.052947490599797475\n",
      "Gradient Descent(41/99): loss=0.4002156437660815, gradient=0.05221229350647201\n",
      "Gradient Descent(42/99): loss=0.3999522901493473, gradient=0.051494158573390895\n",
      "Gradient Descent(43/99): loss=0.3996960524174225, gradient=0.05079221134762415\n",
      "Gradient Descent(44/99): loss=0.3994466812242809, gradient=0.05010565849649762\n",
      "Gradient Descent(45/99): loss=0.39920394041300306, gradient=0.04943377796512439\n",
      "Gradient Descent(46/99): loss=0.39896760591953945, gradient=0.04877591002462732\n",
      "Gradient Descent(47/99): loss=0.39873746480240524, gradient=0.0481314501266688\n",
      "Gradient Descent(48/99): loss=0.39851331438179816, gradient=0.04749984248995074\n",
      "Gradient Descent(49/99): loss=0.3982949614727273, gradient=0.04688057461623183\n",
      "Gradient Descent(50/99): loss=0.3980822216994455, gradient=0.04627317254194218\n",
      "Gradient Descent(51/99): loss=0.3978749188804373, gradient=0.04567719671460195\n",
      "Gradient Descent(52/99): loss=0.3976728844748547, gradient=0.04509223840153643\n",
      "Gradient Descent(53/99): loss=0.39747595708264194, gradient=0.0445179165533852\n",
      "Gradient Descent(54/99): loss=0.3972839819917398, gradient=0.04395387505727205\n",
      "Gradient Descent(55/99): loss=0.39709681076668946, gradient=0.04339978032474086\n",
      "Gradient Descent(56/99): loss=0.39691430087377383, gradient=0.042855319168047495\n",
      "Gradient Descent(57/99): loss=0.3967363153384928, gradient=0.04232019692547963\n",
      "Gradient Descent(58/99): loss=0.3965627224317477, gradient=0.04179413580227381\n",
      "Gradient Descent(59/99): loss=0.3963933953815856, gradient=0.041276873398653026\n",
      "Gradient Descent(60/99): loss=0.3962282121077706, gradient=0.04076816140066142\n",
      "Gradient Descent(61/99): loss=0.39606705497679734, gradient=0.04026776441296755\n",
      "Gradient Descent(62/99): loss=0.395909810575257, gradient=0.039775458915765144\n",
      "Gradient Descent(63/99): loss=0.3957563694997331, gradient=0.03929103233039747\n",
      "Gradient Descent(64/99): loss=0.39560662616161485, gradient=0.03881428218045014\n",
      "Gradient Descent(65/99): loss=0.39546047860541256, gradient=0.03834501533686001\n",
      "Gradient Descent(66/99): loss=0.39531782833932466, gradient=0.03788304733712038\n",
      "Gradient Descent(67/99): loss=0.3951785801769418, gradient=0.03742820176998273\n",
      "Gradient Descent(68/99): loss=0.3950426420891082, gradient=0.036980309718161415\n",
      "Gradient Descent(69/99): loss=0.3949099250650588, gradient=0.036539209252526184\n",
      "Gradient Descent(70/99): loss=0.3947803429820488, gradient=0.03610474497208216\n",
      "Gradient Descent(71/99): loss=0.39465381248277903, gradient=0.035676767584758455\n",
      "Gradient Descent(72/99): loss=0.39453025285998194, gradient=0.0352551335246314\n",
      "Gradient Descent(73/99): loss=0.39440958594761016, gradient=0.03483970460174844\n",
      "Gradient Descent(74/99): loss=0.3942917360181147, gradient=0.03443034768117341\n",
      "Gradient Descent(75/99): loss=0.39417662968535516, gradient=0.03402693438827618\n",
      "Gradient Descent(76/99): loss=0.394064195812726, gradient=0.03362934083763433\n",
      "Gradient Descent(77/99): loss=0.3939543654261239, gradient=0.03323744738322079\n",
      "Gradient Descent(78/99): loss=0.3938470716314105, gradient=0.03285113838781254\n",
      "Gradient Descent(79/99): loss=0.3937422495360644, gradient=0.03247030200978769\n",
      "Gradient Descent(80/99): loss=0.3936398361747363, gradient=0.032094830005680475\n",
      "Gradient Descent(81/99): loss=0.3935397704384448, gradient=0.031724617547039494\n",
      "Gradient Descent(82/99): loss=0.3934419930071818, gradient=0.03135956305029694\n",
      "Gradient Descent(83/99): loss=0.39334644628570736, gradient=0.030999568018481354\n",
      "Gradient Descent(84/99): loss=0.3932530743423311, gradient=0.030644536893743277\n",
      "Gradient Descent(85/99): loss=0.39316182285050155, gradient=0.030294376919755863\n",
      "Gradient Descent(86/99): loss=0.393072639033033, gradient=0.029948998013157216\n",
      "Gradient Descent(87/99): loss=0.39298547160880976, gradient=0.029608312643279806\n",
      "Gradient Descent(88/99): loss=0.3929002707418292, gradient=0.029272235719487526\n",
      "Gradient Descent(89/99): loss=0.39281698799244646, gradient=0.0289406844855083\n",
      "Gradient Descent(90/99): loss=0.3927355762706962, gradient=0.02861357842020774\n",
      "Gradient Descent(91/99): loss=0.39265598979158134, gradient=0.028290839144304535\n",
      "Gradient Descent(92/99): loss=0.39257818403221617, gradient=0.027972390332566942\n",
      "Gradient Descent(93/99): loss=0.3925021156907267, gradient=0.027658157631085013\n",
      "Gradient Descent(94/99): loss=0.39242774264681823, gradient=0.02734806857923701\n",
      "Gradient Descent(95/99): loss=0.3923550239239196, gradient=0.02704205253601044\n",
      "Gradient Descent(96/99): loss=0.39228391965282505, gradient=0.02674004061036555\n",
      "Gradient Descent(97/99): loss=0.3922143910367591, gradient=0.02644196559535592\n",
      "Gradient Descent(98/99): loss=0.392146400317793, gradient=0.026147761905745193\n",
      "Gradient Descent(99/99): loss=0.3920799107445456, gradient=0.025857365518883995\n",
      "Gradient Descent(0/99): loss=0.3931136361536193, gradient=0.02929227999974526\n",
      "Gradient Descent(1/99): loss=0.3930501112864817, gradient=0.02544358319215984\n",
      "Gradient Descent(2/99): loss=0.3929891415998354, gradient=0.02480005277950357\n",
      "Gradient Descent(3/99): loss=0.39293006627263216, gradient=0.02439570497433975\n",
      "Gradient Descent(4/99): loss=0.392872649365054, gradient=0.02404361609961431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(5/99): loss=0.3928167341925784, gradient=0.02372229136432655\n",
      "Gradient Descent(6/99): loss=0.3927622060938461, gradient=0.0234228377871008\n",
      "Gradient Descent(7/99): loss=0.39270897805329036, gradient=0.023139532903066418\n",
      "Gradient Descent(8/99): loss=0.39265698183265385, gradient=0.0228684844311558\n",
      "Gradient Descent(9/99): loss=0.39260616215781596, gradient=0.022607007172818026\n",
      "Gradient Descent(10/99): loss=0.39255647285327666, gradient=0.022353226768856663\n",
      "Gradient Descent(11/99): loss=0.39250787424427225, gradient=0.022105818262715547\n",
      "Gradient Descent(12/99): loss=0.39246033139183045, gradient=0.021863832236825198\n",
      "Gradient Descent(13/99): loss=0.39241381288021565, gradient=0.021626578339902974\n",
      "Gradient Descent(14/99): loss=0.3923682899741303, gradient=0.021393546663418365\n",
      "Gradient Descent(15/99): loss=0.39232373602576187, gradient=0.021164354237525725\n",
      "Gradient Descent(16/99): loss=0.39228012605227475, gradient=0.02093870831425354\n",
      "Gradient Descent(17/99): loss=0.39223743643070574, gradient=0.020716380952845764\n",
      "Gradient Descent(18/99): loss=0.39219564467453, gradient=0.020497191273113686\n",
      "Gradient Descent(19/99): loss=0.39215472926760425, gradient=0.0202809929523353\n",
      "Gradient Descent(20/99): loss=0.39211466953883334, gradient=0.020067665336539153\n",
      "Gradient Descent(21/99): loss=0.3920754455660431, gradient=0.01985710706330775\n",
      "Gradient Descent(22/99): loss=0.39203703810101626, gradient=0.019649231443918794\n",
      "Gradient Descent(23/99): loss=0.39199942851003944, gradient=0.01944396308794584\n",
      "Gradient Descent(24/99): loss=0.39196259872593947, gradient=0.019241235412483237\n",
      "Gradient Descent(25/99): loss=0.39192653120873067, gradient=0.019040988786400904\n",
      "Gradient Descent(26/99): loss=0.39189120891279267, gradient=0.018843169134249462\n",
      "Gradient Descent(27/99): loss=0.39185661525905735, gradient=0.01864772687566371\n",
      "Gradient Descent(28/99): loss=0.3918227341110877, gradient=0.01845461611172607\n",
      "Gradient Descent(29/99): loss=0.3917895497542153, gradient=0.018263793994677306\n",
      "Gradient Descent(30/99): loss=0.3917570468771125, gradient=0.018075220234929166\n",
      "Gradient Descent(31/99): loss=0.3917252105553257, gradient=0.017888856711793303\n",
      "Gradient Descent(32/99): loss=0.39169402623640626, gradient=0.0177046671632429\n",
      "Gradient Descent(33/99): loss=0.3916634797263605, gradient=0.01752261693642113\n",
      "Gradient Descent(34/99): loss=0.39163355717719894, gradient=0.017342672785248505\n",
      "Gradient Descent(35/99): loss=0.39160424507541275, gradient=0.017164802704854536\n",
      "Gradient Descent(36/99): loss=0.3915755302312413, gradient=0.016988975795041054\n",
      "Gradient Descent(37/99): loss=0.3915473997686206, gradient=0.016815162146819246\n",
      "Gradient Descent(38/99): loss=0.3915198411157228, gradient=0.016643332747425788\n",
      "Gradient Descent(39/99): loss=0.3914928419960155, gradient=0.016473459400248316\n",
      "Gradient Descent(40/99): loss=0.3914663904197797, gradient=0.016305514656867963\n",
      "Gradient Descent(41/99): loss=0.39144047467603926, gradient=0.01613947175901126\n",
      "Gradient Descent(42/99): loss=0.3914150833248567, gradient=0.01597530458866058\n",
      "Gradient Descent(43/99): loss=0.39139020518996254, gradient=0.015812987624920997\n",
      "Gradient Descent(44/99): loss=0.3913658293516914, gradient=0.015652495906511722\n",
      "Gradient Descent(45/99): loss=0.3913419451401916, gradient=0.015493804998967867\n",
      "Gradient Descent(46/99): loss=0.391318542128895, gradient=0.015336890965802654\n",
      "Gradient Descent(47/99): loss=0.3912956101282233, gradient=0.015181730343019043\n",
      "Gradient Descent(48/99): loss=0.391273139179516, gradient=0.015028300116462825\n",
      "Gradient Descent(49/99): loss=0.39125111954916336, gradient=0.014876577701599064\n",
      "Gradient Descent(50/99): loss=0.3912295417229362, gradient=0.01472654092536287\n",
      "Gradient Descent(51/99): loss=0.3912083964004964, gradient=0.0145781680097911\n",
      "Gradient Descent(52/99): loss=0.3911876744900802, gradient=0.014431437557191321\n",
      "Gradient Descent(53/99): loss=0.391167367103347, gradient=0.014286328536640415\n",
      "Gradient Descent(54/99): loss=0.39114746555038027, gradient=0.01414282027163835\n",
      "Gradient Descent(55/99): loss=0.39112796133483985, gradient=0.014000892428768483\n",
      "Gradient Descent(56/99): loss=0.3911088461492538, gradient=0.013860525007237941\n",
      "Gradient Descent(57/99): loss=0.39109011187044534, gradient=0.013721698329190643\n",
      "Gradient Descent(58/99): loss=0.3910717505550902, gradient=0.013584393030700108\n",
      "Gradient Descent(59/99): loss=0.39105375443597906, gradient=0.013448589616928774\n",
      "Gradient Descent(60/99): loss=0.3910361159160578, gradient=0.013314270202949724\n",
      "Gradient Descent(61/99): loss=0.39101882756611817, gradient=0.01318141587887573\n",
      "Gradient Descent(62/99): loss=0.3910018821202268, gradient=0.013050008457464974\n",
      "Gradient Descent(63/99): loss=0.39098527247185766, gradient=0.012920030028144108\n",
      "Gradient Descent(64/99): loss=0.3909689916701277, gradient=0.012791462950746159\n",
      "Gradient Descent(65/99): loss=0.3909530329161329, gradient=0.012664289849555872\n",
      "Gradient Descent(66/99): loss=0.3909373895593778, gradient=0.012538493607629804\n",
      "Gradient Descent(67/99): loss=0.3909220550943015, gradient=0.012414057361368993\n",
      "Gradient Descent(68/99): loss=0.39090702315689124, gradient=0.01229096449531967\n",
      "Gradient Descent(69/99): loss=0.3908922875213841, gradient=0.012169198637182145\n",
      "Gradient Descent(70/99): loss=0.39087784209705356, gradient=0.012048743653011881\n",
      "Gradient Descent(71/99): loss=0.39086368092507784, gradient=0.01192958364259589\n",
      "Gradient Descent(72/99): loss=0.39084979817548693, gradient=0.011811702934991394\n",
      "Gradient Descent(73/99): loss=0.3908361881441886, gradient=0.011695086084215224\n",
      "Gradient Descent(74/99): loss=0.3908228452500671, gradient=0.011579717865071948\n",
      "Gradient Descent(75/99): loss=0.39080976403215684, gradient=0.011465583269112303\n",
      "Gradient Descent(76/99): loss=0.39079693914688524, gradient=0.011352667500712992\n",
      "Gradient Descent(77/99): loss=0.39078436536538397, gradient=0.011240955973269576\n",
      "Gradient Descent(78/99): loss=0.3907720375708686, gradient=0.011130434305496775\n",
      "Gradient Descent(79/99): loss=0.3907599507560815, gradient=0.011021088317828812\n",
      "Gradient Descent(80/99): loss=0.3907481000207978, gradient=0.010912904028914988\n",
      "Gradient Descent(81/99): loss=0.3907364805693949, gradient=0.010805867652205052\n",
      "Gradient Descent(82/99): loss=0.39072508770847786, gradient=0.01069996559261969\n",
      "Gradient Descent(83/99): loss=0.3907139168445663, gradient=0.010595184443302366\n",
      "Gradient Descent(84/99): loss=0.39070296348183553, gradient=0.010491510982448854\n",
      "Gradient Descent(85/99): loss=0.3906922232199137, gradient=0.01038893217020951\n",
      "Gradient Descent(86/99): loss=0.39068169175173173, gradient=0.0102874351456634\n",
      "Gradient Descent(87/99): loss=0.39067136486142606, gradient=0.010187007223859616\n",
      "Gradient Descent(88/99): loss=0.39066123842229145, gradient=0.010087635892923342\n",
      "Gradient Descent(89/99): loss=0.3906513083947825, gradient=0.009989308811225258\n",
      "Gradient Descent(90/99): loss=0.39064157082456513, gradient=0.009892013804610278\n",
      "Gradient Descent(91/99): loss=0.3906320218406121, gradient=0.009795738863685056\n",
      "Gradient Descent(92/99): loss=0.3906226576533476, gradient=0.009700472141161095\n",
      "Gradient Descent(93/99): loss=0.39061347455283135, gradient=0.00960620194925256\n",
      "Gradient Descent(94/99): loss=0.3906044689069911, gradient=0.009512916757125987\n",
      "Gradient Descent(95/99): loss=0.39059563715989365, gradient=0.009420605188401343\n",
      "Gradient Descent(96/99): loss=0.39058697583005797, gradient=0.009329256018702185\n",
      "Gradient Descent(97/99): loss=0.3905784815088092, gradient=0.009238858173253314\n",
      "Gradient Descent(98/99): loss=0.3905701508586689, gradient=0.009149400724525709\n",
      "Gradient Descent(99/99): loss=0.39056198061178676, gradient=0.009060872889925766\n",
      "Gradient Descent(0/99): loss=0.3908644167892719, gradient=0.012184585196861877\n",
      "Gradient Descent(1/99): loss=0.3908514479520625, gradient=0.011511971039415718\n",
      "Gradient Descent(2/99): loss=0.3908394474281738, gradient=0.011049401914682995\n",
      "Gradient Descent(3/99): loss=0.3908281815124112, gradient=0.010689860512636866\n",
      "Gradient Descent(4/99): loss=0.3908174943419215, gradient=0.010400300750548567\n",
      "Gradient Descent(5/99): loss=0.3908072784878659, gradient=0.010160225996938358\n",
      "Gradient Descent(6/99): loss=0.39079745818384587, gradient=0.00995567860649207\n",
      "Gradient Descent(7/99): loss=0.39078797875259697, gradient=0.009777039654202867\n",
      "Gradient Descent(8/99): loss=0.390778799805119, gradient=0.009617622969342614\n",
      "Gradient Descent(9/99): loss=0.39076989080412844, gradient=0.00947273003461397\n",
      "Gradient Descent(10/99): loss=0.39076122812645503, gradient=0.009339017741904067\n",
      "Gradient Descent(11/99): loss=0.3907527930893749, gradient=0.009214073397183296\n",
      "Gradient Descent(12/99): loss=0.3907445706019307, gradient=0.009096131506383285\n",
      "Gradient Descent(13/99): loss=0.3907365482290875, gradient=0.008983881597061509\n",
      "Gradient Descent(14/99): loss=0.3907287155318152, gradient=0.008876337446215453\n",
      "Gradient Descent(15/99): loss=0.3907210635944937, gradient=0.008772746993158002\n",
      "Gradient Descent(16/99): loss=0.39071358468141615, gradient=0.008672529522599141\n",
      "Gradient Descent(17/99): loss=0.39070627198361396, gradient=0.008575231299686735\n",
      "Gradient Descent(18/99): loss=0.3906991194297946, gradient=0.008480493828248212\n",
      "Gradient Descent(19/99): loss=0.3906921215434266, gradient=0.008388030848999395\n",
      "Gradient Descent(20/99): loss=0.3906852733334821, gradient=0.008297611465477361\n",
      "Gradient Descent(21/99): loss=0.3906785702100205, gradient=0.008209047621031055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(22/99): loss=0.39067200791835816, gradient=0.008122184658602555\n",
      "Gradient Descent(23/99): loss=0.3906655824871045, gradient=0.008036894366382543\n",
      "Gradient Descent(24/99): loss=0.3906592901870034, gradient=0.007953069341530939\n",
      "Gradient Descent(25/99): loss=0.39065312749792347, gradient=0.007870618841173446\n",
      "Gradient Descent(26/99): loss=0.39064709108214407, gradient=0.007789465574923996\n",
      "Gradient Descent(27/99): loss=0.3906411777626142, gradient=0.007709543124731943\n",
      "Gradient Descent(28/99): loss=0.3906353845051007, gradient=0.007630793934332211\n",
      "Gradient Descent(29/99): loss=0.3906297084034101, gradient=0.007553167715718203\n",
      "Gradient Descent(30/99): loss=0.3906241466670652, gradient=0.00747662017881095\n",
      "Gradient Descent(31/99): loss=0.39061869661094284, gradient=0.00740111201318998\n",
      "Gradient Descent(32/99): loss=0.39061335564649424, gradient=0.007326608067553757\n",
      "Gradient Descent(33/99): loss=0.39060812127425076, gradient=0.007253076685136475\n",
      "Gradient Descent(34/99): loss=0.3906029910773759, gradient=0.007180489162762884\n",
      "Gradient Descent(35/99): loss=0.39059796271607805, gradient=0.007108819308396844\n",
      "Gradient Descent(36/99): loss=0.39059303392273087, gradient=0.007038043077516763\n",
      "Gradient Descent(37/99): loss=0.3905882024975848, gradient=0.006968138272861975\n",
      "Gradient Descent(38/99): loss=0.3905834663049688, gradient=0.00689908429534438\n",
      "Gradient Descent(39/99): loss=0.3905788232699062, gradient=0.0068308619364468364\n",
      "Gradient Descent(40/99): loss=0.39057427137507866, gradient=0.006763453204398958\n",
      "Gradient Descent(41/99): loss=0.3905698086580881, gradient=0.006696841177967116\n",
      "Gradient Descent(42/99): loss=0.39056543320897147, gradient=0.006631009882909984\n",
      "Gradient Descent(43/99): loss=0.3905611431679323, gradient=0.006565944187113628\n",
      "Gradient Descent(44/99): loss=0.39055693672326236, gradient=0.0065016297111797055\n",
      "Gradient Descent(45/99): loss=0.3905528121094272, gradient=0.006438052751850186\n",
      "Gradient Descent(46/99): loss=0.39054876760529256, gradient=0.006375200216134897\n",
      "Gradient Descent(47/99): loss=0.3905448015324796, gradient=0.006313059564397769\n",
      "Gradient Descent(48/99): loss=0.3905409122538289, gradient=0.006251618760971116\n",
      "Gradient Descent(49/99): loss=0.3905370981719644, gradient=0.006190866231119019\n",
      "Gradient Descent(50/99): loss=0.3905333577279461, gradient=0.0061307908233758975\n",
      "Gradient Descent(51/99): loss=0.3905296893999994, gradient=0.006071381776452962\n",
      "Gradient Descent(52/99): loss=0.3905260917023181, gradient=0.006012628690040289\n",
      "Gradient Descent(53/99): loss=0.3905225631839313, gradient=0.005954521498943821\n",
      "Gradient Descent(54/99): loss=0.3905191024276268, gradient=0.00589705045008645\n",
      "Gradient Descent(55/99): loss=0.3905157080489315, gradient=0.005840206081979324\n",
      "Gradient Descent(56/99): loss=0.3905123786951376, gradient=0.005783979206328657\n",
      "Gradient Descent(57/99): loss=0.39050911304437497, gradient=0.0057283608914978764\n",
      "Gradient Descent(58/99): loss=0.39050590980472627, gradient=0.005673342447584194\n",
      "Gradient Descent(59/99): loss=0.39050276771337705, gradient=0.005618915412906768\n",
      "Gradient Descent(60/99): loss=0.39049968553580566, gradient=0.00556507154173255\n",
      "Gradient Descent(61/99): loss=0.39049666206500355, gradient=0.005511802793089938\n",
      "Gradient Descent(62/99): loss=0.3904936961207276, gradient=0.005459101320542905\n",
      "Gradient Descent(63/99): loss=0.3904907865487802, gradient=0.005406959462815095\n",
      "Gradient Descent(64/99): loss=0.39048793222031986, gradient=0.00535536973516797\n",
      "Gradient Descent(65/99): loss=0.3904851320311923, gradient=0.005304324821450634\n",
      "Gradient Descent(66/99): loss=0.3904823849012905, gradient=0.0052538175667500644\n",
      "Gradient Descent(67/99): loss=0.3904796897739343, gradient=0.005203840970577941\n",
      "Gradient Descent(68/99): loss=0.39047704561527363, gradient=0.005154388180540377\n",
      "Gradient Descent(69/99): loss=0.3904744514137095, gradient=0.0051054524864418286\n",
      "Gradient Descent(70/99): loss=0.39047190617933736, gradient=0.005057027314782074\n",
      "Gradient Descent(71/99): loss=0.39046940894340704, gradient=0.005009106223607907\n",
      "Gradient Descent(72/99): loss=0.39046695875780046, gradient=0.004961682897688048\n",
      "Gradient Descent(73/99): loss=0.3904645546945257, gradient=0.004914751143981572\n",
      "Gradient Descent(74/99): loss=0.390462195845228, gradient=0.0048683048873743\n",
      "Gradient Descent(75/99): loss=0.39045988132071485, gradient=0.004822338166660443\n",
      "Gradient Descent(76/99): loss=0.3904576102504959, gradient=0.004776845130749128\n",
      "Gradient Descent(77/99): loss=0.3904553817823374, gradient=0.004731820035077598\n",
      "Gradient Descent(78/99): loss=0.39045319508182824, gradient=0.004687257238214721\n",
      "Gradient Descent(79/99): loss=0.39045104933196195, gradient=0.004643151198640255\n",
      "Gradient Descent(80/99): loss=0.39044894373272615, gradient=0.004599496471686708\n",
      "Gradient Descent(81/99): loss=0.39044687750070933, gradient=0.004556287706631993\n",
      "Gradient Descent(82/99): loss=0.39044484986871336, gradient=0.004513519643932005\n",
      "Gradient Descent(83/99): loss=0.39044286008538187, gradient=0.0044711871125836765\n",
      "Gradient Descent(84/99): loss=0.3904409074148351, gradient=0.004429285027609287\n",
      "Gradient Descent(85/99): loss=0.3904389911363176, gradient=0.004387808387654633\n",
      "Gradient Descent(86/99): loss=0.3904371105438535, gradient=0.004346752272692924\n",
      "Gradient Descent(87/99): loss=0.390435264945913, gradient=0.004306111841828617\n",
      "Gradient Descent(88/99): loss=0.39043345366508636, gradient=0.004265882331194336\n",
      "Gradient Descent(89/99): loss=0.3904316760377682, gradient=0.004226059051935839\n",
      "Gradient Descent(90/99): loss=0.39042993141384713, gradient=0.00418663738827924\n",
      "Gradient Descent(91/99): loss=0.3904282191564088, gradient=0.004147612795676543\n",
      "Gradient Descent(92/99): loss=0.3904265386414396, gradient=0.004108980799024386\n",
      "Gradient Descent(93/99): loss=0.39042488925754587, gradient=0.004070736990952448\n",
      "Gradient Descent(94/99): loss=0.3904232704056731, gradient=0.004032877030177179\n",
      "Gradient Descent(95/99): loss=0.3904216814988378, gradient=0.0039953966399182495\n",
      "Gradient Descent(96/99): loss=0.3904201219618628, gradient=0.003958291606373581\n",
      "Gradient Descent(97/99): loss=0.39041859123112155, gradient=0.003921557777250632\n",
      "Gradient Descent(98/99): loss=0.3904170887542856, gradient=0.003885191060350514\n",
      "Gradient Descent(99/99): loss=0.390415613990083, gradient=0.0038491874222028196\n",
      "Gradient Descent(0/99): loss=0.3900595179176018, gradient=0.010455089600853305\n",
      "Gradient Descent(1/99): loss=0.39005227948199117, gradient=0.008826390692813746\n",
      "Gradient Descent(2/99): loss=0.3900468320458033, gradient=0.007617745162507716\n",
      "Gradient Descent(3/99): loss=0.3900425498657265, gradient=0.006720430731040395\n",
      "Gradient Descent(4/99): loss=0.3900390441394669, gradient=0.006053162876174374\n",
      "Gradient Descent(5/99): loss=0.3900360702274176, gradient=0.005553507127402978\n",
      "Gradient Descent(6/99): loss=0.3900334719072887, gradient=0.005174495720290285\n",
      "Gradient Descent(7/99): loss=0.3900311476604302, gradient=0.004881699424830834\n",
      "Gradient Descent(8/99): loss=0.3900290301699784, gradient=0.0046504541068832115\n",
      "Gradient Descent(9/99): loss=0.39002707377138646, gradient=0.00446338852284706\n",
      "Gradient Descent(10/99): loss=0.39002524671010647, gradient=0.004308386753758206\n",
      "Gradient Descent(11/99): loss=0.3900235263199127, gradient=0.004177020501905441\n",
      "Gradient Descent(12/99): loss=0.3900218959865101, gradient=0.0040634046875857015\n",
      "Gradient Descent(13/99): loss=0.3900203432103998, gradient=0.0039633925520903565\n",
      "Gradient Descent(14/99): loss=0.39001885835246436, gradient=0.003874023234030106\n",
      "Gradient Descent(15/99): loss=0.3900174338079135, gradient=0.003793148063615669\n",
      "Gradient Descent(16/99): loss=0.39001606345219336, gradient=0.003719179423898596\n",
      "Gradient Descent(17/99): loss=0.39001474226192145, gradient=0.003650922053856993\n",
      "Gradient Descent(18/99): loss=0.3900134660501893, gradient=0.003587459217576268\n",
      "Gradient Descent(19/99): loss=0.39001223127786, gradient=0.003528075238573197\n",
      "Gradient Descent(20/99): loss=0.39001103491627637, gradient=0.0034722021609758977\n",
      "Gradient Descent(21/99): loss=0.39000987434541484, gradient=0.0034193824997928862\n",
      "Gradient Descent(22/99): loss=0.3900087472769411, gradient=0.0033692428116377056\n",
      "Gradient Descent(23/99): loss=0.3900076516951072, gradient=0.0033214746249929364\n",
      "Gradient Descent(24/99): loss=0.3900065858106581, gradient=0.0032758204439589265\n",
      "Gradient Descent(25/99): loss=0.3900055480243953, gradient=0.003232063302720039\n",
      "Gradient Descent(26/99): loss=0.3900045368980259, gradient=0.003190018845333897\n",
      "Gradient Descent(27/99): loss=0.3900035511306351, gradient=0.003149529070084929\n",
      "Gradient Descent(28/99): loss=0.39000258953928013, gradient=0.0031104582322318444\n",
      "Gradient Descent(29/99): loss=0.3900016510432897, gradient=0.003072688096030033\n",
      "Gradient Descent(30/99): loss=0.3900007346510388, gradient=0.003036115617703257\n",
      "Gradient Descent(31/99): loss=0.3899998394489366, gradient=0.0030006503655637634\n",
      "Gradient Descent(32/99): loss=0.3899989645921524, gradient=0.002966212563960156\n",
      "Gradient Descent(33/99): loss=0.38999810929675693, gradient=0.002932731493053037\n",
      "Gradient Descent(34/99): loss=0.3899972728330266, gradient=0.002900144170850987\n",
      "Gradient Descent(35/99): loss=0.3899964545197029, gradient=0.0028683942610659577\n",
      "Gradient Descent(36/99): loss=0.38999565371904754, gradient=0.002837431163001562\n",
      "Gradient Descent(37/99): loss=0.38999486983256104, gradient=0.0028072092491711787\n",
      "Gradient Descent(38/99): loss=0.38999410229725606, gradient=0.002777687223536495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(39/99): loss=0.3899933505824015, gradient=0.0027488275787746924\n",
      "Gradient Descent(40/99): loss=0.38999261418666453, gradient=0.002720596135260393\n",
      "Gradient Descent(41/99): loss=0.38999189263558975, gradient=0.002692961647791506\n",
      "Gradient Descent(42/99): loss=0.3899911854793696, gradient=0.0026658954687239556\n",
      "Gradient Descent(43/99): loss=0.3899904922908627, gradient=0.0026393712582730667\n",
      "Gradient Descent(44/99): loss=0.38998981266382765, gradient=0.0026133647344095787\n",
      "Gradient Descent(45/99): loss=0.3899891462113414, gradient=0.002587853456121056\n",
      "Gradient Descent(46/99): loss=0.38998849256438106, gradient=0.0025628166348938015\n",
      "Gradient Descent(47/99): loss=0.38998785137054603, gradient=0.0025382349701485022\n",
      "Gradient Descent(48/99): loss=0.3899872222929028, gradient=0.0025140905050802915\n",
      "Gradient Descent(49/99): loss=0.3899866050090161, gradient=0.002490366192782256\n",
      "Gradient Descent(50/99): loss=0.38998599920976934, gradient=0.002467047021214924\n",
      "Gradient Descent(51/99): loss=0.38998540459873315, gradient=0.002444118046701338\n",
      "Gradient Descent(52/99): loss=0.38998482089129777, gradient=0.002421565559811086\n",
      "Gradient Descent(53/99): loss=0.38998424781395075, gradient=0.0023993766915679604\n",
      "Gradient Descent(54/99): loss=0.38998368510361464, gradient=0.0023775393436482315\n",
      "Gradient Descent(55/99): loss=0.3899831325070398, gradient=0.0023560421258648732\n",
      "Gradient Descent(56/99): loss=0.3899825897802434, gradient=0.002334874300020672\n",
      "Gradient Descent(57/99): loss=0.3899820566879932, gradient=0.0023140257293461112\n",
      "Gradient Descent(58/99): loss=0.3899815330033314, gradient=0.0022934868328486633\n",
      "Gradient Descent(59/99): loss=0.3899810185071319, gradient=0.0022732485439945532\n",
      "Gradient Descent(60/99): loss=0.38998051298769104, gradient=0.002253302273222433\n",
      "Gradient Descent(61/99): loss=0.3899800162403484, gradient=0.00223363987385633\n",
      "Gradient Descent(62/99): loss=0.389979528067132, gradient=0.0022142536110422517\n",
      "Gradient Descent(63/99): loss=0.3899790482764301, gradient=0.002195136133381508\n",
      "Gradient Descent(64/99): loss=0.3899785766826848, gradient=0.002176280446975261\n",
      "Gradient Descent(65/99): loss=0.38997811310610364, gradient=0.0021576798916308803\n",
      "Gradient Descent(66/99): loss=0.38997765737239387, gradient=0.002139328119010919\n",
      "Gradient Descent(67/99): loss=0.3899772093125108, gradient=0.002121219072532028\n",
      "Gradient Descent(68/99): loss=0.3899767687624229, gradient=0.0021033469688440075\n",
      "Gradient Descent(69/99): loss=0.389976335562892, gradient=0.002085706280738609\n",
      "Gradient Descent(70/99): loss=0.38997590955926587, gradient=0.0020682917213547636\n",
      "Gradient Descent(71/99): loss=0.3899754906012844, gradient=0.0020510982295623907\n",
      "Gradient Descent(72/99): loss=0.38997507854289587, gradient=0.0020341209564186937\n",
      "Gradient Descent(73/99): loss=0.3899746732420859, gradient=0.002017355252603349\n",
      "Gradient Descent(74/99): loss=0.38997427456071343, gradient=0.002000796656747973\n",
      "Gradient Descent(75/99): loss=0.38997388236435854, gradient=0.001984440884584303\n",
      "Gradient Descent(76/99): loss=0.3899734965221771, gradient=0.0019682838188436686\n",
      "Gradient Descent(77/99): loss=0.3899731169067647, gradient=0.0019523214998456411\n",
      "Gradient Descent(78/99): loss=0.3899727433940258, gradient=0.0019365501167212878\n",
      "Gradient Descent(79/99): loss=0.38997237586305256, gradient=0.0019209659992208768\n",
      "Gradient Descent(80/99): loss=0.38997201419600774, gradient=0.001905565610060421\n",
      "Gradient Descent(81/99): loss=0.38997165827801444, gradient=0.00189034553776627\n",
      "Gradient Descent(82/99): loss=0.38997130799705243, gradient=0.0018753024899800556\n",
      "Gradient Descent(83/99): loss=0.3899709632438575, gradient=0.001860433287189923\n",
      "Gradient Descent(84/99): loss=0.3899706239118279, gradient=0.0018457348568568209\n",
      "Gradient Descent(85/99): loss=0.3899702898969342, gradient=0.0018312042279074994\n",
      "Gradient Descent(86/99): loss=0.38996996109763327, gradient=0.00181683852556785\n",
      "Gradient Descent(87/99): loss=0.3899696374147875, gradient=0.0018026349665129459\n",
      "Gradient Descent(88/99): loss=0.38996931875158664, gradient=0.0017885908543113984\n",
      "Gradient Descent(89/99): loss=0.38996900501347337, gradient=0.0017747035751442372\n",
      "Gradient Descent(90/99): loss=0.38996869610807317, gradient=0.0017609705937792488\n",
      "Gradient Descent(91/99): loss=0.3899683919451258, gradient=0.0017473894497838478\n",
      "Gradient Descent(92/99): loss=0.3899680924364213, gradient=0.0017339577539604595\n",
      "Gradient Descent(93/99): loss=0.3899677974957377, gradient=0.0017206731849898412\n",
      "Gradient Descent(94/99): loss=0.38996750703878186, gradient=0.0017075334862687846\n",
      "Gradient Descent(95/99): loss=0.3899672209831331, gradient=0.0016945364629293164\n",
      "Gradient Descent(96/99): loss=0.389966939248188, gradient=0.0016816799790282022\n",
      "Gradient Descent(97/99): loss=0.3899666617551087, gradient=0.0016689619548955314\n",
      "Gradient Descent(98/99): loss=0.38996638842677295, gradient=0.0016563803646326382\n",
      "Gradient Descent(99/99): loss=0.3899661191877259, gradient=0.0016439332337496127\n",
      "Gradient Descent(0/99): loss=0.3896843458703095, gradient=0.011105822360142074\n",
      "Gradient Descent(1/99): loss=0.3896792962271055, gradient=0.007489712877630379\n",
      "Gradient Descent(2/99): loss=0.38967570047060845, gradient=0.006231478614249744\n",
      "Gradient Descent(3/99): loss=0.38967304695277505, gradient=0.005336154206764714\n",
      "Gradient Descent(4/99): loss=0.3896710466657924, gradient=0.004621801322590125\n",
      "Gradient Descent(5/99): loss=0.3896695076235214, gradient=0.004044421846562549\n",
      "Gradient Descent(6/99): loss=0.3896682993346084, gradient=0.003575218416190899\n",
      "Gradient Descent(7/99): loss=0.38966733204727677, gradient=0.0031916960542492747\n",
      "Gradient Descent(8/99): loss=0.3896665432862394, gradient=0.0028760781739820213\n",
      "Gradient Descent(9/99): loss=0.38966588899788185, gradient=0.00261435908559266\n",
      "Gradient Descent(10/99): loss=0.3896653376870152, gradient=0.0023955484737352094\n",
      "Gradient Descent(11/99): loss=0.3896648665083078, gradient=0.0022110511008593234\n",
      "Gradient Descent(12/99): loss=0.389664458641458, gradient=0.002054158372014953\n",
      "Gradient Descent(13/99): loss=0.38966410151403236, gradient=0.001919635382724585\n",
      "Gradient Descent(14/99): loss=0.38966378558737325, gradient=0.0018033897593365694\n",
      "Gradient Descent(15/99): loss=0.3896635035189632, gradient=0.0017022097644554682\n",
      "Gradient Descent(16/99): loss=0.3896632495783038, gradient=0.0016135599380958652\n",
      "Gradient Descent(17/99): loss=0.3896630192348731, gradient=0.001535423501769972\n",
      "Gradient Descent(18/99): loss=0.3896628088638998, gradient=0.001466181977854311\n",
      "Gradient Descent(19/99): loss=0.38966261553357473, gradient=0.0014045238764429475\n",
      "Gradient Descent(20/99): loss=0.38966243684913476, gradient=0.0013493757303385972\n",
      "Gradient Descent(21/99): loss=0.38966227083709687, gradient=0.001299850094898141\n",
      "Gradient Descent(22/99): loss=0.389662115858176, gradient=0.0012552063008543033\n",
      "Gradient Descent(23/99): loss=0.38966197054093554, gradient=0.001214820726989414\n",
      "Gradient Descent(24/99): loss=0.38966183373062163, gradient=0.0011781641481999525\n",
      "Gradient Descent(25/99): loss=0.3896617044492438, gradient=0.0011447843326400844\n",
      "Gradient Descent(26/99): loss=0.38966158186410277, gradient=0.0011142925358648547\n",
      "Gradient Descent(27/99): loss=0.3896614652627296, gradient=0.001086352897651642\n",
      "Gradient Descent(28/99): loss=0.389661354032755, gradient=0.0010606740134770155\n",
      "Gradient Descent(29/99): loss=0.389661247645606, gradient=0.0010370021487429951\n",
      "Gradient Descent(30/99): loss=0.3896611456432085, gradient=0.0010151157070436285\n",
      "Gradient Descent(31/99): loss=0.3896610476270687, gradient=0.0009948206676404284\n",
      "Gradient Descent(32/99): loss=0.38966095324925776, gradient=0.0009759467823106992\n",
      "Gradient Descent(33/99): loss=0.38966086220492707, gradient=0.0009583443757298836\n",
      "Gradient Descent(34/99): loss=0.38966077422606415, gradient=0.000941881632403753\n",
      "Gradient Descent(35/99): loss=0.3896606890762629, gradient=0.000926442281178515\n",
      "Gradient Descent(36/99): loss=0.38966060654632384, gradient=0.0009119236086445067\n",
      "Gradient Descent(37/99): loss=0.3896605264505396, gradient=0.0008982347475560903\n",
      "Gradient Descent(38/99): loss=0.38966044862354937, gradient=0.0008852951973130982\n",
      "Gradient Descent(39/99): loss=0.3896603729176636, gradient=0.0008730335417182044\n",
      "Gradient Descent(40/99): loss=0.3896602992005848, gradient=0.0008613863354326086\n",
      "Gradient Descent(41/99): loss=0.3896602273534582, gradient=0.0008502971353575259\n",
      "Gradient Descent(42/99): loss=0.38966015726920006, gradient=0.0008397156569537837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(43/99): loss=0.389660088851061, gradient=0.0008295970385530264\n",
      "Gradient Descent(44/99): loss=0.389660022011387, gradient=0.0008199011991935794\n",
      "Gradient Descent(45/99): loss=0.38965995667054865, gradient=0.0008105922775701878\n",
      "Gradient Descent(46/99): loss=0.3896598927560134, gradient=0.00080163814140886\n",
      "Gradient Descent(47/99): loss=0.3896598302015407, gradient=0.0007930099580398496\n",
      "Gradient Descent(48/99): loss=0.38965976894647975, gradient=0.0007846818181854023\n",
      "Gradient Descent(49/99): loss=0.38965970893515783, gradient=0.0007766304060498258\n",
      "Gradient Descent(50/99): loss=0.38965965011634485, gradient=0.0007688347097194717\n",
      "Gradient Descent(51/99): loss=0.3896595924427843, gradient=0.0007612757666762717\n",
      "Gradient Descent(52/99): loss=0.38965953587078095, gradient=0.0007539364399170719\n",
      "Gradient Descent(53/99): loss=0.389659480359838, gradient=0.0007468012207676436\n",
      "Gradient Descent(54/99): loss=0.3896594258723367, gradient=0.0007398560549963283\n",
      "Gradient Descent(55/99): loss=0.38965937237325415, gradient=0.0007330881892813389\n",
      "Gradient Descent(56/99): loss=0.389659319829912, gradient=0.0007264860354725996\n",
      "Gradient Descent(57/99): loss=0.38965926821175384, gradient=0.0007200390504266917\n",
      "Gradient Descent(58/99): loss=0.3896592174901487, gradient=0.000713737629483552\n",
      "Gradient Descent(59/99): loss=0.3896591676382136, gradient=0.000707573011907113\n",
      "Gradient Descent(60/99): loss=0.3896591186306579, gradient=0.0007015371968295861\n",
      "Gradient Descent(61/99): loss=0.3896590704436407, gradient=0.0006956228684289989\n",
      "Gradient Descent(62/99): loss=0.38965902305464784, gradient=0.0006898233292336114\n",
      "Gradient Descent(63/99): loss=0.3896589764423771, gradient=0.0006841324405875251\n",
      "Gradient Descent(64/99): loss=0.3896589305866377, gradient=0.0006785445694368681\n",
      "Gradient Descent(65/99): loss=0.38965888546826016, gradient=0.000673054540700181\n",
      "Gradient Descent(66/99): loss=0.3896588410690122, gradient=0.0006676575945807967\n",
      "Gradient Descent(67/99): loss=0.3896587973715268, gradient=0.000662349348258125\n",
      "Gradient Descent(68/99): loss=0.3896587543592343, gradient=0.0006571257614646471\n",
      "Gradient Descent(69/99): loss=0.38965871201630226, gradient=0.000651983105515988\n",
      "Gradient Descent(70/99): loss=0.3896586703275808, gradient=0.0006469179354133657\n",
      "Gradient Descent(71/99): loss=0.3896586292785524, gradient=0.0006419270646842574\n",
      "Gradient Descent(72/99): loss=0.3896585888552878, gradient=0.0006370075426663111\n",
      "Gradient Descent(73/99): loss=0.3896585490444038, gradient=0.0006321566339742983\n",
      "Gradient Descent(74/99): loss=0.3896585098330269, gradient=0.0006273717999207858\n",
      "Gradient Descent(75/99): loss=0.3896584712087591, gradient=0.0006226506816866804\n",
      "Gradient Descent(76/99): loss=0.389658433159646, gradient=0.0006179910850619392\n",
      "Gradient Descent(77/99): loss=0.3896583956741499, gradient=0.0006133909665966737\n",
      "Gradient Descent(78/99): loss=0.38965835874112237, gradient=0.0006088484210203511\n",
      "Gradient Descent(79/99): loss=0.38965832234978165, gradient=0.0006043616698032182\n",
      "Gradient Descent(80/99): loss=0.38965828648969036, gradient=0.0005999290507470498\n",
      "Gradient Descent(81/99): loss=0.38965825115073544, gradient=0.000595549008504985\n",
      "Gradient Descent(82/99): loss=0.38965821632311004, gradient=0.0005912200859406597\n",
      "Gradient Descent(83/99): loss=0.38965818199729596, gradient=0.0005869409162465122\n",
      "Gradient Descent(84/99): loss=0.3896581481640494, gradient=0.0005827102157493241\n",
      "Gradient Descent(85/99): loss=0.3896581148143856, gradient=0.0005785267773383013\n",
      "Gradient Descent(86/99): loss=0.38965808193956514, gradient=0.0005743894644583473\n",
      "Gradient Descent(87/99): loss=0.38965804953108313, gradient=0.0005702972056158058\n",
      "Gradient Descent(88/99): loss=0.3896580175806569, gradient=0.0005662489893502464\n",
      "Gradient Descent(89/99): loss=0.38965798608021585, gradient=0.0005622438596302438\n",
      "Gradient Descent(90/99): loss=0.3896579550218913, gradient=0.0005582809116342724\n",
      "Gradient Descent(91/99): loss=0.38965792439800817, gradient=0.0005543592878832696\n",
      "Gradient Descent(92/99): loss=0.3896578942010755, gradient=0.0005504781746932849\n",
      "Gradient Descent(93/99): loss=0.3896578644237802, gradient=0.0005466367989198058\n",
      "Gradient Descent(94/99): loss=0.3896578350589784, gradient=0.0005428344249690318\n",
      "Gradient Descent(95/99): loss=0.3896578060996889, gradient=0.0005390703520522399\n",
      "Gradient Descent(96/99): loss=0.3896577775390873, gradient=0.0005353439116625922\n",
      "Gradient Descent(97/99): loss=0.3896577493705, gradient=0.0005316544652556177\n",
      "Gradient Descent(98/99): loss=0.3896577215873983, gradient=0.0005280014021152754\n",
      "Gradient Descent(99/99): loss=0.3896576941833928, gradient=0.0005243841373908839\n",
      "Gradient Descent(0/99): loss=0.38896691342385625, gradient=0.008187284412118197\n",
      "Gradient Descent(1/99): loss=0.3889641692512029, gradient=0.005535057981054027\n",
      "Gradient Descent(2/99): loss=0.3889622642844417, gradient=0.004549146972200537\n",
      "Gradient Descent(3/99): loss=0.38896089305836284, gradient=0.0038463986929497132\n",
      "Gradient Descent(4/99): loss=0.38895988145740695, gradient=0.0032940814409062455\n",
      "Gradient Descent(5/99): loss=0.3889591155605075, gradient=0.0028573056057020737\n",
      "Gradient Descent(6/99): loss=0.38895851950811255, gradient=0.0025123978251275123\n",
      "Gradient Descent(7/99): loss=0.3889580424024121, gradient=0.002240358735371366\n",
      "Gradient Descent(8/99): loss=0.3889576498395288, gradient=0.0020257191377742213\n",
      "Gradient Descent(9/99): loss=0.3889573183736205, gradient=0.0018559281872083215\n",
      "Gradient Descent(10/99): loss=0.38895703188023756, gradient=0.0017208973249308128\n",
      "Gradient Descent(11/99): loss=0.3889567791565885, gradient=0.0016126210429843251\n",
      "Gradient Descent(12/99): loss=0.3889565523305467, gradient=0.001524837514925748\n",
      "Gradient Descent(13/99): loss=0.3889563458005602, gradient=0.001452716433705488\n",
      "Gradient Descent(14/99): loss=0.38895615552550444, gradient=0.0013925758727446134\n",
      "Gradient Descent(15/99): loss=0.38895597854620356, gradient=0.0013416344496501042\n",
      "Gradient Descent(16/99): loss=0.3889558126610134, gradient=0.0012978030925598937\n",
      "Gradient Descent(17/99): loss=0.3889556562043558, gradient=0.0012595165783976165\n",
      "Gradient Descent(18/99): loss=0.3889555078944041, gradient=0.0012256014783448748\n",
      "Gradient Descent(19/99): loss=0.38895536672747316, gradient=0.0011951751648955184\n",
      "Gradient Descent(20/99): loss=0.38895523190414644, gradient=0.0011675699782470591\n",
      "Gradient Descent(21/99): loss=0.38895510277710343, gradient=0.0011422770154437514\n",
      "Gradient Descent(22/99): loss=0.38895497881389646, gradient=0.0011189048141778879\n",
      "Gradient Descent(23/99): loss=0.3889548595700957, gradient=0.0010971491280084814\n",
      "Gradient Descent(24/99): loss=0.388954744669693, gradient=0.0010767708534558157\n",
      "Gradient Descent(25/99): loss=0.38895463379062767, gradient=0.0010575798979767204\n",
      "Gradient Descent(26/99): loss=0.3889545266539619, gradient=0.0010394233564365181\n",
      "Gradient Descent(27/99): loss=0.38895442301568095, gradient=0.0010221768058541346\n",
      "Gradient Descent(28/99): loss=0.3889543226604062, gradient=0.0010057378575570226\n",
      "Gradient Descent(29/99): loss=0.3889542253965131, gradient=0.0009900213470319622\n",
      "Gradient Descent(30/99): loss=0.38895413105229193, gradient=0.0009749557163043513\n",
      "Gradient Descent(31/99): loss=0.38895403947289503, gradient=0.0009604802690834629\n",
      "Gradient Descent(32/99): loss=0.3889539505178828, gradient=0.000946543068611951\n",
      "Gradient Descent(33/99): loss=0.38895386405922766, gradient=0.0009330993121862868\n",
      "Gradient Descent(34/99): loss=0.3889537799796767, gradient=0.0009201100620071294\n",
      "Gradient Descent(35/99): loss=0.3889536981713949, gradient=0.0009075412446643094\n",
      "Gradient Descent(36/99): loss=0.38895361853483174, gradient=0.0008953628549479548\n",
      "Gradient Descent(37/99): loss=0.38895354097776746, gradient=0.0008835483164903272\n",
      "Gradient Descent(38/99): loss=0.3889534654145016, gradient=0.0008720739638883288\n",
      "Gradient Descent(39/99): loss=0.3889533917651636, gradient=0.0008609186197784208\n",
      "Gradient Descent(40/99): loss=0.3889533199551151, gradient=0.0008500632467839693\n",
      "Gradient Descent(41/99): loss=0.3889532499144799, gradient=0.0008394901501405178\n",
      "Gradient Descent(42/99): loss=0.38895318157756603, gradient=0.0008291848112204264\n",
      "Gradient Descent(43/99): loss=0.3889531148826109, gradient=0.0008191325125123895\n",
      "Gradient Descent(44/99): loss=0.3889530497713899, gradient=0.0008093203251485491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(45/99): loss=0.38895298618891433, gradient=0.0007997364126518609\n",
      "Gradient Descent(46/99): loss=0.388952924083163, gradient=0.0007903699069295728\n",
      "Gradient Descent(47/99): loss=0.3889528634048434, gradient=0.000781210801806523\n",
      "Gradient Descent(48/99): loss=0.38895280410717986, gradient=0.0007722498611748211\n",
      "Gradient Descent(49/99): loss=0.38895274614572045, gradient=0.0007634785393862128\n",
      "Gradient Descent(50/99): loss=0.3889526894781684, gradient=0.0007548889119488685\n",
      "Gradient Descent(51/99): loss=0.3889526340642248, gradient=0.0007464736149372049\n",
      "Gradient Descent(52/99): loss=0.3889525798654513, gradient=0.0007382257918022791\n",
      "Gradient Descent(53/99): loss=0.38895252684514076, gradient=0.0007301390464953776\n",
      "Gradient Descent(54/99): loss=0.38895247496820445, gradient=0.0007222074019988367\n",
      "Gradient Descent(55/99): loss=0.38895242420106624, gradient=0.0007144252635087951\n",
      "Gradient Descent(56/99): loss=0.3889523745115663, gradient=0.0007067873856346125\n",
      "Gradient Descent(57/99): loss=0.38895232586887346, gradient=0.0006992888430811893\n",
      "Gradient Descent(58/99): loss=0.3889522782434058, gradient=0.0006919250043627788\n",
      "Gradient Descent(59/99): loss=0.3889522316067555, gradient=0.0006846915081655057\n",
      "Gradient Descent(60/99): loss=0.38895218593162195, gradient=0.0006775842420327551\n",
      "Gradient Descent(61/99): loss=0.3889521411917481, gradient=0.0006705993230951634\n",
      "Gradient Descent(62/99): loss=0.38895209736186337, gradient=0.000663733080606778\n",
      "Gradient Descent(63/99): loss=0.38895205441762953, gradient=0.0006569820400820183\n",
      "Gradient Descent(64/99): loss=0.3889520123355915, gradient=0.0006503429088565907\n",
      "Gradient Descent(65/99): loss=0.3889519710931309, gradient=0.0006438125629183659\n",
      "Gradient Descent(66/99): loss=0.3889519306684239, gradient=0.0006373880348752676\n",
      "Gradient Descent(67/99): loss=0.388951891040401, gradient=0.0006310665029433993\n",
      "Gradient Descent(68/99): loss=0.38895185218871026, gradient=0.000624845280853257\n",
      "Gradient Descent(69/99): loss=0.3889518140936821, gradient=0.0006187218085846227\n",
      "Gradient Descent(70/99): loss=0.3889517767362981, gradient=0.0006126936438506732\n",
      "Gradient Descent(71/99): loss=0.38895174009816025, gradient=0.0006067584542617819\n",
      "Gradient Descent(72/99): loss=0.38895170416146246, gradient=0.000600914010106013\n",
      "Gradient Descent(73/99): loss=0.38895166890896465, gradient=0.0005951581776919688\n",
      "Gradient Descent(74/99): loss=0.38895163432396773, gradient=0.000589488913203416\n",
      "Gradient Descent(75/99): loss=0.3889516003902904, gradient=0.0005839042570219842\n",
      "Gradient Descent(76/99): loss=0.38895156709224693, gradient=0.000578402328478032\n",
      "Gradient Descent(77/99): loss=0.3889515344146266, gradient=0.0005729813209931276\n",
      "Gradient Descent(78/99): loss=0.3889515023426747, gradient=0.0005676394975825508\n",
      "Gradient Descent(79/99): loss=0.38895147086207327, gradient=0.0005623751866877323\n",
      "Gradient Descent(80/99): loss=0.3889514399589251, gradient=0.0005571867783121372\n",
      "Gradient Descent(81/99): loss=0.38895140961973657, gradient=0.0005520727204364346\n",
      "Gradient Descent(82/99): loss=0.38895137983140166, gradient=0.0005470315156908227\n",
      "Gradient Descent(83/99): loss=0.3889513505811885, gradient=0.000542061718264085\n",
      "Gradient Descent(84/99): loss=0.38895132185672443, gradient=0.000537161931031521\n",
      "Gradient Descent(85/99): loss=0.38895129364598363, gradient=0.00053233080288383\n",
      "Gradient Descent(86/99): loss=0.388951265937274, gradient=0.0005275670262430717\n",
      "Gradient Descent(87/99): loss=0.3889512387192256, gradient=0.0005228693347496746\n",
      "Gradient Descent(88/99): loss=0.3889512119807789, gradient=0.0005182365011089899\n",
      "Gradient Descent(89/99): loss=0.3889511857111748, gradient=0.0005136673350846768\n",
      "Gradient Descent(90/99): loss=0.3889511598999436, gradient=0.0005091606816278302\n",
      "Gradient Descent(91/99): loss=0.38895113453689495, gradient=0.0005047154191322339\n",
      "Gradient Descent(92/99): loss=0.3889511096121101, gradient=0.0005003304578057396\n",
      "Gradient Descent(93/99): loss=0.388951085115931, gradient=0.000496004738149703\n",
      "Gradient Descent(94/99): loss=0.3889510610389527, gradient=0.0004917372295382431\n",
      "Gradient Descent(95/99): loss=0.3889510373720158, gradient=0.00048752692889003174\n",
      "Gradient Descent(96/99): loss=0.38895101410619715, gradient=0.00048337285942583777\n",
      "Gradient Descent(97/99): loss=0.38895099123280336, gradient=0.00047927406950554625\n",
      "Gradient Descent(98/99): loss=0.3889509687433635, gradient=0.00047522963153909774\n",
      "Gradient Descent(99/99): loss=0.38895094662962243, gradient=0.000471238640965565\n",
      "Gradient Descent(0/99): loss=0.39015414211270016, gradient=0.015423078132153227\n",
      "Gradient Descent(1/99): loss=0.3901502025104323, gradient=0.00693192306153774\n",
      "Gradient Descent(2/99): loss=0.390147534477477, gradient=0.005374211156964259\n",
      "Gradient Descent(3/99): loss=0.3901455231742433, gradient=0.0046319970655422\n",
      "Gradient Descent(4/99): loss=0.39014396321574907, gradient=0.004067504566624683\n",
      "Gradient Descent(5/99): loss=0.39014272640145997, gradient=0.003612715631787949\n",
      "Gradient Descent(6/99): loss=0.3901417253263871, gradient=0.0032425388112962703\n",
      "Gradient Descent(7/99): loss=0.39014089919813727, gradient=0.0029390827286119867\n",
      "Gradient Descent(8/99): loss=0.39014020510934294, gradient=0.0026884778712123543\n",
      "Gradient Descent(9/99): loss=0.39013961233646544, gradient=0.0024798947793168483\n",
      "Gradient Descent(10/99): loss=0.39013909856074563, gradient=0.0023048669337158295\n",
      "Gradient Descent(11/99): loss=0.39013864733513526, gradient=0.002156766712011037\n",
      "Gradient Descent(12/99): loss=0.3901382463673815, gradient=0.0020303961244921924\n",
      "Gradient Descent(13/99): loss=0.39013788634304486, gradient=0.0019216675321098112\n",
      "Gradient Descent(14/99): loss=0.3901375601093451, gradient=0.0018273552386495705\n",
      "Gradient Descent(15/99): loss=0.39013726210263894, gradient=0.0017449028266285917\n",
      "Gradient Descent(16/99): loss=0.3901369879421257, gradient=0.0016722741620290988\n",
      "Gradient Descent(17/99): loss=0.3901367341381638, gradient=0.0016078384313950795\n",
      "Gradient Descent(18/99): loss=0.39013649788045285, gradient=0.001550281558347463\n",
      "Gradient Descent(19/99): loss=0.39013627688245417, gradient=0.001498537967103699\n",
      "Gradient Descent(20/99): loss=0.3901360692658402, gradient=0.0014517379772099165\n",
      "Gradient Descent(21/99): loss=0.3901358734737254, gradient=0.0014091671721014347\n",
      "Gradient Descent(22/99): loss=0.3901356882048172, gradient=0.0013702349245949025\n",
      "Gradient Descent(23/99): loss=0.3901355123629258, gradient=0.0013344499219705363\n",
      "Gradient Descent(24/99): loss=0.3901353450178625, gradient=0.0013014010454451687\n",
      "Gradient Descent(25/99): loss=0.390135185374875, gradient=0.0012707423530100878\n",
      "Gradient Descent(26/99): loss=0.3901350327505371, gradient=0.0012421812158382012\n",
      "Gradient Descent(27/99): loss=0.3901348865535672, gradient=0.001215468887418588\n",
      "Gradient Descent(28/99): loss=0.3901347462694484, gradient=0.001190392957911678\n",
      "Gradient Descent(29/99): loss=0.3901346114480019, gradient=0.001166771277111316\n",
      "Gradient Descent(30/99): loss=0.3901344816932773, gradient=0.001144447028111671\n",
      "Gradient Descent(31/99): loss=0.3901343566552744, gradient=0.001123284708206396\n",
      "Gradient Descent(32/99): loss=0.39013423602312236, gradient=0.001103166829714989\n",
      "Gradient Descent(33/99): loss=0.390134119519429, gradient=0.0010839911958982534\n",
      "Gradient Descent(34/99): loss=0.3901340068955754, gradient=0.0010656686393185975\n",
      "Gradient Descent(35/99): loss=0.390133897927777, gradient=0.0010481211344972189\n",
      "Gradient Descent(36/99): loss=0.3901337924137733, gradient=0.001031280215440865\n",
      "Gradient Descent(37/99): loss=0.3901336901700309, gradient=0.0010150856429885443\n",
      "Gradient Descent(38/99): loss=0.3901335910293744, gradient=0.0009994842780369163\n",
      "Gradient Descent(39/99): loss=0.39013349483896886, gradient=0.0009844291253340075\n",
      "Gradient Descent(40/99): loss=0.39013340145859815, gradient=0.0009698785192849823\n",
      "Gradient Descent(41/99): loss=0.3901333107591887, gradient=0.0009557954285302257\n",
      "Gradient Descent(42/99): loss=0.3901332226215398, gradient=0.000942146860273372\n",
      "Gradient Descent(43/99): loss=0.39013313693522955, gradient=0.0009289033487022896\n",
      "Gradient Descent(44/99): loss=0.3901330535976672, gradient=0.0009160385145502089\n",
      "Gradient Descent(45/99): loss=0.3901329725132719, gradient=0.0009035286850315772\n",
      "Gradient Descent(46/99): loss=0.3901328935927565, gradient=0.0008913525651655413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(47/99): loss=0.3901328167525038, gradient=0.0008794909529564979\n",
      "Gradient Descent(48/99): loss=0.3901327419140198, gradient=0.0008679264920972542\n",
      "Gradient Descent(49/99): loss=0.39013266900345506, gradient=0.000856643456850178\n",
      "Gradient Descent(50/99): loss=0.39013259795118127, gradient=0.0008456275645831428\n",
      "Gradient Descent(51/99): loss=0.39013252869141957, gradient=0.0008348658121212719\n",
      "Gradient Descent(52/99): loss=0.39013246116191147, gradient=0.0008243463326480109\n",
      "Gradient Descent(53/99): loss=0.39013239530362614, gradient=0.0008140582703702615\n",
      "Gradient Descent(54/99): loss=0.3901323310605007, gradient=0.0008039916705668281\n",
      "Gradient Descent(55/99): loss=0.3901322683792083, gradient=0.0007941373829807162\n",
      "Gradient Descent(56/99): loss=0.39013220720895164, gradient=0.000784486976805908\n",
      "Gradient Descent(57/99): loss=0.39013214750127667, gradient=0.0007750326657639423\n",
      "Gradient Descent(58/99): loss=0.39013208920990616, gradient=0.0007657672419749257\n",
      "Gradient Descent(59/99): loss=0.3901320322905893, gradient=0.0007566840175052027\n",
      "Gradient Descent(60/99): loss=0.3901319767009667, gradient=0.0007477767726257042\n",
      "Gradient Descent(61/99): loss=0.3901319224004467, gradient=0.000739039709945555\n",
      "Gradient Descent(62/99): loss=0.39013186935009425, gradient=0.0007304674136959107\n",
      "Gradient Descent(63/99): loss=0.3901318175125308, gradient=0.0007220548135352126\n",
      "Gradient Descent(64/99): loss=0.39013176685183976, gradient=0.0007137971523293456\n",
      "Gradient Descent(65/99): loss=0.3901317173334842, gradient=0.0007056899574303614\n",
      "Gradient Descent(66/99): loss=0.390131668924228, gradient=0.0006977290150389076\n",
      "Gradient Descent(67/99): loss=0.3901316215920659, gradient=0.0006899103472880518\n",
      "Gradient Descent(68/99): loss=0.3901315753061577, gradient=0.0006822301917319177\n",
      "Gradient Descent(69/99): loss=0.3901315300367677, gradient=0.0006746849829620299\n",
      "Gradient Descent(70/99): loss=0.3901314857552102, gradient=0.0006672713361080984\n",
      "Gradient Descent(71/99): loss=0.39013144243379677, gradient=0.0006599860320106318\n",
      "Gradient Descent(72/99): loss=0.3901314000457889, gradient=0.0006528260038776932\n",
      "Gradient Descent(73/99): loss=0.39013135856535347, gradient=0.0006457883252615263\n",
      "Gradient Descent(74/99): loss=0.3901313179675221, gradient=0.0006388701992094197\n",
      "Gradient Descent(75/99): loss=0.3901312782281501, gradient=0.0006320689484617259\n",
      "Gradient Descent(76/99): loss=0.39013123932388416, gradient=0.0006253820065833707\n",
      "Gradient Descent(77/99): loss=0.3901312012321242, gradient=0.0006188069099297884\n",
      "Gradient Descent(78/99): loss=0.3901311639309953, gradient=0.00061234129035856\n",
      "Gradient Descent(79/99): loss=0.3901311273993155, gradient=0.0006059828686091385\n",
      "Gradient Descent(80/99): loss=0.3901310916165684, gradient=0.0005997294482808366\n",
      "Gradient Descent(81/99): loss=0.39013105656287694, gradient=0.000593578910348082\n",
      "Gradient Descent(82/99): loss=0.39013102221897805, gradient=0.0005875292081581963\n",
      "Gradient Descent(83/99): loss=0.3901309885661992, gradient=0.0005815783628629843\n",
      "Gradient Descent(84/99): loss=0.39013095558643557, gradient=0.0005757244592413735\n",
      "Gradient Descent(85/99): loss=0.3901309232621298, gradient=0.0005699656418738496\n",
      "Gradient Descent(86/99): loss=0.39013089157625047, gradient=0.0005643001116352592\n",
      "Gradient Descent(87/99): loss=0.39013086051227436, gradient=0.0005587261224742665\n",
      "Gradient Descent(88/99): loss=0.39013083005416715, gradient=0.0005532419784532867\n",
      "Gradient Descent(89/99): loss=0.39013080018636687, gradient=0.0005478460310229639\n",
      "Gradient Descent(90/99): loss=0.39013077089376696, gradient=0.0005425366765106703\n",
      "Gradient Descent(91/99): loss=0.3901307421617005, gradient=0.0005373123538019382\n",
      "Gradient Descent(92/99): loss=0.3901307139759255, gradient=0.0005321715421984613\n",
      "Gradient Descent(93/99): loss=0.3901306863226103, gradient=0.000527112759435653\n",
      "Gradient Descent(94/99): loss=0.39013065918831974, gradient=0.000522134559846368\n",
      "Gradient Descent(95/99): loss=0.3901306325600019, gradient=0.0005172355326574726\n",
      "Gradient Descent(96/99): loss=0.3901306064249761, gradient=0.000512414300407604\n",
      "Gradient Descent(97/99): loss=0.3901305807709196, gradient=0.0005076695174761882\n",
      "Gradient Descent(98/99): loss=0.39013055558585785, gradient=0.0005029998687138141\n",
      "Gradient Descent(99/99): loss=0.39013053085815114, gradient=0.0004984040681654631\n",
      "Gradient Descent(0/99): loss=0.3903650435347366, gradient=0.007270301635794711\n",
      "Gradient Descent(1/99): loss=0.39036136393540577, gradient=0.006258149745030896\n",
      "Gradient Descent(2/99): loss=0.39035847201122276, gradient=0.005528029289739706\n",
      "Gradient Descent(3/99): loss=0.3903561395406072, gradient=0.004950406176493569\n",
      "Gradient Descent(4/99): loss=0.39035421506474166, gradient=0.004485267854872066\n",
      "Gradient Descent(5/99): loss=0.3903525951533525, gradient=0.004105928977674315\n",
      "Gradient Descent(6/99): loss=0.3903512077208577, gradient=0.003792566602166146\n",
      "Gradient Descent(7/99): loss=0.3903500015142874, gradient=0.003530337247292581\n",
      "Gradient Descent(8/99): loss=0.3903489393667036, gradient=0.003308100394581294\n",
      "Gradient Descent(9/99): loss=0.39034799380351676, gradient=0.003117464296895738\n",
      "Gradient Descent(10/99): loss=0.39034714414156385, gradient=0.002952059935967502\n",
      "Gradient Descent(11/99): loss=0.3903463745399265, gradient=0.002807018883423766\n",
      "Gradient Descent(12/99): loss=0.3903456726708277, gradient=0.002678582368009073\n",
      "Gradient Descent(13/99): loss=0.3903450287964773, gradient=0.002563823661554341\n",
      "Gradient Descent(14/99): loss=0.39034443511913874, gradient=0.002460437661191432\n",
      "Gradient Descent(15/99): loss=0.3903438853146293, gradient=0.0023665941174532655\n",
      "Gradient Descent(16/99): loss=0.3903433741930222, gradient=0.002280826418940197\n",
      "Gradient Descent(17/99): loss=0.3903428974485535, gradient=0.002201947940285216\n",
      "Gradient Descent(18/99): loss=0.39034245147113017, gradient=0.002128997703231383\n",
      "Gradient Descent(19/99): loss=0.3903420332056647, gradient=0.002061183153424536\n",
      "Gradient Descent(20/99): loss=0.3903416400430249, gradient=0.0019978518574801773\n",
      "Gradient Descent(21/99): loss=0.39034126973599803, gradient=0.001938462160413967\n",
      "Gradient Descent(22/99): loss=0.39034092033366413, gradient=0.0018825617374212827\n",
      "Gradient Descent(23/99): loss=0.39034059012976086, gradient=0.0018297703616059434\n",
      "Gradient Descent(24/99): loss=0.39034027762143214, gradient=0.0017797680266670008\n",
      "Gradient Descent(25/99): loss=0.3903399814766324, gradient=0.0017322817032918373\n",
      "Gradient Descent(26/99): loss=0.39033970050744127, gradient=0.0016870793736755486\n",
      "Gradient Descent(27/99): loss=0.39033943364870416, gradient=0.0016439612173042171\n",
      "Gradient Descent(28/99): loss=0.3903391799404271, gradient=0.0016027550155056551\n",
      "Gradient Descent(29/99): loss=0.3903389385133366, gradient=0.001563311339337555\n",
      "Gradient Descent(30/99): loss=0.3903387085769296, gradient=0.001525499826012853\n",
      "Gradient Descent(31/99): loss=0.39033848940952076, gradient=0.0014892061253217403\n",
      "Gradient Descent(32/99): loss=0.39033828034990314, gradient=0.0014543293869221897\n",
      "Gradient Descent(33/99): loss=0.39033808079031956, gradient=0.0014207801859201518\n",
      "Gradient Descent(34/99): loss=0.3903378901704998, gradient=0.001388478804854565\n",
      "Gradient Descent(35/99): loss=0.39033770797257755, gradient=0.001357353806440891\n",
      "Gradient Descent(36/99): loss=0.3903375337167266, gradient=0.001327340844244035\n",
      "Gradient Descent(37/99): loss=0.3903373669573984, gradient=0.001298381668620163\n",
      "Gradient Descent(38/99): loss=0.39033720728005744, gradient=0.0012704232933690917\n",
      "Gradient Descent(39/99): loss=0.3903370542983338, gradient=0.0012434172950198136\n",
      "Gradient Descent(40/99): loss=0.39033690765152756, gradient=0.0012173192218742248\n",
      "Gradient Descent(41/99): loss=0.39033676700241, gradient=0.001192088094123313\n",
      "Gradient Descent(42/99): loss=0.3903366320352779, gradient=0.0011676859797317878\n",
      "Gradient Descent(43/99): loss=0.39033650245422197, gradient=0.0011440776335272008\n",
      "Gradient Descent(44/99): loss=0.3903363779815815, gradient=0.0011212301891510858\n",
      "Gradient Descent(45/99): loss=0.3903362583565586, gradient=0.0010991128953397845\n",
      "Gradient Descent(46/99): loss=0.39033614333396927, gradient=0.00107769688947724\n",
      "Gradient Descent(47/99): loss=0.390336032683116, gradient=0.0010569550025679693\n",
      "Gradient Descent(48/99): loss=0.39033592618676516, gradient=0.0010368615907662088\n",
      "Gradient Descent(49/99): loss=0.39033582364021685, gradient=0.0010173923894092687\n",
      "Gradient Descent(50/99): loss=0.39033572485045576, gradient=0.0009985243861692606\n",
      "Gradient Descent(51/99): loss=0.39033562963537405, gradient=0.0009802357104892099\n",
      "Gradient Descent(52/99): loss=0.39033553782306013, gradient=0.0009625055369239789\n",
      "Gradient Descent(53/99): loss=0.3903354492511415, gradient=0.0009453140003833534\n",
      "Gradient Descent(54/99): loss=0.3903353637661826, gradient=0.0009286421215883969\n",
      "Gradient Descent(55/99): loss=0.3903352812231252, gradient=0.0009124717413115959\n",
      "Gradient Descent(56/99): loss=0.39033520148477335, gradient=0.0008967854621895362\n",
      "Gradient Descent(57/99): loss=0.3903351244213143, gradient=0.0008815665970785567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(58/99): loss=0.3903350499098741, gradient=0.000866799123075178\n",
      "Gradient Descent(59/99): loss=0.39033497783410404, gradient=0.0008524676404521958\n",
      "Gradient Descent(60/99): loss=0.3903349080837961, gradient=0.0008385573358682702\n",
      "Gradient Descent(61/99): loss=0.390334840554524, gradient=0.0008250539493003995\n",
      "Gradient Descent(62/99): loss=0.3903347751473081, gradient=0.0008119437442250819\n",
      "Gradient Descent(63/99): loss=0.390334711768302, gradient=0.0007992134806399568\n",
      "Gradient Descent(64/99): loss=0.3903346503285, gradient=0.0007868503905724407\n",
      "Gradient Descent(65/99): loss=0.3903345907434628, gradient=0.0007748421557703849\n",
      "Gradient Descent(66/99): loss=0.39033453293305964, gradient=0.0007631768873093212\n",
      "Gradient Descent(67/99): loss=0.39033447682122924, gradient=0.0007518431068860664\n",
      "Gradient Descent(68/99): loss=0.3903344223357502, gradient=0.0007408297295983006\n",
      "Gradient Descent(69/99): loss=0.3903343694080321, gradient=0.0007301260480354922\n",
      "Gradient Descent(70/99): loss=0.3903343179729133, gradient=0.0007197217175283403\n",
      "Gradient Descent(71/99): loss=0.3903342679684743, gradient=0.0007096067424241246\n",
      "Gradient Descent(72/99): loss=0.3903342193358607, gradient=0.0006997714632707593\n",
      "Gradient Descent(73/99): loss=0.39033417201911674, gradient=0.0006902065448077617\n",
      "Gradient Descent(74/99): loss=0.39033412596502914, gradient=0.0006809029646751741\n",
      "Gradient Descent(75/99): loss=0.39033408112297935, gradient=0.0006718520027613476\n",
      "Gradient Descent(76/99): loss=0.39033403744480466, gradient=0.0006630452311217397\n",
      "Gradient Descent(77/99): loss=0.39033399488466686, gradient=0.0006544745044080581\n",
      "Gradient Descent(78/99): loss=0.3903339533989293, gradient=0.0006461319507552934\n",
      "Gradient Descent(79/99): loss=0.3903339129460392, gradient=0.0006380099630804917\n",
      "Gradient Descent(80/99): loss=0.3903338734864187, gradient=0.0006301011907525422\n",
      "Gradient Descent(81/99): loss=0.3903338349823603, gradient=0.0006223985315982014\n",
      "Gradient Descent(82/99): loss=0.39033379739792956, gradient=0.0006148951242127979\n",
      "Gradient Descent(83/99): loss=0.3903337606988714, gradient=0.0006075843405493455\n",
      "Gradient Descent(84/99): loss=0.3903337248525235, gradient=0.0006004597787620712\n",
      "Gradient Descent(85/99): loss=0.39033368982773337, gradient=0.0005935152562840776\n",
      "Gradient Descent(86/99): loss=0.39033365559478067, gradient=0.0005867448031214439\n",
      "Gradient Descent(87/99): loss=0.3903336221253023, gradient=0.0005801426553483355\n",
      "Gradient Descent(88/99): loss=0.39033358939222407, gradient=0.0005737032487897019\n",
      "Gradient Descent(89/99): loss=0.39033355736969355, gradient=0.0005674212128800979\n",
      "Gradient Descent(90/99): loss=0.3903335260330182, gradient=0.0005612913646887486\n",
      "Gradient Descent(91/99): loss=0.3903334953586065, gradient=0.0005553087031022015\n",
      "Gradient Descent(92/99): loss=0.390333465323912, gradient=0.0005494684031570735\n",
      "Gradient Descent(93/99): loss=0.3903334359073798, gradient=0.0005437658105170522\n",
      "Gradient Descent(94/99): loss=0.390333407088398, gradient=0.0005381964360879368\n",
      "Gradient Descent(95/99): loss=0.3903333788472495, gradient=0.0005327559507669533\n",
      "Gradient Descent(96/99): loss=0.3903333511650666, gradient=0.0005274401803213044\n",
      "Gradient Descent(97/99): loss=0.3903333240237908, gradient=0.0005222451003935986\n",
      "Gradient Descent(98/99): loss=0.3903332974061302, gradient=0.0005171668316302897\n",
      "Gradient Descent(99/99): loss=0.39033327129552303, gradient=0.0005122016349306666\n",
      "Gradient Descent(0/99): loss=0.38998524907073423, gradient=0.010005451850020842\n",
      "Gradient Descent(1/99): loss=0.38997901604574176, gradient=0.00824342168356419\n",
      "Gradient Descent(2/99): loss=0.3899746024840301, gradient=0.006909147250720557\n",
      "Gradient Descent(3/99): loss=0.389971365093283, gradient=0.005893053973147566\n",
      "Gradient Descent(4/99): loss=0.3899689046425003, gradient=0.005116635838122739\n",
      "Gradient Descent(5/99): loss=0.389966970239418, gradient=0.004519553088902733\n",
      "Gradient Descent(6/99): loss=0.3899654019065727, gradient=0.004055661451981578\n",
      "Gradient Descent(7/99): loss=0.38996409587612413, gradient=0.003690177394872716\n",
      "Gradient Descent(8/99): loss=0.38996298349345787, gradient=0.003397308757256438\n",
      "Gradient Descent(9/99): loss=0.38996201831220745, gradient=0.00315820749701322\n",
      "Gradient Descent(10/99): loss=0.3899611681382799, gradient=0.002959247843807357\n",
      "Gradient Descent(11/99): loss=0.3899604100791704, gradient=0.0027906353209930074\n",
      "Gradient Descent(12/99): loss=0.3899597274291976, gradient=0.002645331162690516\n",
      "Gradient Descent(13/99): loss=0.38995910768436537, gradient=0.0025182518540847305\n",
      "Gradient Descent(14/99): loss=0.3899585412582392, gradient=0.0024056910222500804\n",
      "Gradient Descent(15/99): loss=0.3899580206372469, gradient=0.002304910787150298\n",
      "Gradient Descent(16/99): loss=0.38995753981468556, gradient=0.0022138568540291644\n",
      "Gradient Descent(17/99): loss=0.38995709390389277, gradient=0.00213096129268663\n",
      "Gradient Descent(18/99): loss=0.38995667886835744, gradient=0.002055006222711264\n",
      "Gradient Descent(19/99): loss=0.3899562913294406, gradient=0.001985029288578505\n",
      "Gradient Descent(20/99): loss=0.38995592842671684, gradient=0.0019202566942381073\n",
      "Gradient Descent(21/99): loss=0.38995558771372724, gradient=0.0018600604455304166\n",
      "Gradient Descent(22/99): loss=0.38995526708020517, gradient=0.0018039191018757795\n",
      "Gradient Descent(23/99): loss=0.3899549646916756, gradient=0.0017513976156349703\n",
      "Gradient Descent(24/99): loss=0.38995467894249314, gradient=0.001702128436181839\n",
      "Gradient Descent(25/99): loss=0.3899544084187014, gradient=0.0016557981178555413\n",
      "Gradient Descent(26/99): loss=0.38995415186840005, gradient=0.0016121364739951777\n",
      "Gradient Descent(27/99): loss=0.3899539081773722, gradient=0.001570911269375195\n",
      "Gradient Descent(28/99): loss=0.38995367634974165, gradient=0.0015319180957226932\n",
      "Gradient Descent(29/99): loss=0.38995345549187904, gradient=0.0014949765559903864\n",
      "Gradient Descent(30/99): loss=0.3899532447982434, gradient=0.0014599318200828244\n",
      "Gradient Descent(31/99): loss=0.3899530435408181, gradient=0.001426642104121418\n",
      "Gradient Descent(32/99): loss=0.3899528510593727, gradient=0.0013949822774782783\n",
      "Gradient Descent(33/99): loss=0.38995266675341606, gradient=0.0013648401319513287\n",
      "Gradient Descent(34/99): loss=0.38995249007533284, gradient=0.0013361145846081987\n",
      "Gradient Descent(35/99): loss=0.38995232052450035, gradient=0.001308714177991328\n",
      "Gradient Descent(36/99): loss=0.3899521576422191, gradient=0.0012825558220818415\n",
      "Gradient Descent(37/99): loss=0.3899520010073225, gradient=0.001257563734176435\n",
      "Gradient Descent(38/99): loss=0.38995185023236, gradient=0.0012336685417545226\n",
      "Gradient Descent(39/99): loss=0.38995170496025855, gradient=0.0012108065202787593\n",
      "Gradient Descent(40/99): loss=0.3899515648613968, gradient=0.0011889189432151045\n",
      "Gradient Descent(41/99): loss=0.3899514296310255, gradient=0.0011679515257641436\n",
      "Gradient Descent(42/99): loss=0.38995129898698816, gradient=0.0011478539471337305\n",
      "Gradient Descent(43/99): loss=0.3899511726676971, gradient=0.0011285794388580512\n",
      "Gradient Descent(44/99): loss=0.3899510504303316, gradient=0.0011100844288237962\n",
      "Gradient Descent(45/99): loss=0.38995093204923004, gradient=0.0010923282324161478\n",
      "Gradient Descent(46/99): loss=0.3899508173144461, gradient=0.001075272783626011\n",
      "Gradient Descent(47/99): loss=0.3899507060304551, gradient=0.0010588824001324193\n",
      "Gradient Descent(48/99): loss=0.3899505980149847, gradient=0.001043123577340157\n",
      "Gradient Descent(49/99): loss=0.3899504930979624, gradient=0.0010279648071521176\n",
      "Gradient Descent(50/99): loss=0.38995039112055985, gradient=0.0010133764179195773\n",
      "Gradient Descent(51/99): loss=0.38995029193432607, gradient=0.000999330432565923\n",
      "Gradient Descent(52/99): loss=0.38995019540040105, gradient=0.0009858004423418355\n",
      "Gradient Descent(53/99): loss=0.38995010138879643, gradient=0.0009727614940562315\n",
      "Gradient Descent(54/99): loss=0.38995000977774136, gradient=0.00096018998895255\n",
      "Gradient Descent(55/99): loss=0.38994992045308235, gradient=0.000948063591673452\n",
      "Gradient Descent(56/99): loss=0.38994983330773486, gradient=0.0009363611479876099\n",
      "Gradient Descent(57/99): loss=0.38994974824117984, gradient=0.0009250626101470179\n",
      "Gradient Descent(58/99): loss=0.389949665159001, gradient=0.0009141489689086917\n",
      "Gradient Descent(59/99): loss=0.3899495839724589, gradient=0.000903602191394355\n",
      "Gradient Descent(60/99): loss=0.3899495045981004, gradient=0.000893405164080196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(61/99): loss=0.3899494269573956, gradient=0.000883541640310325\n",
      "Gradient Descent(62/99): loss=0.38994935097640543, gradient=0.0008739961918126483\n",
      "Gradient Descent(63/99): loss=0.389949276585473, gradient=0.0008647541637697691\n",
      "Gradient Descent(64/99): loss=0.38994920371893843, gradient=0.0008558016330592455\n",
      "Gradient Descent(65/99): loss=0.3899491323148749, gradient=0.0008471253693309632\n",
      "Gradient Descent(66/99): loss=0.3899490623148446, gradient=0.0008387127986346766\n",
      "Gradient Descent(67/99): loss=0.3899489936636708, gradient=0.0008305519693490516\n",
      "Gradient Descent(68/99): loss=0.3899489263092284, gradient=0.0008226315201971157\n",
      "Gradient Descent(69/99): loss=0.38994886020224784, gradient=0.0008149406501608795\n",
      "Gradient Descent(70/99): loss=0.3899487952961333, gradient=0.0008074690901315821\n",
      "Gradient Descent(71/99): loss=0.3899487315467949, gradient=0.0008002070761535807\n",
      "Gradient Descent(72/99): loss=0.38994866891248947, gradient=0.0007931453241361055\n",
      "Gradient Descent(73/99): loss=0.3899486073536762, gradient=0.0007862750059232185\n",
      "Gradient Descent(74/99): loss=0.3899485468328791, gradient=0.0007795877266244264\n",
      "Gradient Descent(75/99): loss=0.38994848731456055, gradient=0.0007730755031197953\n",
      "Gradient Descent(76/99): loss=0.38994842876500213, gradient=0.000766730743662547\n",
      "Gradient Descent(77/99): loss=0.3899483711521945, gradient=0.0007605462285100369\n",
      "Gradient Descent(78/99): loss=0.38994831444573413, gradient=0.0007545150915212519\n",
      "Gradient Descent(79/99): loss=0.3899482586167273, gradient=0.0007486308026644881\n",
      "Gradient Descent(80/99): loss=0.38994820363769916, gradient=0.0007428871513845699\n",
      "Gradient Descent(81/99): loss=0.389948149482511, gradient=0.0007372782307823393\n",
      "Gradient Descent(82/99): loss=0.38994809612627984, gradient=0.0007317984225641103\n",
      "Gradient Descent(83/99): loss=0.389948043545307, gradient=0.0007264423827210714\n",
      "Gradient Descent(84/99): loss=0.3899479917170071, gradient=0.0007212050279024665\n",
      "Gradient Descent(85/99): loss=0.3899479406198454, gradient=0.0007160815224477847\n",
      "Gradient Descent(86/99): loss=0.3899478902332762, gradient=0.0007110672660465741\n",
      "Gradient Descent(87/99): loss=0.3899478405376873, gradient=0.0007061578819954226\n",
      "Gradient Descent(88/99): loss=0.38994779151434694, gradient=0.0007013492060242123\n",
      "Gradient Descent(89/99): loss=0.38994774314535363, gradient=0.0006966372756647236\n",
      "Gradient Descent(90/99): loss=0.38994769541358987, gradient=0.0006920183201364243\n",
      "Gradient Descent(91/99): loss=0.3899476483026794, gradient=0.0006874887507255425\n",
      "Gradient Descent(92/99): loss=0.3899476017969453, gradient=0.0006830451516343869\n",
      "Gradient Descent(93/99): loss=0.3899475558813716, gradient=0.000678684271279498\n",
      "Gradient Descent(94/99): loss=0.3899475105415678, gradient=0.0006744030140174402\n",
      "Gradient Descent(95/99): loss=0.3899474657637349, gradient=0.0006701984322791687\n",
      "Gradient Descent(96/99): loss=0.38994742153463385, gradient=0.0006660677190930723\n",
      "Gradient Descent(97/99): loss=0.38994737784155475, gradient=0.0006620082009796154\n",
      "Gradient Descent(98/99): loss=0.38994733467228987, gradient=0.0006580173311995792\n",
      "Gradient Descent(99/99): loss=0.38994729201510697, gradient=0.0006540926833394741\n",
      "Gradient Descent(0/99): loss=0.38967941132461076, gradient=0.011119352609102164\n",
      "Gradient Descent(1/99): loss=0.3896744194780854, gradient=0.007452720003215036\n",
      "Gradient Descent(2/99): loss=0.3896708817928495, gradient=0.006184552030522768\n",
      "Gradient Descent(3/99): loss=0.38966828511038754, gradient=0.005282334571011992\n",
      "Gradient Descent(4/99): loss=0.38966634040559034, gradient=0.004560937338227907\n",
      "Gradient Descent(5/99): loss=0.3896648557330307, gradient=0.003976263659802204\n",
      "Gradient Descent(6/99): loss=0.3896637006431167, gradient=0.0034996181983284152\n",
      "Gradient Descent(7/99): loss=0.3896627854298224, gradient=0.0031086064637366154\n",
      "Gradient Descent(8/99): loss=0.38966204766744444, gradient=0.002785537216879221\n",
      "Gradient Descent(9/99): loss=0.38966144335396175, gradient=0.0025164723587152975\n",
      "Gradient Descent(10/99): loss=0.3896609410456408, gradient=0.002290472128990481\n",
      "Gradient Descent(11/99): loss=0.3896605179466911, gradient=0.0020989772337398783\n",
      "Gradient Descent(12/99): loss=0.38966015728321324, gradient=0.0019353037122821135\n",
      "Gradient Descent(13/99): loss=0.3896598465253493, gradient=0.0017942333698352303\n",
      "Gradient Descent(14/99): loss=0.3896595761729126, gradient=0.0016716855208219023\n",
      "Gradient Descent(15/99): loss=0.38965933891778026, gradient=0.0015644572439917357\n",
      "Gradient Descent(16/99): loss=0.38965912905999095, gradient=0.001470020389160882\n",
      "Gradient Descent(17/99): loss=0.38965894209603236, gradient=0.0013863646791044133\n",
      "Gradient Descent(18/99): loss=0.38965877442499963, gradient=0.0013118775369330023\n",
      "Gradient Descent(19/99): loss=0.38965862313620125, gradient=0.001245252672447925\n",
      "Gradient Descent(20/99): loss=0.3896584858536228, gradient=0.001185420861575558\n",
      "Gradient Descent(21/99): loss=0.389658360620513, gradient=0.001131497649838526\n",
      "Gradient Descent(22/99): loss=0.38965824581261654, gradient=0.0010827438433225415\n",
      "Gradient Descent(23/99): loss=0.38965814007210137, gradient=0.001038535596361796\n",
      "Gradient Descent(24/99): loss=0.38965804225663014, gradient=0.0009983416686193578\n",
      "Gradient Descent(25/99): loss=0.389657951399645, gradient=0.0009617060248122099\n",
      "Gradient Descent(26/99): loss=0.38965786667906305, gradient=0.000928234413567039\n",
      "Gradient Descent(27/99): loss=0.38965778739235984, gradient=0.0008975839138563776\n",
      "Gradient Descent(28/99): loss=0.3896577129365536, gradient=0.0008694547018068838\n",
      "Gradient Descent(29/99): loss=0.38965764279199905, gradient=0.0008435834874340445\n",
      "Gradient Descent(30/99): loss=0.38965757650916777, gradient=0.0008197382162977829\n",
      "Gradient Descent(31/99): loss=0.3896575136977928, gradient=0.0007977137379930466\n",
      "Gradient Descent(32/99): loss=0.38965745401790464, gradient=0.0007773282216554986\n",
      "Gradient Descent(33/99): loss=0.38965739717238673, gradient=0.0007584201557761114\n",
      "Gradient Descent(34/99): loss=0.38965734290076465, gradient=0.0007408458112042289\n",
      "Gradient Descent(35/99): loss=0.38965729097400176, gradient=0.000724477076470883\n",
      "Gradient Descent(36/99): loss=0.3896572411901198, gradient=0.0007091995965774537\n",
      "Gradient Descent(37/99): loss=0.38965719337050075, gradient=0.0006949111624437894\n",
      "Gradient Descent(38/99): loss=0.38965714735675216, gradient=0.0006815203099566169\n",
      "Gradient Descent(39/99): loss=0.3896571030080436, gradient=0.0006689450962069953\n",
      "Gradient Descent(40/99): loss=0.38965706019883406, gradient=0.000657112026930668\n",
      "Gradient Descent(41/99): loss=0.38965701881692716, gradient=0.0006459551139876649\n",
      "Gradient Descent(42/99): loss=0.38965697876180533, gradient=0.0006354150453930142\n",
      "Gradient Descent(43/99): loss=0.38965693994319656, gradient=0.0006254384532516556\n",
      "Gradient Descent(44/99): loss=0.38965690227983585, gradient=0.0006159772671888167\n",
      "Gradient Descent(45/99): loss=0.3896568656983986, gradient=0.0006069881426610963\n",
      "Gradient Descent(46/99): loss=0.3896568301325735, gradient=0.0005984319549962223\n",
      "Gradient Descent(47/99): loss=0.38965679552225824, gradient=0.0005902733512235204\n",
      "Gradient Descent(48/99): loss=0.3896567618128589, gradient=0.0005824803527773741\n",
      "Gradient Descent(49/99): loss=0.3896567289546783, gradient=0.0005750240030255465\n",
      "Gradient Descent(50/99): loss=0.38965669690238175, gradient=0.0005678780543214335\n",
      "Gradient Descent(51/99): loss=0.3896566656145282, gradient=0.0005610186899260291\n",
      "Gradient Descent(52/99): loss=0.38965663505315856, gradient=0.0005544242767108173\n",
      "Gradient Descent(53/99): loss=0.38965660518343304, gradient=0.0005480751450440783\n",
      "Gradient Descent(54/99): loss=0.38965657597331166, gradient=0.0005419533926969826\n",
      "Gradient Descent(55/99): loss=0.3896565473932719, gradient=0.000536042709984871\n",
      "Gradient Descent(56/99): loss=0.3896565194160585, gradient=0.0005303282236928319\n",
      "Gradient Descent(57/99): loss=0.38965649201646185, gradient=0.0005247963576285949\n",
      "Gradient Descent(58/99): loss=0.3896564651711206, gradient=0.0005194347079034553\n",
      "Gradient Descent(59/99): loss=0.3896564388583457, gradient=0.0005142319312689254\n",
      "Gradient Descent(60/99): loss=0.38965641305796506, gradient=0.0005091776450364779\n",
      "Gradient Descent(61/99): loss=0.38965638775118305, gradient=0.0005042623372824549\n",
      "Gradient Descent(62/99): loss=0.38965636292045563, gradient=0.0004994772861949425\n",
      "Gradient Descent(63/99): loss=0.3896563385493781, gradient=0.0004948144875531469\n",
      "Gradient Descent(64/99): loss=0.38965631462258554, gradient=0.0004902665894506389\n",
      "Gradient Descent(65/99): loss=0.38965629112566097, gradient=0.00048582683347545474\n",
      "Gradient Descent(66/99): loss=0.3896562680450558, gradient=0.00048148900165371046\n",
      "Gradient Descent(67/99): loss=0.3896562453680154, gradient=0.0004772473685423124\n",
      "Gradient Descent(68/99): loss=0.38965622308251363, gradient=0.0004730966579274162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(69/99): loss=0.3896562011771935, gradient=0.0004690320036474809\n",
      "Gradient Descent(70/99): loss=0.38965617964131194, gradient=0.0004650489141132801\n",
      "Gradient Descent(71/99): loss=0.3896561584646924, gradient=0.00046114324014668424\n",
      "Gradient Descent(72/99): loss=0.3896561376376804, gradient=0.0004573111458005379\n",
      "Gradient Descent(73/99): loss=0.38965611715110243, gradient=0.0004535490818604794\n",
      "Gradient Descent(74/99): loss=0.3896560969962306, gradient=0.00044985376176143703\n",
      "Gradient Descent(75/99): loss=0.3896560771647483, gradient=0.0004462221396806911\n",
      "Gradient Descent(76/99): loss=0.38965605764872085, gradient=0.00044265139059523033\n",
      "Gradient Descent(77/99): loss=0.3896560384405683, gradient=0.0004391388921129539\n",
      "Gradient Descent(78/99): loss=0.3896560195330389, gradient=0.0004356822079080199\n",
      "Gradient Descent(79/99): loss=0.38965600091918756, gradient=0.00043227907260810707\n",
      "Gradient Descent(80/99): loss=0.38965598259235423, gradient=0.0004289273779963936\n",
      "Gradient Descent(81/99): loss=0.3896559645461451, gradient=0.000425625160406281\n",
      "Gradient Descent(82/99): loss=0.38965594677441423, gradient=0.00042237058919790567\n",
      "Gradient Descent(83/99): loss=0.38965592927124887, gradient=0.00041916195621765506\n",
      "Gradient Descent(84/99): loss=0.3896559120309531, gradient=0.0004159976661509207\n",
      "Gradient Descent(85/99): loss=0.389655895048036, gradient=0.00041287622768774616\n",
      "Gradient Descent(86/99): loss=0.3896558783171975, gradient=0.0004097962454280603\n",
      "Gradient Descent(87/99): loss=0.38965586183331796, gradient=0.00040675641246127383\n",
      "Gradient Descent(88/99): loss=0.38965584559144756, gradient=0.0004037555035603732\n",
      "Gradient Descent(89/99): loss=0.3896558295867961, gradient=0.0004007923689364613\n",
      "Gradient Descent(90/99): loss=0.3896558138147248, gradient=0.0003978659285055192\n",
      "Gradient Descent(91/99): loss=0.3896557982707371, gradient=0.0003949751666222103\n",
      "Gradient Descent(92/99): loss=0.3896557829504713, gradient=0.0003921191272413889\n",
      "Gradient Descent(93/99): loss=0.3896557678496944, gradient=0.00038929690947015817\n",
      "Gradient Descent(94/99): loss=0.38965575296429417, gradient=0.0003865076634775591\n",
      "Gradient Descent(95/99): loss=0.38965573829027383, gradient=0.00038375058673160994\n",
      "Gradient Descent(96/99): loss=0.38965572382374625, gradient=0.00038102492053597307\n",
      "Gradient Descent(97/99): loss=0.3896557095609288, gradient=0.0003783299468416924\n",
      "Gradient Descent(98/99): loss=0.3896556954981384, gradient=0.0003756649853105562\n",
      "Gradient Descent(99/99): loss=0.38965568163178715, gradient=0.0003730293906092748\n",
      "Gradient Descent(0/99): loss=0.3889671086644957, gradient=0.00816732088533693\n",
      "Gradient Descent(1/99): loss=0.38896435468853546, gradient=0.00554346708089832\n",
      "Gradient Descent(2/99): loss=0.38896244272526875, gradient=0.004557549653366692\n",
      "Gradient Descent(3/99): loss=0.38896106682928316, gradient=0.003853056504436968\n",
      "Gradient Descent(4/99): loss=0.3889600520118513, gradient=0.003299379447416077\n",
      "Gradient Descent(5/99): loss=0.38895928375436456, gradient=0.002861708789042203\n",
      "Gradient Descent(6/99): loss=0.3889586858087726, gradient=0.0025163228356866294\n",
      "Gradient Descent(7/99): loss=0.38895820702742867, gradient=0.002244165112226229\n",
      "Gradient Descent(8/99): loss=0.38895781284767444, gradient=0.0020297068259991172\n",
      "Gradient Descent(9/99): loss=0.3889574797245364, gradient=0.0018603369765153724\n",
      "Gradient Descent(10/99): loss=0.38895719147339575, gradient=0.0017259095077754262\n",
      "Gradient Descent(11/99): loss=0.38895693685661403, gradient=0.0016183674466798063\n",
      "Gradient Descent(12/99): loss=0.3889567079835555, gradient=0.0015314058575049314\n",
      "Gradient Descent(13/99): loss=0.3889564992445823, gradient=0.0014601604893330034\n",
      "Gradient Descent(14/99): loss=0.3889563065970241, gradient=0.001400924141198179\n",
      "Gradient Descent(15/99): loss=0.38895612708416294, gradient=0.0013508975437460692\n",
      "Gradient Descent(16/99): loss=0.3889559585091666, gradient=0.0013079795648952423\n",
      "Gradient Descent(17/99): loss=0.38895579921256035, gradient=0.0012705972531532838\n",
      "Gradient Descent(18/99): loss=0.3889556479192387, gradient=0.001237572523823337\n",
      "Gradient Descent(19/99): loss=0.3889555036324337, gradient=0.0012080201883768137\n",
      "Gradient Descent(20/99): loss=0.3889553655595872, gradient=0.001181271403334452\n",
      "Gradient Descent(21/99): loss=0.3889552330600312, gradient=0.0011568169541532742\n",
      "Gradient Descent(22/99): loss=0.3889551056076841, gradient=0.0011342655945752525\n",
      "Gradient Descent(23/99): loss=0.3889549827641617, gradient=0.0011133135934211934\n",
      "Gradient Descent(24/99): loss=0.388954864159171, gradient=0.0010937225141520127\n",
      "Gradient Descent(25/99): loss=0.38895474947604275, gradient=0.0010753029902669015\n",
      "Gradient Descent(26/99): loss=0.38895463844092143, gradient=0.0010579028457542395\n",
      "Gradient Descent(27/99): loss=0.3889545308145868, gradient=0.001041398357673873\n",
      "Gradient Descent(28/99): loss=0.38895442638618705, gradient=0.001025687791418526\n",
      "Gradient Descent(29/99): loss=0.38895432496837873, gradient=0.001010686583225088\n",
      "Gradient Descent(30/99): loss=0.38895422639350646, gradient=0.0009963237210158253\n",
      "Gradient Descent(31/99): loss=0.3889541305105708, gradient=0.0009825390013773\n",
      "Gradient Descent(32/99): loss=0.38895403718278976, gradient=0.0009692809310641548\n",
      "Gradient Descent(33/99): loss=0.3889539462856194, gradient=0.000956505106023693\n",
      "Gradient Descent(34/99): loss=0.38895385770512925, gradient=0.000944172947004633\n",
      "Gradient Descent(35/99): loss=0.388953771336659, gradient=0.0009322507037057167\n",
      "Gradient Descent(36/99): loss=0.3889536870836949, gradient=0.0009207086629600713\n",
      "Gradient Descent(37/99): loss=0.38895360485692637, gradient=0.0009095205133638931\n",
      "Gradient Descent(38/99): loss=0.3889535245734435, gradient=0.0008986628309633508\n",
      "Gradient Descent(39/99): loss=0.38895344615605343, gradient=0.0008881146594710338\n",
      "Gradient Descent(40/99): loss=0.38895336953277554, gradient=0.0008778561757367379\n",
      "Gradient Descent(41/99): loss=0.38895329463607403, gradient=0.0008678724376730949\n",
      "Gradient Descent(42/99): loss=0.3889532214026868, gradient=0.0008581469724379643\n",
      "Gradient Descent(43/99): loss=0.3889531497731464, gradient=0.0008486657773285716\n",
      "Gradient Descent(44/99): loss=0.3889530796914402, gradient=0.0008394160771712647\n",
      "Gradient Descent(45/99): loss=0.38895301110471153, gradient=0.0008303861804161344\n",
      "Gradient Descent(46/99): loss=0.38895294396299546, gradient=0.000821565356198648\n",
      "Gradient Descent(47/99): loss=0.3889528782189846, gradient=0.000812943728773449\n",
      "Gradient Descent(48/99): loss=0.3889528138278179, gradient=0.0008045121864224593\n",
      "Gradient Descent(49/99): loss=0.3889527507468937, gradient=0.0007962623024846267\n",
      "Gradient Descent(50/99): loss=0.3889526889357007, gradient=0.0007881862665888334\n",
      "Gradient Descent(51/99): loss=0.38895262835566596, gradient=0.0007802768245146443\n",
      "Gradient Descent(52/99): loss=0.3889525689700179, gradient=0.0007725272253820507\n",
      "Gradient Descent(53/99): loss=0.3889525107436608, gradient=0.0007649311750955195\n",
      "Gradient Descent(54/99): loss=0.3889524536430628, gradient=0.000757482795146537\n",
      "Gradient Descent(55/99): loss=0.3889523976361524, gradient=0.0007501765860269158\n",
      "Gradient Descent(56/99): loss=0.38895234269222495, gradient=0.0007430073946264448\n",
      "Gradient Descent(57/99): loss=0.38895228878185617, gradient=0.0007359703850859926\n",
      "Gradient Descent(58/99): loss=0.38895223587682387, gradient=0.0007290610126602233\n",
      "Gradient Descent(59/99): loss=0.38895218395003534, gradient=0.0007222750002113317\n",
      "Gradient Descent(60/99): loss=0.38895213297546144, gradient=0.0007156083170112358\n",
      "Gradient Descent(61/99): loss=0.3889520829280746, gradient=0.0007090571595770183\n",
      "Gradient Descent(62/99): loss=0.3889520337837933, gradient=0.0007026179343033359\n",
      "Gradient Descent(63/99): loss=0.38895198551942867, gradient=0.0006962872416884512\n",
      "Gradient Descent(64/99): loss=0.38895193811263756, gradient=0.0006900618619781431\n",
      "Gradient Descent(65/99): loss=0.3889518915418765, gradient=0.0006839387420755126\n",
      "Gradient Descent(66/99): loss=0.38895184578636105, gradient=0.0006779149835837391\n",
      "Gradient Descent(67/99): loss=0.38895180082602643, gradient=0.0006719878318661259\n",
      "Gradient Descent(68/99): loss=0.3889517566414922, gradient=0.0006661546660221239\n",
      "Gradient Descent(69/99): loss=0.3889517132140283, gradient=0.0006604129896897345\n",
      "Gradient Descent(70/99): loss=0.38895167052552454, gradient=0.0006547604225957548\n",
      "Gradient Descent(71/99): loss=0.3889516285584599, gradient=0.0006491946927837958\n",
      "Gradient Descent(72/99): loss=0.38895158729587725, gradient=0.0006437136294583805\n",
      "Gradient Descent(73/99): loss=0.38895154672135623, gradient=0.0006383151563891219\n",
      "Gradient Descent(74/99): loss=0.38895150681899, gradient=0.0006329972858262861\n",
      "Gradient Descent(75/99): loss=0.3889514675733623, gradient=0.0006277581128828789\n",
      "Gradient Descent(76/99): loss=0.3889514289695275, gradient=0.0006225958103436219\n",
      "Gradient Descent(77/99): loss=0.38895139099298875, gradient=0.0006175086238644146\n",
      "Gradient Descent(78/99): loss=0.38895135362968164, gradient=0.0006124948675303032\n",
      "Gradient Descent(79/99): loss=0.3889513168659544, gradient=0.0006075529197422543\n",
      "Gradient Descent(80/99): loss=0.38895128068855317, gradient=0.000602681219405871\n",
      "Gradient Descent(81/99): loss=0.38895124508460505, gradient=0.0005978782623979524\n",
      "Gradient Descent(82/99): loss=0.38895121004160366, gradient=0.0005931425982887808\n",
      "Gradient Descent(83/99): loss=0.3889511755473953, gradient=0.000588472827299549\n",
      "Gradient Descent(84/99): loss=0.3889511415901654, gradient=0.0005838675974769158\n",
      "Gradient Descent(85/99): loss=0.38895110815842643, gradient=0.0005793256020676931\n",
      "Gradient Descent(86/99): loss=0.388951075241005, gradient=0.000574845577077734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(87/99): loss=0.38895104282703175, gradient=0.0005704262990015835\n",
      "Gradient Descent(88/99): loss=0.3889510109059297, gradient=0.0005660665827091838\n",
      "Gradient Descent(89/99): loss=0.3889509794674038, gradient=0.0005617652794779394\n",
      "Gradient Descent(90/99): loss=0.3889509485014324, gradient=0.0005575212751592928\n",
      "Gradient Descent(91/99): loss=0.38895091799825654, gradient=0.0005533334884692204\n",
      "Gradient Descent(92/99): loss=0.3889508879483719, gradient=0.0005492008693938947\n",
      "Gradient Descent(93/99): loss=0.3889508583425203, gradient=0.000545122397701158\n",
      "Gradient Descent(94/99): loss=0.3889508291716808, gradient=0.0005410970815508525\n",
      "Gradient Descent(95/99): loss=0.3889508004270633, gradient=0.000537123956195839\n",
      "Gradient Descent(96/99): loss=0.38895077210010026, gradient=0.000533202082767255\n",
      "Gradient Descent(97/99): loss=0.3889507441824393, gradient=0.00052933054713814\n",
      "Gradient Descent(98/99): loss=0.38895071666593767, gradient=0.0005255084588589428\n",
      "Gradient Descent(99/99): loss=0.3889506895426543, gradient=0.0005217349501604163\n",
      "Gradient Descent(0/99): loss=0.390151103694868, gradient=0.015412206723581921\n",
      "Gradient Descent(1/99): loss=0.3901471756680957, gradient=0.006923173234757961\n",
      "Gradient Descent(2/99): loss=0.3901445195064915, gradient=0.005363280326127497\n",
      "Gradient Descent(3/99): loss=0.39014252066961375, gradient=0.004618669735176488\n",
      "Gradient Descent(4/99): loss=0.3901409736616564, gradient=0.004051685868210119\n",
      "Gradient Descent(5/99): loss=0.39013975016205815, gradient=0.003594350397291214\n",
      "Gradient Descent(6/99): loss=0.39013876266279235, gradient=0.00322162256126023\n",
      "Gradient Descent(7/99): loss=0.39013795028731757, gradient=0.00291565470010693\n",
      "Gradient Descent(8/99): loss=0.390137270061265, gradient=0.0026626113835007074\n",
      "Gradient Descent(9/99): loss=0.3901366912085943, gradient=0.0024516878487166128\n",
      "Gradient Descent(10/99): loss=0.3901361913698251, gradient=0.002274433878291637\n",
      "Gradient Descent(11/99): loss=0.3901357540665316, gradient=0.002124231097929416\n",
      "Gradient Descent(12/99): loss=0.39013536698238416, gradient=0.0019958851795130314\n",
      "Gradient Descent(13/99): loss=0.3901350207845184, gradient=0.0018853079707908207\n",
      "Gradient Descent(14/99): loss=0.39013470830608404, gradient=0.0017892703256085257\n",
      "Gradient Descent(15/99): loss=0.3901344239727158, gradient=0.0017052104498358211\n",
      "Gradient Descent(16/99): loss=0.39013416339546403, gradient=0.001631085682739857\n",
      "Gradient Descent(17/99): loss=0.39013392307851996, gradient=0.0015652581016619468\n",
      "Gradient Descent(18/99): loss=0.39013370020694493, gradient=0.001506406334543039\n",
      "Gradient Descent(19/99): loss=0.390133492490748, gradient=0.0014534575873464302\n",
      "Gradient Descent(20/99): loss=0.3901332980490706, gradient=0.0014055352058898415\n",
      "Gradient Descent(21/99): loss=0.390133115323217, gradient=0.001361918143387884\n",
      "Gradient Descent(22/99): loss=0.3901329430106503, gradient=0.0013220095386304676\n",
      "Gradient Descent(23/99): loss=0.39013278001438145, gradient=0.0012853122632611274\n",
      "Gradient Descent(24/99): loss=0.39013262540377674, gradient=0.0012514098039714894\n",
      "Gradient Descent(25/99): loss=0.3901324783839196, gradient=0.0012199512360269139\n",
      "Gradient Descent(26/99): loss=0.3901323382714436, gradient=0.0011906393432245255\n",
      "Gradient Descent(27/99): loss=0.3901322044753066, gradient=0.0011632211665971413\n",
      "Gradient Descent(28/99): loss=0.3901320764813759, gradient=0.0011374804363548927\n",
      "Gradient Descent(29/99): loss=0.39013195383997257, gradient=0.0011132314717008317\n",
      "Gradient Descent(30/99): loss=0.39013183615573965, gradient=0.0010903142313935168\n",
      "Gradient Descent(31/99): loss=0.3901317230793459, gradient=0.00106859027206221\n",
      "Gradient Descent(32/99): loss=0.39013161430064924, gradient=0.0010479394272648298\n",
      "Gradient Descent(33/99): loss=0.39013150954303605, gradient=0.001028257062628092\n",
      "Gradient Descent(34/99): loss=0.39013140855870476, gradient=0.0010094517945326996\n",
      "Gradient Descent(35/99): loss=0.39013131112471916, gradient=0.0009914435842528094\n",
      "Gradient Descent(36/99): loss=0.3901312170396911, gradient=0.0009741621381462636\n",
      "Gradient Descent(37/99): loss=0.390131126120979, gradient=0.0009575455588475283\n",
      "Gradient Descent(38/99): loss=0.39013103820231365, gradient=0.0009415392035037515\n",
      "Gradient Descent(39/99): loss=0.390130953131778, gradient=0.0009260947137124259\n",
      "Gradient Descent(40/99): loss=0.39013087077008085, gradient=0.0009111691885626605\n",
      "Gradient Descent(41/99): loss=0.39013079098907893, gradient=0.0008967244774912606\n",
      "Gradient Descent(42/99): loss=0.3901307136705054, gradient=0.0008827265738756439\n",
      "Gradient Descent(43/99): loss=0.3901306387048728, gradient=0.0008691450936479791\n",
      "Gradient Descent(44/99): loss=0.39013056599052504, gradient=0.0008559528259178132\n",
      "Gradient Descent(45/99): loss=0.39013049543281436, gradient=0.0008431253447758071\n",
      "Gradient Descent(46/99): loss=0.39013042694338423, gradient=0.0008306406732320236\n",
      "Gradient Descent(47/99): loss=0.3901303604395444, gradient=0.0008184789916983039\n",
      "Gradient Descent(48/99): loss=0.39013029584372405, gradient=0.0008066223846246581\n",
      "Gradient Descent(49/99): loss=0.3901302330829911, gradient=0.0007950546198901942\n",
      "Gradient Descent(50/99): loss=0.39013017208863027, gradient=0.0007837609563752782\n",
      "Gradient Descent(51/99): loss=0.39013011279576876, gradient=0.0007727279758278781\n",
      "Gradient Descent(52/99): loss=0.3901300551430488, gradient=0.0007619434357135938\n",
      "Gradient Descent(53/99): loss=0.39012999907233326, gradient=0.0007513961402232717\n",
      "Gradient Descent(54/99): loss=0.3901299445284459, gradient=0.000741075827019568\n",
      "Gradient Descent(55/99): loss=0.3901298914589403, gradient=0.0007309730676492423\n",
      "Gradient Descent(56/99): loss=0.39012983981389093, gradient=0.0007210791798393381\n",
      "Gradient Descent(57/99): loss=0.3901297895457098, gradient=0.0007113861501452019\n",
      "Gradient Descent(58/99): loss=0.39012974060897626, gradient=0.0007018865656281166\n",
      "Gradient Descent(59/99): loss=0.3901296929602902, gradient=0.0006925735534215619\n",
      "Gradient Descent(60/99): loss=0.3901296465581339, gradient=0.0006834407271995169\n",
      "Gradient Descent(61/99): loss=0.3901296013627504, gradient=0.0006744821396909545\n",
      "Gradient Descent(62/99): loss=0.3901295573360316, gradient=0.0006656922404994418\n",
      "Gradient Descent(63/99): loss=0.39012951444141714, gradient=0.0006570658385831596\n",
      "Gradient Descent(64/99): loss=0.39012947264380143, gradient=0.0006485980688343499\n",
      "Gradient Descent(65/99): loss=0.3901294319094509, gradient=0.0006402843622702255\n",
      "Gradient Descent(66/99): loss=0.3901293922059245, gradient=0.0006321204194078896\n",
      "Gradient Descent(67/99): loss=0.3901293535020039, gradient=0.000624102186452555\n",
      "Gradient Descent(68/99): loss=0.3901293157676285, gradient=0.000616225833971582\n",
      "Gradient Descent(69/99): loss=0.3901292789738346, gradient=0.0006084877377702762\n",
      "Gradient Descent(70/99): loss=0.39012924309269986, gradient=0.0006008844617182864\n",
      "Gradient Descent(71/99): loss=0.390129208097292, gradient=0.0005934127423077642\n",
      "Gradient Descent(72/99): loss=0.39012917396162106, gradient=0.000586069474749114\n",
      "Gradient Descent(73/99): loss=0.3901291406605946, gradient=0.0005788517004351915\n",
      "Gradient Descent(74/99): loss=0.3901291081699766, gradient=0.0005717565956236175\n",
      "Gradient Descent(75/99): loss=0.39012907646634915, gradient=0.0005647814612053183\n",
      "Gradient Descent(76/99): loss=0.3901290455270754, gradient=0.0005579237134422986\n",
      "Gradient Descent(77/99): loss=0.390129015330267, gradient=0.0005511808755716585\n",
      "Gradient Descent(78/99): loss=0.3901289858547512, gradient=0.0005445505701845806\n",
      "Gradient Descent(79/99): loss=0.39012895708004186, gradient=0.0005380305122989952\n",
      "Gradient Descent(80/99): loss=0.3901289289863109, gradient=0.000531618503054647\n",
      "Gradient Descent(81/99): loss=0.39012890155436236, gradient=0.0005253124239662857\n",
      "Gradient Descent(82/99): loss=0.39012887476560726, gradient=0.0005191102316789301\n",
      "Gradient Descent(83/99): loss=0.3901288486020394, gradient=0.0005130099531742776\n",
      "Gradient Descent(84/99): loss=0.3901288230462137, gradient=0.0005070096813837368\n",
      "Gradient Descent(85/99): loss=0.3901287980812254, gradient=0.0005011075711677482\n",
      "Gradient Descent(86/99): loss=0.3901287736906888, gradient=0.0004953018356262537\n",
      "Gradient Descent(87/99): loss=0.3901287498587195, gradient=0.0004895907427079014\n",
      "Gradient Descent(88/99): loss=0.39012872656991493, gradient=0.00048397261208943946\n",
      "Gradient Descent(89/99): loss=0.3901287038093385, gradient=0.00047844581230046076\n",
      "Gradient Descent(90/99): loss=0.3901286815625018, gradient=0.00047300875806987225\n",
      "Gradient Descent(91/99): loss=0.39012865981534994, gradient=0.00046765990787411236\n",
      "Gradient Descent(92/99): loss=0.39012863855424634, gradient=0.00046239776166841115\n",
      "Gradient Descent(93/99): loss=0.3901286177659576, gradient=0.00045722085878488857\n",
      "Gradient Descent(94/99): loss=0.39012859743764067, gradient=0.00045212777598245774\n",
      "Gradient Descent(95/99): loss=0.39012857755683017, gradient=0.00044711712563505647\n",
      "Gradient Descent(96/99): loss=0.3901285581114243, gradient=0.00044218755404636215\n",
      "Gradient Descent(97/99): loss=0.3901285390896743, gradient=0.00043733773988023866\n",
      "Gradient Descent(98/99): loss=0.39012852048017194, gradient=0.00043256639269676915\n",
      "Gradient Descent(99/99): loss=0.390128502271839, gradient=0.0004278722515852353\n",
      "Gradient Descent(0/99): loss=0.3903625696743778, gradient=0.007265751624692082\n",
      "Gradient Descent(1/99): loss=0.3903588971845329, gradient=0.006252721869222289\n",
      "Gradient Descent(2/99): loss=0.3903560135593617, gradient=0.005520731924370401\n",
      "Gradient Descent(3/99): loss=0.3903536901495986, gradient=0.004941397885805232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(4/99): loss=0.39035177521423364, gradient=0.0044747260216060954\n",
      "Gradient Descent(5/99): loss=0.39035016514668064, gradient=0.004093997716456749\n",
      "Gradient Descent(6/99): loss=0.39034878774904264, gradient=0.0037793637728737984\n",
      "Gradient Descent(7/99): loss=0.39034759169452843, gradient=0.003515961741537155\n",
      "Gradient Descent(8/99): loss=0.3903465397662398, gradient=0.003292635953284201\n",
      "Gradient Descent(9/99): loss=0.39034560445339134, gradient=0.003100987012989802\n",
      "Gradient Descent(10/99): loss=0.3903447650465921, gradient=0.0029346360147973474\n",
      "Gradient Descent(11/99): loss=0.39034400568476263, gradient=0.0027887084289326897\n",
      "Gradient Descent(12/99): loss=0.39034331402411954, gradient=0.002659440447124256\n",
      "Gradient Descent(13/99): loss=0.3903426803143828, gradient=0.002543898831965692\n",
      "Gradient Descent(14/99): loss=0.3903420967482183, gradient=0.0024397714031127895\n",
      "Gradient Descent(15/99): loss=0.3903415569918745, gradient=0.002345231208581768\n",
      "Gradient Descent(16/99): loss=0.3903410558482335, gradient=0.0022588042532350116\n",
      "Gradient Descent(17/99): loss=0.39034058900513396, gradient=0.002179302326417171\n",
      "Gradient Descent(18/99): loss=0.3903401528478245, gradient=0.002105757841031744\n",
      "Gradient Descent(19/99): loss=0.39033974431694624, gradient=0.0020373773328657752\n",
      "Gradient Descent(20/99): loss=0.3903393607997802, gradient=0.0019735058756150396\n",
      "Gradient Descent(21/99): loss=0.3903390000461083, gradient=0.0019135995738731352\n",
      "Gradient Descent(22/99): loss=0.39033866010250595, gradient=0.0018572041010326856\n",
      "Gradient Descent(23/99): loss=0.39033833926069106, gradient=0.0018039370966391414\n",
      "Gradient Descent(24/99): loss=0.3903380360160285, gradient=0.0017534777468438138\n",
      "Gradient Descent(25/99): loss=0.39033774903532104, gradient=0.0017055498189929455\n",
      "Gradient Descent(26/99): loss=0.3903374771293658, gradient=0.001659922462269459\n",
      "Gradient Descent(27/99): loss=0.39033721923206044, gradient=0.001616394325145201\n",
      "Gradient Descent(28/99): loss=0.3903369743827589, gradient=0.0015747917761656121\n",
      "Gradient Descent(29/99): loss=0.39033674171171967, gradient=0.0015349645961708848\n",
      "Gradient Descent(30/99): loss=0.39033652042813455, gradient=0.0014967817419316398\n",
      "Gradient Descent(31/99): loss=0.39033630981015616, gradient=0.00146012827966496\n",
      "Gradient Descent(32/99): loss=0.39033610919654316, gradient=0.0014249028605896135\n",
      "Gradient Descent(33/99): loss=0.39033591797961587, gradient=0.0013910156359149396\n",
      "Gradient Descent(34/99): loss=0.3903357355992834, gradient=0.0013583865293242143\n",
      "Gradient Descent(35/99): loss=0.39033556153794846, gradient=0.0013269438012344856\n",
      "Gradient Descent(36/99): loss=0.3903353953161353, gradient=0.0012966228519225825\n",
      "Gradient Descent(37/99): loss=0.390335236488719, gradient=0.0012673652207716188\n",
      "Gradient Descent(38/99): loss=0.3903350846416527, gradient=0.0012391177469955037\n",
      "Gradient Descent(39/99): loss=0.3903349393891161, gradient=0.0012118318636824199\n",
      "Gradient Descent(40/99): loss=0.39033480037101165, gradient=0.00118546300220653\n",
      "Gradient Descent(41/99): loss=0.39033466725076277, gradient=0.0011599700882516807\n",
      "Gradient Descent(42/99): loss=0.39033453971336224, gradient=0.0011353151140799894\n",
      "Gradient Descent(43/99): loss=0.3903344174636374, gradient=0.0011114627744231585\n",
      "Gradient Descent(44/99): loss=0.3903343002247007, gradient=0.0010883801556050142\n",
      "Gradient Descent(45/99): loss=0.3903341877365604, gradient=0.0010660364693165529\n",
      "Gradient Descent(46/99): loss=0.3903340797548691, gradient=0.001044402823948216\n",
      "Gradient Descent(47/99): loss=0.39033397604979275, gradient=0.0010234520275923464\n",
      "Gradient Descent(48/99): loss=0.3903338764049856, gradient=0.001003158417823466\n",
      "Gradient Descent(49/99): loss=0.3903337806166579, gradient=0.000983497714178245\n",
      "Gradient Descent(50/99): loss=0.3903336884927238, gradient=0.0009644468899278562\n",
      "Gradient Descent(51/99): loss=0.39033359985202304, gradient=0.0009459840602904253\n",
      "Gradient Descent(52/99): loss=0.3903335145236063, gradient=0.0009280883846870225\n",
      "Gradient Descent(53/99): loss=0.390333432346078, gradient=0.0009107399810263505\n",
      "Gradient Descent(54/99): loss=0.3903333531669904, gradient=0.0008939198503160744\n",
      "Gradient Descent(55/99): loss=0.3903332768422842, gradient=0.0008776098101623511\n",
      "Gradient Descent(56/99): loss=0.39033320323577053, gradient=0.0008617924359378431\n",
      "Gradient Descent(57/99): loss=0.3903331322186509, gradient=0.0008464510085800297\n",
      "Gradient Descent(58/99): loss=0.39033306366907183, gradient=0.0008315694681367157\n",
      "Gradient Descent(59/99): loss=0.39033299747170913, gradient=0.0008171323723028352\n",
      "Gradient Descent(60/99): loss=0.3903329335173837, gradient=0.0008031248593023163\n",
      "Gradient Descent(61/99): loss=0.3903328717026989, gradient=0.000789532614558723\n",
      "Gradient Descent(62/99): loss=0.39033281192970726, gradient=0.0007763418406775916\n",
      "Gradient Descent(63/99): loss=0.3903327541055941, gradient=0.0007635392303274943\n",
      "Gradient Descent(64/99): loss=0.3903326981423847, gradient=0.0007511119416634252\n",
      "Gradient Descent(65/99): loss=0.3903326439566695, gradient=0.0007390475759834624\n",
      "Gradient Descent(66/99): loss=0.39033259146934535, gradient=0.0007273341573503848\n",
      "Gradient Descent(67/99): loss=0.3903325406053734, gradient=0.0007159601139447682\n",
      "Gradient Descent(68/99): loss=0.3903324912935541, gradient=0.0007049142609456923\n",
      "Gradient Descent(69/99): loss=0.3903324434663112, gradient=0.0006941857847614871\n",
      "Gradient Descent(70/99): loss=0.39033239705949446, gradient=0.000683764228454665\n",
      "Gradient Descent(71/99): loss=0.3903323520121886, gradient=0.0006736394782247622\n",
      "Gradient Descent(72/99): loss=0.39033230826653753, gradient=0.0006638017508293406\n",
      "Gradient Descent(73/99): loss=0.39033226576757785, gradient=0.0006542415818382773\n",
      "Gradient Descent(74/99): loss=0.39033222446308063, gradient=0.0006449498146286944\n",
      "Gradient Descent(75/99): loss=0.3903321843034045, gradient=0.0006359175900391421\n",
      "Gradient Descent(76/99): loss=0.390332145241356, gradient=0.0006271363366118045\n",
      "Gradient Descent(77/99): loss=0.39033210723205775, gradient=0.0006185977613591472\n",
      "Gradient Descent(78/99): loss=0.39033207023282523, gradient=0.000610293841000119\n",
      "Gradient Descent(79/99): loss=0.3903320342030496, gradient=0.0006022168136163338\n",
      "Gradient Descent(80/99): loss=0.3903319991040868, gradient=0.0005943591706857641\n",
      "Gradient Descent(81/99): loss=0.3903319648991549, gradient=0.0005867136494556396\n",
      "Gradient Descent(82/99): loss=0.3903319315532343, gradient=0.0005792732256217186\n",
      "Gradient Descent(83/99): loss=0.3903318990329761, gradient=0.0005720311062842965\n",
      "Gradient Descent(84/99): loss=0.39033186730661323, gradient=0.000564980723155789\n",
      "Gradient Descent(85/99): loss=0.39033183634387975, gradient=0.0005581157259971613\n",
      "Gradient Descent(86/99): loss=0.3903318061159299, gradient=0.0005514299762638039\n",
      "Gradient Descent(87/99): loss=0.39033177659526663, gradient=0.0005449175409440116\n",
      "Gradient Descent(88/99): loss=0.3903317477556702, gradient=0.0005385726865748578\n",
      "Gradient Descent(89/99): loss=0.3903317195721329, gradient=0.0005323898734230344\n",
      "Gradient Descent(90/99): loss=0.39033169202079626, gradient=0.0005263637498194309\n",
      "Gradient Descent(91/99): loss=0.390331665078892, gradient=0.0005204891466378997\n",
      "Gradient Descent(92/99): loss=0.3903316387246865, gradient=0.0005147610719100608\n",
      "Gradient Descent(93/99): loss=0.3903316129374273, gradient=0.0005091747055692647\n",
      "Gradient Descent(94/99): loss=0.3903315876972943, gradient=0.000503725394317545\n",
      "Gradient Descent(95/99): loss=0.39033156298535093, gradient=0.0004984086466106895\n",
      "Gradient Descent(96/99): loss=0.3903315387835011, gradient=0.0004932201277571006\n",
      "Gradient Descent(97/99): loss=0.39033151507444497, gradient=0.0004881556551270091\n",
      "Gradient Descent(98/99): loss=0.390331491841641, gradient=0.0004832111934685476\n",
      "Gradient Descent(99/99): loss=0.3903314690692661, gradient=0.00047838285032889564\n",
      "Gradient Descent(0/99): loss=0.3899822199636142, gradient=0.01000867111683258\n",
      "Gradient Descent(1/99): loss=0.38997598692084406, gradient=0.008243973089471952\n",
      "Gradient Descent(2/99): loss=0.3899715762121576, gradient=0.006907499364947851\n",
      "Gradient Descent(3/99): loss=0.38996834358406646, gradient=0.005889341657543672\n",
      "Gradient Descent(4/99): loss=0.3899658891990839, gradient=0.005110967775735089\n",
      "Gradient Descent(5/99): loss=0.38996396177353626, gradient=0.004512037588256583\n",
      "Gradient Descent(6/99): loss=0.3899624010717852, gradient=0.0040464133090146225\n",
      "Gradient Descent(7/99): loss=0.3899611031540913, gradient=0.003679309033873095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(8/99): loss=0.3899599992486749, gradient=0.003384926499090964\n",
      "Gradient Descent(9/99): loss=0.3899590428274589, gradient=0.0031444073734497873\n",
      "Gradient Descent(10/99): loss=0.38995820163790096, gradient=0.0029441135469597123\n",
      "Gradient Descent(11/99): loss=0.3899574527445303, gradient=0.002774238065346372\n",
      "Gradient Descent(12/99): loss=0.3899567794091805, gradient=0.00262773078599433\n",
      "Gradient Descent(13/99): loss=0.3899561691025994, gradient=0.002499498496835338\n",
      "Gradient Descent(14/99): loss=0.3899556122181853, gradient=0.0023858269215061476\n",
      "Gradient Descent(15/99): loss=0.38995510122586186, gradient=0.0022839719260117726\n",
      "Gradient Descent(16/99): loss=0.3899546301051208, gradient=0.0021918743583471314\n",
      "Gradient Descent(17/99): loss=0.3899541939575328, gradient=0.0021079625560393705\n",
      "Gradient Descent(18/99): loss=0.389953788736393, gradient=0.002031015784909137\n",
      "Gradient Descent(19/99): loss=0.3899534110541149, gradient=0.0019600695093368215\n",
      "Gradient Descent(20/99): loss=0.3899530580421612, gradient=0.0018943492062091234\n",
      "Gradient Descent(21/99): loss=0.3899527272471591, gradient=0.0018332236343806658\n",
      "Gradient Descent(22/99): loss=0.38995241655292323, gradient=0.0017761685148140654\n",
      "Gradient Descent(23/99): loss=0.38995212411861563, gradient=0.0017227538552401105\n",
      "Gradient Descent(24/99): loss=0.3899518483335433, gradient=0.0016726069159422906\n",
      "Gradient Descent(25/99): loss=0.3899515877785741, gradient=0.0016254173370524358\n",
      "Gradient Descent(26/99): loss=0.3899513411974415, gradient=0.0015809121110915442\n",
      "Gradient Descent(27/99): loss=0.3899511074717156, gradient=0.001538860066967128\n",
      "Gradient Descent(28/99): loss=0.3899508856017295, gradient=0.0014990559632457805\n",
      "Gradient Descent(29/99): loss=0.38995067469012623, gradient=0.0014613208299825962\n",
      "Gradient Descent(30/99): loss=0.3899504739283639, gradient=0.0014254966698993572\n",
      "Gradient Descent(31/99): loss=0.3899502825854226, gradient=0.0013914432387652843\n",
      "Gradient Descent(32/99): loss=0.3899500999982902, gradient=0.0013590354103342056\n",
      "Gradient Descent(33/99): loss=0.3899499255638963, gradient=0.0013281610035936519\n",
      "Gradient Descent(34/99): loss=0.3899497587322333, gradient=0.001298718979519152\n",
      "Gradient Descent(35/99): loss=0.38994959900046033, gradient=0.0012706179358899499\n",
      "Gradient Descent(36/99): loss=0.38994944590782066, gradient=0.0012437748444694386\n",
      "Gradient Descent(37/99): loss=0.38994929903124154, gradient=0.0012181139866366015\n",
      "Gradient Descent(38/99): loss=0.38994915798150537, gradient=0.0011935660524963866\n",
      "Gradient Descent(39/99): loss=0.3899490223999034, gradient=0.0011700673753737918\n",
      "Gradient Descent(40/99): loss=0.38994889195530075, gradient=0.0011475592789489365\n",
      "Gradient Descent(41/99): loss=0.3899487663415486, gradient=0.0011259875184975083\n",
      "Gradient Descent(42/99): loss=0.38994864527519757, gradient=0.0011053018010453554\n",
      "Gradient Descent(43/99): loss=0.38994852849346745, gradient=0.0010854553719194019\n",
      "Gradient Descent(44/99): loss=0.389948415752439, gradient=0.001066404657336194\n",
      "Gradient Descent(45/99): loss=0.3899483068254394, gradient=0.0010481089544195578\n",
      "Gradient Descent(46/99): loss=0.3899482015015943, gradient=0.0010305301614685432\n",
      "Gradient Descent(47/99): loss=0.389948099584527, gradient=0.0010136325424688598\n",
      "Gradient Descent(48/99): loss=0.38994800089118714, gradient=0.0009973825208074692\n",
      "Gradient Descent(49/99): loss=0.3899479052507908, gradient=0.000981748497948406\n",
      "Gradient Descent(50/99): loss=0.38994781250386257, gradient=0.000966700693492344\n",
      "Gradient Descent(51/99): loss=0.3899477225013643, gradient=0.0009522110035946922\n",
      "Gradient Descent(52/99): loss=0.38994763510390396, gradient=0.0009382528751791841\n",
      "Gradient Descent(53/99): loss=0.38994755018101496, gradient=0.0009248011937714281\n",
      "Gradient Descent(54/99): loss=0.3899474676104967, gradient=0.000911832183102268\n",
      "Gradient Descent(55/99): loss=0.38994738727781236, gradient=0.0008993233149043313\n",
      "Gradient Descent(56/99): loss=0.3899473090755377, gradient=0.0008872532275579835\n",
      "Gradient Descent(57/99): loss=0.38994723290285394, gradient=0.0008756016524371405\n",
      "Gradient Descent(58/99): loss=0.38994715866508334, gradient=0.0008643493469729952\n",
      "Gradient Descent(59/99): loss=0.3899470862732613, gradient=0.0008534780335933323\n",
      "Gradient Descent(60/99): loss=0.38994701564374223, gradient=0.0008429703438162772\n",
      "Gradient Descent(61/99): loss=0.3899469466978352, gradient=0.0008328097668781931\n",
      "Gradient Descent(62/99): loss=0.3899468793614695, gradient=0.000822980602364104\n",
      "Gradient Descent(63/99): loss=0.38994681356488414, gradient=0.0008134679163813858\n",
      "Gradient Descent(64/99): loss=0.3899467492423408, gradient=0.0008042575008832364\n",
      "Gradient Descent(65/99): loss=0.38994668633185825, gradient=0.000795335835800748\n",
      "Gradient Descent(66/99): loss=0.3899466247749664, gradient=0.0007866900536902924\n",
      "Gradient Descent(67/99): loss=0.3899465645164775, gradient=0.0007783079066414681\n",
      "Gradient Descent(68/99): loss=0.38994650550427545, gradient=0.0007701777352266972\n",
      "Gradient Descent(69/99): loss=0.3899464476891175, gradient=0.0007622884393001015\n",
      "Gradient Descent(70/99): loss=0.3899463910244524, gradient=0.0007546294504809917\n",
      "Gradient Descent(71/99): loss=0.38994633546624996, gradient=0.0007471907061768686\n",
      "Gradient Descent(72/99): loss=0.3899462809728424, gradient=0.0007399626250191733\n",
      "Gradient Descent(73/99): loss=0.3899462275047784, gradient=0.0007329360836024647\n",
      "Gradient Descent(74/99): loss=0.389946175024684, gradient=0.0007261023944281742\n",
      "Gradient Descent(75/99): loss=0.38994612349713537, gradient=0.000719453284968825\n",
      "Gradient Descent(76/99): loss=0.38994607288853983, gradient=0.0007129808777753213\n",
      "Gradient Descent(77/99): loss=0.3899460231670247, gradient=0.0007066776715610989\n",
      "Gradient Descent(78/99): loss=0.38994597430233235, gradient=0.0007005365232018886\n",
      "Gradient Descent(79/99): loss=0.3899459262657234, gradient=0.0006945506305975191\n",
      "Gradient Descent(80/99): loss=0.38994587902988737, gradient=0.0006887135163465236\n",
      "Gradient Descent(81/99): loss=0.3899458325688558, gradient=0.0006830190121898618\n",
      "Gradient Descent(82/99): loss=0.3899457868579253, gradient=0.0006774612441825014\n",
      "Gradient Descent(83/99): loss=0.3899457418735819, gradient=0.0006720346185567363\n",
      "Gradient Descent(84/99): loss=0.38994569759343223, gradient=0.0006667338082423704\n",
      "Gradient Descent(85/99): loss=0.38994565399613884, gradient=0.0006615537400127559\n",
      "Gradient Descent(86/99): loss=0.3899456110613586, gradient=0.0006564895822269079\n",
      "Gradient Descent(87/99): loss=0.3899455687696875, gradient=0.0006515367331403542\n",
      "Gradient Descent(88/99): loss=0.38994552710260544, gradient=0.0006466908097588431\n",
      "Gradient Descent(89/99): loss=0.38994548604242746, gradient=0.0006419476372107369\n",
      "Gradient Descent(90/99): loss=0.3899454455722566, gradient=0.0006373032386148311\n",
      "Gradient Descent(91/99): loss=0.3899454056759397, gradient=0.0006327538254219611\n",
      "Gradient Descent(92/99): loss=0.38994536633802684, gradient=0.0006282957882097161\n",
      "Gradient Descent(93/99): loss=0.38994532754373246, gradient=0.0006239256879101935\n",
      "Gradient Descent(94/99): loss=0.38994528927889793, gradient=0.0006196402474521389\n",
      "Gradient Descent(95/99): loss=0.3899452515299597, gradient=0.0006154363437992317\n",
      "Gradient Descent(96/99): loss=0.3899452142839152, gradient=0.0006113110003673974\n",
      "Gradient Descent(97/99): loss=0.3899451775282939, gradient=0.0006072613798042487\n",
      "Gradient Descent(98/99): loss=0.3899451412511288, gradient=0.0006032847771150128\n",
      "Gradient Descent(99/99): loss=0.3899451054409299, gradient=0.0005993786131193897\n",
      "Gradient Descent(0/99): loss=0.3896789957530287, gradient=0.011119332679177038\n",
      "Gradient Descent(1/99): loss=0.38967400200491986, gradient=0.007453902170779235\n",
      "Gradient Descent(2/99): loss=0.3896704620737838, gradient=0.00618631314566619\n",
      "Gradient Descent(3/99): loss=0.389667862926622, gradient=0.005284624704478494\n",
      "Gradient Descent(4/99): loss=0.3896659156077036, gradient=0.004563768672932999\n",
      "Gradient Descent(5/99): loss=0.38966442821906944, gradient=0.003979650948117625\n",
      "Gradient Descent(6/99): loss=0.3896632703443172, gradient=0.003503574763394987\n",
      "Gradient Descent(7/99): loss=0.38966235230211677, gradient=0.0031131394037070617\n",
      "Gradient Descent(8/99): loss=0.38966161168526836, gradient=0.0027906480517434985\n",
      "Gradient Descent(9/99): loss=0.38966100450581453, gradient=0.00252215743231506\n",
      "Gradient Descent(10/99): loss=0.38966049933079966, gradient=0.002296723458248505\n",
      "Gradient Descent(11/99): loss=0.3896600733727206, gradient=0.0021057835201003148\n",
      "Gradient Descent(12/99): loss=0.38965970986404, gradient=0.0019426513095197712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(13/99): loss=0.38965939627975504, gradient=0.0018021070905285084\n",
      "Gradient Descent(14/99): loss=0.3896591231233479, gradient=0.0016800692386822187\n",
      "Gradient Descent(15/99): loss=0.3896588830894245, gradient=0.0015733343002831272\n",
      "Gradient Descent(16/99): loss=0.38965867048000963, gradient=0.0014793738386478167\n",
      "Gradient Descent(17/99): loss=0.3896584807929894, gradient=0.0013961774199947072\n",
      "Gradient Descent(18/99): loss=0.38965831042839455, gradient=0.0013221323670455504\n",
      "Gradient Descent(19/99): loss=0.3896581564761071, gradient=0.0012559323060898622\n",
      "Gradient Descent(20/99): loss=0.3896580165604031, gradient=0.0011965079322365337\n",
      "Gradient Descent(21/99): loss=0.3896578887246051, gradient=0.0011429747149939104\n",
      "Gradient Descent(22/99): loss=0.38965777134436425, gradient=0.0010945934003718336\n",
      "Gradient Descent(23/99): loss=0.3896576630616305, gradient=0.0010507401132841406\n",
      "Gradient Descent(24/99): loss=0.38965756273375574, gradient=0.0010108836291942646\n",
      "Gradient Descent(25/99): loss=0.38965746939380386, gradient=0.0009745679859552198\n",
      "Gradient Descent(26/99): loss=0.3896573822192669, gradient=0.0009413990711489941\n",
      "Gradient Descent(27/99): loss=0.3896573005071645, gradient=0.0009110341730057479\n",
      "Gradient Descent(28/99): loss=0.3896572236540399, gradient=0.0008831737478735066\n",
      "Gradient Descent(29/99): loss=0.38965715113976307, gradient=0.0008575548543054663\n",
      "Gradient Descent(30/99): loss=0.38965708251431785, gradient=0.0008339458494412598\n",
      "Gradient Descent(31/99): loss=0.38965701738695246, gradient=0.0008121420503257091\n",
      "Gradient Descent(32/99): loss=0.3896569554172193, gradient=0.000791962141043371\n",
      "Gradient Descent(33/99): loss=0.38965689630753236, gradient=0.0007732451635765178\n",
      "Gradient Descent(34/99): loss=0.38965683979695936, gradient=0.0007558479717739327\n",
      "Gradient Descent(35/99): loss=0.38965678565601747, gradient=0.0007396430579539574\n",
      "Gradient Descent(36/99): loss=0.389656733682296, gradient=0.0007245166835688533\n",
      "Gradient Descent(37/99): loss=0.3896566836967563, gradient=0.0007103672613089396\n",
      "Gradient Descent(38/99): loss=0.38965663554060115, gradient=0.0006971039476889121\n",
      "Gradient Descent(39/99): loss=0.3896565890726064, gradient=0.0006846454137433168\n",
      "Gradient Descent(40/99): loss=0.38965654416685114, gradient=0.0006729187678319168\n",
      "Gradient Descent(41/99): loss=0.38965650071077146, gradient=0.0006618586093467372\n",
      "Gradient Descent(42/99): loss=0.38965645860349407, gradient=0.0006514061957631583\n",
      "Gradient Descent(43/99): loss=0.38965641775440235, gradient=0.0006415087083077806\n",
      "Gradient Descent(44/99): loss=0.3896563780818984, gradient=0.0006321186037476128\n",
      "Gradient Descent(45/99): loss=0.3896563395123345, gradient=0.0006231930415996348\n",
      "Gradient Descent(46/99): loss=0.38965630197908624, gradient=0.0006146933775262509\n",
      "Gradient Descent(47/99): loss=0.3896562654217469, gradient=0.0006065847149023186\n",
      "Gradient Descent(48/99): loss=0.38965622978542774, gradient=0.0005988355075668012\n",
      "Gradient Descent(49/99): loss=0.38965619502014465, gradient=0.0005914172076499294\n",
      "Gradient Descent(50/99): loss=0.38965616108028384, gradient=0.0005843039531214576\n",
      "Gradient Descent(51/99): loss=0.38965612792413246, gradient=0.0005774722903609625\n",
      "Gradient Descent(52/99): loss=0.38965609551346647, gradient=0.0005709009276216523\n",
      "Gradient Descent(53/99): loss=0.38965606381318835, gradient=0.0005645705157587167\n",
      "Gradient Descent(54/99): loss=0.389656032791006, gradient=0.0005584634530317961\n",
      "Gradient Descent(55/99): loss=0.38965600241715137, gradient=0.0005525637111747259\n",
      "Gradient Descent(56/99): loss=0.3896559726641284, gradient=0.0005468566802646066\n",
      "Gradient Descent(57/99): loss=0.38965594350649285, gradient=0.0005413290302187127\n",
      "Gradient Descent(58/99): loss=0.3896559149206534, gradient=0.0005359685870093938\n",
      "Gradient Descent(59/99): loss=0.38965588688469616, gradient=0.0005307642219155925\n",
      "Gradient Descent(60/99): loss=0.38965585937822905, gradient=0.0005257057523325394\n",
      "Gradient Descent(61/99): loss=0.38965583238224033, gradient=0.0005207838528364858\n",
      "Gradient Descent(62/99): loss=0.3896558058789745, gradient=0.0005159899753579398\n",
      "Gradient Descent(63/99): loss=0.38965577985181954, gradient=0.0005113162774524385\n",
      "Gradient Descent(64/99): loss=0.3896557542852066, gradient=0.0005067555577776803\n",
      "Gradient Descent(65/99): loss=0.3896557291645197, gradient=0.000502301197990697\n",
      "Gradient Descent(66/99): loss=0.3896557044760136, gradient=0.0004979471103709689\n",
      "Gradient Descent(67/99): loss=0.3896556802067413, gradient=0.0004936876905555177\n",
      "Gradient Descent(68/99): loss=0.38965565634448746, gradient=0.0004895177748438818\n",
      "Gradient Descent(69/99): loss=0.38965563287770905, gradient=0.00048543260159199665\n",
      "Gradient Descent(70/99): loss=0.3896556097954804, gradient=0.0004814277762696174\n",
      "Gradient Descent(71/99): loss=0.3896555870874458, gradient=0.000477499239803429\n",
      "Gradient Descent(72/99): loss=0.3896555647437735, gradient=0.0004736432398701347\n",
      "Gradient Descent(73/99): loss=0.38965554275511693, gradient=0.00046985630484168927\n",
      "Gradient Descent(74/99): loss=0.3896555211125762, gradient=0.0004661352201169272\n",
      "Gradient Descent(75/99): loss=0.3896554998076678, gradient=0.00046247700660266707\n",
      "Gradient Descent(76/99): loss=0.38965547883229107, gradient=0.0004588789011342391\n",
      "Gradient Descent(77/99): loss=0.38965545817870295, gradient=0.00045533833864534957\n",
      "Gradient Descent(78/99): loss=0.38965543783949186, gradient=0.0004518529359198905\n",
      "Gradient Descent(79/99): loss=0.3896554178075545, gradient=0.00044842047677427007\n",
      "Gradient Descent(80/99): loss=0.38965539807607563, gradient=0.00044503889853470403\n",
      "Gradient Descent(81/99): loss=0.3896553786385081, gradient=0.0004417062796882878\n",
      "Gradient Descent(82/99): loss=0.38965535948855573, gradient=0.0004384208285985566\n",
      "Gradient Descent(83/99): loss=0.3896553406201567, gradient=0.000435180873187757\n",
      "Gradient Descent(84/99): loss=0.38965532202746933, gradient=0.0004319848514965614\n",
      "Gradient Descent(85/99): loss=0.3896553037048582, gradient=0.0004288313030428731\n",
      "Gradient Descent(86/99): loss=0.3896552856468813, gradient=0.0004257188609064497\n",
      "Gradient Descent(87/99): loss=0.38965526784827964, gradient=0.0004226462444756164\n",
      "Gradient Descent(88/99): loss=0.3896552503039647, gradient=0.000419612252796673\n",
      "Gradient Descent(89/99): loss=0.38965523300901106, gradient=0.0004166157584730172\n",
      "Gradient Descent(90/99): loss=0.38965521595864533, gradient=0.000413655702066148\n",
      "Gradient Descent(91/99): loss=0.38965519914823904, gradient=0.0004107310869545502\n",
      "Gradient Descent(92/99): loss=0.3896551825733014, gradient=0.00040784097461123034\n",
      "Gradient Descent(93/99): loss=0.3896551662294696, gradient=0.0004049844802638796\n",
      "Gradient Descent(94/99): loss=0.38965515011250557, gradient=0.0004021607689047212\n",
      "Gradient Descent(95/99): loss=0.38965513421828785, gradient=0.0003993690516209764\n",
      "Gradient Descent(96/99): loss=0.38965511854280643, gradient=0.00039660858221824297\n",
      "Gradient Descent(97/99): loss=0.3896551030821569, gradient=0.00039387865411256187\n",
      "Gradient Descent(98/99): loss=0.3896550878325371, gradient=0.0003911785974688029\n",
      "Gradient Descent(99/99): loss=0.3896550727902411, gradient=0.0003885077765643843\n",
      "Gradient Descent(0/99): loss=0.38896664790383234, gradient=0.008169575821398094\n",
      "Gradient Descent(1/99): loss=0.38896388868123305, gradient=0.005548255787682803\n",
      "Gradient Descent(2/99): loss=0.3889619719189223, gradient=0.0045629159275253845\n",
      "Gradient Descent(3/99): loss=0.38896059155428336, gradient=0.0038589374873675263\n",
      "Gradient Descent(4/99): loss=0.3889595724935458, gradient=0.0033058726518763273\n",
      "Gradient Descent(5/99): loss=0.3889588001473886, gradient=0.0028688987083858772\n",
      "Gradient Descent(6/99): loss=0.38895819822127764, gradient=0.002524264214585905\n",
      "Gradient Descent(7/99): loss=0.38895771553710096, gradient=0.0022528802482429475\n",
      "Gradient Descent(8/99): loss=0.3889573175123139, gradient=0.002039187385154645\n",
      "Gradient Descent(9/99): loss=0.38895698058902867, gradient=0.0018705493980912694\n",
      "Gradient Descent(10/99): loss=0.3889566885743013, gradient=0.0017368026161144997\n",
      "Gradient Descent(11/99): loss=0.38895643022517956, gradient=0.0016298804452811444\n",
      "Gradient Descent(12/99): loss=0.38895619764768347, gradient=0.0015434751849167719\n",
      "Gradient Descent(13/99): loss=0.3889559852301097, gradient=0.0014727247562180078\n",
      "Gradient Descent(14/99): loss=0.3889557889285481, gradient=0.0014139270840559887\n",
      "Gradient Descent(15/99): loss=0.388955605785565, gradient=0.0013642893650584985\n",
      "Gradient Descent(16/99): loss=0.38895543360394064, gradient=0.001321717168176368\n",
      "Gradient Descent(17/99): loss=0.3889552707240138, gradient=0.0012846438330208919\n",
      "Gradient Descent(18/99): loss=0.388955115870608, gradient=0.0012518968497917888\n",
      "Gradient Descent(19/99): loss=0.38895496804694923, gradient=0.0012225957978704175\n",
      "Gradient Descent(20/99): loss=0.38895482646050317, gradient=0.0011960758237951714\n",
      "Gradient Descent(21/99): loss=0.38895469047063586, gradient=0.0011718310103927302\n",
      "Gradient Descent(22/99): loss=0.3889545595512967, gradient=0.0011494728196623373\n",
      "Gradient Descent(23/99): loss=0.3889544332641231, gradient=0.0011286997413002486\n",
      "Gradient Descent(24/99): loss=0.388954311238831, gradient=0.0011092751630245679\n",
      "Gradient Descent(25/99): loss=0.38895419315874596, gradient=0.0010910112227629678\n",
      "Gradient Descent(26/99): loss=0.3889540787499928, gradient=0.0010737569920493259\n",
      "Gradient Descent(27/99): loss=0.3889539677733181, gradient=0.0010573897892299843\n",
      "Gradient Descent(28/99): loss=0.3889538600178242, gradient=0.0010418087549912912\n",
      "Gradient Descent(29/99): loss=0.38895375529611004, gradient=0.0010269300667198609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(30/99): loss=0.3889536534404532, gradient=0.001012683344493158\n",
      "Gradient Descent(31/99): loss=0.38895355429977824, gradient=0.0009990089279391344\n",
      "Gradient Descent(32/99): loss=0.38895345773721884, gradient=0.000985855793506511\n",
      "Gradient Descent(33/99): loss=0.38895336362813965, gradient=0.0009731799460507005\n",
      "Gradient Descent(34/99): loss=0.3889532718585142, gradient=0.000960943164506591\n",
      "Gradient Descent(35/99): loss=0.38895318232357995, gradient=0.0009491120141505208\n",
      "Gradient Descent(36/99): loss=0.3889530949267175, gradient=0.0009376570613682981\n",
      "Gradient Descent(37/99): loss=0.38895300957850554, gradient=0.0009265522436609281\n",
      "Gradient Descent(38/99): loss=0.38895292619592126, gradient=0.0009157743597515127\n",
      "Gradient Descent(39/99): loss=0.3889528447016561, gradient=0.0009053026534577098\n",
      "Gradient Descent(40/99): loss=0.38895276502369036, gradient=0.000895116572133662\n",
      "Gradient Descent(41/99): loss=0.38895268709427777, gradient=0.0008852032289849042\n",
      "Gradient Descent(42/99): loss=0.38895261085003574, gradient=0.0008755453125142763\n",
      "Gradient Descent(43/99): loss=0.38895253623137405, gradient=0.0008661289562581763\n",
      "Gradient Descent(44/99): loss=0.3889524631821559, gradient=0.0008569415097127239\n",
      "Gradient Descent(45/99): loss=0.3889523916494002, gradient=0.0008479713957528693\n",
      "Gradient Descent(46/99): loss=0.3889523215830161, gradient=0.0008392079888270475\n",
      "Gradient Descent(47/99): loss=0.3889522529355689, gradient=0.0008306415103642975\n",
      "Gradient Descent(48/99): loss=0.3889521856620709, gradient=0.000822262938522598\n",
      "Gradient Descent(49/99): loss=0.3889521197197922, gradient=0.0008140639299469883\n",
      "Gradient Descent(50/99): loss=0.38895205506809344, gradient=0.0008060367516363387\n",
      "Gradient Descent(51/99): loss=0.388951991668273, gradient=0.0007981742213584377\n",
      "Gradient Descent(52/99): loss=0.3889519294834301, gradient=0.0007904696553268271\n",
      "Gradient Descent(53/99): loss=0.38895186847834057, gradient=0.0007829168220739902\n",
      "Gradient Descent(54/99): loss=0.3889518086193432, gradient=0.0007755099016342476\n",
      "Gradient Descent(55/99): loss=0.3889517498742378, gradient=0.0007682434492955562\n",
      "Gradient Descent(56/99): loss=0.38895169221219034, gradient=0.000761112363299383\n",
      "Gradient Descent(57/99): loss=0.38895163560364754, gradient=0.000754111855965116\n",
      "Gradient Descent(58/99): loss=0.3889515800202588, gradient=0.0007472374277970121\n",
      "Gradient Descent(59/99): loss=0.3889515254348027, gradient=0.0007404848441989449\n",
      "Gradient Descent(60/99): loss=0.38895147182112166, gradient=0.0007338501144770886\n",
      "Gradient Descent(61/99): loss=0.38895141915406006, gradient=0.000727329472858106\n",
      "Gradient Descent(62/99): loss=0.38895136740940833, gradient=0.0007209193612885707\n",
      "Gradient Descent(63/99): loss=0.3889513165638508, gradient=0.0007146164138138588\n",
      "Gradient Descent(64/99): loss=0.3889512665949164, gradient=0.0007084174423631776\n",
      "Gradient Descent(65/99): loss=0.3889512174809356, gradient=0.000702319423788825\n",
      "Gradient Descent(66/99): loss=0.3889511692009972, gradient=0.000696319488028818\n",
      "Gradient Descent(67/99): loss=0.3889511217349106, gradient=0.0006904149072780267\n",
      "Gradient Descent(68/99): loss=0.3889510750631702, gradient=0.0006846030860670948\n",
      "Gradient Descent(69/99): loss=0.3889510291669207, gradient=0.0006788815521604982\n",
      "Gradient Descent(70/99): loss=0.3889509840279274, gradient=0.0006732479481957527\n",
      "Gradient Descent(71/99): loss=0.3889509396285458, gradient=0.0006677000239944175\n",
      "Gradient Descent(72/99): loss=0.38895089595169496, gradient=0.000662235629483204\n",
      "Gradient Descent(73/99): loss=0.3889508529808315, gradient=0.0006568527081705651\n",
      "Gradient Descent(74/99): loss=0.3889508106999262, gradient=0.0006515492911294664\n",
      "Gradient Descent(75/99): loss=0.3889507690934415, gradient=0.0006463234914424461\n",
      "Gradient Descent(76/99): loss=0.38895072814630993, gradient=0.0006411734990692104\n",
      "Gradient Descent(77/99): loss=0.38895068784391446, gradient=0.0006360975761012569\n",
      "Gradient Descent(78/99): loss=0.38895064817207037, gradient=0.0006310940523711222\n",
      "Gradient Descent(79/99): loss=0.3889506091170067, gradient=0.0006261613213872633\n",
      "Gradient Descent(80/99): loss=0.3889505706653504, gradient=0.0006212978365675344\n",
      "Gradient Descent(81/99): loss=0.38895053280411035, gradient=0.0006165021077479445\n",
      "Gradient Descent(82/99): loss=0.38895049552066285, gradient=0.0006117726979438765\n",
      "Gradient Descent(83/99): loss=0.38895045880273704, gradient=0.0006071082203444166\n",
      "Gradient Descent(84/99): loss=0.38895042263840235, gradient=0.0006025073355210391\n",
      "Gradient Descent(85/99): loss=0.38895038701605517, gradient=0.0005979687488341649\n",
      "Gradient Descent(86/99): loss=0.38895035192440697, gradient=0.0005934912080217991\n",
      "Gradient Descent(87/99): loss=0.3889503173524744, gradient=0.0005890735009572918\n",
      "Gradient Descent(88/99): loss=0.3889502832895664, gradient=0.0005847144535614746\n",
      "Gradient Descent(89/99): loss=0.38895024972527525, gradient=0.0005804129278592294\n",
      "Gradient Descent(90/99): loss=0.38895021664946616, gradient=0.0005761678201681776\n",
      "Gradient Descent(91/99): loss=0.3889501840522688, gradient=0.0005719780594103581\n",
      "Gradient Descent(92/99): loss=0.388950151924068, gradient=0.0005678426055368938\n",
      "Gradient Descent(93/99): loss=0.38895012025549464, gradient=0.0005637604480579709\n",
      "Gradient Descent(94/99): loss=0.38895008903741823, gradient=0.0005597306046693744\n",
      "Gradient Descent(95/99): loss=0.38895005826093976, gradient=0.0005557521199692174\n",
      "Gradient Descent(96/99): loss=0.38895002791738276, gradient=0.0005518240642576627\n",
      "Gradient Descent(97/99): loss=0.38894999799828744, gradient=0.0005479455324133414\n",
      "Gradient Descent(98/99): loss=0.38894996849540375, gradient=0.0005441156428416383\n",
      "Gradient Descent(99/99): loss=0.38894993940068423, gradient=0.0005403335364884286\n",
      "Gradient Descent(0/99): loss=0.3901497983975837, gradient=0.015409632847057347\n",
      "Gradient Descent(1/99): loss=0.39014587115124766, gradient=0.006922460892951744\n",
      "Gradient Descent(2/99): loss=0.390143215723281, gradient=0.005362609477860252\n",
      "Gradient Descent(3/99): loss=0.39014121769399046, gradient=0.004617818353273718\n",
      "Gradient Descent(4/99): loss=0.3901396715571156, gradient=0.004050631769354248\n",
      "Gradient Descent(5/99): loss=0.3901384489769053, gradient=0.003593089321762011\n",
      "Gradient Descent(6/99): loss=0.3901374624321527, gradient=0.0032201558293609867\n",
      "Gradient Descent(7/99): loss=0.39013665103575174, gradient=0.002913987334392682\n",
      "Gradient Descent(8/99): loss=0.3901359718051087, gradient=0.0026607509528362946\n",
      "Gradient Descent(9/99): loss=0.3901353939579147, gradient=0.0024496435373650123\n",
      "Gradient Descent(10/99): loss=0.3901348951299759, gradient=0.0022722157497464183\n",
      "Gradient Descent(11/99): loss=0.3901344588393602, gradient=0.0021218495382939375\n",
      "Gradient Descent(12/99): loss=0.390134072767148, gradient=0.0019933504986122874\n",
      "Gradient Descent(13/99): loss=0.3901337275785742, gradient=0.0018826301340294034\n",
      "Gradient Descent(14/99): loss=0.39013341610540186, gradient=0.0017864587881110325\n",
      "Gradient Descent(15/99): loss=0.39013313277225936, gradient=0.0017022740662533747\n",
      "Gradient Descent(16/99): loss=0.3901328731894724, gradient=0.001628032670046122\n",
      "Gradient Descent(17/99): loss=0.39013263386071656, gradient=0.0015620960370041773\n",
      "Gradient Descent(18/99): loss=0.39013241197069, gradient=0.0015031421748207272\n",
      "Gradient Descent(19/99): loss=0.39013220522915315, gradient=0.0014500977011417838\n",
      "Gradient Descent(20/99): loss=0.39013201175508194, gradient=0.0014020854114820587\n",
      "Gradient Descent(21/99): loss=0.39013182998967905, gradient=0.0013583837488732138\n",
      "Gradient Descent(22/99): loss=0.3901316586303509, gradient=0.0013183953817657436\n",
      "Gradient Descent(23/99): loss=0.39013149658008717, gradient=0.0012816227497079234\n",
      "Gradient Descent(24/99): loss=0.390131342908258, gradient=0.0012476489432874025\n",
      "Gradient Descent(25/99): loss=0.3901311968199706, gradient=0.0012161226751529176\n",
      "Gradient Descent(26/99): loss=0.39013105763189565, gradient=0.0011867463974439014\n",
      "Gradient Descent(27/99): loss=0.3901309247530397, gradient=0.0011592668480590363\n",
      "Gradient Descent(28/99): loss=0.3901307976693258, gradient=0.0011334674803083434\n",
      "Gradient Descent(29/99): loss=0.39013067593113643, gradient=0.0011091623606012816\n",
      "Gradient Descent(30/99): loss=0.39013055914318123, gradient=0.0010861912170424758\n",
      "Gradient Descent(31/99): loss=0.39013044695619736, gradient=0.0010644153959311715\n",
      "Gradient Descent(32/99): loss=0.39013033906011385, gradient=0.0010437145391414308\n",
      "Gradient Descent(33/99): loss=0.39013023517838924, gradient=0.0010239838377117901\n",
      "Gradient Descent(34/99): loss=0.39013013506329436, gradient=0.0010051317490960272\n",
      "Gradient Descent(35/99): loss=0.3901300384919664, gradient=0.0009870780899751078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(36/99): loss=0.39012994526309047, gradient=0.0009697524352203341\n",
      "Gradient Descent(37/99): loss=0.3901298551940976, gradient=0.000953092767951916\n",
      "Gradient Descent(38/99): loss=0.3901297681187916, gradient=0.0009370443367284594\n",
      "Gradient Descent(39/99): loss=0.39012968388532665, gradient=0.0009215586845207406\n",
      "Gradient Descent(40/99): loss=0.39012960235448296, gradient=0.000906592820866715\n",
      "Gradient Descent(41/99): loss=0.39012952339818724, gradient=0.0008921085139140585\n",
      "Gradient Descent(42/99): loss=0.39012944689824225, gradient=0.0008780716832681539\n",
      "Gradient Descent(43/99): loss=0.39012937274522935, gradient=0.0008644518779254482\n",
      "Gradient Descent(44/99): loss=0.3901293008375601, gradient=0.0008512218262744445\n",
      "Gradient Descent(45/99): loss=0.3901292310806531, gradient=0.0008383570473333648\n",
      "Gradient Descent(46/99): loss=0.3901291633862175, gradient=0.0008258355141734975\n",
      "Gradient Descent(47/99): loss=0.39012909767162823, gradient=0.0008136373619336489\n",
      "Gradient Descent(48/99): loss=0.3901290338593777, gradient=0.0008017446340317922\n",
      "Gradient Descent(49/99): loss=0.39012897187659673, gradient=0.0007901410611715394\n",
      "Gradient Descent(50/99): loss=0.3901289116546317, gradient=0.0007788118685651792\n",
      "Gradient Descent(51/99): loss=0.39012885312867124, gradient=0.0007677436074846784\n",
      "Gradient Descent(52/99): loss=0.39012879623741714, gradient=0.0007569240078260801\n",
      "Gradient Descent(53/99): loss=0.39012874092279093, gradient=0.0007463418488589575\n",
      "Gradient Descent(54/99): loss=0.3901286871296749, gradient=0.0007359868457395885\n",
      "Gradient Descent(55/99): loss=0.39012863480567933, gradient=0.0007258495497119323\n",
      "Gradient Descent(56/99): loss=0.3901285839009354, gradient=0.0007159212602134171\n",
      "Gradient Descent(57/99): loss=0.3901285343679095, gradient=0.0007061939473502418\n",
      "Gradient Descent(58/99): loss=0.39012848616123663, gradient=0.0006966601834193777\n",
      "Gradient Descent(59/99): loss=0.3901284392375696, gradient=0.0006873130823337895\n",
      "Gradient Descent(60/99): loss=0.3901283935554439, gradient=0.0006781462459632926\n",
      "Gradient Descent(61/99): loss=0.39012834907515426, gradient=0.0006691537165338145\n",
      "Gradient Descent(62/99): loss=0.39012830575864355, gradient=0.0006603299343428017\n",
      "Gradient Descent(63/99): loss=0.390128263569402, gradient=0.0006516697001446586\n",
      "Gradient Descent(64/99): loss=0.3901282224723741, gradient=0.0006431681416448731\n",
      "Gradient Descent(65/99): loss=0.39012818243387426, gradient=0.0006348206836132171\n",
      "Gradient Descent(66/99): loss=0.3901281434215099, gradient=0.0006266230211888391\n",
      "Gradient Descent(67/99): loss=0.3901281054041102, gradient=0.0006185710960044715\n",
      "Gradient Descent(68/99): loss=0.3901280683516612, gradient=0.0006106610748028899\n",
      "Gradient Descent(69/99): loss=0.3901280322352447, gradient=0.0006028893302601679\n",
      "Gradient Descent(70/99): loss=0.3901279970269843, gradient=0.0005952524237644197\n",
      "Gradient Descent(71/99): loss=0.39012796269999184, gradient=0.0005877470899306491\n",
      "Gradient Descent(72/99): loss=0.39012792922832157, gradient=0.0005803702226569767\n",
      "Gradient Descent(73/99): loss=0.390127896586924, gradient=0.0005731188625531037\n",
      "Gradient Descent(74/99): loss=0.390127864751606, gradient=0.000565990185589604\n",
      "Gradient Descent(75/99): loss=0.3901278336989917, gradient=0.0005589814928367236\n",
      "Gradient Descent(76/99): loss=0.3901278034064856, gradient=0.0005520902011748701\n",
      "Gradient Descent(77/99): loss=0.39012777385224034, gradient=0.0005453138348734965\n",
      "Gradient Descent(78/99): loss=0.39012774501512315, gradient=0.0005386500179474653\n",
      "Gradient Descent(79/99): loss=0.39012771687468784, gradient=0.0005320964672083145\n",
      "Gradient Descent(80/99): loss=0.3901276894111453, gradient=0.0005256509859403893\n",
      "Gradient Descent(81/99): loss=0.39012766260533815, gradient=0.0005193114581360307\n",
      "Gradient Descent(82/99): loss=0.39012763643871534, gradient=0.0005130758432345573\n",
      "Gradient Descent(83/99): loss=0.39012761089330833, gradient=0.0005069421713134075\n",
      "Gradient Descent(84/99): loss=0.39012758595170927, gradient=0.0005009085386871162\n",
      "Gradient Descent(85/99): loss=0.3901275615970495, gradient=0.0004949731038736967\n",
      "Gradient Descent(86/99): loss=0.3901275378129796, gradient=0.0004891340838927728\n",
      "Gradient Descent(87/99): loss=0.39012751458365036, gradient=0.0004833897508633173\n",
      "Gradient Descent(88/99): loss=0.3901274918936946, gradient=0.00047773842887250295\n",
      "Gradient Descent(89/99): loss=0.39012746972821, gradient=0.00047217849109035584\n",
      "Gradient Descent(90/99): loss=0.39012744807274247, gradient=0.00046670835710648087\n",
      "Gradient Descent(91/99): loss=0.3901274269132708, gradient=0.0004613264904694257\n",
      "Gradient Descent(92/99): loss=0.39012740623619174, gradient=0.00045603139640928536\n",
      "Gradient Descent(93/99): loss=0.39012738602830427, gradient=0.0004508216197274214\n",
      "Gradient Descent(94/99): loss=0.39012736627679834, gradient=0.0004456957428384488\n",
      "Gradient Descent(95/99): loss=0.3901273469692399, gradient=0.0004406523839507944\n",
      "Gradient Descent(96/99): loss=0.3901273280935591, gradient=0.00043569019537409177\n",
      "Gradient Descent(97/99): loss=0.39012730963803816, gradient=0.0004308078619420986\n",
      "Gradient Descent(98/99): loss=0.3901272915912999, gradient=0.00042600409954178856\n",
      "Gradient Descent(99/99): loss=0.3901272739422962, gradient=0.0004212776537393181\n",
      "Gradient Descent(0/99): loss=0.3903611225211951, gradient=0.00726716460169523\n",
      "Gradient Descent(1/99): loss=0.39035744789293897, gradient=0.006254489957145211\n",
      "Gradient Descent(2/99): loss=0.39035456242915506, gradient=0.005522458681644036\n",
      "Gradient Descent(3/99): loss=0.3903522374135369, gradient=0.00494307614007486\n",
      "Gradient Descent(4/99): loss=0.3903503210536325, gradient=0.004476364463201999\n",
      "Gradient Descent(5/99): loss=0.3903487097072686, gradient=0.004095601107572348\n",
      "Gradient Descent(6/99): loss=0.39034733115131753, gradient=0.0037809337875279827\n",
      "Gradient Descent(7/99): loss=0.3903461340404571, gradient=0.003517498346391858\n",
      "Gradient Descent(8/99): loss=0.39034508114445327, gradient=0.003294135811449637\n",
      "Gradient Descent(9/99): loss=0.39034414494054825, gradient=0.003102451914514963\n",
      "Gradient Descent(10/99): loss=0.39034330471071643, gradient=0.0029360643386980687\n",
      "Gradient Descent(11/99): loss=0.3903425445865432, gradient=0.0027900996761219413\n",
      "Gradient Descent(12/99): loss=0.39034185221826273, gradient=0.0026607938253376423\n",
      "Gradient Descent(13/99): loss=0.39034121785054327, gradient=0.0025452137440677147\n",
      "Gradient Descent(14/99): loss=0.39034063367251864, gradient=0.0024410440507642178\n",
      "Gradient Descent(15/99): loss=0.3903400933458195, gradient=0.0023464648900751593\n",
      "Gradient Descent(16/99): loss=0.3903395916701045, gradient=0.002259998794670038\n",
      "Gradient Descent(17/99): loss=0.3903391243303844, gradient=0.00218045764778876\n",
      "Gradient Descent(18/99): loss=0.3903386877094097, gradient=0.0021068739356451677\n",
      "Gradient Descent(19/99): loss=0.3903382787455982, gradient=0.0020384542511325747\n",
      "Gradient Descent(20/99): loss=0.3903378948242382, gradient=0.001974543712687607\n",
      "Gradient Descent(21/99): loss=0.3903375336933172, gradient=0.0019145984604269576\n",
      "Gradient Descent(22/99): loss=0.39033719339778167, gradient=0.0018581641965144783\n",
      "Gradient Descent(23/99): loss=0.3903368722277476, gradient=0.001804859299220544\n",
      "Gradient Descent(24/99): loss=0.39033656867757455, gradient=0.0017543601307759647\n",
      "Gradient Descent(25/99): loss=0.3903362814122689, gradient=0.0017063957706033965\n",
      "Gradient Descent(26/99): loss=0.39033600924184064, gradient=0.001660730059582025\n",
      "Gradient Descent(27/99): loss=0.39033575109903196, gradient=0.0016171642831827417\n",
      "Gradient Descent(28/99): loss=0.39033550602219524, gradient=0.0015755243482383297\n",
      "Gradient Descent(29/99): loss=0.3903352731406523, gradient=0.001535660049740893\n",
      "Gradient Descent(30/99): loss=0.39033505166271526, gradient=0.0014974403574954315\n",
      "Gradient Descent(31/99): loss=0.3903348408657097, gradient=0.0014607503497146104\n",
      "Gradient Descent(32/99): loss=0.39033464008805296, gradient=0.0014254855268533296\n",
      "Gradient Descent(33/99): loss=0.39033444872085526, gradient=0.0013915624767697962\n",
      "Gradient Descent(34/99): loss=0.390334266203333, gradient=0.0013588978642306018\n",
      "Gradient Descent(35/99): loss=0.39033409201723296, gradient=0.0013274199581163376\n",
      "Gradient Descent(36/99): loss=0.3903339256824582, gradient=0.0012970641661520746\n",
      "Gradient Descent(37/99): loss=0.3903337667532943, gradient=0.0012677720341920172\n",
      "Gradient Descent(38/99): loss=0.3903336148151364, gradient=0.001239490406984765\n",
      "Gradient Descent(39/99): loss=0.3903334694816325, gradient=0.0012121707222592926\n",
      "Gradient Descent(40/99): loss=0.39033333039218193, gradient=0.001185768415179877\n",
      "Gradient Descent(41/99): loss=0.3903331972097293, gradient=0.0011602424144151271\n",
      "Gradient Descent(42/99): loss=0.39033306961881287, gradient=0.0011355547144529835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(43/99): loss=0.3903329473238281, gradient=0.0011116700115395572\n",
      "Gradient Descent(44/99): loss=0.39033283004747715, gradient=0.0010885553928496564\n",
      "Gradient Descent(45/99): loss=0.3903327175293781, gradient=0.001066180070310229\n",
      "Gradient Descent(46/99): loss=0.39033260952481263, gradient=0.0010445151519799516\n",
      "Gradient Descent(47/99): loss=0.3903325058035949, gradient=0.0010235334450985202\n",
      "Gradient Descent(48/99): loss=0.3903324061490445, gradient=0.0010032092859121743\n",
      "Gradient Descent(49/99): loss=0.39033231035705374, gradient=0.0009835183921971956\n",
      "Gradient Descent(50/99): loss=0.3903322182352348, gradient=0.0009644377350738897\n",
      "Gradient Descent(51/99): loss=0.3903321296021406, gradient=0.0009459454272579208\n",
      "Gradient Descent(52/99): loss=0.39033204428654944, gradient=0.0009280206253535339\n",
      "Gradient Descent(53/99): loss=0.3903319621268072, gradient=0.0009106434441720805\n",
      "Gradient Descent(54/99): loss=0.39033188297022015, gradient=0.00089379488137533\n",
      "Gradient Descent(55/99): loss=0.3903318066724962, gradient=0.0008774567510039923\n",
      "Gradient Descent(56/99): loss=0.390331733097225, gradient=0.0008616116246722145\n",
      "Gradient Descent(57/99): loss=0.39033166211539805, gradient=0.0008462427793899496\n",
      "Gradient Descent(58/99): loss=0.39033159360496195, gradient=0.0008313341511303052\n",
      "Gradient Descent(59/99): loss=0.3903315274504042, gradient=0.000816870293385721\n",
      "Gradient Descent(60/99): loss=0.39033146354236553, gradient=0.0008028363400669166\n",
      "Gradient Descent(61/99): loss=0.3903314017772795, gradient=0.0007892179721892566\n",
      "Gradient Descent(62/99): loss=0.3903313420570368, gradient=0.0007760013878683801\n",
      "Gradient Descent(63/99): loss=0.39033128428866976, gradient=0.000763173275213007\n",
      "Gradient Descent(64/99): loss=0.39033122838405837, gradient=0.0007507207877589102\n",
      "Gradient Descent(65/99): loss=0.3903311742596551, gradient=0.000738631522134415\n",
      "Gradient Descent(66/99): loss=0.39033112183622587, gradient=0.0007268934976899197\n",
      "Gradient Descent(67/99): loss=0.39033107103860903, gradient=0.0007154951378576324\n",
      "Gradient Descent(68/99): loss=0.3903310217954863, gradient=0.0007044252530382269\n",
      "Gradient Descent(69/99): loss=0.3903309740391713, gradient=0.0006936730248365702\n",
      "Gradient Descent(70/99): loss=0.39033092770540745, gradient=0.0006832279914911119\n",
      "Gradient Descent(71/99): loss=0.3903308827331797, gradient=0.0006730800343606757\n",
      "Gradient Descent(72/99): loss=0.3903308390645372, gradient=0.0006632193653488363\n",
      "Gradient Descent(73/99): loss=0.3903307966444268, gradient=0.0006536365151613825\n",
      "Gradient Descent(74/99): loss=0.3903307554205343, gradient=0.0006443223223038814\n",
      "Gradient Descent(75/99): loss=0.39033071534313774, gradient=0.0006352679227388745\n",
      "Gradient Descent(76/99): loss=0.3903306763649669, gradient=0.0006264647401302913\n",
      "Gradient Descent(77/99): loss=0.3903306384410722, gradient=0.0006179044766128588\n",
      "Gradient Descent(78/99): loss=0.3903306015287004, gradient=0.000609579104030519\n",
      "Gradient Descent(79/99): loss=0.3903305655871775, gradient=0.0006014808555952506\n",
      "Gradient Descent(80/99): loss=0.39033053057779815, gradient=0.0005936022179231173\n",
      "Gradient Descent(81/99): loss=0.39033049646372153, gradient=0.0005859359234099616\n",
      "Gradient Descent(82/99): loss=0.390330463209873, gradient=0.0005784749429131258\n",
      "Gradient Descent(83/99): loss=0.3903304307828508, gradient=0.000571212478710366\n",
      "Gradient Descent(84/99): loss=0.3903303991508388, gradient=0.0005641419577101947\n",
      "Gradient Descent(85/99): loss=0.39033036828352347, gradient=0.0005572570248912255\n",
      "Gradient Descent(86/99): loss=0.3903303381520147, gradient=0.0005505515369513089\n",
      "Gradient Descent(87/99): loss=0.390330308728773, gradient=0.0005440195561487745\n",
      "Gradient Descent(88/99): loss=0.39033027998753833, gradient=0.0005376553443218047\n",
      "Gradient Descent(89/99): loss=0.3903302519032658, gradient=0.0005314533570723561\n",
      "Gradient Descent(90/99): loss=0.3903302244520598, gradient=0.0005254082381040337\n",
      "Gradient Descent(91/99): loss=0.3903301976111188, gradient=0.0005195148137042564\n",
      "Gradient Descent(92/99): loss=0.3903301713586766, gradient=0.0005137680873624068\n",
      "Gradient Descent(93/99): loss=0.39033014567395025, gradient=0.0005081632345168973\n",
      "Gradient Descent(94/99): loss=0.3903301205370905, gradient=0.0005026955974257436\n",
      "Gradient Descent(95/99): loss=0.39033009592913326, gradient=0.0004973606801546299\n",
      "Gradient Descent(96/99): loss=0.39033007183195595, gradient=0.000492154143679158\n",
      "Gradient Descent(97/99): loss=0.39033004822823497, gradient=0.0004870718010970141\n",
      "Gradient Descent(98/99): loss=0.3903300251014042, gradient=0.0004821096129472455\n",
      "Gradient Descent(99/99): loss=0.39033000243561866, gradient=0.00047726368263429286\n",
      "Gradient Descent(0/99): loss=0.389980775193171, gradient=0.010008845355850806\n",
      "Gradient Descent(1/99): loss=0.38997454230324274, gradient=0.00824390053643973\n",
      "Gradient Descent(2/99): loss=0.38997013185994783, gradient=0.006907328049916819\n",
      "Gradient Descent(3/99): loss=0.3899668996203929, gradient=0.005889037556787925\n",
      "Gradient Descent(4/99): loss=0.3899644457471271, gradient=0.005110496016002436\n",
      "Gradient Descent(5/99): loss=0.3899625189498608, gradient=0.004511369982665409\n",
      "Gradient Descent(6/99): loss=0.3899609589805661, gradient=0.004045537123764941\n",
      "Gradient Descent(7/99): loss=0.389959661888272, gradient=0.003678215755224472\n",
      "Gradient Descent(8/99): loss=0.38995855889061454, gradient=0.0033836130724882573\n",
      "Gradient Descent(9/99): loss=0.38995760345001507, gradient=0.0031428743268470775\n",
      "Gradient Descent(10/99): loss=0.38995676330559725, gradient=0.0029423635011048784\n",
      "Gradient Descent(11/99): loss=0.38995601551465275, gradient=0.0027722746920682364\n",
      "Gradient Descent(12/99): loss=0.3899553433327421, gradient=0.002625558156141415\n",
      "Gradient Descent(13/99): loss=0.389954734225164, gradient=0.0024971207151987603\n",
      "Gradient Descent(14/99): loss=0.38995417858055864, gradient=0.00238324794949059\n",
      "Gradient Descent(15/99): loss=0.38995366886466903, gradient=0.002281195515554342\n",
      "Gradient Descent(16/99): loss=0.38995319905328846, gradient=0.0021889040454568932\n",
      "Gradient Descent(17/99): loss=0.38995276424469255, gradient=0.002104801682329116\n",
      "Gradient Descent(18/99): loss=0.38995236039028564, gradient=0.002027662194482446\n",
      "Gradient Descent(19/99): loss=0.38995198409869475, gradient=0.001956531587451902\n",
      "Gradient Descent(20/99): loss=0.3899516324989641, gradient=0.0018906299265146471\n",
      "Gradient Descent(21/99): loss=0.3899513031355204, gradient=0.0018293258877202152\n",
      "Gradient Descent(22/99): loss=0.3899509938896622, gradient=0.0017720980245054343\n",
      "Gradient Descent(23/99): loss=0.38995070291923883, gradient=0.0017185104909366335\n",
      "Gradient Descent(24/99): loss=0.38995042861211443, gradient=0.0016681917923992726\n",
      "Gradient Descent(25/99): loss=0.389950169547285, gradient=0.0016208349855095449\n",
      "Gradient Descent(26/99): loss=0.38994992446722404, gradient=0.0015761640309308643\n",
      "Gradient Descent(27/99): loss=0.38994969225195164, gradient=0.0015339502802660012\n",
      "Gradient Descent(28/99): loss=0.3899494719005556, gradient=0.0014939872190307963\n",
      "Gradient Descent(29/99): loss=0.38994926251452017, gradient=0.0014560958905134051\n",
      "Gradient Descent(30/99): loss=0.38994906328422574, gradient=0.0014201183142833603\n",
      "Gradient Descent(31/99): loss=0.38994887347764917, gradient=0.0013859142656557331\n",
      "Gradient Descent(32/99): loss=0.38994869243084285, gradient=0.0013533586398176056\n",
      "Gradient Descent(33/99): loss=0.38994851953986415, gradient=0.0013223392783507147\n",
      "Gradient Descent(34/99): loss=0.3899483542538931, gradient=0.0012927551653304802\n",
      "Gradient Descent(35/99): loss=0.3899481960693299, gradient=0.0012645149215475375\n",
      "Gradient Descent(36/99): loss=0.3899480445247111, gradient=0.0012375355411493802\n",
      "Gradient Descent(37/99): loss=0.3899478991963042, gradient=0.0012117413267836513\n",
      "Gradient Descent(38/99): loss=0.3899477596942761, gradient=0.001187062988268666\n",
      "Gradient Descent(39/99): loss=0.38994762565934504, gradient=0.0011634368766947115\n",
      "Gradient Descent(40/99): loss=0.3899474967598409, gradient=0.0011408043312106542\n",
      "Gradient Descent(41/99): loss=0.38994737268911733, gradient=0.001119111119961419\n",
      "Gradient Descent(42/99): loss=0.3899472531632606, gradient=0.0010983069599819525\n",
      "Gradient Descent(43/99): loss=0.38994713791905866, gradient=0.0010783451035307058\n",
      "Gradient Descent(44/99): loss=0.38994702671219117, gradient=0.0010591819805014725\n",
      "Gradient Descent(45/99): loss=0.3899469193156112, gradient=0.0010407768883046674\n",
      "Gradient Descent(46/99): loss=0.3899468155180973, gradient=0.0010230917220370966\n",
      "Gradient Descent(47/99): loss=0.38994671512295076, gradient=0.0010060907389320003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(48/99): loss=0.3899466179468223, gradient=0.0009897403520469725\n",
      "Gradient Descent(49/99): loss=0.3899465238186509, gradient=0.0009740089489459397\n",
      "Gradient Descent(50/99): loss=0.3899464325787049, gradient=0.0009588667317948817\n",
      "Gradient Descent(51/99): loss=0.38994634407770845, gradient=0.000944285575844434\n",
      "Gradient Descent(52/99): loss=0.3899462581760515, gradient=0.0009302389037329628\n",
      "Gradient Descent(53/99): loss=0.3899461747430647, gradient=0.0009167015734329156\n",
      "Gradient Descent(54/99): loss=0.38994609365636135, gradient=0.0009036497779863305\n",
      "Gradient Descent(55/99): loss=0.38994601480123364, gradient=0.0008910609554518952\n",
      "Gradient Descent(56/99): loss=0.38994593807010003, gradient=0.0008789137077156256\n",
      "Gradient Descent(57/99): loss=0.3899458633619974, gradient=0.0008671877270139747\n",
      "Gradient Descent(58/99): loss=0.3899457905821171, gradient=0.0008558637291844268\n",
      "Gradient Descent(59/99): loss=0.38994571964137437, gradient=0.0008449233927993056\n",
      "Gradient Descent(60/99): loss=0.3899456504560137, gradient=0.0008343493034586976\n",
      "Gradient Descent(61/99): loss=0.38994558294724563, gradient=0.0008241249026209227\n",
      "Gradient Descent(62/99): loss=0.3899455170409101, gradient=0.0008142344404360296\n",
      "Gradient Descent(63/99): loss=0.3899454526671649, gradient=0.0008046629321223055\n",
      "Gradient Descent(64/99): loss=0.38994538976020016, gradient=0.0007953961174895095\n",
      "Gradient Descent(65/99): loss=0.3899453282579699, gradient=0.0007864204232671081\n",
      "Gradient Descent(66/99): loss=0.38994526810194696, gradient=0.0007777229279423393\n",
      "Gradient Descent(67/99): loss=0.38994520923689385, gradient=0.0007692913288530134\n",
      "Gradient Descent(68/99): loss=0.38994515161065024, gradient=0.0007611139113138477\n",
      "Gradient Descent(69/99): loss=0.38994509517393594, gradient=0.0007531795195848156\n",
      "Gradient Descent(70/99): loss=0.38994503988016743, gradient=0.0007454775295150344\n",
      "Gradient Descent(71/99): loss=0.3899449856852878, gradient=0.0007379978227168005\n",
      "Gradient Descent(72/99): loss=0.38994493254760815, gradient=0.0007307307621436175\n",
      "Gradient Descent(73/99): loss=0.38994488042765857, gradient=0.0007236671689608585\n",
      "Gradient Descent(74/99): loss=0.3899448292880532, gradient=0.0007167983006122814\n",
      "Gradient Descent(75/99): loss=0.3899447790933589, gradient=0.0007101158299961352\n",
      "Gradient Descent(76/99): loss=0.38994472980997796, gradient=0.0007036118256756309\n",
      "Gradient Descent(77/99): loss=0.3899446814060357, gradient=0.0006972787330557858\n",
      "Gradient Descent(78/99): loss=0.3899446338512763, gradient=0.0006911093564670795\n",
      "Gradient Descent(79/99): loss=0.3899445871169652, gradient=0.0006850968421019328\n",
      "Gradient Descent(80/99): loss=0.3899445411757993, gradient=0.0006792346617553957\n",
      "Gradient Descent(81/99): loss=0.38994449600182074, gradient=0.0006735165973261817\n",
      "Gradient Descent(82/99): loss=0.3899444515703381, gradient=0.0006679367260383504\n",
      "Gradient Descent(83/99): loss=0.389944407857853, gradient=0.0006624894063463721\n",
      "Gradient Descent(84/99): loss=0.3899443648419886, gradient=0.0006571692644907272\n",
      "Gradient Descent(85/99): loss=0.38994432250142685, gradient=0.0006519711816722558\n",
      "Gradient Descent(86/99): loss=0.38994428081584576, gradient=0.0006468902818164733\n",
      "Gradient Descent(87/99): loss=0.3899442397658634, gradient=0.0006419219199011081\n",
      "Gradient Descent(88/99): loss=0.3899441993329846, gradient=0.0006370616708208889\n",
      "Gradient Descent(89/99): loss=0.3899441594995498, gradient=0.0006323053187664383\n",
      "Gradient Descent(90/99): loss=0.38994412024868913, gradient=0.0006276488470939165\n",
      "Gradient Descent(91/99): loss=0.38994408156427807, gradient=0.000623088428664643\n",
      "Gradient Descent(92/99): loss=0.38994404343089606, gradient=0.000618620416633852\n",
      "Gradient Descent(93/99): loss=0.38994400583378813, gradient=0.0006142413356695974\n",
      "Gradient Descent(94/99): loss=0.3899439687588273, gradient=0.0006099478735828341\n",
      "Gradient Descent(95/99): loss=0.38994393219248263, gradient=0.000605736873351139\n",
      "Gradient Descent(96/99): loss=0.38994389612178504, gradient=0.0006016053255192538\n",
      "Gradient Descent(97/99): loss=0.389943860534298, gradient=0.0005975503609595505\n",
      "Gradient Descent(98/99): loss=0.38994382541808964, gradient=0.000593569243977495\n",
      "Gradient Descent(99/99): loss=0.3899437907617052, gradient=0.0005896593657464483\n",
      "Gradient Descent(0/99): loss=0.38967833098490967, gradient=0.011114411211109792\n",
      "Gradient Descent(1/99): loss=0.38967333857949404, gradient=0.007452693815358705\n",
      "Gradient Descent(2/99): loss=0.38966979922367095, gradient=0.006185740372362596\n",
      "Gradient Descent(3/99): loss=0.3896672002571462, gradient=0.0052843811420205345\n",
      "Gradient Descent(4/99): loss=0.3896652528725052, gradient=0.004563787718038706\n",
      "Gradient Descent(5/99): loss=0.3896637652623767, gradient=0.00397988980427507\n",
      "Gradient Descent(6/99): loss=0.38966260706698985, gradient=0.0035040040015678576\n",
      "Gradient Descent(7/99): loss=0.38966168864212936, gradient=0.0031137340861358154\n",
      "Gradient Descent(8/99): loss=0.38966094760465425, gradient=0.0027913881962990574\n",
      "Gradient Descent(9/99): loss=0.38966033998225946, gradient=0.0025230269085965646\n",
      "Gradient Descent(10/99): loss=0.38965983435219725, gradient=0.0022977091709119237\n",
      "Gradient Descent(11/99): loss=0.38965940793362336, gradient=0.0021068747864812607\n",
      "Gradient Descent(12/99): loss=0.3896590439633357, gradient=0.0019438393704329952\n",
      "Gradient Descent(13/99): loss=0.38965872991913436, gradient=0.0018033847156528277\n",
      "Gradient Descent(14/99): loss=0.3896584563062906, gradient=0.0016814304031758757\n",
      "Gradient Descent(15/99): loss=0.38965821582052657, gradient=0.0015747739185100042\n",
      "Gradient Descent(16/99): loss=0.3896580027645344, gradient=0.0014808875459095341\n",
      "Gradient Descent(17/99): loss=0.38965781263657, gradient=0.001397761395620169\n",
      "Gradient Descent(18/99): loss=0.3896576418368352, gradient=0.0013237831933122558\n",
      "Gradient Descent(19/99): loss=0.3896574874552533, gradient=0.0012576468579899485\n",
      "Gradient Descent(20/99): loss=0.38965734711605704, gradient=0.0011982832934491598\n",
      "Gradient Descent(21/99): loss=0.3896572188624709, gradient=0.0011448081157533106\n",
      "Gradient Descent(22/99): loss=0.3896571010700167, gradient=0.0010964821732811468\n",
      "Gradient Descent(23/99): loss=0.38965699238049634, gradient=0.0010526816635241657\n",
      "Gradient Descent(24/99): loss=0.3896568916511037, gradient=0.0010128754159964002\n",
      "Gradient Descent(25/99): loss=0.38965679791474356, gradient=0.0009766075126338826\n",
      "Gradient Descent(26/99): loss=0.3896567103487509, gradient=0.000943483881404681\n",
      "Gradient Descent(27/99): loss=0.3896566282499925, gradient=0.0009131618516039914\n",
      "Gradient Descent(28/99): loss=0.3896565510148648, gradient=0.0008853419241688184\n",
      "Gradient Descent(29/99): loss=0.389656478123099, gradient=0.0008597612074064216\n",
      "Gradient Descent(30/99): loss=0.3896564091245474, gradient=0.0008361881140999253\n",
      "Gradient Descent(31/99): loss=0.38965634362833546, gradient=0.0008144180228801392\n",
      "Gradient Descent(32/99): loss=0.38965628129389923, gradient=0.0007942696849457504\n",
      "Gradient Descent(33/99): loss=0.3896562218235446, gradient=0.0007755822142053883\n",
      "Gradient Descent(34/99): loss=0.3896561649562384, gradient=0.0007582125403583181\n",
      "Gradient Descent(35/99): loss=0.3896561104624032, gradient=0.0007420332345355031\n",
      "Gradient Descent(36/99): loss=0.3896560581395402, gradient=0.0007269306389996813\n",
      "Gradient Descent(37/99): loss=0.389656007808529, gradient=0.0007128032483324687\n",
      "Gradient Descent(38/99): loss=0.38965595931049424, gradient=0.0006995603011825866\n",
      "Gradient Descent(39/99): loss=0.38965591250414033, gradient=0.0006871205502195585\n",
      "Gradient Descent(40/99): loss=0.38965586726347845, gradient=0.0006754111843024281\n",
      "Gradient Descent(41/99): loss=0.3896558234758811, gradient=0.000664366881654819\n",
      "Gradient Descent(42/99): loss=0.389655781040416, gradient=0.0006539289764843382\n",
      "Gradient Descent(43/99): loss=0.38965573986640933, gradient=0.0006440447243116225\n",
      "Gradient Descent(44/99): loss=0.38965569987221044, gradient=0.0006346666535040638\n",
      "Gradient Descent(45/99): loss=0.38965566098412113, gradient=0.0006257519923027736\n",
      "Gradient Descent(46/99): loss=0.38965562313546936, gradient=0.0006172621620987806\n",
      "Gradient Descent(47/99): loss=0.38965558626580304, gradient=0.0006091623289336835\n",
      "Gradient Descent(48/99): loss=0.38965555032019017, gradient=0.0006014210062295772\n",
      "Gradient Descent(49/99): loss=0.3896555152486055, gradient=0.0005940097026304117\n",
      "Gradient Descent(50/99): loss=0.3896554810053954, gradient=0.0005869026095937788\n",
      "Gradient Descent(51/99): loss=0.38965544754880976, gradient=0.0005800763240274346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(52/99): loss=0.38965541484058785, gradient=0.0005735096018368663\n",
      "Gradient Descent(53/99): loss=0.3896553828455973, gradient=0.0005671831387507987\n",
      "Gradient Descent(54/99): loss=0.38965535153151243, gradient=0.0005610793752298306\n",
      "Gradient Descent(55/99): loss=0.38965532086853205, gradient=0.0005551823226486982\n",
      "Gradient Descent(56/99): loss=0.3896552908291296, gradient=0.0005494774082816974\n",
      "Gradient Descent(57/99): loss=0.38965526138782947, gradient=0.0005439513369174388\n",
      "Gradient Descent(58/99): loss=0.38965523252101103, gradient=0.0005385919671917776\n",
      "Gradient Descent(59/99): loss=0.3896552042067319, gradient=0.0005333882009561213\n",
      "Gradient Descent(60/99): loss=0.3896551764245716, gradient=0.0005283298842013662\n",
      "Gradient Descent(61/99): loss=0.38965514915549104, gradient=0.0005234077182342113\n",
      "Gradient Descent(62/99): loss=0.38965512238170846, gradient=0.0005186131799582062\n",
      "Gradient Descent(63/99): loss=0.38965509608658583, gradient=0.0005139384502483693\n",
      "Gradient Descent(64/99): loss=0.38965507025452856, gradient=0.0005093763495278763\n",
      "Gradient Descent(65/99): loss=0.38965504487089575, gradient=0.0005049202797604719\n",
      "Gradient Descent(66/99): loss=0.38965501992191826, gradient=0.0005005641721640815\n",
      "Gradient Descent(67/99): loss=0.38965499539462484, gradient=0.000496302440032223\n",
      "Gradient Descent(68/99): loss=0.3896549712767769, gradient=0.0004921299361203947\n",
      "Gradient Descent(69/99): loss=0.38965494755680813, gradient=0.0004880419141174077\n",
      "Gradient Descent(70/99): loss=0.38965492422377096, gradient=0.0004840339937755562\n",
      "Gradient Descent(71/99): loss=0.3896549012672868, gradient=0.0004801021293224018\n",
      "Gradient Descent(72/99): loss=0.3896548786775025, gradient=0.0004762425808188313\n",
      "Gradient Descent(73/99): loss=0.38965485644504944, gradient=0.00047245188816500495\n",
      "Gradient Descent(74/99): loss=0.3896548345610077, gradient=0.0004687268474891246\n",
      "Gradient Descent(75/99): loss=0.3896548130168719, gradient=0.0004650644896823487\n",
      "Gradient Descent(76/99): loss=0.3896547918045212, gradient=0.0004614620608694185\n",
      "Gradient Descent(77/99): loss=0.38965477091619277, gradient=0.0004579170046257504\n",
      "Gradient Descent(78/99): loss=0.3896547503444543, gradient=0.000454426945773516\n",
      "Gradient Descent(79/99): loss=0.3896547300821837, gradient=0.0004509896756048786\n",
      "Gradient Descent(80/99): loss=0.3896547101225458, gradient=0.00044760313839830153\n",
      "Gradient Descent(81/99): loss=0.3896546904589745, gradient=0.00044426541910531816\n",
      "Gradient Descent(82/99): loss=0.38965467108515484, gradient=0.00044097473210016685\n",
      "Gradient Descent(83/99): loss=0.38965465199500626, gradient=0.0004377294108931373\n",
      "Gradient Descent(84/99): loss=0.3896546331826688, gradient=0.0004345278987199868\n",
      "Gradient Descent(85/99): loss=0.3896546146424888, gradient=0.0004313687399277805\n",
      "Gradient Descent(86/99): loss=0.38965459636900657, gradient=0.00042825057208550785\n",
      "Gradient Descent(87/99): loss=0.3896545783569446, gradient=0.00042517211875416874\n",
      "Gradient Descent(88/99): loss=0.38965456060119774, gradient=0.00042213218285874494\n",
      "Gradient Descent(89/99): loss=0.3896545430968229, gradient=0.00041912964060781616\n",
      "Gradient Descent(90/99): loss=0.38965452583902965, gradient=0.00041616343591392404\n",
      "Gradient Descent(91/99): loss=0.3896545088231727, gradient=0.0004132325752703661\n",
      "Gradient Descent(92/99): loss=0.38965449204474395, gradient=0.00041033612304556714\n",
      "Gradient Descent(93/99): loss=0.38965447549936505, gradient=0.0004074731971586964\n",
      "Gradient Descent(94/99): loss=0.38965445918278097, gradient=0.0004046429651041782\n",
      "Gradient Descent(95/99): loss=0.38965444309085406, gradient=0.0004018446402954002\n",
      "Gradient Descent(96/99): loss=0.3896544272195582, gradient=0.0003990774787009366\n",
      "Gradient Descent(97/99): loss=0.3896544115649737, gradient=0.0003963407757481712\n",
      "Gradient Descent(98/99): loss=0.3896543961232826, gradient=0.000393633863472528\n",
      "Gradient Descent(99/99): loss=0.389654380890763, gradient=0.0003909561078915869\n",
      "Gradient Descent(0/99): loss=0.38896575666870115, gradient=0.008171636411343195\n",
      "Gradient Descent(1/99): loss=0.38896299633778136, gradient=0.005549354528090344\n",
      "Gradient Descent(2/99): loss=0.3889610786727455, gradient=0.004563943714792061\n",
      "Gradient Descent(3/99): loss=0.38895969751903253, gradient=0.0038599876548785082\n",
      "Gradient Descent(4/99): loss=0.3889586777456857, gradient=0.0033069725101590986\n",
      "Gradient Descent(5/99): loss=0.38895790473977204, gradient=0.002870066216997795\n",
      "Gradient Descent(6/99): loss=0.38895730219114266, gradient=0.002525511514117892\n",
      "Gradient Descent(7/99): loss=0.38895681891115164, gradient=0.002254214103772882\n",
      "Gradient Descent(8/99): loss=0.3889564203100824, gradient=0.0020406097838086106\n",
      "Gradient Descent(9/99): loss=0.38895608282511257, gradient=0.0018720584880631043\n",
      "Gradient Descent(10/99): loss=0.38895579025987204, gradient=0.0017383938487645606\n",
      "Gradient Descent(11/99): loss=0.3889555313690109, gradient=0.0016315477068079164\n",
      "Gradient Descent(12/99): loss=0.3889552982568587, gradient=0.0015452117412313195\n",
      "Gradient Descent(13/99): loss=0.3889550853105128, gradient=0.0014745239270360238\n",
      "Gradient Descent(14/99): loss=0.38895488848520854, gradient=0.0014157826497951168\n",
      "Gradient Descent(15/99): loss=0.38895470482289984, gradient=0.0013661957611608726\n",
      "Gradient Descent(16/99): loss=0.38895453212592845, gradient=0.0013236695355449846\n",
      "Gradient Descent(17/99): loss=0.38895436873431777, gradient=0.0012866379847382297\n",
      "Gradient Descent(18/99): loss=0.38895421337266534, gradient=0.001253929197603713\n",
      "Gradient Descent(19/99): loss=0.3889540650440359, gradient=0.0012246632657544308\n",
      "Gradient Descent(20/99): loss=0.3889539229557806, gradient=0.0011981757636771952\n",
      "Gradient Descent(21/99): loss=0.3889537864671841, gradient=0.001173961127037864\n",
      "Gradient Descent(22/99): loss=0.38895365505214063, gradient=0.0011516311070547244\n",
      "Gradient Descent(23/99): loss=0.38895352827224955, gradient=0.0011308844303561144\n",
      "Gradient Descent(24/99): loss=0.38895340575720283, gradient=0.001111484679376368\n",
      "Gradient Descent(25/99): loss=0.388953287190311, gradient=0.0010932441529957293\n",
      "Gradient Descent(26/99): loss=0.3889531722976915, gradient=0.001076012056797878\n",
      "Gradient Descent(27/99): loss=0.3889530608400872, gradient=0.0010596658217312393\n",
      "Gradient Descent(28/99): loss=0.388952952606602, gradient=0.001044104683924491\n",
      "Gradient Descent(29/99): loss=0.3889528474098373, gradient=0.001029244902397909\n",
      "Gradient Descent(30/99): loss=0.38895274508207567, gradient=0.0010150161676684226\n",
      "Gradient Descent(31/99): loss=0.3889526454722467, gradient=0.0010013588806537961\n",
      "Gradient Descent(32/99): loss=0.38895254844349053, gradient=0.0009882220715482113\n",
      "Gradient Descent(33/99): loss=0.3889524538711782, gradient=0.0009755617926762543\n",
      "Gradient Descent(34/99): loss=0.3889523616412891, gradient=0.0009633398651755378\n",
      "Gradient Descent(35/99): loss=0.38895227164906665, gradient=0.0009515228920688153\n",
      "Gradient Descent(36/99): loss=0.3889521837978967, gradient=0.000940081473687755\n",
      "Gradient Descent(37/99): loss=0.3889520979983635, gradient=0.0009289895782144814\n",
      "Gradient Descent(38/99): loss=0.38895201416744823, gradient=0.0009182240322294995\n",
      "Gradient Descent(39/99): loss=0.3889519322278461, gradient=0.00090776410495025\n",
      "Gradient Descent(40/99): loss=0.3889518521076964, gradient=0.000897587458008818\n",
      "Gradient Descent(41/99): loss=0.38895177373907874, gradient=0.0008876849857226044\n",
      "Gradient Descent(42/99): loss=0.38895169705861493, gradient=0.0008780374340972727\n",
      "Gradient Descent(43/99): loss=0.38895162200671785, gradient=0.0008686309559056356\n",
      "Gradient Descent(44/99): loss=0.388951548527253, gradient=0.0008594529184153477\n",
      "Gradient Descent(45/99): loss=0.3889514765672409, gradient=0.000850491760973458\n",
      "Gradient Descent(46/99): loss=0.38895140607659123, gradient=0.0008417368733406224\n",
      "Gradient Descent(47/99): loss=0.3889513370078696, gradient=0.0008331784912175088\n",
      "Gradient Descent(48/99): loss=0.3889512693160868, gradient=0.0008248076060947845\n",
      "Gradient Descent(49/99): loss=0.3889512029585123, gradient=0.0008166158870994049\n",
      "Gradient Descent(50/99): loss=0.388951137894504, gradient=0.0008085956129381748\n",
      "Gradient Descent(51/99): loss=0.388951074085358, gradient=0.0008007396123800256\n",
      "Gradient Descent(52/99): loss=0.3889510114941701, gradient=0.0007930412119918157\n",
      "Gradient Descent(53/99): loss=0.38895095008571207, gradient=0.0007854941900642043\n",
      "Gradient Descent(54/99): loss=0.3889508898263182, gradient=0.0007780927358411067\n",
      "Gradient Descent(55/99): loss=0.3889508306837835, gradient=0.0007708314133136842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(56/99): loss=0.3889507726272684, gradient=0.0007637051289574564\n",
      "Gradient Descent(57/99): loss=0.3889507156272137, gradient=0.0007567091028911724\n",
      "Gradient Descent(58/99): loss=0.38895065965526204, gradient=0.0007498388430145325\n",
      "Gradient Descent(59/99): loss=0.3889506046841856, gradient=0.0007430901217512557\n",
      "Gradient Descent(60/99): loss=0.3889505506878191, gradient=0.0007364589550774803\n",
      "Gradient Descent(61/99): loss=0.38895049764099965, gradient=0.0007299415835637748\n",
      "Gradient Descent(62/99): loss=0.38895044551950975, gradient=0.000723534455196088\n",
      "Gradient Descent(63/99): loss=0.38895039430002476, gradient=0.0007172342097753573\n",
      "Gradient Descent(64/99): loss=0.38895034396006567, gradient=0.000711037664720588\n",
      "Gradient Descent(65/99): loss=0.3889502944779534, gradient=0.0007049418021257652\n",
      "Gradient Descent(66/99): loss=0.3889502458327672, gradient=0.0006989437569384769\n",
      "Gradient Descent(67/99): loss=0.38895019800430763, gradient=0.0006930408061460659\n",
      "Gradient Descent(68/99): loss=0.3889501509730584, gradient=0.0006872303588684191\n",
      "Gradient Descent(69/99): loss=0.3889501047201546, gradient=0.0006815099472691799\n",
      "Gradient Descent(70/99): loss=0.3889500592273509, gradient=0.000675877218207016\n",
      "Gradient Descent(71/99): loss=0.3889500144769917, gradient=0.0006703299255581581\n",
      "Gradient Descent(72/99): loss=0.3889499704519858, gradient=0.0006648659231479439\n",
      "Gradient Descent(73/99): loss=0.38894992713577853, gradient=0.0006594831582377735\n",
      "Gradient Descent(74/99): loss=0.38894988451232965, gradient=0.0006541796655168525\n",
      "Gradient Descent(75/99): loss=0.38894984256608967, gradient=0.0006489535615561234\n",
      "Gradient Descent(76/99): loss=0.3889498012819794, gradient=0.0006438030396842444\n",
      "Gradient Descent(77/99): loss=0.3889497606453706, gradient=0.0006387263652496564\n",
      "Gradient Descent(78/99): loss=0.38894972064206595, gradient=0.0006337218712372729\n",
      "Gradient Descent(79/99): loss=0.38894968125828244, gradient=0.0006287879542098886\n",
      "Gradient Descent(80/99): loss=0.3889496424806353, gradient=0.0006239230705481311\n",
      "Gradient Descent(81/99): loss=0.38894960429612047, gradient=0.0006191257329651326\n",
      "Gradient Descent(82/99): loss=0.3889495666921021, gradient=0.0006143945072732249\n",
      "Gradient Descent(83/99): loss=0.38894952965629637, gradient=0.0006097280093834795\n",
      "Gradient Descent(84/99): loss=0.3889494931767598, gradient=0.000605124902519254\n",
      "Gradient Descent(85/99): loss=0.3889494572418761, gradient=0.0006005838946271806\n",
      "Gradient Descent(86/99): loss=0.38894942184034426, gradient=0.0005961037359703535\n",
      "Gradient Descent(87/99): loss=0.3889493869611672, gradient=0.0005916832168894855\n",
      "Gradient Descent(88/99): loss=0.38894935259364094, gradient=0.0005873211657192724\n",
      "Gradient Descent(89/99): loss=0.38894931872734484, gradient=0.0005830164468480538\n",
      "Gradient Descent(90/99): loss=0.3889492853521305, gradient=0.000578767958909851\n",
      "Gradient Descent(91/99): loss=0.3889492524581145, gradient=0.0005745746330989649\n",
      "Gradient Descent(92/99): loss=0.38894922003566806, gradient=0.0005704354315973974\n",
      "Gradient Descent(93/99): loss=0.3889491880754087, gradient=0.0005663493461073505\n",
      "Gradient Descent(94/99): loss=0.38894915656819257, gradient=0.0005623153964803874\n",
      "Gradient Descent(95/99): loss=0.38894912550510685, gradient=0.0005583326294359929\n",
      "Gradient Descent(96/99): loss=0.3889490948774618, gradient=0.0005544001173635597\n",
      "Gradient Descent(97/99): loss=0.38894906467678375, gradient=0.0005505169572008514\n",
      "Gradient Descent(98/99): loss=0.3889490348948092, gradient=0.0005466822693836454\n",
      "Gradient Descent(99/99): loss=0.3889490055234769, gradient=0.0005428951968614796\n",
      "Gradient Descent(0/99): loss=0.3901486534137936, gradient=0.01540806447346347\n",
      "Gradient Descent(1/99): loss=0.39014472670978984, gradient=0.006921969906996366\n",
      "Gradient Descent(2/99): loss=0.39014207169882953, gradient=0.00536221375720115\n",
      "Gradient Descent(3/99): loss=0.3901400740547872, gradient=0.00461740230957662\n",
      "Gradient Descent(4/99): loss=0.3901385282820932, gradient=0.0040501841702114735\n",
      "Gradient Descent(5/99): loss=0.39013730605012453, gradient=0.0035926070810615267\n",
      "Gradient Descent(6/99): loss=0.3901363198407899, gradient=0.0032196375555395506\n",
      "Gradient Descent(7/99): loss=0.3901355087691904, gradient=0.0029134326515770866\n",
      "Gradient Descent(8/99): loss=0.39013482985433845, gradient=0.002660160201956383\n",
      "Gradient Descent(9/99): loss=0.3901342523151536, gradient=0.002449017550615271\n",
      "Gradient Descent(10/99): loss=0.3901337537884128, gradient=0.0022715556753688205\n",
      "Gradient Descent(11/99): loss=0.3901333177929703, gradient=0.0021211567081483263\n",
      "Gradient Descent(12/99): loss=0.3901329320105567, gradient=0.0019926263316567268\n",
      "Gradient Descent(13/99): loss=0.3901325871069521, gradient=0.001881876068810426\n",
      "Gradient Descent(14/99): loss=0.39013227591437977, gradient=0.001785676237713403\n",
      "Gradient Descent(15/99): loss=0.3901319928578608, gradient=0.0017014643899150048\n",
      "Gradient Descent(16/99): loss=0.39013173354805725, gradient=0.0016271971566419333\n",
      "Gradient Descent(17/99): loss=0.3901314944889333, gradient=0.0015612358966546095\n",
      "Gradient Descent(18/99): loss=0.3901312728654378, gradient=0.0015022585358451849\n",
      "Gradient Descent(19/99): loss=0.39013106638754697, gradient=0.0014491916103820568\n",
      "Gradient Descent(20/99): loss=0.39013087317442574, gradient=0.0014011578366471752\n",
      "Gradient Descent(21/99): loss=0.3901306916674409, gradient=0.0013574355820170987\n",
      "Gradient Descent(22/99): loss=0.39013052056414305, gradient=0.001317427443332536\n",
      "Gradient Descent(23/99): loss=0.39013035876764834, gradient=0.0012806357927978268\n",
      "Gradient Descent(24/99): loss=0.39013020534743925, gradient=0.0012466436579377208\n",
      "Gradient Descent(25/99): loss=0.39013005950872115, gradient=0.0012150996925180241\n",
      "Gradient Descent(26/99): loss=0.3901299205682531, gradient=0.0011857062938120102\n",
      "Gradient Descent(27/99): loss=0.39012978793511954, gradient=0.0011582101486754235\n",
      "Gradient Descent(28/99): loss=0.3901296610953138, gradient=0.001132394662992535\n",
      "Gradient Descent(29/99): loss=0.39012953959928187, gradient=0.0011080738591553512\n",
      "Gradient Descent(30/99): loss=0.3901294230517897, gradient=0.001085087424453236\n",
      "Gradient Descent(31/99): loss=0.390129311103626, gradient=0.0010632966673710072\n",
      "Gradient Descent(32/99): loss=0.3901292034447668, gradient=0.0010425811947745462\n",
      "Gradient Descent(33/99): loss=0.3901290997987139, gradient=0.0010228361653133752\n",
      "Gradient Descent(34/99): loss=0.39012899991777733, gradient=0.0010039700064916636\n",
      "Gradient Descent(35/99): loss=0.39012890357913055, gradient=0.0009859025073092292\n",
      "Gradient Descent(36/99): loss=0.3901288105814921, gradient=0.0009685632170617351\n",
      "Gradient Descent(37/99): loss=0.39012872074232496, gradient=0.0009518900952452896\n",
      "Gradient Descent(38/99): loss=0.39012863389546154, gradient=0.000935828368600801\n",
      "Gradient Descent(39/99): loss=0.3901285498890841, gradient=0.0009203295599514263\n",
      "Gradient Descent(40/99): loss=0.3901284685839984, gradient=0.000905350660229955\n",
      "Gradient Descent(41/99): loss=0.3901283898521559, gradient=0.0008908534204026296\n",
      "Gradient Descent(42/99): loss=0.39012831357538286, gradient=0.0008768037442066134\n",
      "Gradient Descent(43/99): loss=0.39012823964428245, gradient=0.0008631711659809773\n",
      "Gradient Descent(44/99): loss=0.39012816795728766, gradient=0.0008499284005732229\n",
      "Gradient Descent(45/99): loss=0.3901280984198379, gradient=0.0008370509544897587\n",
      "Gradient Descent(46/99): loss=0.3901280309436614, gradient=0.0008245167892389227\n",
      "Gradient Descent(47/99): loss=0.3901279654461525, gradient=0.0008123060292719095\n",
      "Gradient Descent(48/99): loss=0.39012790184982177, gradient=0.0008004007081265919\n",
      "Gradient Descent(49/99): loss=0.3901278400818179, gradient=0.0007887845473720714\n",
      "Gradient Descent(50/99): loss=0.39012778007350446, gradient=0.0007774427637750114\n",
      "Gradient Descent(51/99): loss=0.390127721760087, gradient=0.000766361900798524\n",
      "Gradient Descent(52/99): loss=0.39012766508028324, gradient=0.0007555296811192769\n",
      "Gradient Descent(53/99): loss=0.39012760997603163, gradient=0.0007449348773331548\n",
      "Gradient Descent(54/99): loss=0.39012755639222946, gradient=0.0007345671984289748\n",
      "Gradient Descent(55/99): loss=0.390127504276502, gradient=0.0007244171899529862\n",
      "Gradient Descent(56/99): loss=0.3901274535789958, gradient=0.0007144761460814636\n",
      "Gradient Descent(57/99): loss=0.39012740425219194, gradient=0.0007047360320656947\n",
      "Gradient Descent(58/99): loss=0.3901273562507395, gradient=0.0006951894157258911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(59/99): loss=0.39012730953130553, gradient=0.0006858294068514686\n",
      "Gradient Descent(60/99): loss=0.3901272640524391, gradient=0.0006766496035182694\n",
      "Gradient Descent(61/99): loss=0.3901272197744486, gradient=0.0006676440444667189\n",
      "Gradient Descent(62/99): loss=0.39012717665929025, gradient=0.0006588071667976917\n",
      "Gradient Descent(63/99): loss=0.39012713467046733, gradient=0.0006501337683401739\n",
      "Gradient Descent(64/99): loss=0.39012709377293686, gradient=0.000641618974129047\n",
      "Gradient Descent(65/99): loss=0.39012705393302577, gradient=0.0006332582065032012\n",
      "Gradient Descent(66/99): loss=0.39012701511835446, gradient=0.0006250471583972575\n",
      "Gradient Descent(67/99): loss=0.39012697729776374, gradient=0.000616981769452826\n",
      "Gradient Descent(68/99): loss=0.39012694044125157, gradient=0.000609058204623805\n",
      "Gradient Descent(69/99): loss=0.3901269045199117, gradient=0.0006012728349888904\n",
      "Gradient Descent(70/99): loss=0.390126869505879, gradient=0.0005936222205208834\n",
      "Gradient Descent(71/99): loss=0.3901268353722767, gradient=0.0005861030945924058\n",
      "Gradient Descent(72/99): loss=0.3901268020931703, gradient=0.0005787123500243562\n",
      "Gradient Descent(73/99): loss=0.39012676964352117, gradient=0.0005714470265065815\n",
      "Gradient Descent(74/99): loss=0.3901267379991467, gradient=0.0005643042992407919\n",
      "Gradient Descent(75/99): loss=0.3901267071366817, gradient=0.000557281468672831\n",
      "Gradient Descent(76/99): loss=0.3901266770335413, gradient=0.000550375951197467\n",
      "Gradient Descent(77/99): loss=0.3901266476678876, gradient=0.0005435852707322965\n",
      "Gradient Descent(78/99): loss=0.3901266190185987, gradient=0.0005369070510689341\n",
      "Gradient Descent(79/99): loss=0.3901265910652375, gradient=0.0005303390089202952\n",
      "Gradient Descent(80/99): loss=0.39012656378802496, gradient=0.000523878947592395\n",
      "Gradient Descent(81/99): loss=0.3901265371678131, gradient=0.0005175247512160293\n",
      "Gradient Descent(82/99): loss=0.3901265111860601, gradient=0.000511274379482234\n",
      "Gradient Descent(83/99): loss=0.3901264858248068, gradient=0.0005051258628304411\n",
      "Gradient Descent(84/99): loss=0.3901264610666541, gradient=0.0004990772980445692\n",
      "Gradient Descent(85/99): loss=0.39012643689474213, gradient=0.000493126844216732\n",
      "Gradient Descent(86/99): loss=0.39012641329272996, gradient=0.00048727271904295123\n",
      "Gradient Descent(87/99): loss=0.39012639024477724, gradient=0.0004815131954187995\n",
      "Gradient Descent(88/99): loss=0.3901263677355253, gradient=0.00047584659830607546\n",
      "Gradient Descent(89/99): loss=0.39012634575007976, gradient=0.00047027130184547485\n",
      "Gradient Descent(90/99): loss=0.3901263242739947, gradient=0.0004647857266917829\n",
      "Gradient Descent(91/99): loss=0.3901263032932568, gradient=0.00045938833755134243\n",
      "Gradient Descent(92/99): loss=0.3901262827942701, gradient=0.00045407764090316533\n",
      "Gradient Descent(93/99): loss=0.3901262627638422, gradient=0.00044885218288729913\n",
      "Gradient Descent(94/99): loss=0.3901262431891701, gradient=0.00044371054734527896\n",
      "Gradient Descent(95/99): loss=0.3901262240578269, gradient=0.0004386513539995016\n",
      "Gradient Descent(96/99): loss=0.3901262053577505, gradient=0.00043367325675918144\n",
      "Gradient Descent(97/99): loss=0.39012618707723, gradient=0.0004287749421422328\n",
      "Gradient Descent(98/99): loss=0.39012616920489496, gradient=0.0004239551278030506\n",
      "Gradient Descent(99/99): loss=0.39012615172970466, gradient=0.0004192125611573206\n",
      "Gradient Descent(0/99): loss=0.3903597263572092, gradient=0.0072679058518627425\n",
      "Gradient Descent(1/99): loss=0.3903560504184499, gradient=0.006255559121774679\n",
      "Gradient Descent(2/99): loss=0.39035316376621165, gradient=0.005523566450380174\n",
      "Gradient Descent(3/99): loss=0.3903508376766406, gradient=0.004944193873216116\n",
      "Gradient Descent(4/99): loss=0.39034892034397956, gradient=0.00447748118159447\n",
      "Gradient Descent(5/99): loss=0.39034730811381746, gradient=0.004096708702059298\n",
      "Gradient Descent(6/99): loss=0.3903459287523604, gradient=0.0037820259848277225\n",
      "Gradient Descent(7/99): loss=0.3903447309050958, gradient=0.0035185704032136577\n",
      "Gradient Descent(8/99): loss=0.3903436773353713, gradient=0.003295179324395537\n",
      "Gradient Descent(9/99): loss=0.3903427405118426, gradient=0.0031034700047378595\n",
      "Gradient Descent(10/99): loss=0.3903418997111552, gradient=0.0029370541527470404\n",
      "Gradient Descent(11/99): loss=0.39034113905982126, gradient=0.0027910608053958773\n",
      "Gradient Descent(12/99): loss=0.3903404462039868, gradient=0.0026617253018560855\n",
      "Gradient Descent(13/99): loss=0.3903398113848046, gradient=0.0025461148846856138\n",
      "Gradient Descent(14/99): loss=0.39033922678677163, gradient=0.002441921281763217\n",
      "Gradient Descent(15/99): loss=0.3903386860737023, gradient=0.002347297191775052\n",
      "Gradient Descent(16/99): loss=0.39033818403951376, gradient=0.0022608001371512824\n",
      "Gradient Descent(17/99): loss=0.390337716367226, gradient=0.002181227845506757\n",
      "Gradient Descent(18/99): loss=0.3903372794378241, gradient=0.00210761286166771\n",
      "Gradient Descent(19/99): loss=0.39033687018815294, gradient=0.0020391618232358404\n",
      "Gradient Descent(20/99): loss=0.3903364860020897, gradient=0.0019752198838366096\n",
      "Gradient Descent(21/99): loss=0.39033612462634576, gradient=0.0019152432122225573\n",
      "Gradient Descent(22/99): loss=0.39033578410470826, gradient=0.0018587775346284195\n",
      "Gradient Descent(23/99): loss=0.39033546272623243, gradient=0.0018054412501511883\n",
      "Gradient Descent(24/99): loss=0.39033515898450205, gradient=0.0017549094915096128\n",
      "Gradient Descent(25/99): loss=0.3903348715433708, gradient=0.0017069140430408883\n",
      "Gradient Descent(26/99): loss=0.39033459921216296, gradient=0.0016612164145229482\n",
      "Gradient Descent(27/99): loss=0.39033434092269487, gradient=0.001617619715722866\n",
      "Gradient Descent(28/99): loss=0.39033409571259986, gradient=0.0015759489371579669\n",
      "Gradient Descent(29/99): loss=0.39033386271052495, gradient=0.0015360538893597073\n",
      "Gradient Descent(30/99): loss=0.39033364112414703, gradient=0.0014978035561804773\n",
      "Gradient Descent(31/99): loss=0.39033343023019257, gradient=0.0014610830285157136\n",
      "Gradient Descent(32/99): loss=0.39033322936607406, gradient=0.001425790980007822\n",
      "Gradient Descent(33/99): loss=0.3903330379228409, gradient=0.0013918375821498138\n",
      "Gradient Descent(34/99): loss=0.3903328553391987, gradient=0.0013591427768643006\n",
      "Gradient Descent(35/99): loss=0.3903326810964104, gradient=0.0013276348408506821\n",
      "Gradient Descent(36/99): loss=0.3903325147139203, gradient=0.0012972491887957695\n",
      "Gradient Descent(37/99): loss=0.39033235574557795, gradient=0.0012679273727061369\n",
      "Gradient Descent(38/99): loss=0.3903322037763637, gradient=0.0012396162427211694\n",
      "Gradient Descent(39/99): loss=0.3903320584202372, gradient=0.001212261261714896\n",
      "Gradient Descent(40/99): loss=0.3903319193154603, gradient=0.001185830031141036\n",
      "Gradient Descent(41/99): loss=0.3903317861246255, gradient=0.0011602752949032818\n",
      "Gradient Descent(42/99): loss=0.390331658531936, gradient=0.001135559050596209\n",
      "Gradient Descent(43/99): loss=0.390331536241469, gradient=0.0011116459970138303\n",
      "Gradient Descent(44/99): loss=0.390331418975623, gradient=0.0010885032233726652\n",
      "Gradient Descent(45/99): loss=0.39033130647372816, gradient=0.0010660999431969939\n",
      "Gradient Descent(46/99): loss=0.3903311984907922, gradient=0.0010444072657541622\n",
      "Gradient Descent(47/99): loss=0.3903310947963679, gradient=0.0010233979991541367\n",
      "Gradient Descent(48/99): loss=0.3903309951735269, gradient=0.001003046480220354\n",
      "Gradient Descent(49/99): loss=0.3903308994179255, gradient=0.0009833284270533795\n",
      "Gradient Descent(50/99): loss=0.3903308073369517, gradient=0.0009642208108810689\n",
      "Gradient Descent(51/99): loss=0.3903307187489449, gradient=0.0009457017443420767\n",
      "Gradient Descent(52/99): loss=0.390330633482481, gradient=0.0009277503838072119\n",
      "Gradient Descent(53/99): loss=0.3903305513757121, gradient=0.0009103468437230344\n",
      "Gradient Descent(54/99): loss=0.3903304722757621, gradient=0.0008934721212765376\n",
      "Gradient Descent(55/99): loss=0.3903303960381641, gradient=0.0008771080299425423\n",
      "Gradient Descent(56/99): loss=0.39033032252634214, gradient=0.0008612371406937871\n",
      "Gradient Descent(57/99): loss=0.3903302516111303, gradient=0.0008458427298366824\n",
      "Gradient Descent(58/99): loss=0.39033018317032586, gradient=0.0008309087325894688\n",
      "Gradient Descent(59/99): loss=0.3903301170882739, gradient=0.000816419701647174\n",
      "Gradient Descent(60/99): loss=0.39033005325548004, gradient=0.0008023607700874929\n",
      "Gradient Descent(61/99): loss=0.39032999156824977, gradient=0.0007887176180619532\n",
      "Gradient Descent(62/99): loss=0.39032993192835125, gradient=0.0007754764427953149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(63/99): loss=0.39032987424270144, gradient=0.0007626239314805901\n",
      "Gradient Descent(64/99): loss=0.39032981842307013, gradient=0.0007501472367138213\n",
      "Gradient Descent(65/99): loss=0.3903297643858053, gradient=0.0007380339541598203\n",
      "Gradient Descent(66/99): loss=0.390329712051573, gradient=0.0007262721021807273\n",
      "Gradient Descent(67/99): loss=0.39032966134511743, gradient=0.000714850103194397\n",
      "Gradient Descent(68/99): loss=0.390329612195031, gradient=0.0007037567665590011\n",
      "Gradient Descent(69/99): loss=0.39032956453354173, gradient=0.0006929812728064496\n",
      "Gradient Descent(70/99): loss=0.3903295182963118, gradient=0.0006825131590689665\n",
      "Gradient Descent(71/99): loss=0.39032947342224994, gradient=0.0006723423055631683\n",
      "Gradient Descent(72/99): loss=0.39032942985333197, gradient=0.0006624589230115285\n",
      "Gradient Descent(73/99): loss=0.3903293875344348, gradient=0.0006528535408970449\n",
      "Gradient Descent(74/99): loss=0.39032934641317873, gradient=0.0006435169964581247\n",
      "Gradient Descent(75/99): loss=0.390329306439779, gradient=0.0006344404243431993\n",
      "Gradient Descent(76/99): loss=0.3903292675669059, gradient=0.0006256152468528339\n",
      "Gradient Descent(77/99): loss=0.3903292297495526, gradient=0.0006170331647072302\n",
      "Gradient Descent(78/99): loss=0.39032919294491314, gradient=0.0006086861482828991\n",
      "Gradient Descent(79/99): loss=0.3903291571122613, gradient=0.0006005664292702947\n",
      "Gradient Descent(80/99): loss=0.3903291222128437, gradient=0.0005926664927089218\n",
      "Gradient Descent(81/99): loss=0.3903290882097731, gradient=0.0005849790693624638\n",
      "Gradient Descent(82/99): loss=0.39032905506793086, gradient=0.0005774971284005423\n",
      "Gradient Descent(83/99): loss=0.3903290227538735, gradient=0.0005702138703577891\n",
      "Gradient Descent(84/99): loss=0.3903289912357452, gradient=0.000563122720345121\n",
      "Gradient Descent(85/99): loss=0.3903289604831945, gradient=0.000556217321490008\n",
      "Gradient Descent(86/99): loss=0.39032893046729533, gradient=0.0005494915285871757\n",
      "Gradient Descent(87/99): loss=0.3903289011604738, gradient=0.0005429394019417582\n",
      "Gradient Descent(88/99): loss=0.39032887253643805, gradient=0.0005365552013907018\n",
      "Gradient Descent(89/99): loss=0.39032884457011086, gradient=0.0005303333804893882\n",
      "Gradient Descent(90/99): loss=0.39032881723756874, gradient=0.0005242685808521833\n",
      "Gradient Descent(91/99): loss=0.390328790515981, gradient=0.0005183556266376837\n",
      "Gradient Descent(92/99): loss=0.39032876438355524, gradient=0.0005125895191700125\n",
      "Gradient Descent(93/99): loss=0.3903287388194826, gradient=0.0005069654316896392\n",
      "Gradient Descent(94/99): loss=0.39032871380388995, gradient=0.0005014787042272734\n",
      "Gradient Descent(95/99): loss=0.39032868931779036, gradient=0.0004961248385959762\n",
      "Gradient Descent(96/99): loss=0.3903286653430386, gradient=0.0004908994934970841\n",
      "Gradient Descent(97/99): loss=0.39032864186229044, gradient=0.00048579847973645614\n",
      "Gradient Descent(98/99): loss=0.39032861885896014, gradient=0.0004808177555477405\n",
      "Gradient Descent(99/99): loss=0.39032859631718325, gradient=0.00047595342202035967\n",
      "Gradient Descent(0/99): loss=0.38997962244494094, gradient=0.010008620878261928\n",
      "Gradient Descent(1/99): loss=0.38997338961076133, gradient=0.008243806253350262\n",
      "Gradient Descent(2/99): loss=0.3899689789142625, gradient=0.006907470462356608\n",
      "Gradient Descent(3/99): loss=0.3899657462707546, gradient=0.0058893588767679195\n",
      "Gradient Descent(4/99): loss=0.3899632919356583, gradient=0.005110940354053759\n",
      "Gradient Descent(5/99): loss=0.3899613646724104, gradient=0.004511885674737492\n",
      "Gradient Descent(6/99): loss=0.3899598042573272, gradient=0.0040460941687139795\n",
      "Gradient Descent(7/99): loss=0.38995850675332605, gradient=0.0036787860880343358\n",
      "Gradient Descent(8/99): loss=0.3899574033842056, gradient=0.0033841756682044485\n",
      "Gradient Descent(9/99): loss=0.38995644761427095, gradient=0.0031434138492581268\n",
      "Gradient Descent(10/99): loss=0.3899556071822258, gradient=0.0029428690047644964\n",
      "Gradient Descent(11/99): loss=0.389954859143762, gradient=0.0027727384936011876\n",
      "Gradient Descent(12/99): loss=0.38995418675229077, gradient=0.002625974928067556\n",
      "Gradient Descent(13/99): loss=0.3899535774707609, gradient=0.0024974868007630754\n",
      "Gradient Descent(14/99): loss=0.3899530216854488, gradient=0.0023835608665097024\n",
      "Gradient Descent(15/99): loss=0.3899525118598154, gradient=0.002281453607640695\n",
      "Gradient Descent(16/99): loss=0.3899520419696513, gradient=0.0021890962636822446\n",
      "Gradient Descent(17/99): loss=0.3899516071089598, gradient=0.0021049372687682063\n",
      "Gradient Descent(18/99): loss=0.3899512032262015, gradient=0.0020277462136430585\n",
      "Gradient Descent(19/99): loss=0.3899508269293908, gradient=0.001956558641261635\n",
      "Gradient Descent(20/99): loss=0.3899504753459738, gradient=0.00189060009126724\n",
      "Gradient Descent(21/99): loss=0.3899501460188974, gradient=0.0018292393780369109\n",
      "Gradient Descent(22/99): loss=0.3899498368280895, gradient=0.0017719551667891667\n",
      "Gradient Descent(23/99): loss=0.389949545931219, gradient=0.0017183049605159475\n",
      "Gradient Descent(24/99): loss=0.38994927171316834, gradient=0.0016679347400588862\n",
      "Gradient Descent(25/99): loss=0.38994901275247423, gradient=0.0016205232914100205\n",
      "Gradient Descent(26/99): loss=0.3899487677909596, gradient=0.0015757958890705928\n",
      "Gradient Descent(27/99): loss=0.3899485357072923, gradient=0.0015335288490109487\n",
      "Gradient Descent(28/99): loss=0.3899483154996753, gradient=0.0014935131955040738\n",
      "Gradient Descent(29/99): loss=0.38994810626876664, gradient=0.0014555700075254937\n",
      "Gradient Descent(30/99): loss=0.3899479072041734, gradient=0.0014195413362120335\n",
      "Gradient Descent(31/99): loss=0.389947717573148, gradient=0.0013852869847683761\n",
      "Gradient Descent(32/99): loss=0.389947536711065, gradient=0.0013526818729504586\n",
      "Gradient Descent(33/99): loss=0.3899473640133463, gradient=0.0013216138638848787\n",
      "Gradient Descent(34/99): loss=0.3899471989285751, gradient=0.0012919819604161305\n",
      "Gradient Descent(35/99): loss=0.389947040952593, gradient=0.00126369479954058\n",
      "Gradient Descent(36/99): loss=0.38994688962341134, gradient=0.001236669389230007\n",
      "Gradient Descent(37/99): loss=0.38994674451680506, gradient=0.0012108300437331616\n",
      "Gradient Descent(38/99): loss=0.3899466052424789, gradient=0.0011861074823844707\n",
      "Gradient Descent(39/99): loss=0.38994647144071715, gradient=0.001162438063827287\n",
      "Gradient Descent(40/99): loss=0.38994634277944207, gradient=0.0011397631329094337\n",
      "Gradient Descent(41/99): loss=0.3899462189516244, gradient=0.0011180284617183683\n",
      "Gradient Descent(42/99): loss=0.3899460996729916, gradient=0.0010971837695646534\n",
      "Gradient Descent(43/99): loss=0.38994598467999425, gradient=0.001077182309397848\n",
      "Gradient Descent(44/99): loss=0.3899458737279952, gradient=0.0010579805102956376\n",
      "Gradient Descent(45/99): loss=0.3899457665896506, gradient=0.0010395376674182683\n",
      "Gradient Descent(46/99): loss=0.38994566305345946, gradient=0.0010218156722488388\n",
      "Gradient Descent(47/99): loss=0.3899455629224615, gradient=0.0010047787771115823\n",
      "Gradient Descent(48/99): loss=0.3899454660130614, gradient=0.0009883933889269273\n",
      "Gradient Descent(49/99): loss=0.3899453721539676, gradient=0.0009726278879596307\n",
      "Gradient Descent(50/99): loss=0.38994528118523136, gradient=0.0009574524679804177\n",
      "Gradient Descent(51/99): loss=0.3899451929573747, gradient=0.0009428389948141445\n",
      "Gradient Descent(52/99): loss=0.3899451073305958, gradient=0.0009287608807086287\n",
      "Gradient Descent(53/99): loss=0.3899450241740472, gradient=0.000915192972346478\n",
      "Gradient Descent(54/99): loss=0.38994494336517455, gradient=0.0009021114506465218\n",
      "Gradient Descent(55/99): loss=0.38994486478911244, gradient=0.0008894937407762023\n",
      "Gradient Descent(56/99): loss=0.3899447883381318, gradient=0.0008773184310276544\n",
      "Gradient Descent(57/99): loss=0.3899447139111322, gradient=0.0008655651994056202\n",
      "Gradient Descent(58/99): loss=0.3899446414131744, gradient=0.0008542147469423071\n",
      "Gradient Descent(59/99): loss=0.3899445707550525, gradient=0.0008432487368942828\n",
      "Gradient Descent(60/99): loss=0.3899445018528974, gradient=0.0008326497390977718\n",
      "Gradient Descent(61/99): loss=0.3899444346278131, gradient=0.0008224011788598983\n",
      "Gradient Descent(62/99): loss=0.3899443690055393, gradient=0.0008124872898512602\n",
      "Gradient Descent(63/99): loss=0.3899443049161411, gradient=0.000802893070539857\n",
      "Gradient Descent(64/99): loss=0.3899442422937203, gradient=0.000793604243769593\n",
      "Gradient Descent(65/99): loss=0.3899441810761496, gradient=0.0007846072191416875\n",
      "Gradient Descent(66/99): loss=0.38994412120482513, gradient=0.0007758890579035465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(67/99): loss=0.38994406262443726, gradient=0.0007674374400895442\n",
      "Gradient Descent(68/99): loss=0.3899440052827596, gradient=0.0007592406336933451\n",
      "Gradient Descent(69/99): loss=0.3899439491304485, gradient=0.0007512874656790164\n",
      "Gradient Descent(70/99): loss=0.3899438941208623, gradient=0.0007435672946649361\n",
      "Gradient Descent(71/99): loss=0.3899438402098895, gradient=0.0007360699851356069\n",
      "Gradient Descent(72/99): loss=0.3899437873557896, gradient=0.0007287858830538887\n",
      "Gradient Descent(73/99): loss=0.38994373551904604, gradient=0.000721705792763699\n",
      "Gradient Descent(74/99): loss=0.38994368466222745, gradient=0.0007148209550855628\n",
      "Gradient Descent(75/99): loss=0.3899436347498602, gradient=0.0007081230265190736\n",
      "Gradient Descent(76/99): loss=0.38994358574830756, gradient=0.0007016040594770108\n",
      "Gradient Descent(77/99): loss=0.3899435376256589, gradient=0.0006952564834834414\n",
      "Gradient Descent(78/99): loss=0.389943490351625, gradient=0.0006890730872760546\n",
      "Gradient Descent(79/99): loss=0.3899434438974404, gradient=0.0006830470017589104\n",
      "Gradient Descent(80/99): loss=0.3899433982357728, gradient=0.0006771716837572204\n",
      "Gradient Descent(81/99): loss=0.38994335334063757, gradient=0.0006714409005304855\n",
      "Gradient Descent(82/99): loss=0.38994330918731857, gradient=0.0006658487150036815\n",
      "Gradient Descent(83/99): loss=0.3899432657522944, gradient=0.0006603894716808508\n",
      "Gradient Descent(84/99): loss=0.3899432230131668, gradient=0.0006550577832064266\n",
      "Gradient Descent(85/99): loss=0.38994318094859776, gradient=0.0006498485175444646\n",
      "Gradient Descent(86/99): loss=0.3899431395382476, gradient=0.0006447567857460167\n",
      "Gradient Descent(87/99): loss=0.3899430987627172, gradient=0.0006397779302779477\n",
      "Gradient Descent(88/99): loss=0.38994305860349593, gradient=0.0006349075138884803\n",
      "Gradient Descent(89/99): loss=0.38994301904291023, gradient=0.0006301413089849231\n",
      "Gradient Descent(90/99): loss=0.38994298006407724, gradient=0.0006254752875015071\n",
      "Gradient Descent(91/99): loss=0.3899429416508604, gradient=0.0006209056112361519\n",
      "Gradient Descent(92/99): loss=0.3899429037878283, gradient=0.0006164286226353925\n",
      "Gradient Descent(93/99): loss=0.389942866460216, gradient=0.0006120408360087424\n",
      "Gradient Descent(94/99): loss=0.3899428296538878, gradient=0.0006077389291535988\n",
      "Gradient Descent(95/99): loss=0.38994279335530474, gradient=0.0006035197353732084\n",
      "Gradient Descent(96/99): loss=0.3899427575514898, gradient=0.0005993802358707409\n",
      "Gradient Descent(97/99): loss=0.3899427222300005, gradient=0.0005953175525031788\n",
      "Gradient Descent(98/99): loss=0.3899426873788991, gradient=0.0005913289408795176\n",
      "Gradient Descent(99/99): loss=0.3899426529867256, gradient=0.0005874117837881358\n",
      "Gradient Descent(0/99): loss=0.3896776499934, gradient=0.011108895125524\n",
      "Gradient Descent(1/99): loss=0.38967265935677536, gradient=0.007451174826875982\n",
      "Gradient Descent(2/99): loss=0.3896691209597109, gradient=0.006184852551879087\n",
      "Gradient Descent(3/99): loss=0.3896665225420953, gradient=0.005283786781941951\n",
      "Gradient Descent(4/99): loss=0.3896645754535747, gradient=0.004563409429637334\n",
      "Gradient Descent(5/99): loss=0.38966308798233595, gradient=0.003979674906430716\n",
      "Gradient Descent(6/99): loss=0.38966192982658227, gradient=0.00350391870087039\n",
      "Gradient Descent(7/99): loss=0.38966101138013004, gradient=0.0031137487757698433\n",
      "Gradient Descent(8/99): loss=0.38966027028406897, gradient=0.0027914797139767487\n",
      "Gradient Descent(9/99): loss=0.3896596625815125, gradient=0.0025231772287261614\n",
      "Gradient Descent(10/99): loss=0.3896591568594951, gradient=0.0022979043544701324\n",
      "Gradient Descent(11/99): loss=0.3896587303433397, gradient=0.002107104125952156\n",
      "Gradient Descent(12/99): loss=0.3896583662736896, gradient=0.001944094691596148\n",
      "Gradient Descent(13/99): loss=0.38965805213070087, gradient=0.0018036598104062879\n",
      "Gradient Descent(14/99): loss=0.38965777842104493, gradient=0.001681720573591231\n",
      "Gradient Descent(15/99): loss=0.3896575378412336, gradient=0.0015750756150537178\n",
      "Gradient Descent(16/99): loss=0.38965732469436204, gradient=0.0014811980842789603\n",
      "Gradient Descent(17/99): loss=0.38965713447884553, gradient=0.001398078738000013\n",
      "Gradient Descent(18/99): loss=0.38965696359489727, gradient=0.001324105781415774\n",
      "Gradient Descent(19/99): loss=0.3896568091323622, gradient=0.0012579734869963112\n",
      "Gradient Descent(20/99): loss=0.38965666871534455, gradient=0.00119861301774221\n",
      "Gradient Descent(21/99): loss=0.3896565403869158, gradient=0.0011451401790653932\n",
      "Gradient Descent(22/99): loss=0.3896564225224329, gradient=0.00109681595735832\n",
      "Gradient Descent(23/99): loss=0.38965631376353405, gradient=0.0010530166507118522\n",
      "Gradient Descent(24/99): loss=0.3896562129672546, gradient=0.0010132111621971012\n",
      "Gradient Descent(25/99): loss=0.38965611916634885, gradient=0.000976943627930323\n",
      "Gradient Descent(26/99): loss=0.3896560315380118, gradient=0.0009438200163019737\n",
      "Gradient Descent(27/99): loss=0.3896559493789807, gradient=0.0009134976873554553\n",
      "Gradient Descent(28/99): loss=0.3896558720855335, gradient=0.0008856771660417816\n",
      "Gradient Descent(29/99): loss=0.3896557991372931, gradient=0.0008600955800423911\n",
      "Gradient Descent(30/99): loss=0.3896557300840132, gradient=0.0008365213583483501\n",
      "Gradient Descent(31/99): loss=0.38965566453472944, gradient=0.0008147498936536482\n",
      "Gradient Descent(32/99): loss=0.38965560214879646, gradient=0.0007945999497713904\n",
      "Gradient Descent(33/99): loss=0.3896555426284465, gradient=0.0007759106522395627\n",
      "Gradient Descent(34/99): loss=0.3896554857125788, gradient=0.0007585389417018187\n",
      "Gradient Descent(35/99): loss=0.389655431171555, gradient=0.000742357399735539\n",
      "Gradient Descent(36/99): loss=0.38965537880282064, gradient=0.000727252378662449\n",
      "Gradient Descent(37/99): loss=0.38965532842720446, gradient=0.0007131223827950487\n",
      "Gradient Descent(38/99): loss=0.3896552798857847, gradient=0.0006998766602117517\n",
      "Gradient Descent(39/99): loss=0.3896552330372237, gradient=0.0006874339727181156\n",
      "Gradient Descent(40/99): loss=0.389655187755493, gradient=0.0006757215180114852\n",
      "Gradient Descent(41/99): loss=0.38965514392792994, gradient=0.000664673982846914\n",
      "Gradient Descent(42/99): loss=0.3896551014535685, gradient=0.0006542327096459995\n",
      "Gradient Descent(43/99): loss=0.3896550602417047, gradient=0.0006443449618157643\n",
      "Gradient Descent(44/99): loss=0.38965502021065956, gradient=0.000634963275274158\n",
      "Gradient Descent(45/99): loss=0.38965498128670856, gradient=0.0006260448854716269\n",
      "Gradient Descent(46/99): loss=0.389654943403155, gradient=0.0006175512206644604\n",
      "Gradient Descent(47/99): loss=0.3896549064995243, gradient=0.0006094474534155791\n",
      "Gradient Descent(48/99): loss=0.38965487052086273, gradient=0.0006017021033268606\n",
      "Gradient Descent(49/99): loss=0.38965483541712453, gradient=0.0005942866848854722\n",
      "Gradient Descent(50/99): loss=0.38965480114263795, gradient=0.0005871753950622342\n",
      "Gradient Descent(51/99): loss=0.3896547676556342, gradient=0.0005803448359566937\n",
      "Gradient Descent(52/99): loss=0.3896547349178362, gradient=0.0005737737683540697\n",
      "Gradient Descent(53/99): loss=0.38965470289409493, gradient=0.0005674428925617146\n",
      "Gradient Descent(54/99): loss=0.3896546715520697, gradient=0.0005613346533289293\n",
      "Gradient Descent(55/99): loss=0.3896546408619449, gradient=0.0005554330660413091\n",
      "Gradient Descent(56/99): loss=0.38965461079617947, gradient=0.0005497235617183194\n",
      "Gradient Descent(57/99): loss=0.38965458132928515, gradient=0.0005441928486408149\n",
      "Gradient Descent(58/99): loss=0.38965455243762814, gradient=0.00053882878869603\n",
      "Gradient Descent(59/99): loss=0.38965452409925383, gradient=0.0005336202867586945\n",
      "Gradient Descent(60/99): loss=0.38965449629372984, gradient=0.0005285571916270005\n",
      "Gradient Descent(61/99): loss=0.38965446900200573, gradient=0.0005236302072110817\n",
      "Gradient Descent(62/99): loss=0.38965444220628886, gradient=0.0005188308128256288\n",
      "Gradient Descent(63/99): loss=0.38965441588993044, gradient=0.0005141511915758987\n",
      "Gradient Descent(64/99): loss=0.38965439003732566, gradient=0.0005095841659453773\n",
      "Gradient Descent(65/99): loss=0.38965436463382364, gradient=0.0005051231397986634\n",
      "Gradient Descent(66/99): loss=0.38965433966564506, gradient=0.000500762046105299\n",
      "Gradient Descent(67/99): loss=0.3896543151198098, gradient=0.0004964952997706525\n",
      "Gradient Descent(68/99): loss=0.38965429098407, gradient=0.0004923177550316063\n",
      "Gradient Descent(69/99): loss=0.38965426724685026, gradient=0.000488224666936596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(70/99): loss=0.3896542438971944, gradient=0.00048421165648411063\n",
      "Gradient Descent(71/99): loss=0.3896542209247152, gradient=0.00048027467904232697\n",
      "Gradient Descent(72/99): loss=0.3896541983195512, gradient=0.00047640999571453406\n",
      "Gradient Descent(73/99): loss=0.38965417607232616, gradient=0.0004726141473521285\n",
      "Gradient Descent(74/99): loss=0.3896541541741114, gradient=0.0004688839309498314\n",
      "Gradient Descent(75/99): loss=0.3896541326163947, gradient=0.00046521637818705775\n",
      "Gradient Descent(76/99): loss=0.3896541113910472, gradient=0.00046160873590407896\n",
      "Gradient Descent(77/99): loss=0.38965409049029837, gradient=0.000458058448324775\n",
      "Gradient Descent(78/99): loss=0.38965406990670937, gradient=0.0004545631408576687\n",
      "Gradient Descent(79/99): loss=0.38965404963315026, gradient=0.00045112060532412926\n",
      "Gradient Descent(80/99): loss=0.38965402966277946, gradient=0.0004477287864788388\n",
      "Gradient Descent(81/99): loss=0.3896540099890235, gradient=0.00044438576970115697\n",
      "Gradient Descent(82/99): loss=0.38965399060556094, gradient=0.0004410897697482028\n",
      "Gradient Descent(83/99): loss=0.38965397150630493, gradient=0.0004378391204720244\n",
      "Gradient Descent(84/99): loss=0.3896539526853886, gradient=0.00043463226541253446\n",
      "Gradient Descent(85/99): loss=0.38965393413715216, gradient=0.00043146774918625936\n",
      "Gradient Descent(86/99): loss=0.38965391585612935, gradient=0.0004283442095998825\n",
      "Gradient Descent(87/99): loss=0.38965389783703724, gradient=0.00042526037042323473\n",
      "Gradient Descent(88/99): loss=0.389653880074764, gradient=0.00042221503476352077\n",
      "Gradient Descent(89/99): loss=0.38965386256436085, gradient=0.0004192070789874886\n",
      "Gradient Descent(90/99): loss=0.3896538453010317, gradient=0.0004162354471437673\n",
      "Gradient Descent(91/99): loss=0.38965382828012507, gradient=0.0004132991458419126\n",
      "Gradient Descent(92/99): loss=0.3896538114971276, gradient=0.00041039723954827586\n",
      "Gradient Descent(93/99): loss=0.3896537949476552, gradient=0.0004075288462636971\n",
      "Gradient Descent(94/99): loss=0.38965377862744766, gradient=0.00040469313354933573\n",
      "Gradient Descent(95/99): loss=0.38965376253236156, gradient=0.00040188931487205145\n",
      "Gradient Descent(96/99): loss=0.3896537466583659, gradient=0.00039911664624178066\n",
      "Gradient Descent(97/99): loss=0.3896537310015356, gradient=0.00039637442311661055\n",
      "Gradient Descent(98/99): loss=0.3896537155580473, gradient=0.0003936619775529863\n",
      "Gradient Descent(99/99): loss=0.38965370032417435, gradient=0.0003909786755809944\n",
      "Gradient Descent(0/99): loss=0.38896482663256526, gradient=0.008173583491116587\n",
      "Gradient Descent(1/99): loss=0.3889620659045938, gradient=0.0055498134181637526\n",
      "Gradient Descent(2/99): loss=0.38896014800728546, gradient=0.004564224203285198\n",
      "Gradient Descent(3/99): loss=0.38895876669902685, gradient=0.003860204452931137\n",
      "Gradient Descent(4/99): loss=0.38895774682319295, gradient=0.003307140704714912\n",
      "Gradient Descent(5/99): loss=0.38895697375091504, gradient=0.0028701925507392877\n",
      "Gradient Descent(6/99): loss=0.38895637116150267, gradient=0.002525601073064805\n",
      "Gradient Descent(7/99): loss=0.388955887859035, gradient=0.00225427126383179\n",
      "Gradient Descent(8/99): loss=0.38895548924870454, gradient=0.002040638574830831\n",
      "Gradient Descent(9/99): loss=0.3889551517640592, gradient=0.0018720627424006428\n",
      "Gradient Descent(10/99): loss=0.3889548592061049, gradient=0.001738377215232748\n",
      "Gradient Descent(11/99): loss=0.3889546003275722, gradient=0.0016315135969615002\n",
      "Gradient Descent(12/99): loss=0.3889543672313717, gradient=0.0015451632619115502\n",
      "Gradient Descent(13/99): loss=0.38895415430354496, gradient=0.0014744638314578135\n",
      "Gradient Descent(14/99): loss=0.3889539574985362, gradient=0.0014157133185287393\n",
      "Gradient Descent(15/99): loss=0.388953773857706, gradient=0.0013661192106032682\n",
      "Gradient Descent(16/99): loss=0.3889536011829494, gradient=0.0013235874455594678\n",
      "Gradient Descent(17/99): loss=0.3889534378139539, gradient=0.0012865517365787137\n",
      "Gradient Descent(18/99): loss=0.3889532824750657, gradient=0.0012538399151507578\n",
      "Gradient Descent(19/99): loss=0.3889531341691619, gradient=0.0012245718555394301\n",
      "Gradient Descent(20/99): loss=0.38895299210345546, gradient=0.001198082951239049\n",
      "Gradient Descent(21/99): loss=0.388952855637131, gradient=0.001173867488599229\n",
      "Gradient Descent(22/99): loss=0.38895272424401095, gradient=0.0011515370963819593\n",
      "Gradient Descent(23/99): loss=0.38895259748564576, gradient=0.0011307904011349922\n",
      "Gradient Descent(24/99): loss=0.38895247499169633, gradient=0.001111390903639108\n",
      "Gradient Descent(25/99): loss=0.38895235644545423, gradient=0.001093150836188651\n",
      "Gradient Descent(26/99): loss=0.388952241573029, gradient=0.0010759193500508959\n",
      "Gradient Descent(27/99): loss=0.38895213013516367, gradient=0.001059573831829073\n",
      "Gradient Descent(28/99): loss=0.388952021920968, gradient=0.0010440134814058883\n",
      "Gradient Descent(29/99): loss=0.3889519167430541, gradient=0.0010291545281373594\n",
      "Gradient Descent(30/99): loss=0.3889518144337184, gradient=0.0010149266382318842\n",
      "Gradient Descent(31/99): loss=0.3889517148419069, gradient=0.0010012701926636\n",
      "Gradient Descent(32/99): loss=0.38895161783077836, gradient=0.0009881342052483675\n",
      "Gradient Descent(33/99): loss=0.3889515232757234, gradient=0.0009754747148506462\n",
      "Gradient Descent(34/99): loss=0.388951431062742, gradient=0.0009632535315425823\n",
      "Gradient Descent(35/99): loss=0.38895134108709906, gradient=0.000951437249251105\n",
      "Gradient Descent(36/99): loss=0.38895125325220137, gradient=0.0009399964608362183\n",
      "Gradient Descent(37/99): loss=0.388951167468655, gradient=0.0009289051283512614\n",
      "Gradient Descent(38/99): loss=0.388951083653462, gradient=0.0009181400733607203\n",
      "Gradient Descent(39/99): loss=0.38895100172933944, gradient=0.0009076805609909412\n",
      "Gradient Descent(40/99): loss=0.3889509216247513, gradient=0.000897500697455089\n",
      "Gradient Descent(41/99): loss=0.38895084327145285, gradient=0.0008875987568421191\n",
      "Gradient Descent(42/99): loss=0.3889507666060902, gradient=0.0008779516303475366\n",
      "Gradient Descent(43/99): loss=0.38895069156909967, gradient=0.0008685454710990828\n",
      "Gradient Descent(44/99): loss=0.3889506181043693, gradient=0.0008593676468342607\n",
      "Gradient Descent(45/99): loss=0.38895054615894115, gradient=0.0008504065974797658\n",
      "Gradient Descent(46/99): loss=0.3889504756827458, gradient=0.0008416517134764367\n",
      "Gradient Descent(47/99): loss=0.3889504066283687, gradient=0.0008330932312935967\n",
      "Gradient Descent(48/99): loss=0.3889503389508399, gradient=0.0008247221432672214\n",
      "Gradient Descent(49/99): loss=0.38895027260744736, gradient=0.0008165301194344145\n",
      "Gradient Descent(50/99): loss=0.38895020755756643, gradient=0.0008085094394655314\n",
      "Gradient Descent(51/99): loss=0.38895014376251025, gradient=0.0008006529331365253\n",
      "Gradient Descent(52/99): loss=0.38895008118539065, gradient=0.0007929539280561005\n",
      "Gradient Descent(53/99): loss=0.38895001979099514, gradient=0.0007854062035833351\n",
      "Gradient Descent(54/99): loss=0.38894995954567335, gradient=0.0007780039500507442\n",
      "Gradient Descent(55/99): loss=0.38894990041723415, gradient=0.0007707417325522157\n",
      "Gradient Descent(56/99): loss=0.3889498423748518, gradient=0.0007636144586755322\n",
      "Gradient Descent(57/99): loss=0.3889497853889806, gradient=0.0007566173496569809\n",
      "Gradient Descent(58/99): loss=0.3889497294312756, gradient=0.0007497459145157769\n",
      "Gradient Descent(59/99): loss=0.38894967447452133, gradient=0.0007429959267940717\n",
      "Gradient Descent(60/99): loss=0.38894962049256404, gradient=0.0007363634035833301\n",
      "Gradient Descent(61/99): loss=0.3889495674602519, gradient=0.0007298445865641829\n",
      "Gradient Descent(62/99): loss=0.388949515353378, gradient=0.000723435924826075\n",
      "Gradient Descent(63/99): loss=0.38894946414862824, gradient=0.0007171340592653522\n",
      "Gradient Descent(64/99): loss=0.38894941382353326, gradient=0.0007109358083875334\n",
      "Gradient Descent(65/99): loss=0.38894936435642335, gradient=0.0007048381553634899\n",
      "Gradient Descent(66/99): loss=0.38894931572638697, gradient=0.0006988382362072939\n",
      "Gradient Descent(67/99): loss=0.3889492679132329, gradient=0.0006929333289620746\n",
      "Gradient Descent(68/99): loss=0.3889492208974536, gradient=0.0006871208437923877\n",
      "Gradient Descent(69/99): loss=0.3889491746601915, gradient=0.0006813983138952746\n",
      "Gradient Descent(70/99): loss=0.388949129183209, gradient=0.0006757633871513573\n",
      "Gradient Descent(71/99): loss=0.3889490844488582, gradient=0.0006702138184471941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(72/99): loss=0.38894904044005413, gradient=0.0006647474626070643\n",
      "Gradient Descent(73/99): loss=0.388948997140249, gradient=0.0006593622678795546\n",
      "Gradient Descent(74/99): loss=0.3889489545334086, gradient=0.0006540562699297119\n",
      "Gradient Descent(75/99): loss=0.38894891260399, gradient=0.0006488275862929928\n",
      "Gradient Descent(76/99): loss=0.38894887133691947, gradient=0.0006436744112510624\n",
      "Gradient Descent(77/99): loss=0.388948830717574, gradient=0.0006385950110943581\n",
      "Gradient Descent(78/99): loss=0.388948790731762, gradient=0.0006335877197387662\n",
      "Gradient Descent(79/99): loss=0.38894875136570517, gradient=0.0006286509346670748\n",
      "Gradient Descent(80/99): loss=0.38894871260602315, gradient=0.0006237831131693611\n",
      "Gradient Descent(81/99): loss=0.38894867443971687, gradient=0.0006189827688573728\n",
      "Gradient Descent(82/99): loss=0.3889486368541543, gradient=0.0006142484684317956\n",
      "Gradient Descent(83/99): loss=0.3889485998370561, gradient=0.0006095788286817975\n",
      "Gradient Descent(84/99): loss=0.38894856337648276, gradient=0.000604972513698715\n",
      "Gradient Descent(85/99): loss=0.3889485274608212, gradient=0.0006004282322872452\n",
      "Gradient Descent(86/99): loss=0.3889484920787743, gradient=0.0005959447355586773\n",
      "Gradient Descent(87/99): loss=0.38894845721934806, gradient=0.0005915208146924339\n",
      "Gradient Descent(88/99): loss=0.38894842287184156, gradient=0.0005871552988523104\n",
      "Gradient Descent(89/99): loss=0.38894838902583695, gradient=0.0005828470532464356\n",
      "Gradient Descent(90/99): loss=0.3889483556711893, gradient=0.0005785949773194759\n",
      "Gradient Descent(91/99): loss=0.3889483227980172, gradient=0.0005743980030672732\n",
      "Gradient Descent(92/99): loss=0.3889482903966946, gradient=0.0005702550934644468\n",
      "Gradient Descent(93/99): loss=0.38894825845784103, gradient=0.0005661652409970468\n",
      "Gradient Descent(94/99): loss=0.38894822697231546, gradient=0.0005621274662917524\n",
      "Gradient Descent(95/99): loss=0.38894819593120633, gradient=0.0005581408168346592\n",
      "Gradient Descent(96/99): loss=0.38894816532582627, gradient=0.0005542043657733321\n",
      "Gradient Descent(97/99): loss=0.38894813514770343, gradient=0.0005503172107952624\n",
      "Gradient Descent(98/99): loss=0.3889481053885756, gradient=0.0005464784730778682\n",
      "Gradient Descent(99/99): loss=0.3889480760403834, gradient=0.0005426872963042087\n",
      "Gradient Descent(0/99): loss=0.39014757372205217, gradient=0.015406699023835095\n",
      "Gradient Descent(1/99): loss=0.3901436475407687, gradient=0.006921512119081686\n",
      "Gradient Descent(2/99): loss=0.39014099291426074, gradient=0.005361851858943914\n",
      "Gradient Descent(3/99): loss=0.3901389956062345, gradient=0.004617042917565395\n",
      "Gradient Descent(4/99): loss=0.3901374501344022, gradient=0.004049818139091083\n",
      "Gradient Descent(5/99): loss=0.3901362281767277, gradient=0.003592231501793861\n",
      "Gradient Descent(6/99): loss=0.3901352422208566, gradient=0.003219250593577249\n",
      "Gradient Descent(7/99): loss=0.3901344313861915, gradient=0.0029130330860298254\n",
      "Gradient Descent(8/99): loss=0.3901337526949563, gradient=0.002659747264639909\n",
      "Gradient Descent(9/99): loss=0.3901331753685253, gradient=0.002448590809820427\n",
      "Gradient Descent(10/99): loss=0.3901326770455743, gradient=0.002271114945196502\n",
      "Gradient Descent(11/99): loss=0.3901322412464459, gradient=0.00212070197831955\n",
      "Gradient Descent(12/99): loss=0.39013185565405084, gradient=0.001992157714077221\n",
      "Gradient Descent(13/99): loss=0.3901315109351126, gradient=0.0018813937576273568\n",
      "Gradient Descent(14/99): loss=0.39013119992261713, gradient=0.0017851804799776886\n",
      "Gradient Descent(15/99): loss=0.3901309170422059, gradient=0.0017009554643721912\n",
      "Gradient Descent(16/99): loss=0.3901306579050499, gradient=0.0016266753586091723\n",
      "Gradient Descent(17/99): loss=0.3901304190155337, gradient=0.0015607015273649098\n",
      "Gradient Descent(18/99): loss=0.3901301975589554, gradient=0.001501711895020012\n",
      "Gradient Descent(19/99): loss=0.39012999124558434, gradient=0.0014486329911124558\n",
      "Gradient Descent(20/99): loss=0.3901297981948316, gradient=0.0014005875219148894\n",
      "Gradient Descent(21/99): loss=0.39012961684827135, gradient=0.0013568538423791151\n",
      "Gradient Descent(22/99): loss=0.3901294459036322, gradient=0.0013168345354327448\n",
      "Gradient Descent(23/99): loss=0.3901292842641809, gradient=0.0012800319584693601\n",
      "Gradient Descent(24/99): loss=0.3901291309995296, gradient=0.001246029123730714\n",
      "Gradient Descent(25/99): loss=0.3901289853149955, gradient=0.0012144746695353578\n",
      "Gradient Descent(26/99): loss=0.3901288465274342, gradient=0.0011850709777693842\n",
      "Gradient Descent(27/99): loss=0.390128714046014, gradient=0.0011575647201251908\n",
      "Gradient Descent(28/99): loss=0.3901285873568018, gradient=0.0011317392876660777\n",
      "Gradient Descent(29/99): loss=0.39012846601030726, gradient=0.001107408688390497\n",
      "Gradient Descent(30/99): loss=0.39012834961135257, gradient=0.0010844125956808038\n",
      "Gradient Descent(31/99): loss=0.3901282378107762, gradient=0.0010626123046405995\n",
      "Gradient Descent(32/99): loss=0.3901281302985981, gradient=0.001041887409304275\n",
      "Gradient Descent(33/99): loss=0.39012802679835884, gradient=0.0010221330560511923\n",
      "Gradient Descent(34/99): loss=0.3901279270624036, gradient=0.0010032576606792713\n",
      "Gradient Descent(35/99): loss=0.3901278308679372, gradient=0.0009851810010413147\n",
      "Gradient Descent(36/99): loss=0.3901277380137062, gradient=0.0009678326158350833\n",
      "Gradient Descent(37/99): loss=0.3901276483171992, gradient=0.0009511504544940177\n",
      "Gradient Descent(38/99): loss=0.39012756161227213, gradient=0.0009350797342145785\n",
      "Gradient Descent(39/99): loss=0.390127477747128, gradient=0.0009195719687749958\n",
      "Gradient Descent(40/99): loss=0.39012739658259277, gradient=0.0009045841405426133\n",
      "Gradient Descent(41/99): loss=0.39012731799063594, gradient=0.000890077992377324\n",
      "Gradient Descent(42/99): loss=0.3901272418531002, gradient=0.0008760194203479549\n",
      "Gradient Descent(43/99): loss=0.39012716806060466, gradient=0.0008623779515429283\n",
      "Gradient Descent(44/99): loss=0.3901270965115974, gradient=0.0008491262939562948\n",
      "Gradient Descent(45/99): loss=0.3901270271115311, gradient=0.0008362399476188071\n",
      "Gradient Descent(46/99): loss=0.39012695977214795, gradient=0.0008236968679215767\n",
      "Gradient Descent(47/99): loss=0.3901268944108541, gradient=0.0008114771735390746\n",
      "Gradient Descent(48/99): loss=0.3901268309501729, gradient=0.0007995628925556333\n",
      "Gradient Descent(49/99): loss=0.390126769317264, gradient=0.000787937741393013\n",
      "Gradient Descent(50/99): loss=0.3901267094435021, gradient=0.0007765869319614956\n",
      "Gradient Descent(51/99): loss=0.39012665126410345, gradient=0.0007654970031434621\n",
      "Gradient Descent(52/99): loss=0.39012659471779626, gradient=0.0007546556732966463\n",
      "Gradient Descent(53/99): loss=0.39012653974652856, gradient=0.0007440517109466007\n",
      "Gradient Descent(54/99): loss=0.39012648629520763, gradient=0.000733674821247471\n",
      "Gradient Descent(55/99): loss=0.39012643431146843, gradient=0.0007235155461351916\n",
      "Gradient Descent(56/99): loss=0.3901263837454664, gradient=0.0007135651763884657\n",
      "Gradient Descent(57/99): loss=0.3901263345496917, gradient=0.0007038156740637576\n",
      "Gradient Descent(58/99): loss=0.39012628667880234, gradient=0.0006942596039790676\n",
      "Gradient Descent(59/99): loss=0.3901262400894737, gradient=0.0006848900731051648\n",
      "Gradient Descent(60/99): loss=0.3901261947402633, gradient=0.0006757006768739912\n",
      "Gradient Descent(61/99): loss=0.39012615059148803, gradient=0.000666685451548889\n",
      "Gradient Descent(62/99): loss=0.390126107605112, gradient=0.0006578388319127287\n",
      "Gradient Descent(63/99): loss=0.39012606574464653, gradient=0.0006491556136285338\n",
      "Gradient Descent(64/99): loss=0.3901260249750563, gradient=0.0006406309197105244\n",
      "Gradient Descent(65/99): loss=0.3901259852626761, gradient=0.0006322601706161599\n",
      "Gradient Descent(66/99): loss=0.3901259465751334, gradient=0.0006240390575318078\n",
      "Gradient Descent(67/99): loss=0.3901259088812766, gradient=0.000615963518478863\n",
      "Gradient Descent(68/99): loss=0.39012587215111116, gradient=0.0006080297169137122\n",
      "Gradient Descent(69/99): loss=0.3901258363557378, gradient=0.0006002340225358073\n",
      "Gradient Descent(70/99): loss=0.3901258014672978, gradient=0.0005925729940524326\n",
      "Gradient Descent(71/99): loss=0.39012576745892147, gradient=0.000585043363680368\n",
      "Gradient Descent(72/99): loss=0.3901257343046813, gradient=0.000577642023190616\n",
      "Gradient Descent(73/99): loss=0.39012570197954494, gradient=0.000570366011325687\n",
      "Gradient Descent(74/99): loss=0.39012567045933655, gradient=0.0005632125024390669\n",
      "Gradient Descent(75/99): loss=0.39012563972069675, gradient=0.0005561787962246127\n",
      "Gradient Descent(76/99): loss=0.3901256097410472, gradient=0.0005492623084186921\n",
      "Gradient Descent(77/99): loss=0.3901255804985562, gradient=0.0005424605623714281\n",
      "Gradient Descent(78/99): loss=0.39012555197210724, gradient=0.0005357711813955278\n",
      "Gradient Descent(79/99): loss=0.3901255241412698, gradient=0.0005291918818116509\n",
      "Gradient Descent(80/99): loss=0.39012549698627, gradient=0.0005227204666178708\n",
      "Gradient Descent(81/99): loss=0.3901254704879659, gradient=0.0005163548197199725\n",
      "Gradient Descent(82/99): loss=0.390125444627821, gradient=0.0005100929006650203\n",
      "Gradient Descent(83/99): loss=0.3901254193878814, gradient=0.000503932739828072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(84/99): loss=0.39012539475075314, gradient=0.0004978724340068935\n",
      "Gradient Descent(85/99): loss=0.39012537069958186, gradient=0.000491910142384535\n",
      "Gradient Descent(86/99): loss=0.39012534721803194, gradient=0.00048604408282383973\n",
      "Gradient Descent(87/99): loss=0.3901253242902676, gradient=0.00048027252846199304\n",
      "Gradient Descent(88/99): loss=0.39012530190093503, gradient=0.0004745938045764069\n",
      "Gradient Descent(89/99): loss=0.39012528003514435, gradient=0.00046900628569643835\n",
      "Gradient Descent(90/99): loss=0.390125258678455, gradient=0.00046350839293795584\n",
      "Gradient Descent(91/99): loss=0.3901252378168575, gradient=0.0004580985915400133\n",
      "Gradient Descent(92/99): loss=0.3901252174367611, gradient=0.0004527753885852957\n",
      "Gradient Descent(93/99): loss=0.3901251975249772, gradient=0.0004475373308880299\n",
      "Gradient Descent(94/99): loss=0.39012517806870706, gradient=0.0004423830030339101\n",
      "Gradient Descent(95/99): loss=0.39012515905552847, gradient=0.00043731102555879126\n",
      "Gradient Descent(96/99): loss=0.39012514047338287, gradient=0.0004323200532544034\n",
      "Gradient Descent(97/99): loss=0.39012512231056357, gradient=0.00042740877358972976\n",
      "Gradient Descent(98/99): loss=0.39012510455570415, gradient=0.00042257590523837424\n",
      "Gradient Descent(99/99): loss=0.39012508719776745, gradient=0.0004178201967029873\n",
      "Gradient Descent(0/99): loss=0.39035838648850496, gradient=0.007268494390481906\n",
      "Gradient Descent(1/99): loss=0.3903547094476407, gradient=0.0062564590709536395\n",
      "Gradient Descent(2/99): loss=0.3903518217811404, gradient=0.005524514521310404\n",
      "Gradient Descent(3/99): loss=0.3903494947676303, gradient=0.004945158721754367\n",
      "Gradient Descent(4/99): loss=0.39034757659497843, gradient=0.004478448962629226\n",
      "Gradient Descent(5/99): loss=0.390345963600866, gradient=0.00409766951917289\n",
      "Gradient Descent(6/99): loss=0.39034458354365353, gradient=0.0037829725608351456\n",
      "Gradient Descent(7/99): loss=0.3903433850615272, gradient=0.003519497492110589\n",
      "Gradient Descent(8/99): loss=0.39034233091411474, gradient=0.0032960736341950342\n",
      "Gradient Descent(9/99): loss=0.3903413935606349, gradient=0.0031043400441930825\n",
      "Gradient Descent(10/99): loss=0.3903405522734891, gradient=0.002937896132985681\n",
      "Gradient Descent(11/99): loss=0.39033979117448514, gradient=0.0027918752488844427\n",
      "Gradient Descent(12/99): loss=0.39033909790620186, gradient=0.0026625112791718477\n",
      "Gradient Descent(13/99): loss=0.3903384627067092, gradient=0.0025468717568302587\n",
      "Gradient Descent(14/99): loss=0.3903378777578295, gradient=0.0024426486113883522\n",
      "Gradient Descent(15/99): loss=0.39033733672391163, gradient=0.0023479818657561754\n",
      "Gradient Descent(16/99): loss=0.3903368343935675, gradient=0.002261455535989682\n",
      "Gradient Descent(17/99): loss=0.39033636644807645, gradient=0.0021818537810516063\n",
      "Gradient Descent(18/99): loss=0.39033592926688104, gradient=0.0021082091999414343\n",
      "Gradient Descent(19/99): loss=0.39033551978544656, gradient=0.002039728471302423\n",
      "Gradient Descent(20/99): loss=0.3903351353864099, gradient=0.0019757567810592515\n",
      "Gradient Descent(21/99): loss=0.3903347738153601, gradient=0.001915750324385531\n",
      "Gradient Descent(22/99): loss=0.39033443311506166, gradient=0.0018592548499131627\n",
      "Gradient Descent(23/99): loss=0.3903341115736338, gradient=0.0018058887763011495\n",
      "Gradient Descent(24/99): loss=0.39033380768418563, gradient=0.0017553248116977797\n",
      "Gradient Descent(25/99): loss=0.3903335201092796, gradient=0.0017073000167922737\n",
      "Gradient Descent(26/99): loss=0.39033324765779215, gradient=0.0016615713250004744\n",
      "Gradient Descent(27/99): loss=0.39033298926055776, gradient=0.0016179454013311352\n",
      "Gradient Descent(28/99): loss=0.3903327439545796, gradient=0.0015762454308448413\n",
      "Gradient Descent(29/99): loss=0.3903325108679112, gradient=0.0015363212426889783\n",
      "Gradient Descent(30/99): loss=0.3903322892076685, gradient=0.0014980418373444521\n",
      "Gradient Descent(31/99): loss=0.3903320782500474, gradient=0.001461292320555063\n",
      "Gradient Descent(32/99): loss=0.3903318773319589, gradient=0.0014259713792072837\n",
      "Gradient Descent(33/99): loss=0.390331685843975, gradient=0.001391989196585955\n",
      "Gradient Descent(34/99): loss=0.3903315032243503, gradient=0.0013592657250879139\n",
      "Gradient Descent(35/99): loss=0.39033132895391837, gradient=0.0013277292506897339\n",
      "Gradient Descent(36/99): loss=0.39033116255171485, gradient=0.0012973151962679533\n",
      "Gradient Descent(37/99): loss=0.390331003571202, gradient=0.0012679651210322778\n",
      "Gradient Descent(38/99): loss=0.3903308515969912, gradient=0.0012396258814313763\n",
      "Gradient Descent(39/99): loss=0.3903307062419866, gradient=0.0012122489253746432\n",
      "Gradient Descent(40/99): loss=0.39033056714487924, gradient=0.0011857896968187343\n",
      "Gradient Descent(41/99): loss=0.39033043396794054, gradient=0.0011602071319628277\n",
      "Gradient Descent(42/99): loss=0.390330306395068, gradient=0.0011354632316845688\n",
      "Gradient Descent(43/99): loss=0.3903301841300478, gradient=0.0011115226975939596\n",
      "Gradient Descent(44/99): loss=0.39033006689500227, gradient=0.001088352621312082\n",
      "Gradient Descent(45/99): loss=0.39032995443016366, gradient=0.0010659109703316557\n",
      "Gradient Descent(46/99): loss=0.39032984648902685, gradient=0.0010441917506242458\n",
      "Gradient Descent(47/99): loss=0.3903297428409133, gradient=0.0010231561141510314\n",
      "Gradient Descent(48/99): loss=0.39032964326867386, gradient=0.0010027783990842315\n",
      "Gradient Descent(49/99): loss=0.3903295475677547, gradient=0.0009830343246692297\n",
      "Gradient Descent(50/99): loss=0.3903294555453442, gradient=0.0009639008630491814\n",
      "Gradient Descent(51/99): loss=0.39032936701959214, gradient=0.0009453561275843138\n",
      "Gradient Descent(52/99): loss=0.3903292818188935, gradient=0.0009273792752061028\n",
      "Gradient Descent(53/99): loss=0.3903291997812296, gradient=0.0009099504207887056\n",
      "Gradient Descent(54/99): loss=0.3903291207535606, gradient=0.0008930505618367133\n",
      "Gradient Descent(55/99): loss=0.3903290445912648, gradient=0.0008766615120516588\n",
      "Gradient Descent(56/99): loss=0.3903289711576189, gradient=0.0008607658425575365\n",
      "Gradient Descent(57/99): loss=0.39032890032331735, gradient=0.0008453468297490697\n",
      "Gradient Descent(58/99): loss=0.3903288319660239, gradient=0.0008303884088794674\n",
      "Gradient Descent(59/99): loss=0.3903287659699575, gradient=0.000815875132632747\n",
      "Gradient Descent(60/99): loss=0.39032870222550353, gradient=0.0008017921340348527\n",
      "Gradient Descent(61/99): loss=0.39032864062885375, gradient=0.0007881250931484894\n",
      "Gradient Descent(62/99): loss=0.39032858108166796, gradient=0.0007748602070741628\n",
      "Gradient Descent(63/99): loss=0.39032852349075986, gradient=0.0007619841628460251\n",
      "Gradient Descent(64/99): loss=0.39032846776780145, gradient=0.0007494841128661252\n",
      "Gradient Descent(65/99): loss=0.3903284138290478, gradient=0.0007373476525687726\n",
      "Gradient Descent(66/99): loss=0.39032836159507767, gradient=0.0007255628000469776\n",
      "Gradient Descent(67/99): loss=0.3903283109905505, gradient=0.0007141179774082723\n",
      "Gradient Descent(68/99): loss=0.3903282619439795, gradient=0.0007030019936561154\n",
      "Gradient Descent(69/99): loss=0.390328214387517, gradient=0.0006922040289199379\n",
      "Gradient Descent(70/99): loss=0.39032816825675365, gradient=0.00068171361987847\n",
      "Gradient Descent(71/99): loss=0.39032812349052964, gradient=0.0006715206462399382\n",
      "Gradient Descent(72/99): loss=0.39032808003075625, gradient=0.0006616153181603669\n",
      "Gradient Descent(73/99): loss=0.39032803782224895, gradient=0.0006519881644946891\n",
      "Gradient Descent(74/99): loss=0.39032799681256963, gradient=0.000642630021788665\n",
      "Gradient Descent(75/99): loss=0.39032795695187783, gradient=0.0006335320239305413\n",
      "Gradient Descent(76/99): loss=0.39032791819279133, gradient=0.0006246855923908169\n",
      "Gradient Descent(77/99): loss=0.39032788049025374, gradient=0.0006160824269875699\n",
      "Gradient Descent(78/99): loss=0.3903278438014104, gradient=0.0006077144971214214\n",
      "Gradient Descent(79/99): loss=0.390327808085491, gradient=0.0005995740334319735\n",
      "Gradient Descent(80/99): loss=0.39032777330369856, gradient=0.0005916535198321225\n",
      "Gradient Descent(81/99): loss=0.3903277394191051, gradient=0.000583945685882826\n",
      "Gradient Descent(82/99): loss=0.39032770639655334, gradient=0.0005764434994749924\n",
      "Gradient Descent(83/99): loss=0.3903276742025632, gradient=0.0005691401597892918\n",
      "Gradient Descent(84/99): loss=0.3903276428052434, gradient=0.0005620290905082221\n",
      "Gradient Descent(85/99): loss=0.3903276121742094, gradient=0.0005551039332581653\n",
      "Gradient Descent(86/99): loss=0.3903275822805035, gradient=0.0005483585412617675\n",
      "Gradient Descent(87/99): loss=0.390327553096522, gradient=0.0005417869731834644\n",
      "Gradient Descent(88/99): loss=0.39032752459594394, gradient=0.0005353834871536364\n",
      "Gradient Descent(89/99): loss=0.3903274967536655, gradient=0.0005291425349579826\n",
      "Gradient Descent(90/99): loss=0.3903274695457369, gradient=0.000523058756381682\n",
      "Gradient Descent(91/99): loss=0.39032744294930277, gradient=0.0005171269736978997\n",
      "Gradient Descent(92/99): loss=0.3903274169425474, gradient=0.0005113421862931758\n",
      "Gradient Descent(93/99): loss=0.39032739150464024, gradient=0.0005056995654221215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(94/99): loss=0.39032736661568634, gradient=0.0005001944490854222\n",
      "Gradient Descent(95/99): loss=0.3903273422566784, gradient=0.0004948223370264419\n",
      "Gradient Descent(96/99): loss=0.39032731840945334, gradient=0.0004895788858415256\n",
      "Gradient Descent(97/99): loss=0.39032729505664726, gradient=0.0004844599042006948\n",
      "Gradient Descent(98/99): loss=0.39032727218165736, gradient=0.00047946134817560004\n",
      "Gradient Descent(99/99): loss=0.39032724976860284, gradient=0.0004745793166719505\n",
      "Gradient Descent(0/99): loss=0.38997855863420855, gradient=0.010008325554992035\n",
      "Gradient Descent(1/99): loss=0.3899723258614059, gradient=0.008243693580335476\n",
      "Gradient Descent(2/99): loss=0.3899679148516404, gradient=0.006907644665988863\n",
      "Gradient Descent(3/99): loss=0.3899646816999578, gradient=0.0058897594414250055\n",
      "Gradient Descent(4/99): loss=0.3899622267684954, gradient=0.005111509133590179\n",
      "Gradient Descent(5/99): loss=0.38996029888567973, gradient=0.00451256305560174\n",
      "Gradient Descent(6/99): loss=0.3899587378552126, gradient=0.004046853752230402\n",
      "Gradient Descent(7/99): loss=0.3899574397580658, gradient=0.0036795972542015155\n",
      "Gradient Descent(8/99): loss=0.38995633582707323, gradient=0.0033850149216741077\n",
      "Gradient Descent(9/99): loss=0.3899553795304474, gradient=0.003144263584197577\n",
      "Gradient Descent(10/99): loss=0.38995453860793206, gradient=0.0029437162709106115\n",
      "Gradient Descent(11/99): loss=0.3899537901146976, gradient=0.002773573883329604\n",
      "Gradient Descent(12/99): loss=0.389953117302824, gradient=0.002626791653267734\n",
      "Gradient Descent(13/99): loss=0.38995250763354583, gradient=0.0024982799739642425\n",
      "Gradient Descent(14/99): loss=0.38995195149561374, gradient=0.0023843086229860576\n",
      "Gradient Descent(15/99): loss=0.38995144134611887, gradient=0.0022821713436349794\n",
      "Gradient Descent(16/99): loss=0.389950971156838, gradient=0.0021897923271306753\n",
      "Gradient Descent(17/99): loss=0.38995053602228974, gradient=0.0021056005870697995\n",
      "Gradient Descent(18/99): loss=0.3899501318893214, gradient=0.002028375873705145\n",
      "Gradient Descent(19/99): loss=0.38994975536444154, gradient=0.001957154012809442\n",
      "Gradient Descent(20/99): loss=0.3899494035736933, gradient=0.0018911607624822462\n",
      "Gradient Descent(21/99): loss=0.3899490740587182, gradient=0.001829765109277292\n",
      "Gradient Descent(22/99): loss=0.3899487647004994, gradient=0.0017724322474569126\n",
      "Gradient Descent(23/99): loss=0.38994847365182717, gradient=0.0017187549235608251\n",
      "Gradient Descent(24/99): loss=0.38994819929786584, gradient=0.0016683502443646807\n",
      "Gradient Descent(25/99): loss=0.3899479402161898, gradient=0.0016209044746757313\n",
      "Gradient Descent(26/99): loss=0.38994769514843486, gradient=0.0015761382649425483\n",
      "Gradient Descent(27/99): loss=0.38994746297163324, gradient=0.0015338375991774994\n",
      "Gradient Descent(28/99): loss=0.3899472426831995, gradient=0.0014937886528701648\n",
      "Gradient Descent(29/99): loss=0.38994703338305514, gradient=0.001455812542767595\n",
      "Gradient Descent(30/99): loss=0.38994683426011567, gradient=0.0014197513529554504\n",
      "Gradient Descent(31/99): loss=0.3899466445809867, gradient=0.001385464915276826\n",
      "Gradient Descent(32/99): loss=0.3899464636804349, gradient=0.0013528281742510313\n",
      "Gradient Descent(33/99): loss=0.38994629095331157, gradient=0.0013217290142704106\n",
      "Gradient Descent(34/99): loss=0.3899461258476636, gradient=0.0012920664562818271\n",
      "Gradient Descent(35/99): loss=0.3899459678588292, gradient=0.0012637491525178306\n",
      "Gradient Descent(36/99): loss=0.3899458165243461, gradient=0.001236694123587524\n",
      "Gradient Descent(37/99): loss=0.38994567141954434, gradient=0.0012108256940177077\n",
      "Gradient Descent(38/99): loss=0.38994553215371, gradient=0.0011860745912782079\n",
      "Gradient Descent(39/99): loss=0.38994539836673286, gradient=0.0011623771801998488\n",
      "Gradient Descent(40/99): loss=0.3899452697261653, gradient=0.0011396748100459312\n",
      "Gradient Descent(41/99): loss=0.38994514592462926, gradient=0.0011179132557065038\n",
      "Gradient Descent(42/99): loss=0.3899450266775244, gradient=0.0010970422378253756\n",
      "Gradient Descent(43/99): loss=0.38994491172099266, gradient=0.0010770150093464736\n",
      "Gradient Descent(44/99): loss=0.389944800810106, gradient=0.0010577879981213767\n",
      "Gradient Descent(45/99): loss=0.3899446937172474, gradient=0.0010393204969715716\n",
      "Gradient Descent(46/99): loss=0.38994459023065925, gradient=0.0010215743940274573\n",
      "Gradient Descent(47/99): loss=0.3899444901531384, gradient=0.001004513937337251\n",
      "Gradient Descent(48/99): loss=0.38994439330086245, gradient=0.000988105528705319\n",
      "Gradient Descent(49/99): loss=0.38994429950232484, gradient=0.000972317542517525\n",
      "Gradient Descent(50/99): loss=0.3899442085973752, gradient=0.0009571201659746603\n",
      "Gradient Descent(51/99): loss=0.38994412043634546, gradient=0.0009424852577072396\n",
      "Gradient Descent(52/99): loss=0.38994403487925555, gradient=0.0009283862222068975\n",
      "Gradient Descent(53/99): loss=0.3899439517950892, gradient=0.0009147978978970132\n",
      "Gradient Descent(54/99): loss=0.38994387106113343, gradient=0.0009016964569890897\n",
      "Gradient Descent(55/99): loss=0.38994379256237427, gradient=0.0008890593155474278\n",
      "Gradient Descent(56/99): loss=0.3899437161909421, gradient=0.0008768650524141935\n",
      "Gradient Descent(57/99): loss=0.3899436418456041, gradient=0.0008650933358439782\n",
      "Gradient Descent(58/99): loss=0.38994356943129665, gradient=0.0008537248568624429\n",
      "Gradient Descent(59/99): loss=0.3899434988586968, gradient=0.0008427412685050626\n",
      "Gradient Descent(60/99): loss=0.38994343004382515, gradient=0.0008321251302115213\n",
      "Gradient Descent(61/99): loss=0.38994336290768145, gradient=0.0008218598567540776\n",
      "Gradient Descent(62/99): loss=0.38994329737590816, gradient=0.0008119296711653482\n",
      "Gradient Descent(63/99): loss=0.3899432333784776, gradient=0.0008023195612049129\n",
      "Gradient Descent(64/99): loss=0.38994317084940516, gradient=0.000793015238968888\n",
      "Gradient Descent(65/99): loss=0.38994310972648155, gradient=0.0007840031033002318\n",
      "Gradient Descent(66/99): loss=0.38994304995102624, gradient=0.0007752702047044377\n",
      "Gradient Descent(67/99): loss=0.3899429914676569, gradient=0.0007668042125156427\n",
      "Gradient Descent(68/99): loss=0.3899429342240782, gradient=0.0007585933840918492\n",
      "Gradient Descent(69/99): loss=0.3899428781708829, gradient=0.0007506265358475581\n",
      "Gradient Descent(70/99): loss=0.3899428232613684, gradient=0.0007428930159574406\n",
      "Gradient Descent(71/99): loss=0.3899427694513658, gradient=0.0007353826785855078\n",
      "Gradient Descent(72/99): loss=0.3899427166990809, gradient=0.0007280858595140378\n",
      "Gradient Descent(73/99): loss=0.38994266496494573, gradient=0.0007209933530605923\n",
      "Gradient Descent(74/99): loss=0.3899426142114814, gradient=0.0007140963901864718\n",
      "Gradient Descent(75/99): loss=0.38994256440316916, gradient=0.000707386617710792\n",
      "Gradient Descent(76/99): loss=0.3899425155063294, gradient=0.0007008560785546923\n",
      "Gradient Descent(77/99): loss=0.38994246748901135, gradient=0.000694497192947939\n",
      "Gradient Descent(78/99): loss=0.38994242032088783, gradient=0.0006883027405388047\n",
      "Gradient Descent(79/99): loss=0.3899423739731577, gradient=0.0006822658433529317\n",
      "Gradient Descent(80/99): loss=0.38994232841845483, gradient=0.0006763799495531654\n",
      "Gradient Descent(81/99): loss=0.3899422836307625, gradient=0.0006706388179565295\n",
      "Gradient Descent(82/99): loss=0.38994223958533497, gradient=0.000665036503268531\n",
      "Gradient Descent(83/99): loss=0.38994219625862175, gradient=0.0006595673419985201\n",
      "Gradient Descent(84/99): loss=0.38994215362819856, gradient=0.0006542259390224057\n",
      "Gradient Descent(85/99): loss=0.38994211167270143, gradient=0.0006490071547619817\n",
      "Gradient Descent(86/99): loss=0.38994207037176726, gradient=0.0006439060929520732\n",
      "Gradient Descent(87/99): loss=0.38994202970597386, gradient=0.0006389180889683782\n",
      "Gradient Descent(88/99): loss=0.3899419896567897, gradient=0.0006340386986911864\n",
      "Gradient Descent(89/99): loss=0.3899419502065207, gradient=0.000629263687881016\n",
      "Gradient Descent(90/99): loss=0.3899419113382651, gradient=0.0006245890220439371\n",
      "Gradient Descent(91/99): loss=0.38994187303586836, gradient=0.0006200108567649144\n",
      "Gradient Descent(92/99): loss=0.3899418352838821, gradient=0.0006155255284896084\n",
      "Gradient Descent(93/99): loss=0.3899417980675248, gradient=0.0006111295457347656\n",
      "Gradient Descent(94/99): loss=0.3899417613726464, gradient=0.0006068195807091006\n",
      "Gradient Descent(95/99): loss=0.3899417251856926, gradient=0.0006025924613268762\n",
      "Gradient Descent(96/99): loss=0.38994168949367364, gradient=0.000598445163597596\n",
      "Gradient Descent(97/99): loss=0.3899416542841338, gradient=0.0005943748043750243\n",
      "Gradient Descent(98/99): loss=0.3899416195451228, gradient=0.0005903786344507332\n",
      "Gradient Descent(99/99): loss=0.3899415852651693, gradient=0.0005864540319764825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=0.38967699523469174, gradient=0.011103453399653952\n",
      "Gradient Descent(1/99): loss=0.38967200637506094, gradient=0.007449655540156053\n",
      "Gradient Descent(2/99): loss=0.38966846896555746, gradient=0.0061839442778306875\n",
      "Gradient Descent(3/99): loss=0.38966587113638873, gradient=0.005283157064326815\n",
      "Gradient Descent(4/99): loss=0.3896639243909686, gradient=0.004562981569236207\n",
      "Gradient Descent(5/99): loss=0.38966243711190784, gradient=0.0039793928555175336\n",
      "Gradient Descent(6/99): loss=0.38966127905217923, gradient=0.0035037526172558464\n",
      "Gradient Descent(7/99): loss=0.3896603606425932, gradient=0.0031136692214803977\n",
      "Gradient Descent(8/99): loss=0.38965961954774464, gradient=0.0027914637383541986\n",
      "Gradient Descent(9/99): loss=0.3896590118256463, gradient=0.002523207067755984\n",
      "Gradient Descent(10/99): loss=0.3896585060727378, gradient=0.0022979663749891316\n",
      "Gradient Descent(11/99): loss=0.3896580795202336, gradient=0.0021071879589974526\n",
      "Gradient Descent(12/99): loss=0.3896577154124203, gradient=0.0019441925216130364\n",
      "Gradient Descent(13/99): loss=0.3896574012316597, gradient=0.0018037657973857812\n",
      "Gradient Descent(14/99): loss=0.3896571274859129, gradient=0.0016818303895260722\n",
      "Gradient Descent(15/99): loss=0.38965688687240085, gradient=0.0015751860776943715\n",
      "Gradient Descent(16/99): loss=0.3896566736945626, gradient=0.0014813068720711364\n",
      "Gradient Descent(17/99): loss=0.38965648345093223, gradient=0.0013981841712230165\n",
      "Gradient Descent(18/99): loss=0.3896563125417051, gradient=0.001324206656123106\n",
      "Gradient Descent(19/99): loss=0.3896561580566289, gradient=0.0012580689502492868\n",
      "Gradient Descent(20/99): loss=0.3896560176196685, gradient=0.001198702474599645\n",
      "Gradient Descent(21/99): loss=0.3896558892737352, gradient=0.0011452232236603581\n",
      "Gradient Descent(22/99): loss=0.38965577139402113, gradient=0.001096892322043705\n",
      "Gradient Descent(23/99): loss=0.3896556626220018, gradient=0.001053086168642534\n",
      "Gradient Descent(24/99): loss=0.38965556181455796, gradient=0.0010132737398387958\n",
      "Gradient Descent(25/99): loss=0.389655468004299, gradient=0.000976999224867033\n",
      "Gradient Descent(26/99): loss=0.3896553803682863, gradient=0.0009438686303934123\n",
      "Gradient Descent(27/99): loss=0.3896552982031355, gradient=0.000913539343810629\n",
      "Gradient Descent(28/99): loss=0.3896552209050139, gradient=0.0008857119093589974\n",
      "Gradient Descent(29/99): loss=0.3896551479534442, gradient=0.0008601234680505192\n",
      "Gradient Descent(30/99): loss=0.3896550788980897, gradient=0.0008365424577924373\n",
      "Gradient Descent(31/99): loss=0.3896550133479044, gradient=0.0008147642769204777\n",
      "Gradient Descent(32/99): loss=0.3896549509621699, gradient=0.0007946076924600987\n",
      "Gradient Descent(33/99): loss=0.38965489144305193, gradient=0.000775911831362726\n",
      "Gradient Descent(34/99): loss=0.38965483452939037, gradient=0.0007585336343590475\n",
      "Gradient Descent(35/99): loss=0.3896547799914932, gradient=0.0007423456821439201\n",
      "Gradient Descent(36/99): loss=0.3896547276267567, gradient=0.0007272343254569074\n",
      "Gradient Descent(37/99): loss=0.3896546772559662, gradient=0.0007130980665344713\n",
      "Gradient Descent(38/99): loss=0.3896546287201601, gradient=0.0006998461510419645\n",
      "Gradient Descent(39/99): loss=0.38965458187796415, gradient=0.0006873973381543196\n",
      "Gradient Descent(40/99): loss=0.3896545366033169, gradient=0.0006756788228116477\n",
      "Gradient Descent(41/99): loss=0.3896544927835255, gradient=0.0006646252889541561\n",
      "Gradient Descent(42/99): loss=0.3896544503175966, gradient=0.0006541780761826461\n",
      "Gradient Descent(43/99): loss=0.3896544091148007, gradient=0.0006442844451154365\n",
      "Gradient Descent(44/99): loss=0.38965436909343526, gradient=0.0006348969289418843\n",
      "Gradient Descent(45/99): loss=0.3896543301797547, gradient=0.0006259727604633417\n",
      "Gradient Descent(46/99): loss=0.38965429230704224, gradient=0.0006174733653797679\n",
      "Gradient Descent(47/99): loss=0.3896542554148047, gradient=0.0006093639137986297\n",
      "Gradient Descent(48/99): loss=0.38965421944807094, gradient=0.0006016129229719123\n",
      "Gradient Descent(49/99): loss=0.3896541843567799, gradient=0.0005941919051438234\n",
      "Gradient Descent(50/99): loss=0.38965415009524407, gradient=0.0005870750551487012\n",
      "Gradient Descent(51/99): loss=0.389654116621681, gradient=0.0005802389730539065\n",
      "Gradient Descent(52/99): loss=0.3896540838978003, gradient=0.0005736624177135928\n",
      "Gradient Descent(53/99): loss=0.3896540518884402, gradient=0.0005673260876011452\n",
      "Gradient Descent(54/99): loss=0.38965402056124865, gradient=0.0005612124257244727\n",
      "Gradient Descent(55/99): loss=0.38965398988639854, gradient=0.0005553054458157985\n",
      "Gradient Descent(56/99): loss=0.3896539598363385, gradient=0.0005495905773243195\n",
      "Gradient Descent(57/99): loss=0.38965393038556967, gradient=0.0005440545270388857\n",
      "Gradient Descent(58/99): loss=0.3896539015104492, gradient=0.0005386851554285677\n",
      "Gradient Descent(59/99): loss=0.3896538731890128, gradient=0.0005334713660189359\n",
      "Gradient Descent(60/99): loss=0.38965384540081954, gradient=0.0005284030063240798\n",
      "Gradient Descent(61/99): loss=0.38965381812681077, gradient=0.0005234707790307802\n",
      "Gradient Descent(62/99): loss=0.3896537913491849, gradient=0.0005186661622875893\n",
      "Gradient Descent(63/99): loss=0.3896537650512859, gradient=0.0005139813380870459\n",
      "Gradient Descent(64/99): loss=0.3896537392175014, gradient=0.0005094091278502518\n",
      "Gradient Descent(65/99): loss=0.3896537138331732, gradient=0.0005049429344266398\n",
      "Gradient Descent(66/99): loss=0.3896536888845153, gradient=0.0005005766898149135\n",
      "Gradient Descent(67/99): loss=0.38965366435854004, gradient=0.0004963048079915015\n",
      "Gradient Descent(68/99): loss=0.3896536402429939, gradient=0.0004921221423038171\n",
      "Gradient Descent(69/99): loss=0.3896536165262944, gradient=0.0004880239469479609\n",
      "Gradient Descent(70/99): loss=0.3896535931974792, gradient=0.00048400584210528776\n",
      "Gradient Descent(71/99): loss=0.38965357024615566, gradient=0.0004800637823602036\n",
      "Gradient Descent(72/99): loss=0.38965354766245625, gradient=0.0004761940280638131\n",
      "Gradient Descent(73/99): loss=0.3896535254369985, gradient=0.0004723931193452251\n",
      "Gradient Descent(74/99): loss=0.3896535035608491, gradient=0.00046865785250560196\n",
      "Gradient Descent(75/99): loss=0.3896534820254897, gradient=0.00046498525855779465\n",
      "Gradient Descent(76/99): loss=0.3896534608227868, gradient=0.0004613725837014327\n",
      "Gradient Descent(77/99): loss=0.38965343994496454, gradient=0.0004578172715444635\n",
      "Gradient Descent(78/99): loss=0.3896534193845787, gradient=0.0004543169469030518\n",
      "Gradient Descent(79/99): loss=0.3896533991344951, gradient=0.0004508694010287575\n",
      "Gradient Descent(80/99): loss=0.38965337918786686, gradient=0.0004474725781281691\n",
      "Gradient Descent(81/99): loss=0.38965335953811653, gradient=0.0004441245630530665\n",
      "Gradient Descent(82/99): loss=0.38965334017891773, gradient=0.0004408235700529328\n",
      "Gradient Descent(83/99): loss=0.38965332110417916, gradient=0.0004375679324913086\n",
      "Gradient Descent(84/99): loss=0.3896533023080298, gradient=0.0004343560934376395\n",
      "Gradient Descent(85/99): loss=0.3896532837848054, gradient=0.00043118659705572833\n",
      "Gradient Descent(86/99): loss=0.38965326552903584, gradient=0.00042805808071633703\n",
      "Gradient Descent(87/99): loss=0.38965324753543384, gradient=0.00042496926776973475\n",
      "Gradient Descent(88/99): loss=0.389653229798884, gradient=0.00042191896091897966\n",
      "Gradient Descent(89/99): loss=0.38965321231443306, gradient=0.0004189060361418422\n",
      "Gradient Descent(90/99): loss=0.38965319507728163, gradient=0.000415929437112499\n",
      "Gradient Descent(91/99): loss=0.3896531780827746, gradient=0.0004129881700798684\n",
      "Gradient Descent(92/99): loss=0.3896531613263944, gradient=0.0004100812991633119\n",
      "Gradient Descent(93/99): loss=0.38965314480375385, gradient=0.0004072079420293646\n",
      "Gradient Descent(94/99): loss=0.3896531285105889, gradient=0.00040436726591765436\n",
      "Gradient Descent(95/99): loss=0.38965311244275325, gradient=0.0004015584839854175\n",
      "Gradient Descent(96/99): loss=0.389653096596212, gradient=0.00039878085194468776\n",
      "Gradient Descent(97/99): loss=0.38965308096703694, gradient=0.00039603366496681827\n",
      "Gradient Descent(98/99): loss=0.38965306555140145, gradient=0.00039331625483240464\n",
      "Gradient Descent(99/99): loss=0.3896530503455756, gradient=0.0003906279873062288\n",
      "Gradient Descent(0/99): loss=0.3889639262311419, gradient=0.008175457325580874\n",
      "Gradient Descent(1/99): loss=0.38896116522388274, gradient=0.005550158077564088\n",
      "Gradient Descent(2/99): loss=0.38895924720356967, gradient=0.00456437903148517\n",
      "Gradient Descent(3/99): loss=0.3889578658424408, gradient=0.0038602841520948496\n",
      "Gradient Descent(4/99): loss=0.3889568459612036, gradient=0.0033071578628134923\n",
      "Gradient Descent(5/99): loss=0.38895607291649537, gradient=0.0028701518493927758\n",
      "Gradient Descent(6/99): loss=0.3889554703781103, gradient=0.002525506276235147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(7/99): loss=0.38895498714347576, gradient=0.0022541262085134235\n",
      "Gradient Descent(8/99): loss=0.38895458861308774, gradient=0.0020404474810902322\n",
      "Gradient Descent(9/99): loss=0.3889542512171082, gradient=0.0018718302273321682\n",
      "Gradient Descent(10/99): loss=0.38895395875406874, gradient=0.0017381081238158895\n",
      "Gradient Descent(11/99): loss=0.3889536999748678, gradient=0.0016312127590296945\n",
      "Gradient Descent(12/99): loss=0.3889534669810489, gradient=0.0015448352639996952\n",
      "Gradient Descent(13/99): loss=0.3889532541576244, gradient=0.0014741128531717052\n",
      "Gradient Descent(14/99): loss=0.3889530574582619, gradient=0.0014153430455681089\n",
      "Gradient Descent(15/99): loss=0.388952873923733, gradient=0.0013657328129349023\n",
      "Gradient Descent(16/99): loss=0.38895270135548643, gradient=0.0013231876003357561\n",
      "Gradient Descent(17/99): loss=0.3889525380928725, gradient=0.0012861406761299626\n",
      "Gradient Descent(18/99): loss=0.38895238285998296, gradient=0.0012534194852098948\n",
      "Gradient Descent(19/99): loss=0.3889522346595043, gradient=0.0012241435742070621\n",
      "Gradient Descent(20/99): loss=0.3889520926985082, gradient=0.0011976480634950132\n",
      "Gradient Descent(21/99): loss=0.38895195633607643, gradient=0.0011734270140823795\n",
      "Gradient Descent(22/99): loss=0.3889518250459583, gradient=0.0011510918699074195\n",
      "Gradient Descent(23/99): loss=0.3889516983896542, gradient=0.0011303411063424782\n",
      "Gradient Descent(24/99): loss=0.38895157599679214, gradient=0.0011109381005757193\n",
      "Gradient Descent(25/99): loss=0.38895145755064575, gradient=0.0010926949837473457\n",
      "Gradient Descent(26/99): loss=0.3889513427773162, gradient=0.0010754608241520121\n",
      "Gradient Descent(27/99): loss=0.3889512314375476, gradient=0.001059112940132788\n",
      "Gradient Descent(28/99): loss=0.3889511233204564, gradient=0.0010435504752230854\n",
      "Gradient Descent(29/99): loss=0.3889510182386666, gradient=0.001028689612089724\n",
      "Gradient Descent(30/99): loss=0.38895091602449, gradient=0.001014459978106684\n",
      "Gradient Descent(31/99): loss=0.388950816526891, gradient=0.0010008019218216628\n",
      "Gradient Descent(32/99): loss=0.3889507196090485, gradient=0.0009876644298699021\n",
      "Gradient Descent(33/99): loss=0.38895062514637496, gradient=0.0009750035182466742\n",
      "Gradient Descent(34/99): loss=0.38895053302489324, gradient=0.0009627809777117129\n",
      "Gradient Descent(35/99): loss=0.38895044313989124, gradient=0.0009509633858258752\n",
      "Gradient Descent(36/99): loss=0.3889503553948002, gradient=0.0009395213215333908\n",
      "Gradient Descent(37/99): loss=0.3889502697002495, gradient=0.0009284287350175692\n",
      "Gradient Descent(38/99): loss=0.3889501859732654, gradient=0.0009176624376876292\n",
      "Gradient Descent(39/99): loss=0.3889501041365884, gradient=0.0009072016859577893\n",
      "Gradient Descent(40/99): loss=0.3889500241193017, gradient=0.0008970136194563696\n",
      "Gradient Descent(41/99): loss=0.38894994585250864, gradient=0.0008871109687980511\n",
      "Gradient Descent(42/99): loss=0.3889498692728863, gradient=0.0008774630720792483\n",
      "Gradient Descent(43/99): loss=0.3889497943209007, gradient=0.0008680560815716881\n",
      "Gradient Descent(44/99): loss=0.38894972094046887, gradient=0.0008588773642898953\n",
      "Gradient Descent(45/99): loss=0.38894964907865975, gradient=0.0008499153595846762\n",
      "Gradient Descent(46/99): loss=0.3889495786854303, gradient=0.0008411594574699059\n",
      "Gradient Descent(47/99): loss=0.388949509713391, gradient=0.0008325998941305533\n",
      "Gradient Descent(48/99): loss=0.3889494421175963, gradient=0.0008242276617499548\n",
      "Gradient Descent(49/99): loss=0.3889493758553566, gradient=0.0008160344303319093\n",
      "Gradient Descent(50/99): loss=0.38894931088606977, gradient=0.0008080124796195125\n",
      "Gradient Descent(51/99): loss=0.3889492471710704, gradient=0.0008001546395543558\n",
      "Gradient Descent(52/99): loss=0.3889491846734908, gradient=0.0007924542379915858\n",
      "Gradient Descent(53/99): loss=0.388949123358138, gradient=0.000784905054606478\n",
      "Gradient Descent(54/99): loss=0.3889490631913806, gradient=0.0007775012801073934\n",
      "Gradient Descent(55/99): loss=0.3889490041410455, gradient=0.0007702374800150337\n",
      "Gradient Descent(56/99): loss=0.3889489461763246, gradient=0.0007631085623871867\n",
      "Gradient Descent(57/99): loss=0.38894888926768895, gradient=0.0007561097489665786\n",
      "Gradient Descent(58/99): loss=0.38894883338680986, gradient=0.0007492365493095945\n",
      "Gradient Descent(59/99): loss=0.3889487785064874, gradient=0.0007424847375212011\n",
      "Gradient Descent(60/99): loss=0.38894872460058294, gradient=0.0007358503312771622\n",
      "Gradient Descent(61/99): loss=0.3889486716439589, gradient=0.000729329572860038\n",
      "Gradient Descent(62/99): loss=0.38894861961242266, gradient=0.0007229189119759171\n",
      "Gradient Descent(63/99): loss=0.3889485684826733, gradient=0.0007166149901494564\n",
      "Gradient Descent(64/99): loss=0.38894851823225424, gradient=0.000710414626524259\n",
      "Gradient Descent(65/99): loss=0.3889484688395083, gradient=0.0007043148049166815\n",
      "Gradient Descent(66/99): loss=0.3889484202835364, gradient=0.0006983126619922318\n",
      "Gradient Descent(67/99): loss=0.3889483725441586, gradient=0.0006924054764497017\n",
      "Gradient Descent(68/99): loss=0.38894832560187836, gradient=0.0006865906591125264\n",
      "Gradient Descent(69/99): loss=0.38894827943784943, gradient=0.0006808657438384043\n",
      "Gradient Descent(70/99): loss=0.3889482340338444, gradient=0.0006752283791697619\n",
      "Gradient Descent(71/99): loss=0.3889481893722252, gradient=0.000669676320654907\n",
      "Gradient Descent(72/99): loss=0.3889481454359169, gradient=0.0006642074237793339\n",
      "Gradient Descent(73/99): loss=0.3889481022083812, gradient=0.0006588196374515774\n",
      "Gradient Descent(74/99): loss=0.3889480596735928, gradient=0.0006535109979947215\n",
      "Gradient Descent(75/99): loss=0.388948017816017, gradient=0.0006482796235997724\n",
      "Gradient Descent(76/99): loss=0.38894797662058933, gradient=0.0006431237092012648\n",
      "Gradient Descent(77/99): loss=0.3889479360726949, gradient=0.0006380415217391535\n",
      "Gradient Descent(78/99): loss=0.3889478961581492, gradient=0.0006330313957750704\n",
      "Gradient Descent(79/99): loss=0.38894785686318245, gradient=0.0006280917294337972\n",
      "Gradient Descent(80/99): loss=0.3889478181744213, gradient=0.0006232209806429751\n",
      "Gradient Descent(81/99): loss=0.388947780078874, gradient=0.0006184176636474182\n",
      "Gradient Descent(82/99): loss=0.3889477425639152, gradient=0.0006136803457761777\n",
      "Gradient Descent(83/99): loss=0.38894770561727243, gradient=0.0006090076444416984\n",
      "Gradient Descent(84/99): loss=0.388947669227012, gradient=0.0006043982243533852\n",
      "Gradient Descent(85/99): loss=0.3889476333815282, gradient=0.000599850794928696\n",
      "Gradient Descent(86/99): loss=0.3889475980695288, gradient=0.0005953641078861788\n",
      "Gradient Descent(87/99): loss=0.3889475632800263, gradient=0.0005909369550066545\n",
      "Gradient Descent(88/99): loss=0.3889475290023251, gradient=0.0005865681660497819\n",
      "Gradient Descent(89/99): loss=0.38894749522601313, gradient=0.0005822566068134534\n",
      "Gradient Descent(90/99): loss=0.3889474619409505, gradient=0.0005780011773262398\n",
      "Gradient Descent(91/99): loss=0.3889474291372607, gradient=0.0005738008101615812\n",
      "Gradient Descent(92/99): loss=0.3889473968053228, gradient=0.000569654468865773\n",
      "Gradient Descent(93/99): loss=0.3889473649357611, gradient=0.0005655611464900647\n",
      "Gradient Descent(94/99): loss=0.3889473335194391, gradient=0.0005615198642201639\n",
      "Gradient Descent(95/99): loss=0.3889473025474499, gradient=0.0005575296700948585\n",
      "Gradient Descent(96/99): loss=0.3889472720111103, gradient=0.0005535896378079614\n",
      "Gradient Descent(97/99): loss=0.3889472419019525, gradient=0.0005496988655868503\n",
      "Gradient Descent(98/99): loss=0.38894721221171835, gradient=0.0005458564751423485\n",
      "Gradient Descent(99/99): loss=0.3889471829323526, gradient=0.0005420616106844775\n",
      "Gradient Descent(0/99): loss=0.3901465442050107, gradient=0.015405408052235163\n",
      "Gradient Descent(1/99): loss=0.3901426185448165, gradient=0.006921076675654667\n",
      "Gradient Descent(2/99): loss=0.390139964295603, gradient=0.005361510124475562\n",
      "Gradient Descent(3/99): loss=0.3901379673144331, gradient=0.0046167074868469875\n",
      "Gradient Descent(4/99): loss=0.3901364221312775, gradient=0.004049480466542956\n",
      "Gradient Descent(5/99): loss=0.3901352004336173, gradient=0.003591888815656285\n",
      "Gradient Descent(6/99): loss=0.39013421471525, gradient=0.0032189010640383646\n",
      "Gradient Descent(7/99): loss=0.3901334041002951, gradient=0.0029126754099268286\n",
      "Gradient Descent(8/99): loss=0.3901327256144961, gradient=0.0026593805309090635\n",
      "Gradient Descent(9/99): loss=0.3901321484819272, gradient=0.002448214404406286\n",
      "Gradient Descent(10/99): loss=0.3901316503433516, gradient=0.0022707284771433184\n",
      "Gradient Descent(11/99): loss=0.3901312147207469, gradient=0.0021203052223258156\n",
      "Gradient Descent(12/99): loss=0.390130829298314, gradient=0.001991750566284736\n",
      "Gradient Descent(13/99): loss=0.3901304847438069, gradient=0.0018809762019780791\n",
      "Gradient Descent(14/99): loss=0.3901301738910394, gradient=0.0017847525628796705\n",
      "Gradient Descent(15/99): loss=0.39012989116632457, gradient=0.001700517275783364\n",
      "Gradient Descent(16/99): loss=0.3901296321813815, gradient=0.0016262270179972919\n",
      "Gradient Descent(17/99): loss=0.39012939344104575, gradient=0.001560243173322833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(18/99): loss=0.39012917213098824, gradient=0.0015012436775949324\n",
      "Gradient Descent(19/99): loss=0.3901289659617891, gradient=0.0014481550661395682\n",
      "Gradient Descent(20/99): loss=0.39012877305311944, gradient=0.0014001000468298708\n",
      "Gradient Descent(21/99): loss=0.39012859184677273, gradient=0.0013563569731240253\n",
      "Gradient Descent(22/99): loss=0.39012842104066214, gradient=0.0013163284241756172\n",
      "Gradient Descent(23/99): loss=0.3901282595382128, gradient=0.001279516751933345\n",
      "Gradient Descent(24/99): loss=0.39012810640917095, gradient=0.0012455049619846284\n",
      "Gradient Descent(25/99): loss=0.3901279608589694, gradient=0.0012139416851361079\n",
      "Gradient Descent(26/99): loss=0.39012782220456266, gradient=0.0011845292951754008\n",
      "Gradient Descent(27/99): loss=0.39012768985520563, gradient=0.0011570144553192112\n",
      "Gradient Descent(28/99): loss=0.3901275632970385, gradient=0.0011311805479412413\n",
      "Gradient Descent(29/99): loss=0.39012744208063665, gradient=0.0011068415722629745\n",
      "Gradient Descent(30/99): loss=0.3901273258108785, gradient=0.0010838371929007748\n",
      "Gradient Descent(31/99): loss=0.3901272141386524, gradient=0.0010620286962793833\n",
      "Gradient Descent(32/99): loss=0.3901271067540218, gradient=0.0010412956678991989\n",
      "Gradient Descent(33/99): loss=0.3901270033805661, gradient=0.001021533245794637\n",
      "Gradient Descent(34/99): loss=0.390126903770665, gradient=0.0010026498376397193\n",
      "Gradient Descent(35/99): loss=0.3901268077015535, gradient=0.000984565213407693\n",
      "Gradient Descent(36/99): loss=0.3901267149720063, gradient=0.0009672089041772101\n",
      "Gradient Descent(37/99): loss=0.3901266253995359, gradient=0.0009505188520328378\n",
      "Gradient Descent(38/99): loss=0.39012653881802095, gradient=0.0009344402670982225\n",
      "Gradient Descent(39/99): loss=0.390126455075685, gradient=0.0009189246563566022\n",
      "Gradient Descent(40/99): loss=0.39012637403337225, gradient=0.0009039289956576426\n",
      "Gradient Descent(41/99): loss=0.39012629556306955, gradient=0.0008894150216174931\n",
      "Gradient Descent(42/99): loss=0.390126219546635, gradient=0.0008753486243310029\n",
      "Gradient Descent(43/99): loss=0.3901261458747029, gradient=0.0008616993251764661\n",
      "Gradient Descent(44/99): loss=0.3901260744457345, gradient=0.0008484398266951268\n",
      "Gradient Descent(45/99): loss=0.39012600516519574, gradient=0.0008355456237147564\n",
      "Gradient Descent(46/99): loss=0.3901259379448406, gradient=0.0008229946666660748\n",
      "Gradient Descent(47/99): loss=0.3901258727020869, gradient=0.0008107670694974245\n",
      "Gradient Descent(48/99): loss=0.39012580935946867, gradient=0.0007988448557935226\n",
      "Gradient Descent(49/99): loss=0.3901257478441562, gradient=0.0007872117376949377\n",
      "Gradient Descent(50/99): loss=0.39012568808753423, gradient=0.0007758529230410344\n",
      "Gradient Descent(51/99): loss=0.3901256300248286, gradient=0.000764754946845932\n",
      "Gradient Descent(52/99): loss=0.3901255735947768, gradient=0.0007539055237937296\n",
      "Gradient Descent(53/99): loss=0.3901255187393359, gradient=0.0007432934189235034\n",
      "Gradient Descent(54/99): loss=0.3901254654034216, gradient=0.0007329083340830475\n",
      "Gradient Descent(55/99): loss=0.39012541353467767, gradient=0.0007227408080743798\n",
      "Gradient Descent(56/99): loss=0.3901253630832677, gradient=0.000712782128708408\n",
      "Gradient Descent(57/99): loss=0.39012531400168987, gradient=0.0007030242552329019\n",
      "Gradient Descent(58/99): loss=0.39012526624460997, gradient=0.0006934597498103604\n",
      "Gradient Descent(59/99): loss=0.39012521976871134, gradient=0.0006840817169026338\n",
      "Gradient Descent(60/99): loss=0.39012517453255896, gradient=0.0006748837495740019\n",
      "Gradient Descent(61/99): loss=0.3901251304964767, gradient=0.0006658598818555132\n",
      "Gradient Descent(62/99): loss=0.3901250876224365, gradient=0.0006570045464281113\n",
      "Gradient Descent(63/99): loss=0.3901250458739562, gradient=0.0006483125369779558\n",
      "Gradient Descent(64/99): loss=0.39012500521600785, gradient=0.0006397789746629771\n",
      "Gradient Descent(65/99): loss=0.3901249656149329, gradient=0.0006313992782002232\n",
      "Gradient Descent(66/99): loss=0.39012492703836543, gradient=0.0006231691371474001\n",
      "Gradient Descent(67/99): loss=0.39012488945516066, gradient=0.0006150844880049155\n",
      "Gradient Descent(68/99): loss=0.3901248528353299, gradient=0.0006071414928121348\n",
      "Gradient Descent(69/99): loss=0.39012481714998076, gradient=0.00059933651995176\n",
      "Gradient Descent(70/99): loss=0.3901247823712607, gradient=0.0005916661269114178\n",
      "Gradient Descent(71/99): loss=0.39012474847230644, gradient=0.0005841270447822652\n",
      "Gradient Descent(72/99): loss=0.39012471542719535, gradient=0.0005767161643005994\n",
      "Gradient Descent(73/99): loss=0.3901246832109021, gradient=0.0005694305232625483\n",
      "Gradient Descent(74/99): loss=0.39012465179925604, gradient=0.0005622672951611081\n",
      "Gradient Descent(75/99): loss=0.3901246211689036, gradient=0.0005552237789130752\n",
      "Gradient Descent(76/99): loss=0.39012459129727184, gradient=0.0005482973895590803\n",
      "Gradient Descent(77/99): loss=0.39012456216253455, gradient=0.000541485649832844\n",
      "Gradient Descent(78/99): loss=0.39012453374358097, gradient=0.0005347861825080669\n",
      "Gradient Descent(79/99): loss=0.390124506019985, gradient=0.0005281967034422436\n",
      "Gradient Descent(80/99): loss=0.39012447897197877, gradient=0.0005217150152445259\n",
      "Gradient Descent(81/99): loss=0.39012445258042466, gradient=0.0005153390015045821\n",
      "Gradient Descent(82/99): loss=0.3901244268267912, gradient=0.0005090666215248104\n",
      "Gradient Descent(83/99): loss=0.3901244016931297, gradient=0.0005028959055060016\n",
      "Gradient Descent(84/99): loss=0.39012437716205056, gradient=0.0004968249501408818\n",
      "Gradient Descent(85/99): loss=0.39012435321670436, gradient=0.0004908519145758343\n",
      "Gradient Descent(86/99): loss=0.39012432984075973, gradient=0.00048497501670431936\n",
      "Gradient Descent(87/99): loss=0.3901243070183853, gradient=0.0004791925297609118\n",
      "Gradient Descent(88/99): loss=0.39012428473423155, gradient=0.00047350277918635866\n",
      "Gradient Descent(89/99): loss=0.3901242629734135, gradient=0.0004679041397385262\n",
      "Gradient Descent(90/99): loss=0.3901242417214938, gradient=0.0004623950328267001\n",
      "Gradient Descent(91/99): loss=0.39012422096446797, gradient=0.000456973924047437\n",
      "Gradient Descent(92/99): loss=0.3901242006887479, gradient=0.00045163932090494573\n",
      "Gradient Descent(93/99): loss=0.3901241808811503, gradient=0.00044638977069822567\n",
      "Gradient Descent(94/99): loss=0.39012416152887947, gradient=0.0004412238585607366\n",
      "Gradient Descent(95/99): loss=0.39012414261951717, gradient=0.0004361402056390145\n",
      "Gradient Descent(96/99): loss=0.3901241241410082, gradient=0.00043113746739785843\n",
      "Gradient Descent(97/99): loss=0.39012410608164977, gradient=0.00042621433204141976\n",
      "Gradient Descent(98/99): loss=0.3901240884300788, gradient=0.00042136951904049586\n",
      "Gradient Descent(99/99): loss=0.39012407117526143, gradient=0.0004166017777566962\n",
      "Gradient Descent(0/99): loss=0.39035710352952335, gradient=0.007269038500160545\n",
      "Gradient Descent(1/99): loss=0.3903534254656104, gradient=0.006257303052273562\n",
      "Gradient Descent(2/99): loss=0.3903505368534423, gradient=0.005525407183459965\n",
      "Gradient Descent(3/99): loss=0.3903482089757663, gradient=0.004946069074520312\n",
      "Gradient Descent(4/99): loss=0.3903462900159527, gradient=0.004479363012283199\n",
      "Gradient Descent(5/99): loss=0.39034467630500175, gradient=0.0040985773196341516\n",
      "Gradient Descent(6/99): loss=0.39034329559435904, gradient=0.0037838668624942937\n",
      "Gradient Descent(7/99): loss=0.3903420965156093, gradient=0.003520373115116873\n",
      "Gradient Descent(8/99): loss=0.3903410418279071, gradient=0.003296908191301666\n",
      "Gradient Descent(9/99): loss=0.39034010397793384, gradient=0.003105152719810843\n",
      "Gradient Descent(10/99): loss=0.39033926223490645, gradient=0.0029386812586925065\n",
      "Gradient Descent(11/99): loss=0.3903385007158283, gradient=0.002792635030213672\n",
      "Gradient Descent(12/99): loss=0.3903378070600863, gradient=0.002663244690997109\n",
      "Gradient Descent(13/99): loss=0.3903371715029734, gradient=0.002547578072799675\n",
      "Gradient Descent(14/99): loss=0.39033658622389317, gradient=0.0024433273154397094\n",
      "Gradient Descent(15/99): loss=0.3903360448906887, gradient=0.002348607492202298\n",
      "Gradient Descent(16/99): loss=0.39033554228371375, gradient=0.0022620544179297996\n",
      "Gradient Descent(17/99): loss=0.39033507408271506, gradient=0.002182425619753242\n",
      "Gradient Descent(18/99): loss=0.3903346366657688, gradient=0.002108753761483964\n",
      "Gradient Descent(19/99): loss=0.39033422696711395, gradient=0.002040245572486871\n",
      "Gradient Descent(20/99): loss=0.3903338423682792, gradient=0.001976246279818883\n",
      "Gradient Descent(21/99): loss=0.3903334806138468, gradient=0.0019162121131149647\n",
      "Gradient Descent(22/99): loss=0.39033313974566003, gradient=0.0018596888506811242\n",
      "Gradient Descent(23/99): loss=0.3903328180509905, gradient=0.0018062949372809093\n",
      "Gradient Descent(24/99): loss=0.3903325140229254, gradient=0.0017556983265083416\n",
      "Gradient Descent(25/99): loss=0.39033222632233266, gradient=0.0017076463921786795\n",
      "Gradient Descent(26/99): loss=0.39033195375798746, gradient=0.0016618871058296301\n",
      "Gradient Descent(27/99): loss=0.3903316952595218, gradient=0.001618234077088519\n",
      "Gradient Descent(28/99): loss=0.3903314498633785, gradient=0.0015765069515956253\n",
      "Gradient Descent(29/99): loss=0.3903312166970781, gradient=0.0015365555869792\n",
      "Gradient Descent(30/99): loss=0.39033099496723095, gradient=0.0014982490087703036\n",
      "Gradient Descent(31/99): loss=0.39033078394955156, gradient=0.0014614723447515959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(32/99): loss=0.39033058298049256, gradient=0.0014261243011951788\n",
      "Gradient Descent(33/99): loss=0.3903303914501903, gradient=0.001392115078427283\n",
      "Gradient Descent(34/99): loss=0.3903302087964832, gradient=0.0013593646438104796\n",
      "Gradient Descent(35/99): loss=0.3903300344998087, gradient=0.001327801296446606\n",
      "Gradient Descent(36/99): loss=0.3903298680788256, gradient=0.0012973604707045218\n",
      "Gradient Descent(37/99): loss=0.39032970908663583, gradient=0.0012679837358368633\n",
      "Gradient Descent(38/99): loss=0.39032955710750833, gradient=0.001239617957049325\n",
      "Gradient Descent(39/99): loss=0.3903294117540205, gradient=0.001212214589868281\n",
      "Gradient Descent(40/99): loss=0.39032927266455175, gradient=0.0011857290848581088\n",
      "Gradient Descent(41/99): loss=0.3903291395010768, gradient=0.00116012038393347\n",
      "Gradient Descent(42/99): loss=0.3903290119472106, gradient=0.001135350492900268\n",
      "Gradient Descent(43/99): loss=0.3903288897064697, gradient=0.0011113841176041428\n",
      "Gradient Descent(44/99): loss=0.3903287725007205, gradient=0.0010881883532938429\n",
      "Gradient Descent(45/99): loss=0.39032866006878514, gradient=0.001065732418622329\n",
      "Gradient Descent(46/99): loss=0.3903285521651884, gradient=0.0010439874271889794\n",
      "Gradient Descent(47/99): loss=0.3903284485590242, gradient=0.0010229261907371094\n",
      "Gradient Descent(48/99): loss=0.390328349032927, gradient=0.001002523049113703\n",
      "Gradient Descent(49/99): loss=0.39032825338213745, gradient=0.000982753722913841\n",
      "Gradient Descent(50/99): loss=0.3903281614136477, gradient=0.0009635951854030537\n",
      "Gradient Descent(51/99): loss=0.39032807294542154, gradient=0.000945025550865036\n",
      "Gradient Descent(52/99): loss=0.39032798780567696, gradient=0.0009270239769795636\n",
      "Gradient Descent(53/99): loss=0.3903279058322267, gradient=0.0009095705792155654\n",
      "Gradient Descent(54/99): loss=0.3903278268736389, gradient=0.0008926259779151661\n",
      "Gradient Descent(55/99): loss=0.3903277507832234, gradient=0.0008762135143576932\n",
      "Gradient Descent(56/99): loss=0.3903276774241216, gradient=0.0008602945883906854\n",
      "Gradient Descent(57/99): loss=0.3903276066668992, gradient=0.0008448524765652431\n",
      "Gradient Descent(58/99): loss=0.3903275383890981, gradient=0.0008298711145544755\n",
      "Gradient Descent(59/99): loss=0.39032747247482064, gradient=0.0008153350554083607\n",
      "Gradient Descent(60/99): loss=0.39032740881434186, gradient=0.0008012294324574393\n",
      "Gradient Descent(61/99): loss=0.39032734730374824, gradient=0.0007875399260143963\n",
      "Gradient Descent(62/99): loss=0.39032728784459975, gradient=0.0007742527333802581\n",
      "Gradient Descent(63/99): loss=0.39032723034361533, gradient=0.0007613545417436075\n",
      "Gradient Descent(64/99): loss=0.39032717471237754, gradient=0.0007488325036163469\n",
      "Gradient Descent(65/99): loss=0.3903271208670552, gradient=0.000736674214498429\n",
      "Gradient Descent(66/99): loss=0.3903270687281455, gradient=0.0007248676925036058\n",
      "Gradient Descent(67/99): loss=0.39032701822023114, gradient=0.0007134013597133794\n",
      "Gradient Descent(68/99): loss=0.3903269692717517, gradient=0.0007022640250563975\n",
      "Gradient Descent(69/99): loss=0.39032692181478973, gradient=0.0006914448685357495\n",
      "Gradient Descent(70/99): loss=0.3903268757848696, gradient=0.0006809334266491484\n",
      "Gradient Descent(71/99): loss=0.39032683112076855, gradient=0.0006707195788662775\n",
      "Gradient Descent(72/99): loss=0.3903267877643382, gradient=0.000660793535043697\n",
      "Gradient Descent(73/99): loss=0.3903267456603369, gradient=0.0006511458236730356\n",
      "Gradient Descent(74/99): loss=0.39032670475627257, gradient=0.0006417672808699432\n",
      "Gradient Descent(75/99): loss=0.39032666500225366, gradient=0.000632649040023138\n",
      "Gradient Descent(76/99): loss=0.39032662635084864, gradient=0.000623782522031786\n",
      "Gradient Descent(77/99): loss=0.39032658875695553, gradient=0.0006151594260687627\n",
      "Gradient Descent(78/99): loss=0.3903265521776751, gradient=0.0006067717208141642\n",
      "Gradient Descent(79/99): loss=0.39032651657219486, gradient=0.0005986116361103121\n",
      "Gradient Descent(80/99): loss=0.3903264819016788, gradient=0.0005906716549954522\n",
      "Gradient Descent(81/99): loss=0.3903264481291606, gradient=0.0005829445060781549\n",
      "Gradient Descent(82/99): loss=0.3903264152194475, gradient=0.0005754231562193191\n",
      "Gradient Descent(83/99): loss=0.39032638313902457, gradient=0.0005681008034925018\n",
      "Gradient Descent(84/99): loss=0.3903263518559686, gradient=0.0005609708703970585\n",
      "Gradient Descent(85/99): loss=0.39032632133986345, gradient=0.0005540269973015456\n",
      "Gradient Descent(86/99): loss=0.390326291561723, gradient=0.0005472630360979484\n",
      "Gradient Descent(87/99): loss=0.39032626249391467, gradient=0.0005406730440494356\n",
      "Gradient Descent(88/99): loss=0.39032623411009126, gradient=0.0005342512778169696\n",
      "Gradient Descent(89/99): loss=0.39032620638512366, gradient=0.000527992187651833\n",
      "Gradient Descent(90/99): loss=0.3903261792950379, gradient=0.0005218904117424806\n",
      "Gradient Descent(91/99): loss=0.39032615281695593, gradient=0.0005159407707070382\n",
      "Gradient Descent(92/99): loss=0.39032612692904023, gradient=0.0005101382622219857\n",
      "Gradient Descent(93/99): loss=0.390326101610439, gradient=0.0005044780557808865\n",
      "Gradient Descent(94/99): loss=0.390326076841238, gradient=0.0004989554875764673\n",
      "Gradient Descent(95/99): loss=0.39032605260241116, gradient=0.0004935660555013534\n",
      "Gradient Descent(96/99): loss=0.3903260288757769, gradient=0.0004883054142627637\n",
      "Gradient Descent(97/99): loss=0.39032600564395487, gradient=0.0004831693706074432\n",
      "Gradient Descent(98/99): loss=0.39032598289032583, gradient=0.0004781538786542647\n",
      "Gradient Descent(99/99): loss=0.39032596059899366, gradient=0.000473255035330881\n",
      "Gradient Descent(0/99): loss=0.38997754753764485, gradient=0.010008013968115572\n",
      "Gradient Descent(1/99): loss=0.3899713148540763, gradient=0.008243557414281455\n",
      "Gradient Descent(2/99): loss=0.3899669035511717, gradient=0.006907798930472897\n",
      "Gradient Descent(3/99): loss=0.38996366990456216, gradient=0.005890144358285829\n",
      "Gradient Descent(4/99): loss=0.3899612143841132, gradient=0.005112067390649123\n",
      "Gradient Descent(5/99): loss=0.38995928588849127, gradient=0.004513224753210307\n",
      "Gradient Descent(6/99): loss=0.38995772424378133, gradient=0.004047604883059002\n",
      "Gradient Descent(7/99): loss=0.3899564255498988, gradient=0.0036804074194827555\n",
      "Gradient Descent(8/99): loss=0.38995532104943476, gradient=0.003385860661551391\n",
      "Gradient Descent(9/99): loss=0.38995436421509283, gradient=0.003145127198974809\n",
      "Gradient Descent(10/99): loss=0.3899535227881316, gradient=0.002944584637589345\n",
      "Gradient Descent(11/99): loss=0.38995277382358845, gradient=0.0027744373814022343\n",
      "Gradient Descent(12/99): loss=0.3899521005811522, gradient=0.0026276104227862334\n",
      "Gradient Descent(13/99): loss=0.38995149051180567, gradient=0.0024990803103129887\n",
      "Gradient Descent(14/99): loss=0.38995093399815933, gradient=0.0023851057157131816\n",
      "Gradient Descent(15/99): loss=0.38995042350003956, gradient=0.002282944910419108\n",
      "Gradient Descent(16/99): loss=0.38994995298758683, gradient=0.0021905404167850257\n",
      "Gradient Descent(17/99): loss=0.3899495175537542, gradient=0.0021063217618261932\n",
      "Gradient Descent(18/99): loss=0.3899491131439093, gradient=0.002029069079446982\n",
      "Gradient Descent(19/99): loss=0.38994873636317134, gradient=0.0019578184874433752\n",
      "Gradient Descent(20/99): loss=0.3899483843362855, gradient=0.0018917959705572294\n",
      "Gradient Descent(21/99): loss=0.3899480546036807, gradient=0.0018303706946190747\n",
      "Gradient Descent(22/99): loss=0.38994774504738544, gradient=0.0017729949590860722\n",
      "Gradient Descent(23/99): loss=0.38994745381658863, gradient=0.0017192888267157749\n",
      "Gradient Descent(24/99): loss=0.38994717929552797, gradient=0.0016688552195961016\n",
      "Gradient Descent(25/99): loss=0.3899469200609047, gradient=0.001621380505369139\n",
      "Gradient Descent(26/99): loss=0.38994667485493034, gradient=0.0015765762325508947\n",
      "Gradient Descent(27/99): loss=0.3899464425523128, gradient=0.0015342472473091086\n",
      "Gradient Descent(28/99): loss=0.3899462221497459, gradient=0.0014941701990298381\n",
      "Gradient Descent(29/99): loss=0.3899460127464742, gradient=0.0014561662423222279\n",
      "Gradient Descent(30/99): loss=0.38994581353078017, gradient=0.001420077494822189\n",
      "Gradient Descent(31/99): loss=0.389945623768674, gradient=0.0013857638179568756\n",
      "Gradient Descent(32/99): loss=0.3899454427943633, gradient=0.001353100182168995\n",
      "Gradient Descent(33/99): loss=0.3899452700021739, gradient=0.001321974494396942\n",
      "Gradient Descent(34/99): loss=0.3899451048396584, gradient=0.0012922857950227302\n",
      "Gradient Descent(35/99): loss=0.3899449468016898, gradient=0.0012639427528562499\n",
      "Gradient Descent(36/99): loss=0.38994479542536753, gradient=0.001236862402465265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(37/99): loss=0.38994465028561004, gradient=0.0012109690799418824\n",
      "Gradient Descent(38/99): loss=0.38994451099131416, gradient=0.001186193522138216\n",
      "Gradient Descent(39/99): loss=0.38994437718200514, gradient=0.0011624721012799789\n",
      "Gradient Descent(40/99): loss=0.3899442485248902, gradient=0.001139746172218701\n",
      "Gradient Descent(41/99): loss=0.38994412471226686, gradient=0.0011179615137921098\n",
      "Gradient Descent(42/99): loss=0.3899440054592295, gradient=0.001097067849103891\n",
      "Gradient Descent(43/99): loss=0.3899438905016321, gradient=0.0010770184322093294\n",
      "Gradient Descent(44/99): loss=0.38994377959427534, gradient=0.001057769690850434\n",
      "Gradient Descent(45/99): loss=0.3899436725092869, gradient=0.0010392809166346946\n",
      "Gradient Descent(46/99): loss=0.3899435690346681, gradient=0.001021513995480381\n",
      "Gradient Descent(47/99): loss=0.38994346897298965, gradient=0.0010044331723227961\n",
      "Gradient Descent(48/99): loss=0.38994337214021496, gradient=0.0009880048450417605\n",
      "Gradient Descent(49/99): loss=0.38994327836463627, gradient=0.0009721973833687605\n",
      "Gradient Descent(50/99): loss=0.38994318748591406, gradient=0.0009569809691954279\n",
      "Gradient Descent(51/99): loss=0.38994309935420124, gradient=0.0009423274552578338\n",
      "Gradient Descent(52/99): loss=0.38994301382934926, gradient=0.00092821023963181\n",
      "Gradient Descent(53/99): loss=0.3899429307801837, gradient=0.0009146041538628594\n",
      "Gradient Descent(54/99): loss=0.3899428500838416, gradient=0.0009014853628775836\n",
      "Gradient Descent(55/99): loss=0.38994277162516855, gradient=0.0008888312750995152\n",
      "Gradient Descent(56/99): loss=0.3899426952961618, gradient=0.000876620461421645\n",
      "Gradient Descent(57/99): loss=0.38994262099546384, gradient=0.0008648325818855331\n",
      "Gradient Descent(58/99): loss=0.38994254862789296, gradient=0.0008534483190813474\n",
      "Gradient Descent(59/99): loss=0.3899424781040147, gradient=0.0008424493174250475\n",
      "Gradient Descent(60/99): loss=0.38994240933974544, gradient=0.000831818127588957\n",
      "Gradient Descent(61/99): loss=0.389942342255986, gradient=0.0008215381554638893\n",
      "Gradient Descent(62/99): loss=0.38994227677828536, gradient=0.000811593615117852\n",
      "Gradient Descent(63/99): loss=0.3899422128365282, gradient=0.0008019694852922238\n",
      "Gradient Descent(64/99): loss=0.38994215036464724, gradient=0.0007926514690380988\n",
      "Gradient Descent(65/99): loss=0.389942089300355, gradient=0.000783625956151732\n",
      "Gradient Descent(66/99): loss=0.38994202958489665, gradient=0.0007748799881134249\n",
      "Gradient Descent(67/99): loss=0.3899419711628211, gradient=0.0007664012252749489\n",
      "Gradient Descent(68/99): loss=0.38994191398176703, gradient=0.0007581779160742895\n",
      "Gradient Descent(69/99): loss=0.38994185799226533, gradient=0.0007501988680861966\n",
      "Gradient Descent(70/99): loss=0.38994180314755444, gradient=0.0007424534207420433\n",
      "Gradient Descent(71/99): loss=0.38994174940341086, gradient=0.0007349314195738734\n",
      "Gradient Descent(72/99): loss=0.389941696717988, gradient=0.0007276231918562552\n",
      "Gradient Descent(73/99): loss=0.38994164505166856, gradient=0.0007205195235353669\n",
      "Gradient Descent(74/99): loss=0.38994159436692755, gradient=0.0007136116373476839\n",
      "Gradient Descent(75/99): loss=0.3899415446282015, gradient=0.0007068911720434106\n",
      "Gradient Descent(76/99): loss=0.3899414958017701, gradient=0.0007003501626383332\n",
      "Gradient Descent(77/99): loss=0.3899414478556429, gradient=0.0006939810216274999\n",
      "Gradient Descent(78/99): loss=0.3899414007594557, gradient=0.0006877765211006199\n",
      "Gradient Descent(79/99): loss=0.38994135448437206, gradient=0.0006817297757056913\n",
      "Gradient Descent(80/99): loss=0.3899413090029928, gradient=0.0006758342264123791\n",
      "Gradient Descent(81/99): loss=0.3899412642892698, gradient=0.0006700836250321795\n",
      "Gradient Descent(82/99): loss=0.3899412203184278, gradient=0.0006644720194543396\n",
      "Gradient Descent(83/99): loss=0.38994117706688775, gradient=0.0006589937395627974\n",
      "Gradient Descent(84/99): loss=0.3899411345121988, gradient=0.0006536433837992957\n",
      "Gradient Descent(85/99): loss=0.3899410926329722, gradient=0.0006484158063427534\n",
      "Gradient Descent(86/99): loss=0.3899410514088202, gradient=0.0006433061048755507\n",
      "Gradient Descent(87/99): loss=0.38994101082029875, gradient=0.0006383096089104163\n",
      "Gradient Descent(88/99): loss=0.3899409708488542, gradient=0.0006334218686523096\n",
      "Gradient Descent(89/99): loss=0.38994093147677267, gradient=0.0006286386443721182\n",
      "Gradient Descent(90/99): loss=0.38994089268713256, gradient=0.0006239558962693305\n",
      "Gradient Descent(91/99): loss=0.3899408544637614, gradient=0.0006193697748027893\n",
      "Gradient Descent(92/99): loss=0.38994081679119325, gradient=0.0006148766114691935\n",
      "Gradient Descent(93/99): loss=0.38994077965463025, gradient=0.0006104729100103274\n",
      "Gradient Descent(94/99): loss=0.38994074303990645, gradient=0.0006061553380301342\n",
      "Gradient Descent(95/99): loss=0.38994070693345284, gradient=0.0006019207190048818\n",
      "Gradient Descent(96/99): loss=0.38994067132226556, gradient=0.0005977660246685817\n",
      "Gradient Descent(97/99): loss=0.38994063619387503, gradient=0.0005936883677582961\n",
      "Gradient Descent(98/99): loss=0.38994060153631865, gradient=0.0005896849951033332\n",
      "Gradient Descent(99/99): loss=0.38994056733811294, gradient=0.0005857532810434965\n",
      "Gradient Descent(0/99): loss=0.38967637173784814, gradient=0.011098191708826755\n",
      "Gradient Descent(1/99): loss=0.3896713845929344, gradient=0.0074481870044605615\n",
      "Gradient Descent(2/99): loss=0.38966784814037336, gradient=0.006183064918821873\n",
      "Gradient Descent(3/99): loss=0.3896652508835262, gradient=0.005282545372423816\n",
      "Gradient Descent(4/99): loss=0.3896633044747525, gradient=0.004562563380685578\n",
      "Gradient Descent(5/99): loss=0.3896618173900955, gradient=0.003979106635289363\n",
      "Gradient Descent(6/99): loss=0.38966065943197525, gradient=0.0035035771324783754\n",
      "Gradient Descent(7/99): loss=0.38965974106692464, gradient=0.0031135759686572078\n",
      "Gradient Descent(8/99): loss=0.3896589999822312, gradient=0.002791430512529133\n",
      "Gradient Descent(9/99): loss=0.3896583922502803, gradient=0.0025232167021587946\n",
      "Gradient Descent(10/99): loss=0.3896578864765759, gradient=0.0022980057159170523\n",
      "Gradient Descent(11/99): loss=0.38965745989800255, gradient=0.0021072470230841904\n",
      "Gradient Descent(12/99): loss=0.38965709576234625, gradient=0.001944263804079479\n",
      "Gradient Descent(13/99): loss=0.38965678155408223, gradient=0.0018038437095378378\n",
      "Gradient Descent(14/99): loss=0.38965650778240224, gradient=0.0016819108076583829\n",
      "Gradient Descent(15/99): loss=0.38965626714519946, gradient=0.0015752659869094773\n",
      "Gradient Descent(16/99): loss=0.38965605394623487, gradient=0.0014813840892948077\n",
      "Gradient Descent(17/99): loss=0.389655863684149, gradient=0.0013982571327638162\n",
      "Gradient Descent(18/99): loss=0.3896556927591131, gradient=0.001324274256661973\n",
      "Gradient Descent(19/99): loss=0.38965553826077665, gradient=0.0012581304219981728\n",
      "Gradient Descent(20/99): loss=0.38965539781296543, gradient=0.0011987572973083292\n",
      "Gradient Descent(21/99): loss=0.3896552694584346, gradient=0.0011452710579866732\n",
      "Gradient Descent(22/99): loss=0.38965515157221503, gradient=0.0010969329604234412\n",
      "Gradient Descent(23/99): loss=0.38965504279562424, gradient=0.0010531194991551223\n",
      "Gradient Descent(24/99): loss=0.3896549419853932, gradient=0.0010132997196615654\n",
      "Gradient Descent(25/99): loss=0.3896548481739918, gradient=0.0009770178607738838\n",
      "Gradient Descent(26/99): loss=0.38965476053835374, gradient=0.0009438799644148306\n",
      "Gradient Descent(27/99): loss=0.3896546783749773, gradient=0.0009135434426719233\n",
      "Gradient Descent(28/99): loss=0.3896546010799244, gradient=0.0008857088566864861\n",
      "Gradient Descent(29/99): loss=0.3896545281326219, gradient=0.0008601133586101382\n",
      "Gradient Descent(30/99): loss=0.38965445908264745, gradient=0.0008365253932256791\n",
      "Gradient Descent(31/99): loss=0.3896543935388782, gradient=0.0008147403625890821\n",
      "Gradient Descent(32/99): loss=0.3896543311605259, gradient=0.0007945770351162306\n",
      "Gradient Descent(33/99): loss=0.3896542716496943, gradient=0.0007758745374370842\n",
      "Gradient Descent(34/99): loss=0.38965421474516726, gradient=0.0007584898087146584\n",
      "Gradient Descent(35/99): loss=0.38965416021720245, gradient=0.0007422954271825663\n",
      "Gradient Descent(36/99): loss=0.38965410786315047, gradient=0.0007271777404937813\n",
      "Gradient Descent(37/99): loss=0.38965405750375576, gradient=0.0007130352473771617\n",
      "Gradient Descent(38/99): loss=0.38965400898001995, gradient=0.0006997771897254984\n",
      "Gradient Descent(39/99): loss=0.38965396215053444, gradient=0.0006873223227946191\n",
      "Gradient Descent(40/99): loss=0.3896539168892082, gradient=0.0006755978375493537\n",
      "Gradient Descent(41/99): loss=0.38965387308332, gradient=0.0006645384139654189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(42/99): loss=0.3896538306318512, gradient=0.0006540853877399632\n",
      "Gradient Descent(43/99): loss=0.38965378944404944, gradient=0.0006441860156846837\n",
      "Gradient Descent(44/99): loss=0.38965374943819053, gradient=0.0006347928273046273\n",
      "Gradient Descent(45/99): loss=0.3896537105405092, gradient=0.0006258630518562719\n",
      "Gradient Descent(46/99): loss=0.3896536726842708, gradient=0.0006173581116444718\n",
      "Gradient Descent(47/99): loss=0.38965363580896517, gradient=0.0006092431735369311\n",
      "Gradient Descent(48/99): loss=0.3896535998596056, gradient=0.0006014867517026749\n",
      "Gradient Descent(49/99): loss=0.3896535647861169, gradient=0.0005940603554586706\n",
      "Gradient Descent(50/99): loss=0.38965353054279794, gradient=0.0005869381768644129\n",
      "Gradient Descent(51/99): loss=0.389653497087853, gradient=0.0005800968133596279\n",
      "Gradient Descent(52/99): loss=0.38965346438298054, gradient=0.0005735150213127765\n",
      "Gradient Descent(53/99): loss=0.3896534323930074, gradient=0.0005671734968463656\n",
      "Gradient Descent(54/99): loss=0.3896534010855708, gradient=0.000561054680745798\n",
      "Gradient Descent(55/99): loss=0.3896533704308334, gradient=0.0005551425846419536\n",
      "Gradient Descent(56/99): loss=0.3896533404012351, gradient=0.000549422635997002\n",
      "Gradient Descent(57/99): loss=0.3896533109712677, gradient=0.000543881539720086\n",
      "Gradient Descent(58/99): loss=0.38965328211727934, gradient=0.0005385071545014346\n",
      "Gradient Descent(59/99): loss=0.3896532538172981, gradient=0.0005332883821822273\n",
      "Gradient Descent(60/99): loss=0.38965322605087505, gradient=0.0005282150686806963\n",
      "Gradient Descent(61/99): loss=0.3896531987989439, gradient=0.0005232779151706899\n",
      "Gradient Descent(62/99): loss=0.38965317204369637, gradient=0.0005184683983652034\n",
      "Gradient Descent(63/99): loss=0.3896531457684689, gradient=0.0005137786988940625\n",
      "Gradient Descent(64/99): loss=0.38965311995764285, gradient=0.0005092016368835231\n",
      "Gradient Descent(65/99): loss=0.38965309459655356, gradient=0.000504730613952071\n",
      "Gradient Descent(66/99): loss=0.3896530696714085, gradient=0.0005003595609271923\n",
      "Gradient Descent(67/99): loss=0.3896530451692149, gradient=0.000496082890670217\n",
      "Gradient Descent(68/99): loss=0.389653021077712, gradient=0.0004918954554661765\n",
      "Gradient Descent(69/99): loss=0.3896529973853133, gradient=0.00048779250849843647\n",
      "Gradient Descent(70/99): loss=0.3896529740810501, gradient=0.0004837696689823343\n",
      "Gradient Descent(71/99): loss=0.38965295115452464, gradient=0.00047982289058017853\n",
      "Gradient Descent(72/99): loss=0.38965292859586437, gradient=0.0004759484327625663\n",
      "Gradient Descent(73/99): loss=0.3896529063956821, gradient=0.00047214283481735944\n",
      "Gradient Descent(74/99): loss=0.38965288454503944, gradient=0.00046840289224137966\n",
      "Gradient Descent(75/99): loss=0.389652863035413, gradient=0.0004647256352783898\n",
      "Gradient Descent(76/99): loss=0.3896528418586653, gradient=0.00046110830939213307\n",
      "Gradient Descent(77/99): loss=0.38965282100701554, gradient=0.00045754835748620416\n",
      "Gradient Descent(78/99): loss=0.3896528004730156, gradient=0.00045404340370229523\n",
      "Gradient Descent(79/99): loss=0.38965278024952704, gradient=0.00045059123864602695\n",
      "Gradient Descent(80/99): loss=0.38965276032969864, gradient=0.00044718980590495455\n",
      "Gradient Descent(81/99): loss=0.38965274070694883, gradient=0.0004438371897375992\n",
      "Gradient Descent(82/99): loss=0.38965272137494755, gradient=0.00044053160382476604\n",
      "Gradient Descent(83/99): loss=0.38965270232759935, gradient=0.0004372713809845057\n",
      "Gradient Descent(84/99): loss=0.3896526835590298, gradient=0.0004340549637631605\n",
      "Gradient Descent(85/99): loss=0.3896526650635708, gradient=0.0004308808958226011\n",
      "Gradient Descent(86/99): loss=0.38965264683574885, gradient=0.0004277478140520051\n",
      "Gradient Descent(87/99): loss=0.38965262887027297, gradient=0.0004246544413392615\n",
      "Gradient Descent(88/99): loss=0.3896526111620244, gradient=0.000421599579943822\n",
      "Gradient Descent(89/99): loss=0.3896525937060466, gradient=0.00041858210541733515\n",
      "Gradient Descent(90/99): loss=0.38965257649753665, gradient=0.0004156009610248727\n",
      "Gradient Descent(91/99): loss=0.3896525595318366, gradient=0.00041265515262253375\n",
      "Gradient Descent(92/99): loss=0.3896525428044257, gradient=0.00040974374395223354\n",
      "Gradient Descent(93/99): loss=0.3896525263109134, gradient=0.00040686585231811774\n",
      "Gradient Descent(94/99): loss=0.38965251004703305, gradient=0.0004040206446115345\n",
      "Gradient Descent(95/99): loss=0.38965249400863483, gradient=0.0004012073336551846\n",
      "Gradient Descent(96/99): loss=0.38965247819168153, gradient=0.00039842517483963967\n",
      "Gradient Descent(97/99): loss=0.3896524625922421, gradient=0.00039567346302740643\n",
      "Gradient Descent(98/99): loss=0.38965244720648695, gradient=0.0003929515297023782\n",
      "Gradient Descent(99/99): loss=0.38965243203068367, gradient=0.0003902587403440548\n",
      "Gradient Descent(0/99): loss=0.38896306443711115, gradient=0.00817726816944011\n",
      "Gradient Descent(1/99): loss=0.3889603031650901, gradient=0.005550475007219147\n",
      "Gradient Descent(2/99): loss=0.3889583850349264, gradient=0.004564511489951615\n",
      "Gradient Descent(3/99): loss=0.3889570036309306, gradient=0.003860343447160331\n",
      "Gradient Descent(4/99): loss=0.38895598375332374, gradient=0.0033071555305353786\n",
      "Gradient Descent(5/99): loss=0.3889552107445749, gradient=0.0028700918846083228\n",
      "Gradient Descent(6/99): loss=0.3889546082653315, gradient=0.002525391956965997\n",
      "Gradient Descent(7/99): loss=0.3889541251065158, gradient=0.0022539610461863664\n",
      "Gradient Descent(8/99): loss=0.38895372666400824, gradient=0.0020402355004392084\n",
      "Gradient Descent(9/99): loss=0.38895338936461626, gradient=0.0018715759513723842\n",
      "Gradient Descent(10/99): loss=0.388953097004405, gradient=0.0017378163768035318\n",
      "Gradient Descent(11/99): loss=0.38895283833243793, gradient=0.0016308883984366572\n",
      "Gradient Descent(12/99): loss=0.38895260544888194, gradient=0.0015444829320396141\n",
      "Gradient Descent(13/99): loss=0.3889523927377114, gradient=0.0014737367982759566\n",
      "Gradient Descent(14/99): loss=0.38895219615180715, gradient=0.0014149470255971812\n",
      "Gradient Descent(15/99): loss=0.3889520127313448, gradient=0.0013653200676039298\n",
      "Gradient Descent(16/99): loss=0.38895184027731977, gradient=0.0013227608712540616\n",
      "Gradient Descent(17/99): loss=0.38895167712873957, gradient=0.0012857022537319266\n",
      "Gradient Descent(18/99): loss=0.3889515220094378, gradient=0.001252971266951929\n",
      "Gradient Descent(19/99): loss=0.38895137392190765, gradient=0.0012236871240197298\n",
      "Gradient Descent(20/99): loss=0.38895123207307764, gradient=0.001197184666982801\n",
      "Gradient Descent(21/99): loss=0.38895109582192566, gradient=0.001172957727022818\n",
      "Gradient Descent(22/99): loss=0.3889509646421269, gradient=0.0011506175594348414\n",
      "Gradient Descent(23/99): loss=0.38895083809513104, gradient=0.0011298624851775482\n",
      "Gradient Descent(24/99): loss=0.3889507158105341, gradient=0.001110455755099091\n",
      "Gradient Descent(25/99): loss=0.38895059747159133, gradient=0.00109220939684463\n",
      "Gradient Descent(26/99): loss=0.3889504828043967, gradient=0.0010749723937297144\n",
      "Gradient Descent(27/99): loss=0.38895037156969525, gradient=0.0010586219941048644\n",
      "Gradient Descent(28/99): loss=0.3889502635566111, gradient=0.0010430572836478422\n",
      "Gradient Descent(29/99): loss=0.3889501585777802, gradient=0.0010281943970162745\n",
      "Gradient Descent(30/99): loss=0.3889500564655307, gradient=0.0010139629215856654\n",
      "Gradient Descent(31/99): loss=0.3889499570688463, gradient=0.0010003031724438998\n",
      "Gradient Descent(32/99): loss=0.38894986025092654, gradient=0.0009871641081234666\n",
      "Gradient Descent(33/99): loss=0.3889497658872062, gradient=0.0009745017209231051\n",
      "Gradient Descent(34/99): loss=0.3889496738637317, gradient=0.0009622777815449557\n",
      "Gradient Descent(35/99): loss=0.3889495840758149, gradient=0.0009504588505097819\n",
      "Gradient Descent(36/99): loss=0.3889494964269107, gradient=0.000939015492234303\n",
      "Gradient Descent(37/99): loss=0.3889494108276736, gradient=0.0009279216444747942\n",
      "Gradient Descent(38/99): loss=0.38894932719515407, gradient=0.0009171541079770616\n",
      "Gradient Descent(39/99): loss=0.38894924545211657, gradient=0.000906692129977946\n",
      "Gradient Descent(40/99): loss=0.3889491655288338, gradient=0.000896489218462869\n",
      "Gradient Descent(41/99): loss=0.3889490873551116, gradient=0.0008865864101400537\n",
      "Gradient Descent(42/99): loss=0.38894901086766664, gradient=0.0008769382628926941\n",
      "Gradient Descent(43/99): loss=0.38894893600700303, gradient=0.0008675309315034195\n",
      "Gradient Descent(44/99): loss=0.38894886271707224, gradient=0.0008583517851930672\n",
      "Gradient Descent(45/99): loss=0.388948790944977, gradient=0.0008493892653188241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(46/99): loss=0.3889487206407058, gradient=0.0008406327637673414\n",
      "Gradient Descent(47/99): loss=0.3889486517568995, gradient=0.0008320725185058811\n",
      "Gradient Descent(48/99): loss=0.388948584248641, gradient=0.0008236995234353834\n",
      "Gradient Descent(49/99): loss=0.38894851807326797, gradient=0.0008155054502276347\n",
      "Gradient Descent(50/99): loss=0.3889484531902042, gradient=0.0008074825802528633\n",
      "Gradient Descent(51/99): loss=0.3889483895608086, gradient=0.000799623745042953\n",
      "Gradient Descent(52/99): loss=0.3889483271482374, gradient=0.000791922274008292\n",
      "Gradient Descent(53/99): loss=0.38894826591731974, gradient=0.0007843719483445272\n",
      "Gradient Descent(54/99): loss=0.388948205834446, gradient=0.0007769669602453417\n",
      "Gradient Descent(55/99): loss=0.38894814686746315, gradient=0.0007697018766808961\n",
      "Gradient Descent(56/99): loss=0.388948088985583, gradient=0.0007625716071222928\n",
      "Gradient Descent(57/99): loss=0.38894803215929485, gradient=0.0007555713746889266\n",
      "Gradient Descent(58/99): loss=0.3889479763602883, gradient=0.0007486966902771001\n",
      "Gradient Descent(59/99): loss=0.3889479215613803, gradient=0.0007419433292952033\n",
      "Gradient Descent(60/99): loss=0.3889478677364486, gradient=0.0007353073106861519\n",
      "Gradient Descent(61/99): loss=0.3889478148603716, gradient=0.0007287848779641734\n",
      "Gradient Descent(62/99): loss=0.3889477629089716, gradient=0.0007223724820321361\n",
      "Gradient Descent(63/99): loss=0.3889477118589621, gradient=0.0007160667655777743\n",
      "Gradient Descent(64/99): loss=0.3889476616879005, gradient=0.0007098645488749227\n",
      "Gradient Descent(65/99): loss=0.38894761237414316, gradient=0.000703762816838395\n",
      "Gradient Descent(66/99): loss=0.3889475638968033, gradient=0.0006977587072017387\n",
      "Gradient Descent(67/99): loss=0.3889475162357138, gradient=0.0006918494997025789\n",
      "Gradient Descent(68/99): loss=0.3889474693713896, gradient=0.0006860326061748881\n",
      "Gradient Descent(69/99): loss=0.38894742328499643, gradient=0.0006803055614602213\n",
      "Gradient Descent(70/99): loss=0.388947377958317, gradient=0.0006746660150588601\n",
      "Gradient Descent(71/99): loss=0.3889473333737247, gradient=0.0006691117234526416\n",
      "Gradient Descent(72/99): loss=0.38894728951415414, gradient=0.0006636405430369226\n",
      "Gradient Descent(73/99): loss=0.3889472463630769, gradient=0.0006582504236076467\n",
      "Gradient Descent(74/99): loss=0.3889472039044774, gradient=0.0006529394023539163\n",
      "Gradient Descent(75/99): loss=0.3889471621228301, gradient=0.0006477055983122214\n",
      "Gradient Descent(76/99): loss=0.3889471210030792, gradient=0.0006425472072430402\n",
      "Gradient Descent(77/99): loss=0.38894708053061783, gradient=0.0006374624968934384\n",
      "Gradient Descent(78/99): loss=0.38894704069127045, gradient=0.0006324498026142957\n",
      "Gradient Descent(79/99): loss=0.38894700147127476, gradient=0.0006275075233023111\n",
      "Gradient Descent(80/99): loss=0.3889469628572651, gradient=0.0006226341176406483\n",
      "Gradient Descent(81/99): loss=0.38894692483625687, gradient=0.0006178281006138532\n",
      "Gradient Descent(82/99): loss=0.3889468873956321, gradient=0.0006130880402753352\n",
      "Gradient Descent(83/99): loss=0.38894685052312505, gradient=0.0006084125547473294\n",
      "Gradient Descent(84/99): loss=0.38894681420680904, gradient=0.0006038003094349217\n",
      "Gradient Descent(85/99): loss=0.38894677843508413, gradient=0.0005992500144376465\n",
      "Gradient Descent(86/99): loss=0.388946743196665, gradient=0.0005947604221428905\n",
      "Gradient Descent(87/99): loss=0.38894670848056945, gradient=0.000590330324987765\n",
      "Gradient Descent(88/99): loss=0.38894667427610824, gradient=0.0005859585533757326\n",
      "Gradient Descent(89/99): loss=0.38894664057287454, gradient=0.0005816439737367033\n",
      "Gradient Descent(90/99): loss=0.38894660736073394, gradient=0.0005773854867195716\n",
      "Gradient Descent(91/99): loss=0.3889465746298157, gradient=0.0005731820255070354\n",
      "Gradient Descent(92/99): loss=0.3889465423705031, gradient=0.000569032554243572\n",
      "Gradient Descent(93/99): loss=0.3889465105734261, gradient=0.0005649360665681253\n",
      "Gradient Descent(94/99): loss=0.3889464792294523, gradient=0.0005608915842436034\n",
      "Gradient Descent(95/99): loss=0.38894644832968, gradient=0.0005568981558760223\n",
      "Gradient Descent(96/99): loss=0.38894641786542977, gradient=0.0005529548557164728\n",
      "Gradient Descent(97/99): loss=0.38894638782823887, gradient=0.0005490607825400806\n",
      "Gradient Descent(98/99): loss=0.3889463582098529, gradient=0.0005452150585959861\n",
      "Gradient Descent(99/99): loss=0.3889463290022205, gradient=0.0005414168286233141\n",
      "Gradient Descent(0/99): loss=0.3901455595163374, gradient=0.015404169765020188\n",
      "Gradient Descent(1/99): loss=0.3901416343979722, gradient=0.00692066219487445\n",
      "Gradient Descent(2/99): loss=0.390138980535044, gradient=0.005361186929421061\n",
      "Gradient Descent(3/99): loss=0.39013698388909973, gradient=0.004616391342361166\n",
      "Gradient Descent(4/99): loss=0.3901354389998648, gradient=0.004049163171844137\n",
      "Gradient Descent(5/99): loss=0.3901342175654949, gradient=0.0035915677282126177\n",
      "Gradient Descent(6/99): loss=0.39013323208619, gradient=0.003218574430088709\n",
      "Gradient Descent(7/99): loss=0.3901324216911503, gradient=0.0029123419657679983\n",
      "Gradient Descent(8/99): loss=0.3901317434098984, gradient=0.0026590393765899695\n",
      "Gradient Descent(9/99): loss=0.390131166469423, gradient=0.002447864915584245\n",
      "Gradient Descent(10/99): loss=0.3901306685127457, gradient=0.0022703702371114675\n",
      "Gradient Descent(11/99): loss=0.3901302330636144, gradient=0.002119937969475752\n",
      "Gradient Descent(12/99): loss=0.3901298478076309, gradient=0.001991374153804422\n",
      "Gradient Descent(13/99): loss=0.39012950341366565, gradient=0.0018805905672062105\n",
      "Gradient Descent(14/99): loss=0.39012919271643076, gradient=0.001784357704186077\n",
      "Gradient Descent(15/99): loss=0.3901289101429663, gradient=0.0017001132352292736\n",
      "Gradient Descent(16/99): loss=0.39012865130558466, gradient=0.0016258138683742742\n",
      "Gradient Descent(17/99): loss=0.3901284127096081, gradient=0.001559821008473185\n",
      "Gradient Descent(18/99): loss=0.3901281915411097, gradient=0.001500812605173554\n",
      "Gradient Descent(19/99): loss=0.39012798551100347, gradient=0.0014477152021871514\n",
      "Gradient Descent(20/99): loss=0.3901277927392401, gradient=0.0013996515116908533\n",
      "Gradient Descent(21/99): loss=0.39012761166784726, gradient=0.0013558998883674552\n",
      "Gradient Descent(22/99): loss=0.3901274409949366, gradient=0.0013158629102686344\n",
      "Gradient Descent(23/99): loss=0.3901272796241006, gradient=0.00127904292648571\n",
      "Gradient Descent(24/99): loss=0.3901271266252287, gradient=0.0012450229384300294\n",
      "Gradient Descent(25/99): loss=0.39012698120387584, gradient=0.0012134515717510616\n",
      "Gradient Descent(26/99): loss=0.3901268426771023, gradient=0.0011840311943616014\n",
      "Gradient Descent(27/99): loss=0.3901267104542523, gradient=0.0011565084630928875\n",
      "Gradient Descent(28/99): loss=0.3901265840215457, gradient=0.001130666753585715\n",
      "Gradient Descent(29/99): loss=0.3901264629296252, gradient=0.0011063200581108998\n",
      "Gradient Descent(30/99): loss=0.3901263467834286, gradient=0.0010833080342188973\n",
      "Gradient Descent(31/99): loss=0.3901262352338961, gradient=0.001061491961234354\n",
      "Gradient Descent(32/99): loss=0.3901261279711373, gradient=0.0010407514175875066\n",
      "Gradient Descent(33/99): loss=0.3901260247187714, gradient=0.001020981534322377\n",
      "Gradient Descent(34/99): loss=0.39012592522921363, gradient=0.0010020907122413182\n",
      "Gradient Descent(35/99): loss=0.39012582927973116, gradient=0.0009839987145942356\n",
      "Gradient Descent(36/99): loss=0.39012573666912614, gradient=0.000966635065906902\n",
      "Gradient Descent(37/99): loss=0.39012564721493764, gradient=0.0009499377018981214\n",
      "Gradient Descent(38/99): loss=0.39012556075106675, gradient=0.0009338518265241201\n",
      "Gradient Descent(39/99): loss=0.39012547712575774, gradient=0.0009183289408069654\n",
      "Gradient Descent(40/99): loss=0.39012539619987374, gradient=0.0009033260148458779\n",
      "Gradient Descent(41/99): loss=0.3901253178454192, gradient=0.0008888047797193401\n",
      "Gradient Descent(42/99): loss=0.3901252419442685, gradient=0.0008747311201978735\n",
      "Gradient Descent(43/99): loss=0.39012516838707056, gradient=0.0008610745525470423\n",
      "Gradient Descent(44/99): loss=0.3901250970723005, gradient=0.0008478077744045956\n",
      "Gradient Descent(45/99): loss=0.390125027905437, gradient=0.0008349062759007224\n",
      "Gradient Descent(46/99): loss=0.390124960798247, gradient=0.000822348002970077\n",
      "Gradient Descent(47/99): loss=0.3901248956681591, gradient=0.0008101130652620265\n",
      "Gradient Descent(48/99): loss=0.39012483243771867, gradient=0.000798183482254023\n",
      "Gradient Descent(49/99): loss=0.3901247710341067, gradient=0.0007865429621659222\n",
      "Gradient Descent(50/99): loss=0.39012471138871757, gradient=0.000775176709097414\n",
      "Gradient Descent(51/99): loss=0.3901246534367868, gradient=0.0007640712544981729\n",
      "Gradient Descent(52/99): loss=0.39012459711706154, gradient=0.0007532143096575586\n",
      "Gradient Descent(53/99): loss=0.3901245423715076, gradient=0.0007425946363839172\n",
      "Gradient Descent(54/99): loss=0.39012448914504944, gradient=0.0007322019334523942\n",
      "Gradient Descent(55/99): loss=0.3901244373853397, gradient=0.0007220267367453698\n",
      "Gradient Descent(56/99): loss=0.39012438704254965, gradient=0.0007120603313013207\n",
      "Gradient Descent(57/99): loss=0.39012433806918595, gradient=0.0007022946737376076\n",
      "Gradient Descent(58/99): loss=0.39012429041992197, gradient=0.0006927223237230181\n",
      "Gradient Descent(59/99): loss=0.390124244051449, gradient=0.0006833363833576244\n",
      "Gradient Descent(60/99): loss=0.39012419892233935, gradient=0.0006741304434707981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(61/99): loss=0.3901241549929247, gradient=0.0006650985359809911\n",
      "Gradient Descent(62/99): loss=0.39012411222518384, gradient=0.0006562350915743295\n",
      "Gradient Descent(63/99): loss=0.39012407058264165, gradient=0.0006475349020557186\n",
      "Gradient Descent(64/99): loss=0.3901240300302774, gradient=0.0006389930868112156\n",
      "Gradient Descent(65/99): loss=0.39012399053443964, gradient=0.0006306050628915276\n",
      "Gradient Descent(66/99): loss=0.3901239520627687, gradient=0.0006223665182898745\n",
      "Gradient Descent(67/99): loss=0.39012391458412665, gradient=0.0006142733880403901\n",
      "Gradient Descent(68/99): loss=0.3901238780685314, gradient=0.0006063218328112509\n",
      "Gradient Descent(69/99): loss=0.3901238424870967, gradient=0.0005985082197057325\n",
      "Gradient Descent(70/99): loss=0.39012380781197653, gradient=0.0005908291050210125\n",
      "Gradient Descent(71/99): loss=0.3901237740163132, gradient=0.0005832812187437785\n",
      "Gradient Descent(72/99): loss=0.390123741074191, gradient=0.0005758614505895259\n",
      "Gradient Descent(73/99): loss=0.39012370896059007, gradient=0.0005685668374146241\n",
      "Gradient Descent(74/99): loss=0.3901236776513453, gradient=0.0005613945518510705\n",
      "Gradient Descent(75/99): loss=0.39012364712310943, gradient=0.0005543418920315244\n",
      "Gradient Descent(76/99): loss=0.3901236173533151, gradient=0.0005474062722871364\n",
      "Gradient Descent(77/99): loss=0.39012358832014116, gradient=0.0005405852147150806\n",
      "Gradient Descent(78/99): loss=0.3901235600024825, gradient=0.0005338763415239868\n",
      "Gradient Descent(79/99): loss=0.3901235323799184, gradient=0.0005272773680758054\n",
      "Gradient Descent(80/99): loss=0.39012350543268626, gradient=0.0005207860965526732\n",
      "Gradient Descent(81/99): loss=0.39012347914165363, gradient=0.0005144004101841152\n",
      "Gradient Descent(82/99): loss=0.3901234534882939, gradient=0.0005081182679784803\n",
      "Gradient Descent(83/99): loss=0.39012342845466286, gradient=0.0005019376999070839\n",
      "Gradient Descent(84/99): loss=0.3901234040233768, gradient=0.0004958568024971875\n",
      "Gradient Descent(85/99): loss=0.39012338017759013, gradient=0.0004898737347925566\n",
      "Gradient Descent(86/99): loss=0.39012335690097627, gradient=0.00048398671464629716\n",
      "Gradient Descent(87/99): loss=0.39012333417770884, gradient=0.00047819401531404954\n",
      "Gradient Descent(88/99): loss=0.3901233119924425, gradient=0.00047249396231842524\n",
      "Gradient Descent(89/99): loss=0.39012329033029614, gradient=0.000466884930559573\n",
      "Gradient Descent(90/99): loss=0.3901232691768374, gradient=0.00046136534164863883\n",
      "Gradient Descent(91/99): loss=0.3901232485180654, gradient=0.0004559336614435871\n",
      "Gradient Descent(92/99): loss=0.3901232283403971, gradient=0.00045058839776885673\n",
      "Gradient Descent(93/99): loss=0.3901232086306519, gradient=0.0004453280983023811\n",
      "Gradient Descent(94/99): loss=0.39012318937603874, gradient=0.00044015134861488274\n",
      "Gradient Descent(95/99): loss=0.3901231705641431, gradient=0.00043505677034822246\n",
      "Gradient Descent(96/99): loss=0.39012315218291355, gradient=0.0004300430195203369\n",
      "Gradient Descent(97/99): loss=0.3901231342206509, gradient=0.00042510878494631226\n",
      "Gradient Descent(98/99): loss=0.3901231166659959, gradient=0.00042025278676523437\n",
      "Gradient Descent(99/99): loss=0.3901230995079178, gradient=0.0004154737750643906\n",
      "Gradient Descent(0/99): loss=0.3903558749169275, gradient=0.007269556686852351\n",
      "Gradient Descent(1/99): loss=0.3903521958966248, gradient=0.006258110491566674\n",
      "Gradient Descent(2/99): loss=0.39034930639761645, gradient=0.005526262284685619\n",
      "Gradient Descent(3/99): loss=0.3903469777068055, gradient=0.0049469417108016205\n",
      "Gradient Descent(4/99): loss=0.39034505800444547, gradient=0.004480239503350682\n",
      "Gradient Descent(5/99): loss=0.39034344361572887, gradient=0.004099447967943063\n",
      "Gradient Descent(6/99): loss=0.39034206228596463, gradient=0.0037847246253961482\n",
      "Gradient Descent(7/99): loss=0.39034086264076695, gradient=0.003521212980096163\n",
      "Gradient Descent(8/99): loss=0.39033980744462854, gradient=0.0032976899497144598\n",
      "Gradient Descent(9/99): loss=0.39033886912520704, gradient=0.003105916391717646\n",
      "Gradient Descent(10/99): loss=0.3903380269502804, gradient=0.002939417435937696\n",
      "Gradient Descent(11/99): loss=0.3903372650315921, gradient=0.002793349145255106\n",
      "Gradient Descent(12/99): loss=0.3903365710058471, gradient=0.0026639354732107276\n",
      "Gradient Descent(13/99): loss=0.39033593510598025, gradient=0.002548244574163122\n",
      "Gradient Descent(14/99): loss=0.39033534950931836, gradient=0.0024439688229506546\n",
      "Gradient Descent(15/99): loss=0.39033480787041275, gradient=0.002349274752721001\n",
      "Gradient Descent(16/99): loss=0.390334305002438, gradient=0.0022625934719191928\n",
      "Gradient Descent(17/99): loss=0.3903338365591328, gradient=0.002182941529342964\n",
      "Gradient Descent(18/99): loss=0.3903333989174944, gradient=0.0021092460368362827\n",
      "Gradient Descent(19/99): loss=0.3903329890107808, gradient=0.0020407138023312878\n",
      "Gradient Descent(20/99): loss=0.390332604219622, gradient=0.0019766901184618503\n",
      "Gradient Descent(21/99): loss=0.39033224228777297, gradient=0.0019166312710355929\n",
      "Gradient Descent(22/99): loss=0.3903319012563106, gradient=0.0018600830874410676\n",
      "Gradient Descent(23/99): loss=0.39033157941179397, gradient=0.0018066640559238052\n",
      "Gradient Descent(24/99): loss=0.39033127524813294, gradient=0.001756032823364805\n",
      "Gradient Descent(25/99): loss=0.3903309874236806, gradient=0.0017079569297566734\n",
      "Gradient Descent(26/99): loss=0.3903307147477507, gradient=0.0016621668152630312\n",
      "Gradient Descent(27/99): loss=0.3903304561483122, gradient=0.0016184896587303963\n",
      "Gradient Descent(28/99): loss=0.39033021066135165, gradient=0.00157673818197815\n",
      "Gradient Descent(29/99): loss=0.3903299774139479, gradient=0.0015367622928808176\n",
      "Gradient Descent(30/99): loss=0.39032975561228384, gradient=0.001498431060723772\n",
      "Gradient Descent(31/99): loss=0.39032954453166196, gradient=0.00146162965140093\n",
      "Gradient Descent(32/99): loss=0.3903293435081374, gradient=0.0014262568043834245\n",
      "Gradient Descent(33/99): loss=0.3903291519314639, gradient=0.0013922227489150537\n",
      "Gradient Descent(34/99): loss=0.39032896923911176, gradient=0.0013594474775409163\n",
      "Gradient Descent(35/99): loss=0.3903287949111653, gradient=0.0013278593112841526\n",
      "Gradient Descent(36/99): loss=0.3903286284659435, gradient=0.001297393703585919\n",
      "Gradient Descent(37/99): loss=0.3903284694562233, gradient=0.001267992240281484\n",
      "Gradient Descent(38/99): loss=0.39032831746596147, gradient=0.0012396018009831354\n",
      "Gradient Descent(39/99): loss=0.39032817210743664, gradient=0.0012121738537220806\n",
      "Gradient Descent(40/99): loss=0.3903280330187424, gradient=0.0011856638599058817\n",
      "Gradient Descent(41/99): loss=0.3903278998615806, gradient=0.0011600307708414207\n",
      "Gradient Descent(42/99): loss=0.3903277723193041, gradient=0.001135236600460632\n",
      "Gradient Descent(43/99): loss=0.39032765009518056, gradient=0.001111246061631258\n",
      "Gradient Descent(44/99): loss=0.39032753291083705, gradient=0.0010880262556628212\n",
      "Gradient Descent(45/99): loss=0.39032742050486885, gradient=0.0010655464064328766\n",
      "Gradient Descent(46/99): loss=0.3903273126315837, gradient=0.0010437776320392214\n",
      "Gradient Descent(47/99): loss=0.3903272090598681, gradient=0.0010226927480938835\n",
      "Gradient Descent(48/99): loss=0.3903271095721594, gradient=0.00100226609776732\n",
      "Gradient Descent(49/99): loss=0.39032701396350983, gradient=0.000982473404506855\n",
      "Gradient Descent(50/99): loss=0.39032692204073277, gradient=0.0009632916440234957\n",
      "Gradient Descent(51/99): loss=0.3903268336216209, gradient=0.0009446989326958795\n",
      "Gradient Descent(52/99): loss=0.3903267485342301, gradient=0.0009266744299968849\n",
      "Gradient Descent(53/99): loss=0.3903266666162178, gradient=0.0009091982529287077\n",
      "Gradient Descent(54/99): loss=0.39032658771423734, gradient=0.0008922514007663913\n",
      "Gradient Descent(55/99): loss=0.3903265116833746, gradient=0.0008758156886724613\n",
      "Gradient Descent(56/99): loss=0.3903264383866282, gradient=0.0008598736889638037\n",
      "Gradient Descent(57/99): loss=0.39032636769442847, gradient=0.0008444086789948395\n",
      "Gradient Descent(58/99): loss=0.39032629948418796, gradient=0.000829404594774345\n",
      "Gradient Descent(59/99): loss=0.39032623363988644, gradient=0.0008148459895618364\n",
      "Gradient Descent(60/99): loss=0.39032617005168296, gradient=0.0008007179967977306\n",
      "Gradient Descent(61/99): loss=0.3903261086155527, gradient=0.000787006296813125\n",
      "Gradient Descent(62/99): loss=0.39032604923295117, gradient=0.0007736970868424659\n",
      "Gradient Descent(63/99): loss=0.3903259918104969, gradient=0.0007607770539274036\n",
      "Gradient Descent(64/99): loss=0.39032593626227513, gradient=0.0007481976139403072\n",
      "Gradient Descent(65/99): loss=0.39032588250154526, gradient=0.0007360192858057524\n",
      "Gradient Descent(66/99): loss=0.3903258304487341, gradient=0.0007241928552973773\n",
      "Gradient Descent(67/99): loss=0.39032578002835516, gradient=0.0007127067431749261\n",
      "Gradient Descent(68/99): loss=0.3903257311687836, gradient=0.0007015497586309544\n",
      "Gradient Descent(69/99): loss=0.39032568380203947, gradient=0.0006907110819861897\n",
      "Gradient Descent(70/99): loss=0.3903256378635892, gradient=0.0006801802500181128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(71/99): loss=0.3903255932921536, gradient=0.0006699471424256125\n",
      "Gradient Descent(72/99): loss=0.39032555002953107, gradient=0.0006600019692374925\n",
      "Gradient Descent(73/99): loss=0.3903255080204293, gradient=0.0006503352590557216\n",
      "Gradient Descent(74/99): loss=0.3903254672123087, gradient=0.0006409378480406183\n",
      "Gradient Descent(75/99): loss=0.3903254275552312, gradient=0.0006318008695564435\n",
      "Gradient Descent(76/99): loss=0.39032538900172237, gradient=0.0006229157444061894\n",
      "Gradient Descent(77/99): loss=0.39032535150663816, gradient=0.0006142741715923397\n",
      "Gradient Descent(78/99): loss=0.39032531502704026, gradient=0.000605868119548588\n",
      "Gradient Descent(79/99): loss=0.3903252795220792, gradient=0.0005976898177933575\n",
      "Gradient Descent(80/99): loss=0.39032524495288234, gradient=0.0005897317489623231\n",
      "Gradient Descent(81/99): loss=0.39032521128245, gradient=0.0005819866411823571\n",
      "Gradient Descent(82/99): loss=0.3903251784755565, gradient=0.0005744474607531958\n",
      "Gradient Descent(83/99): loss=0.39032514649865674, gradient=0.0005671074051080796\n",
      "Gradient Descent(84/99): loss=0.3903251153197976, gradient=0.0005599598960276433\n",
      "Gradient Descent(85/99): loss=0.390325084908536, gradient=0.0005529985730842138\n",
      "Gradient Descent(86/99): loss=0.39032505523585753, gradient=0.0005462172872976961\n",
      "Gradient Descent(87/99): loss=0.3903250262741057, gradient=0.0005396100949850689\n",
      "Gradient Descent(88/99): loss=0.3903249979969089, gradient=0.0005331712517893407\n",
      "Gradient Descent(89/99): loss=0.3903249703791145, gradient=0.0005268952068743452\n",
      "Gradient Descent(90/99): loss=0.390324943396727, gradient=0.0005207765972748812\n",
      "Gradient Descent(91/99): loss=0.3903249170268475, gradient=0.0005148102423919216\n",
      "Gradient Descent(92/99): loss=0.39032489124761827, gradient=0.0005089911386249308\n",
      "Gradient Descent(93/99): loss=0.3903248660381688, gradient=0.0005033144541340038\n",
      "Gradient Descent(94/99): loss=0.3903248413785664, gradient=0.000497775523725895\n",
      "Gradient Descent(95/99): loss=0.3903248172497681, gradient=0.0004923698438584341\n",
      "Gradient Descent(96/99): loss=0.3903247936335754, gradient=0.00048709306775944293\n",
      "Gradient Descent(97/99): loss=0.39032477051259246, gradient=0.00048194100065580717\n",
      "Gradient Descent(98/99): loss=0.3903247478701849, gradient=0.00047690959511005945\n",
      "Gradient Descent(99/99): loss=0.39032472569044235, gradient=0.0004719949464615752\n",
      "Gradient Descent(0/99): loss=0.38997658040814426, gradient=0.010007693970118005\n",
      "Gradient Descent(1/99): loss=0.38997034785809115, gradient=0.008243384209561453\n",
      "Gradient Descent(2/99): loss=0.3899659363057696, gradient=0.006907913308099941\n",
      "Gradient Descent(3/99): loss=0.3899627022053685, gradient=0.005890487793473883\n",
      "Gradient Descent(4/99): loss=0.38996024613396624, gradient=0.005112583864237758\n",
      "Gradient Descent(5/99): loss=0.38995831706894946, gradient=0.004513824068129871\n",
      "Gradient Descent(6/99): loss=0.3899567548476851, gradient=0.004048297299103817\n",
      "Gradient Descent(7/99): loss=0.3899554555898031, gradient=0.003681163130367586\n",
      "Gradient Descent(8/99): loss=0.3899543505483311, gradient=0.0033866565282842636\n",
      "Gradient Descent(9/99): loss=0.3899533932010473, gradient=0.00314594563294154\n",
      "Gradient Descent(10/99): loss=0.38995255130804196, gradient=0.0029453560977666005\n",
      "Gradient Descent(11/99): loss=0.3899518019079473, gradient=0.002775203959421621\n",
      "Gradient Descent(12/99): loss=0.3899511282505562, gradient=0.0026283993795718314\n",
      "Gradient Descent(13/99): loss=0.3899505177941793, gradient=0.002499856034237022\n",
      "Gradient Descent(14/99): loss=0.3899499609200634, gradient=0.002385864449840226\n",
      "Gradient Descent(15/99): loss=0.38994945008659476, gradient=0.00228368385008671\n",
      "Gradient Descent(16/99): loss=0.38994897926247524, gradient=0.0021912574568933017\n",
      "Gradient Descent(17/99): loss=0.3899485435392605, gradient=0.002107015317419285\n",
      "Gradient Descent(18/99): loss=0.3899481388609851, gradient=0.002029737958450132\n",
      "Gradient Descent(19/99): loss=0.38994776183150665, gradient=0.001958461799900542\n",
      "Gradient Descent(20/99): loss=0.389947409574383, gradient=0.0018924130631765112\n",
      "Gradient Descent(21/99): loss=0.389947079638184, gradient=0.0018309073370138653\n",
      "Gradient Descent(22/99): loss=0.3899467698885973, gradient=0.0017735359198299393\n",
      "Gradient Descent(23/99): loss=0.38994647847929814, gradient=0.0017198044519789466\n",
      "Gradient Descent(24/99): loss=0.389946203793698, gradient=0.0016693452444022112\n",
      "Gradient Descent(25/99): loss=0.3899459444077138, gradient=0.001621844780528207\n",
      "Gradient Descent(26/99): loss=0.3899456990655475, gradient=0.001576996736914596\n",
      "Gradient Descent(27/99): loss=0.38994546663817187, gradient=0.0015346427923070487\n",
      "Gradient Descent(28/99): loss=0.38994524612162795, gradient=0.0014945409171704034\n",
      "Gradient Descent(29/99): loss=0.3899450366145504, gradient=0.0014565122994501234\n",
      "Gradient Descent(30/99): loss=0.3899448373046492, gradient=0.001420399088157871\n",
      "Gradient Descent(31/99): loss=0.3899446474573969, gradient=0.0013860611737878657\n",
      "Gradient Descent(32/99): loss=0.3899444664064958, gradient=0.0013533735533367462\n",
      "Gradient Descent(33/99): loss=0.3899442935457957, gradient=0.0013222241576940782\n",
      "Gradient Descent(34/99): loss=0.38994412832240133, gradient=0.0012925120485882526\n",
      "Gradient Descent(35/99): loss=0.38994397023076244, gradient=0.0012641459136309762\n",
      "Gradient Descent(36/99): loss=0.38994381880758133, gradient=0.00123704280375074\n",
      "Gradient Descent(37/99): loss=0.38994367362739923, gradient=0.0012111270690900219\n",
      "Gradient Descent(38/99): loss=0.3899435342987601, gradient=0.0011863294583869162\n",
      "Gradient Descent(39/99): loss=0.38994340046085363, gradient=0.0011625863537415228\n",
      "Gradient Descent(40/99): loss=0.38994327178057236, gradient=0.0011398391180220359\n",
      "Gradient Descent(41/99): loss=0.3899431479499164, gradient=0.0011180335363748118\n",
      "Gradient Descent(42/99): loss=0.38994302868369896, gradient=0.0010971193366481563\n",
      "Gradient Descent(43/99): loss=0.3899429137175094, gradient=0.001077049776214027\n",
      "Gradient Descent(44/99): loss=0.38994280280589905, gradient=0.0010577812848306283\n",
      "Gradient Descent(45/99): loss=0.38994269572075957, gradient=0.001039273154939861\n",
      "Gradient Descent(46/99): loss=0.3899425922498701, gradient=0.0010214872722224392\n",
      "Gradient Descent(47/99): loss=0.38994249219559196, gradient=0.0010043878804057878\n",
      "Gradient Descent(48/99): loss=0.38994239537369024, gradient=0.0009879413752850194\n",
      "Gradient Descent(49/99): loss=0.38994230161227184, gradient=0.0009721161237163956\n",
      "Gradient Descent(50/99): loss=0.38994221075082025, gradient=0.0009568823040052983\n",
      "Gradient Descent(51/99): loss=0.389942122639323, gradient=0.000942211764663688\n",
      "Gradient Descent(52/99): loss=0.38994203713747516, gradient=0.0009280778989733562\n",
      "Gradient Descent(53/99): loss=0.3899419541139542, gradient=0.0009144555331780366\n",
      "Gradient Descent(54/99): loss=0.3899418734457589, gradient=0.0009013208264529406\n",
      "Gradient Descent(55/99): loss=0.38994179501760323, gradient=0.0008886511810739605\n",
      "Gradient Descent(56/99): loss=0.3899417187213606, gradient=0.0008764251614402847\n",
      "Gradient Descent(57/99): loss=0.38994164445555624, gradient=0.0008646224207993274\n",
      "Gradient Descent(58/99): loss=0.3899415721248988, gradient=0.0008532236346897857\n",
      "Gradient Descent(59/99): loss=0.3899415016398501, gradient=0.0008422104402587241\n",
      "Gradient Descent(60/99): loss=0.3899414329162276, gradient=0.0008315653807293743\n",
      "Gradient Descent(61/99): loss=0.38994136587484013, gradient=0.00082127185439742\n",
      "Gradient Descent(62/99): loss=0.38994130044114944, gradient=0.0008113140676220178\n",
      "Gradient Descent(63/99): loss=0.3899412365449577, gradient=0.0008016769913512789\n",
      "Gradient Descent(64/99): loss=0.3899411741201197, gradient=0.0007923463207865166\n",
      "Gradient Descent(65/99): loss=0.38994111310427443, gradient=0.000783308437842831\n",
      "Gradient Descent(66/99): loss=0.38994105343859803, gradient=0.0007745503761115394\n",
      "Gradient Descent(67/99): loss=0.3899409950675738, gradient=0.0007660597880691975\n",
      "Gradient Descent(68/99): loss=0.3899409379387785, gradient=0.0007578249143119902\n",
      "Gradient Descent(69/99): loss=0.3899408820026845, gradient=0.0007498345546246042\n",
      "Gradient Descent(70/99): loss=0.3899408272124756, gradient=0.000742078040716216\n",
      "Gradient Descent(71/99): loss=0.3899407735238754, gradient=0.0007345452104796416\n",
      "Gradient Descent(72/99): loss=0.38994072089498855, gradient=0.0007272263836465836\n",
      "Gradient Descent(73/99): loss=0.38994066928615145, gradient=0.0007201123387286022\n",
      "Gradient Descent(74/99): loss=0.38994061865979446, gradient=0.0007131942911466134\n",
      "Gradient Descent(75/99): loss=0.38994056898031265, gradient=0.0007064638724636195\n",
      "Gradient Descent(76/99): loss=0.38994052021394643, gradient=0.0006999131106448094\n",
      "Gradient Descent(77/99): loss=0.389940472328668, gradient=0.0006935344112784126\n",
      "Gradient Descent(78/99): loss=0.38994042529407796, gradient=0.0006873205396969778\n",
      "Gradient Descent(79/99): loss=0.38994037908130663, gradient=0.0006812646039461683\n",
      "Gradient Descent(80/99): loss=0.3899403336629229, gradient=0.0006753600385522768\n",
      "Gradient Descent(81/99): loss=0.3899402890128492, gradient=0.0006696005890454249\n",
      "Gradient Descent(82/99): loss=0.3899402451062812, gradient=0.0006639802971983106\n",
      "Gradient Descent(83/99): loss=0.3899402019196137, gradient=0.0006584934869445793\n",
      "Gradient Descent(84/99): loss=0.3899401594303701, gradient=0.0006531347509431436\n",
      "Gradient Descent(85/99): loss=0.3899401176171379, gradient=0.0006478989377579002\n",
      "Gradient Descent(86/99): loss=0.3899400764595058, gradient=0.0006427811396238869\n",
      "Gradient Descent(87/99): loss=0.38994003593800847, gradient=0.00063777668077372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(88/99): loss=0.3899399960340718, gradient=0.0006328811062980405\n",
      "Gradient Descent(89/99): loss=0.3899399567299618, gradient=0.0006280901715178916\n",
      "Gradient Descent(90/99): loss=0.38993991800873895, gradient=0.0006233998318454294\n",
      "Gradient Descent(91/99): loss=0.38993987985421286, gradient=0.0006188062331125425\n",
      "Gradient Descent(92/99): loss=0.38993984225090067, gradient=0.000614305702346663\n",
      "Gradient Descent(93/99): loss=0.3899398051839891, gradient=0.0006098947389753353\n",
      "Gradient Descent(94/99): loss=0.38993976863929647, gradient=0.0006055700064403306\n",
      "Gradient Descent(95/99): loss=0.38993973260323983, gradient=0.0006013283242045237\n",
      "Gradient Descent(96/99): loss=0.38993969706280124, gradient=0.0005971666601341165\n",
      "Gradient Descent(97/99): loss=0.3899396620054987, gradient=0.0005930821232402969\n",
      "Gradient Descent(98/99): loss=0.3899396274193565, gradient=0.0005890719567651221\n",
      "Gradient Descent(99/99): loss=0.38993959329287975, gradient=0.0005851335315958932\n",
      "Gradient Descent(0/99): loss=0.389675778030182, gradient=0.011093120552441493\n",
      "Gradient Descent(1/99): loss=0.3896707925256133, gradient=0.007446775865988937\n",
      "Gradient Descent(2/99): loss=0.38966725699100624, gradient=0.00618222159297605\n",
      "Gradient Descent(3/99): loss=0.3896646602819574, gradient=0.005281959803384731\n",
      "Gradient Descent(4/99): loss=0.3896627141954734, gradient=0.004562164154793788\n",
      "Gradient Descent(5/99): loss=0.38966122730222735, gradient=0.00397882011063673\n",
      "Gradient Descent(6/99): loss=0.38966006944569176, gradient=0.003503397871921004\n",
      "Gradient Descent(7/99): loss=0.3896591511269423, gradient=0.0031134764557778725\n",
      "Gradient Descent(8/99): loss=0.3896584100552297, gradient=0.002791389320656412\n",
      "Gradient Descent(9/99): loss=0.3896578023168457, gradient=0.002523217276430908\n",
      "Gradient Descent(10/99): loss=0.38965729652606484, gradient=0.0022980353818108485\n",
      "Gradient Descent(11/99): loss=0.3896568699252562, gradient=0.002107296170801116\n",
      "Gradient Descent(12/99): loss=0.38965650576558886, gradient=0.0019443252187884364\n",
      "Gradient Descent(13/99): loss=0.3896561915335794, gradient=0.0018039120266596551\n",
      "Gradient Descent(14/99): loss=0.389655917739608, gradient=0.001681982076025281\n",
      "Gradient Descent(15/99): loss=0.3896556770822138, gradient=0.0015753373238730957\n",
      "Gradient Descent(16/99): loss=0.3896554638654673, gradient=0.001481453412571091\n",
      "Gradient Descent(17/99): loss=0.3896552735881088, gradient=0.0013983229548249951\n",
      "Gradient Descent(18/99): loss=0.3896551026502847, gradient=0.001324335528971749\n",
      "Gradient Descent(19/99): loss=0.3896549481415477, gradient=0.0012581864181831592\n",
      "Gradient Descent(20/99): loss=0.38965480768558847, gradient=0.001198807526236665\n",
      "Gradient Descent(21/99): loss=0.38965467932501024, gradient=0.0011453151995040967\n",
      "Gradient Descent(22/99): loss=0.3896545614346882, gradient=0.0010969708180655477\n",
      "Gradient Descent(23/99): loss=0.389654452655787, gradient=0.0010531509654697533\n",
      "Gradient Descent(24/99): loss=0.3896543518448937, gradient=0.0010133247508371823\n",
      "Gradient Descent(25/99): loss=0.38965425803434395, gradient=0.0009770364581029704\n",
      "Gradient Descent(26/99): loss=0.3896541704009487, gradient=0.0009438921607618842\n",
      "Gradient Descent(27/99): loss=0.3896540882410946, gradient=0.0009135492925999336\n",
      "Gradient Descent(28/99): loss=0.38965401095074254, gradient=0.0008857084292562691\n",
      "Gradient Descent(29/99): loss=0.389653938009229, gradient=0.0008601067321340802\n",
      "Gradient Descent(30/99): loss=0.3896538689660497, gradient=0.0008365126514531547\n",
      "Gradient Descent(31/99): loss=0.3896538034300089, gradient=0.0008147215919429435\n",
      "Gradient Descent(32/99): loss=0.389653741060253, gradient=0.0007945523227031424\n",
      "Gradient Descent(33/99): loss=0.3896536815588271, gradient=0.0007758439696293204\n",
      "Gradient Descent(34/99): loss=0.38965362466446274, gradient=0.000758453470155497\n",
      "Gradient Descent(35/99): loss=0.38965357014737023, gradient=0.0007422534001053898\n",
      "Gradient Descent(36/99): loss=0.38965351780485824, gradient=0.0007271301042745288\n",
      "Gradient Descent(37/99): loss=0.38965346745763324, gradient=0.0007129820782596922\n",
      "Gradient Descent(38/99): loss=0.38965341894666206, gradient=0.0006997185606746685\n",
      "Gradient Descent(39/99): loss=0.38965337213050566, gradient=0.0006872583034432613\n",
      "Gradient Descent(40/99): loss=0.3896533268830446, gradient=0.0006755284942130599\n",
      "Gradient Descent(41/99): loss=0.38965328309153247, gradient=0.0006644638097058577\n",
      "Gradient Descent(42/99): loss=0.3896532406549272, gradient=0.0006540055824617698\n",
      "Gradient Descent(43/99): loss=0.3896531994824552, gradient=0.0006441010662546993\n",
      "Gradient Descent(44/99): loss=0.3896531594923735, gradient=0.0006347027876848463\n",
      "Gradient Descent(45/99): loss=0.3896531206108985, gradient=0.0006257679732441776\n",
      "Gradient Descent(46/99): loss=0.3896530827712793, gradient=0.0006172580426159407\n",
      "Gradient Descent(47/99): loss=0.38965304591299105, gradient=0.0006091381601883543\n",
      "Gradient Descent(48/99): loss=0.38965300998103297, gradient=0.0006013768377896065\n",
      "Gradient Descent(49/99): loss=0.3896529749253163, gradient=0.0005939455825294745\n",
      "Gradient Descent(50/99): loss=0.3896529407001281, gradient=0.0005868185843875408\n",
      "Gradient Descent(51/99): loss=0.389652907263662, gradient=0.0005799724388443376\n",
      "Gradient Descent(52/99): loss=0.3896528745776048, gradient=0.0005733859004221726\n",
      "Gradient Descent(53/99): loss=0.3896528426067741, gradient=0.0005670396635035705\n",
      "Gradient Descent(54/99): loss=0.38965281131879714, gradient=0.0005609161672326864\n",
      "Gradient Descent(55/99): loss=0.3896527806838288, gradient=0.0005549994216909588\n",
      "Gradient Descent(56/99): loss=0.3896527506742991, gradient=0.0005492748528763704\n",
      "Gradient Descent(57/99): loss=0.389652721264693, gradient=0.0005437291643128521\n",
      "Gradient Descent(58/99): loss=0.38965269243135053, gradient=0.0005383502133784814\n",
      "Gradient Descent(59/99): loss=0.3896526641522925, gradient=0.0005331269006700904\n",
      "Gradient Descent(60/99): loss=0.38965263640706327, gradient=0.0005280490709242888\n",
      "Gradient Descent(61/99): loss=0.38965260917658995, gradient=0.0005231074241913331\n",
      "Gradient Descent(62/99): loss=0.38965258244305745, gradient=0.0005182934361148204\n",
      "Gradient Descent(63/99): loss=0.38965255618979633, gradient=0.000513599286305105\n",
      "Gradient Descent(64/99): loss=0.3896525304011823, gradient=0.0005090177939158393\n",
      "Gradient Descent(65/99): loss=0.38965250506254456, gradient=0.0005045423596362995\n",
      "Gradient Descent(66/99): loss=0.38965248016008547, gradient=0.0005001669134055177\n",
      "Gradient Descent(67/99): loss=0.38965245568080664, gradient=0.0004958858672343792\n",
      "Gradient Descent(68/99): loss=0.3896524316124427, gradient=0.0004916940725932809\n",
      "Gradient Descent(69/99): loss=0.38965240794340145, gradient=0.00048758678188449875\n",
      "Gradient Descent(70/99): loss=0.3896523846627101, gradient=0.00048355961357386\n",
      "Gradient Descent(71/99): loss=0.38965236175996587, gradient=0.0004796085206041292\n",
      "Gradient Descent(72/99): loss=0.38965233922529136, gradient=0.00047572976175449465\n",
      "Gradient Descent(73/99): loss=0.3896523170492955, gradient=0.00047191987564825915\n",
      "Gradient Descent(74/99): loss=0.3896522952230351, gradient=0.00046817565714309676\n",
      "Gradient Descent(75/99): loss=0.3896522737379832, gradient=0.0004644941358677636\n",
      "Gradient Descent(76/99): loss=0.38965225258599767, gradient=0.0004608725566940424\n",
      "Gradient Descent(77/99): loss=0.38965223175929425, gradient=0.00045730836195547055\n",
      "Gradient Descent(78/99): loss=0.38965221125042065, gradient=0.00045379917524475524\n",
      "Gradient Descent(79/99): loss=0.3896521910522345, gradient=0.0004503427866385446\n",
      "Gradient Descent(80/99): loss=0.38965217115788137, gradient=0.00044693713921461196\n",
      "Gradient Descent(81/99): loss=0.3896521515607758, gradient=0.00044358031674013624\n",
      "Gradient Descent(82/99): loss=0.38965213225458417, gradient=0.00044027053242223655\n",
      "Gradient Descent(83/99): loss=0.3896521132332079, gradient=0.00043700611862219446\n",
      "Gradient Descent(84/99): loss=0.3896520944907692, gradient=0.0004337855174458915\n",
      "Gradient Descent(85/99): loss=0.3896520760215966, gradient=0.0004306072721304523\n",
      "Gradient Descent(86/99): loss=0.3896520578202132, gradient=0.0004274700191552902\n",
      "Gradient Descent(87/99): loss=0.38965203988132513, gradient=0.00042437248101307216\n",
      "Gradient Descent(88/99): loss=0.38965202219981077, gradient=0.00042131345958191206\n",
      "Gradient Descent(89/99): loss=0.38965200477071027, gradient=0.0004182918300457009\n",
      "Gradient Descent(90/99): loss=0.38965198758921815, gradient=0.00041530653531459105\n",
      "Gradient Descent(91/99): loss=0.3896519706506733, gradient=0.00041235658090214665\n",
      "Gradient Descent(92/99): loss=0.3896519539505522, gradient=0.00040944103022010254\n",
      "Gradient Descent(93/99): loss=0.3896519374844618, gradient=0.0004065590002537445\n",
      "Gradient Descent(94/99): loss=0.38965192124813286, gradient=0.0004037096575868532\n",
      "Gradient Descent(95/99): loss=0.3896519052374129, gradient=0.0004008922147454176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(96/99): loss=0.38965188944826173, gradient=0.00039810592683364076\n",
      "Gradient Descent(97/99): loss=0.38965187387674627, gradient=0.0003953500884377956\n",
      "Gradient Descent(98/99): loss=0.3896518585190344, gradient=0.00039262403077522406\n",
      "Gradient Descent(99/99): loss=0.3896518433713913, gradient=0.0003899271190682796\n",
      "Gradient Descent(0/99): loss=0.3889622402135964, gradient=0.008179021494305392\n",
      "Gradient Descent(1/99): loss=0.38895947866833125, gradient=0.00555077402642174\n",
      "Gradient Descent(2/99): loss=0.3889575604229461, gradient=0.004564635415818567\n",
      "Gradient Descent(3/99): loss=0.38895617896769885, gradient=0.0038603998743402326\n",
      "Gradient Descent(4/99): loss=0.3889551590855598, gradient=0.0033071548064991375\n",
      "Gradient Descent(5/99): loss=0.38895438610482336, gradient=0.0028700372274604214\n",
      "Gradient Descent(6/99): loss=0.38895378367728506, gradient=0.002525286058446988\n",
      "Gradient Descent(7/99): loss=0.3889533005873656, gradient=0.0022538069520578077\n",
      "Gradient Descent(8/99): loss=0.38895290222629414, gradient=0.0020400368506620714\n",
      "Gradient Descent(9/99): loss=0.38895256501745173, gradient=0.001871336942685979\n",
      "Gradient Descent(10/99): loss=0.388952272754361, gradient=0.0017375415544137872\n",
      "Gradient Descent(11/99): loss=0.38895201418417974, gradient=0.001630582382535588\n",
      "Gradient Descent(12/99): loss=0.38895178140563785, gradient=0.0015441501658837683\n",
      "Gradient Descent(13/99): loss=0.38895156880162046, gradient=0.001473381366207809\n",
      "Gradient Descent(14/99): loss=0.3889513723241816, gradient=0.0014145725522022316\n",
      "Gradient Descent(15/99): loss=0.3889511890128684, gradient=0.0013649296856567705\n",
      "Gradient Descent(16/99): loss=0.3889510166681997, gradient=0.0013223572372621718\n",
      "Gradient Descent(17/99): loss=0.38895085362882226, gradient=0.00128528759070542\n",
      "Gradient Descent(18/99): loss=0.3889506986182979, gradient=0.0012525474189763622\n",
      "Gradient Descent(19/99): loss=0.38895055063891726, gradient=0.0012232556126868944\n",
      "Gradient Descent(20/99): loss=0.38895040889745935, gradient=0.0011967467441495612\n",
      "Gradient Descent(21/99): loss=0.3889502727527938, gradient=0.0011725144213791186\n",
      "Gradient Descent(22/99): loss=0.3889501416785192, gradient=0.001150169716181893\n",
      "Gradient Descent(23/99): loss=0.38895001523603384, gradient=0.0011294107990943959\n",
      "Gradient Descent(24/99): loss=0.3889498930549005, gradient=0.001110000797721861\n",
      "Gradient Descent(25/99): loss=0.38894977481835696, gradient=0.0010917516386261326\n",
      "Gradient Descent(26/99): loss=0.38894966025249017, gradient=0.0010745122220291758\n",
      "Gradient Descent(27/99): loss=0.38894954911804785, gradient=0.0010581597277711818\n",
      "Gradient Descent(28/99): loss=0.3889494412041621, gradient=0.0010425931848453353\n",
      "Gradient Descent(29/99): loss=0.3889493363234833, gradient=0.001027728680831645\n",
      "Gradient Descent(30/99): loss=0.388949234308357, gradient=0.0010134957638527646\n",
      "Gradient Descent(31/99): loss=0.3889491350077869, gradient=0.0009998347161380378\n",
      "Gradient Descent(32/99): loss=0.3889490382849949, gradient=0.000986694468605895\n",
      "Gradient Descent(33/99): loss=0.38894894401543995, gradient=0.0009740309902589477\n",
      "Gradient Descent(34/99): loss=0.3889488520851925, gradient=0.0009618060320729056\n",
      "Gradient Descent(35/99): loss=0.3889487623895894, gradient=0.0009499861378051312\n",
      "Gradient Descent(36/99): loss=0.3889486748321119, gradient=0.0009385418575784308\n",
      "Gradient Descent(37/99): loss=0.388948589323439, gradient=0.0009274471169218437\n",
      "Gradient Descent(38/99): loss=0.3889485057806468, gradient=0.000916678706090128\n",
      "Gradient Descent(39/99): loss=0.388948424126525, gradient=0.0009062158632940807\n",
      "Gradient Descent(40/99): loss=0.38894834429364905, gradient=0.0008959854390459384\n",
      "Gradient Descent(41/99): loss=0.3889482662092676, gradient=0.000886083843131555\n",
      "Gradient Descent(42/99): loss=0.3889481898101531, gradient=0.0008764367324474683\n",
      "Gradient Descent(43/99): loss=0.3889481150368612, gradient=0.0008670302713304867\n",
      "Gradient Descent(44/99): loss=0.3889480418333922, gradient=0.0008578518374222812\n",
      "Gradient Descent(45/99): loss=0.38894797014689403, gradient=0.0008488898796290546\n",
      "Gradient Descent(46/99): loss=0.38894789992739737, gradient=0.0008401337966993805\n",
      "Gradient Descent(47/99): loss=0.38894783112758213, gradient=0.0008315738329052985\n",
      "Gradient Descent(48/99): loss=0.38894776370256906, gradient=0.0008232009879885913\n",
      "Gradient Descent(49/99): loss=0.3889476976097303, gradient=0.0008150069390651624\n",
      "Gradient Descent(50/99): loss=0.38894763280852274, gradient=0.0008069839726019497\n",
      "Gradient Descent(51/99): loss=0.38894756926033625, gradient=0.0007991249249174574\n",
      "Gradient Descent(52/99): loss=0.3889475069283558, gradient=0.0007914231299273333\n",
      "Gradient Descent(53/99): loss=0.3889474457774383, gradient=0.000783872373074281\n",
      "Gradient Descent(54/99): loss=0.3889473857739996, gradient=0.0007764668505598403\n",
      "Gradient Descent(55/99): loss=0.38894732688591177, gradient=0.0007692011331393822\n",
      "Gradient Descent(56/99): loss=0.38894726908240945, gradient=0.0007620701338607519\n",
      "Gradient Descent(57/99): loss=0.38894721233400414, gradient=0.0007550690792247246\n",
      "Gradient Descent(58/99): loss=0.38894715661240603, gradient=0.0007481934833255201\n",
      "Gradient Descent(59/99): loss=0.3889471018904517, gradient=0.0007414391245971309\n",
      "Gradient Descent(60/99): loss=0.3889470481420376, gradient=0.0007348020248460064\n",
      "Gradient Descent(61/99): loss=0.38894699534206, gradient=0.0007282784302979198\n",
      "Gradient Descent(62/99): loss=0.3889469434663578, gradient=0.000721864794424364\n",
      "Gradient Descent(63/99): loss=0.3889468924916608, gradient=0.0007155577623476232\n",
      "Gradient Descent(64/99): loss=0.38894684239554184, gradient=0.0007093541566504192\n",
      "Gradient Descent(65/99): loss=0.38894679315637115, gradient=0.0007032509644385708\n",
      "Gradient Descent(66/99): loss=0.3889467447532764, gradient=0.0006972453255263345\n",
      "Gradient Descent(67/99): loss=0.3889466971661033, gradient=0.0006913345216284421\n",
      "Gradient Descent(68/99): loss=0.3889466503753798, gradient=0.0006855159664593967\n",
      "Gradient Descent(69/99): loss=0.3889466043622832, gradient=0.0006797871966505774\n",
      "Gradient Descent(70/99): loss=0.38894655910860804, gradient=0.0006741458634074895\n",
      "Gradient Descent(71/99): loss=0.3889465145967383, gradient=0.0006685897248378457\n",
      "Gradient Descent(72/99): loss=0.38894647080961964, gradient=0.0006631166388887817\n",
      "Gradient Descent(73/99): loss=0.3889464277307335, gradient=0.0006577245568386505\n",
      "Gradient Descent(74/99): loss=0.3889463853440739, gradient=0.0006524115172940872\n",
      "Gradient Descent(75/99): loss=0.3889463436341245, gradient=0.000647175640648415\n",
      "Gradient Descent(76/99): loss=0.3889463025858384, gradient=0.000642015123962018\n",
      "Gradient Descent(77/99): loss=0.38894626218461736, gradient=0.0006369282362287262\n",
      "Gradient Descent(78/99): loss=0.38894622241629356, gradient=0.0006319133139962972\n",
      "Gradient Descent(79/99): loss=0.38894618326711317, gradient=0.0006269687573115648\n",
      "Gradient Descent(80/99): loss=0.38894614472371725, gradient=0.0006220930259639444\n",
      "Gradient Descent(81/99): loss=0.3889461067731293, gradient=0.0006172846360031254\n",
      "Gradient Descent(82/99): loss=0.38894606940273774, gradient=0.0006125421565090291\n",
      "Gradient Descent(83/99): loss=0.3889460326002837, gradient=0.0006078642065939763\n",
      "Gradient Descent(84/99): loss=0.3889459963538468, gradient=0.0006032494526191269\n",
      "Gradient Descent(85/99): loss=0.38894596065183334, gradient=0.0005986966056077279\n",
      "Gradient Descent(86/99): loss=0.38894592548296403, gradient=0.000594204418840633\n",
      "Gradient Descent(87/99): loss=0.3889458908362623, gradient=0.0005897716856199215\n",
      "Gradient Descent(88/99): loss=0.38894585670104453, gradient=0.0005853972371867321\n",
      "Gradient Descent(89/99): loss=0.3889458230669092, gradient=0.0005810799407834021\n",
      "Gradient Descent(90/99): loss=0.3889457899237268, gradient=0.0005768186978468363\n",
      "Gradient Descent(91/99): loss=0.3889457572616317, gradient=0.0005726124423249416\n",
      "Gradient Descent(92/99): loss=0.38894572507101194, gradient=0.0005684601391056631\n",
      "Gradient Descent(93/99): loss=0.388945693342502, gradient=0.0005643607825506855\n",
      "Gradient Descent(94/99): loss=0.38894566206697406, gradient=0.0005603133951261671\n",
      "Gradient Descent(95/99): loss=0.3889456312355305, gradient=0.0005563170261225865\n",
      "Gradient Descent(96/99): loss=0.38894560083949614, gradient=0.0005523707504577313\n",
      "Gradient Descent(97/99): loss=0.38894557087041215, gradient=0.0005484736675563985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(98/99): loss=0.3889455413200281, gradient=0.0005446249003011355\n",
      "Gradient Descent(99/99): loss=0.38894551218029627, gradient=0.0005408235940490428\n",
      "Gradient Descent(0/99): loss=0.3901446157987512, gradient=0.015402981030962463\n",
      "Gradient Descent(1/99): loss=0.3901406912852225, gradient=0.006920268947461967\n",
      "Gradient Descent(2/99): loss=0.39013803784432705, gradient=0.005360882601680578\n",
      "Gradient Descent(3/99): loss=0.3901360415669091, gradient=0.004616094389188647\n",
      "Gradient Descent(4/99): loss=0.39013449699796576, gradient=0.004048865692560095\n",
      "Gradient Descent(5/99): loss=0.3901332758487869, gradient=0.0035912672045424407\n",
      "Gradient Descent(6/99): loss=0.39013229062659077, gradient=0.003218269189732974\n",
      "Gradient Descent(7/99): loss=0.3901314804664621, gradient=0.0029120307959649384\n",
      "Gradient Descent(8/99): loss=0.39013080240227094, gradient=0.0026587214037499395\n",
      "Gradient Descent(9/99): loss=0.3901302256643928, gradient=0.002447539522423312\n",
      "Gradient Descent(10/99): loss=0.39012972789847983, gradient=0.002270036999411489\n",
      "Gradient Descent(11/99): loss=0.3901292926303481, gradient=0.002119596607626626\n",
      "Gradient Descent(12/99): loss=0.3901289075472386, gradient=0.0019910244958738334\n",
      "Gradient Descent(13/99): loss=0.39012856331933016, gradient=0.0018802325209721514\n",
      "Gradient Descent(14/99): loss=0.3901282527823857, gradient=0.001783991236066801\n",
      "Gradient Descent(15/99): loss=0.3901279703642966, gradient=0.001699738354457998\n",
      "Gradient Descent(16/99): loss=0.39012771167806776, gradient=0.001625430615118973\n",
      "Gradient Descent(17/99): loss=0.39012747322958924, gradient=0.0015594294449000137\n",
      "Gradient Descent(18/99): loss=0.3901272522054023, gradient=0.0015004128086974692\n",
      "Gradient Descent(19/99): loss=0.3901270463168085, gradient=0.0014473072603466155\n",
      "Gradient Descent(20/99): loss=0.39012685368408145, gradient=0.0013992355182328446\n",
      "Gradient Descent(21/99): loss=0.3901266727495197, gradient=0.0013554759402406781\n",
      "Gradient Descent(22/99): loss=0.3901265022114625, gradient=0.0013154311053044567\n",
      "Gradient Descent(23/99): loss=0.39012634097369453, gradient=0.001278603361605801\n",
      "Gradient Descent(24/99): loss=0.390126188106269, gradient=0.001244575708261657\n",
      "Gradient Descent(25/99): loss=0.3901260428148803, gradient=0.001212996767562066\n",
      "Gradient Descent(26/99): loss=0.3901259044167074, gradient=0.0011835689032480884\n",
      "Gradient Descent(27/99): loss=0.39012577232119783, gradient=0.0011560387673685405\n",
      "Gradient Descent(28/99): loss=0.39012564601465943, gradient=0.001130189730332837\n",
      "Gradient Descent(29/99): loss=0.39012552504781134, gradient=0.0011058357788620106\n",
      "Gradient Descent(30/99): loss=0.39012540902565856, gradient=0.0010828165647433913\n",
      "Gradient Descent(31/99): loss=0.3901252975991993, gradient=0.0010609933614102371\n",
      "Gradient Descent(32/99): loss=0.3901251904585935, gradient=0.0010402457413416304\n",
      "Gradient Descent(33/99): loss=0.39012508732750556, gradient=0.001020468829625686\n",
      "Gradient Descent(34/99): loss=0.3901249879583902, gradient=0.0010015710211480789\n",
      "Gradient Descent(35/99): loss=0.3901248921285496, gradient=0.0009834720733161817\n",
      "Gradient Descent(36/99): loss=0.390124799636818, gradient=0.0009661015049148539\n",
      "Gradient Descent(37/99): loss=0.39012471030076135, gradient=0.0009493972460448689\n",
      "Gradient Descent(38/99): loss=0.3901246239543064, gradient=0.0009333044951837309\n",
      "Gradient Descent(39/99): loss=0.3901245404457204, gradient=0.0009177747480262841\n",
      "Gradient Descent(40/99): loss=0.39012445963588727, gradient=0.0009027649695047602\n",
      "Gradient Descent(41/99): loss=0.3901243813968302, gradient=0.000888236885697384\n",
      "Gradient Descent(42/99): loss=0.3901243056104409, gradient=0.0008741563765446867\n",
      "Gradient Descent(43/99): loss=0.390124232167385, gradient=0.0008604929536547103\n",
      "Gradient Descent(44/99): loss=0.3901241609661522, gradient=0.000847219310180954\n",
      "Gradient Descent(45/99): loss=0.3901240919122354, gradient=0.0008343109319417885\n",
      "Gradient Descent(46/99): loss=0.3901240249174142, gradient=0.0008217457607312792\n",
      "Gradient Descent(47/99): loss=0.3901239598991306, gradient=0.0008095039022268471\n",
      "Gradient Descent(48/99): loss=0.3901238967799412, gradient=0.0007975673721001096\n",
      "Gradient Descent(49/99): loss=0.3901238354870382, gradient=0.0007859198749276253\n",
      "Gradient Descent(50/99): loss=0.39012377595182685, gradient=0.0007745466113246594\n",
      "Gradient Descent(51/99): loss=0.3901237181095535, gradient=0.0007634341094114745\n",
      "Gradient Descent(52/99): loss=0.3901236618989744, gradient=0.0007525700772987087\n",
      "Gradient Descent(53/99): loss=0.39012360726206563, gradient=0.0007419432737625197\n",
      "Gradient Descent(54/99): loss=0.39012355414376076, gradient=0.0007315433946879458\n",
      "Gradient Descent(55/99): loss=0.3901235024917216, gradient=0.0007213609732048436\n",
      "Gradient Descent(56/99): loss=0.39012345225612816, gradient=0.0007113872917325644\n",
      "Gradient Descent(57/99): loss=0.3901234033894961, gradient=0.0007016143043980738\n",
      "Gradient Descent(58/99): loss=0.39012335584650687, gradient=0.000692034568504407\n",
      "Gradient Descent(59/99): loss=0.39012330958386, gradient=0.0006826411839061738\n",
      "Gradient Descent(60/99): loss=0.3901232645601363, gradient=0.0006734277393034504\n",
      "Gradient Descent(61/99): loss=0.39012322073567507, gradient=0.0006643882645976227\n",
      "Gradient Descent(62/99): loss=0.3901231780724627, gradient=0.0006555171885658695\n",
      "Gradient Descent(63/99): loss=0.3901231365340323, gradient=0.0006468093012087744\n",
      "Gradient Descent(64/99): loss=0.3901230960853704, gradient=0.0006382597202088977\n",
      "Gradient Descent(65/99): loss=0.39012305669283265, gradient=0.0006298638610109234\n",
      "Gradient Descent(66/99): loss=0.39012301832406704, gradient=0.0006216174100960884\n",
      "Gradient Descent(67/99): loss=0.3901229809479424, gradient=0.0006135163010776438\n",
      "Gradient Descent(68/99): loss=0.3901229445344842, gradient=0.0006055566932906567\n",
      "Gradient Descent(69/99): loss=0.3901229090548128, gradient=0.0005977349525905843\n",
      "Gradient Descent(70/99): loss=0.39012287448108873, gradient=0.0005900476341091509\n",
      "Gradient Descent(71/99): loss=0.39012284078646187, gradient=0.0005824914667475914\n",
      "Gradient Descent(72/99): loss=0.3901228079450221, gradient=0.0005750633392133697\n",
      "Gradient Descent(73/99): loss=0.39012277593175626, gradient=0.0005677602874302172\n",
      "Gradient Descent(74/99): loss=0.39012274472250635, gradient=0.0005605794831707676\n",
      "Gradient Descent(75/99): loss=0.39012271429393064, gradient=0.0005535182237794719\n",
      "Gradient Descent(76/99): loss=0.390122684623468, gradient=0.0005465739228688415\n",
      "Gradient Descent(77/99): loss=0.3901226556893035, gradient=0.0005397441018852537\n",
      "Gradient Descent(78/99): loss=0.3901226274703379, gradient=0.0005330263824526776\n",
      "Gradient Descent(79/99): loss=0.39012259994615667, gradient=0.0005264184794133496\n",
      "Gradient Descent(80/99): loss=0.39012257309700227, gradient=0.0005199181944931094\n",
      "Gradient Descent(81/99): loss=0.3901225469037484, gradient=0.0005135234105275908\n",
      "Gradient Descent(82/99): loss=0.39012252134787395, gradient=0.0005072320861923809\n",
      "Gradient Descent(83/99): loss=0.3901224964114401, gradient=0.0005010422511863238\n",
      "Gradient Descent(84/99): loss=0.3901224720770684, gradient=0.0004949520018234667\n",
      "Gradient Descent(85/99): loss=0.3901224483279187, gradient=0.0004889594969929242\n",
      "Gradient Descent(86/99): loss=0.3901224251476695, gradient=0.0004830629544508252\n",
      "Gradient Descent(87/99): loss=0.3901224025204991, gradient=0.000477260647413029\n",
      "Gradient Descent(88/99): loss=0.3901223804310676, gradient=0.00047155090141876087\n",
      "Gradient Descent(89/99): loss=0.3901223588644989, gradient=0.0004659320914409845\n",
      "Gradient Descent(90/99): loss=0.39012233780636485, gradient=0.0004604026392191072\n",
      "Gradient Descent(91/99): loss=0.39012231724266977, gradient=0.0004549610107944902\n",
      "Gradient Descent(92/99): loss=0.3901222971598349, gradient=0.0004496057142297705\n",
      "Gradient Descent(93/99): loss=0.390122277544684, gradient=0.0004443352974956339\n",
      "Gradient Descent(94/99): loss=0.3901222583844308, gradient=0.00043914834650988615\n",
      "Gradient Descent(95/99): loss=0.39012223966666487, gradient=0.0004340434833153313\n",
      "Gradient Descent(96/99): loss=0.3901222213793394, gradient=0.0004290193643848714\n",
      "Gradient Descent(97/99): loss=0.39012220351075894, gradient=0.00042407467904206236\n",
      "Gradient Descent(98/99): loss=0.39012218604956816, gradient=0.0004192081479881778\n",
      "Gradient Descent(99/99): loss=0.3901221689847409, gradient=0.00041441852192596024\n",
      "Gradient Descent(0/99): loss=0.3903546979005106, gradient=0.007270051581567947\n",
      "Gradient Descent(1/99): loss=0.3903510180084853, gradient=0.006258883959586604\n",
      "Gradient Descent(2/99): loss=0.3903481276971581, gradient=0.0055270818744891795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(3/99): loss=0.39034579825696014, gradient=0.0049477782669912615\n",
      "Gradient Descent(4/99): loss=0.3903438778670248, gradient=0.004481079762583811\n",
      "Gradient Descent(5/99): loss=0.39034226284789064, gradient=0.004100282555918029\n",
      "Gradient Descent(6/99): loss=0.39034088093981545, gradient=0.0037855467590169668\n",
      "Gradient Descent(7/99): loss=0.3903396807633229, gradient=0.003522017851085718\n",
      "Gradient Descent(8/99): loss=0.3903386250993282, gradient=0.0032984023805010024\n",
      "Gradient Descent(9/99): loss=0.3903376863435338, gradient=0.0031066170903262633\n",
      "Gradient Descent(10/99): loss=0.3903368437654955, gradient=0.0029400896235800365\n",
      "Gradient Descent(11/99): loss=0.39033608147063764, gradient=0.0027940045122869604\n",
      "Gradient Descent(12/99): loss=0.3903353870938423, gradient=0.0026645722335098656\n",
      "Gradient Descent(13/99): loss=0.3903347508663802, gradient=0.002548861319450818\n",
      "Gradient Descent(14/99): loss=0.3903341649640684, gradient=0.002444564434877269\n",
      "Gradient Descent(15/99): loss=0.3903336230400888, gradient=0.0023498483377748596\n",
      "Gradient Descent(16/99): loss=0.39033311992685826, gradient=0.0022630492578843047\n",
      "Gradient Descent(17/99): loss=0.39033265125365313, gradient=0.0021833796844495895\n",
      "Gradient Descent(18/99): loss=0.3903322133968273, gradient=0.0021096656467654126\n",
      "Gradient Descent(19/99): loss=0.39033180328902495, gradient=0.0020411140774687538\n",
      "Gradient Descent(20/99): loss=0.39033141831028895, gradient=0.0019770703763013747\n",
      "Gradient Descent(21/99): loss=0.39033105620381187, gradient=0.0019169909225059602\n",
      "Gradient Descent(22/99): loss=0.3903307150101322, gradient=0.0018604216259051867\n",
      "Gradient Descent(23/99): loss=0.39033039301529127, gradient=0.0018069810479773604\n",
      "Gradient Descent(24/99): loss=0.3903300887156116, gradient=0.0017563096297638337\n",
      "Gradient Descent(25/99): loss=0.39032980076527235, gradient=0.0017082143270716718\n",
      "Gradient Descent(26/99): loss=0.3903295279753268, gradient=0.0016623913112606143\n",
      "Gradient Descent(27/99): loss=0.39032926927113754, gradient=0.0016186942479028213\n",
      "Gradient Descent(28/99): loss=0.3903290236883978, gradient=0.0015769223324096381\n",
      "Gradient Descent(29/99): loss=0.3903287903538846, gradient=0.0015369255627154864\n",
      "Gradient Descent(30/99): loss=0.39032856847347397, gradient=0.0014985730859857957\n",
      "Gradient Descent(31/99): loss=0.3903283573221594, gradient=0.0014617501354822985\n",
      "Gradient Descent(32/99): loss=0.39032815623568845, gradient=0.0014263555089680865\n",
      "Gradient Descent(33/99): loss=0.39032796460351055, gradient=0.0013922994861406603\n",
      "Gradient Descent(34/99): loss=0.3903277818627966, gradient=0.0013595021032261681\n",
      "Gradient Descent(35/99): loss=0.3903276074933371, gradient=0.0013278917190751593\n",
      "Gradient Descent(36/99): loss=0.39032744101316513, gradient=0.0012974038198948394\n",
      "Gradient Descent(37/99): loss=0.39032728197477895, gradient=0.0012679800199083464\n",
      "Gradient Descent(38/99): loss=0.39032712996186547, gradient=0.001239567223327747\n",
      "Gradient Descent(39/99): loss=0.39032698458644177, gradient=0.0012121169195054174\n",
      "Gradient Descent(40/99): loss=0.3903268454863493, gradient=0.001185584588332017\n",
      "Gradient Descent(41/99): loss=0.390326712323047, gradient=0.001159929197140139\n",
      "Gradient Descent(42/99): loss=0.39032658477965326, gradient=0.0011351127737599948\n",
      "Gradient Descent(43/99): loss=0.39032646255921105, gradient=0.0011111000431153772\n",
      "Gradient Descent(44/99): loss=0.3903263453831318, gradient=0.0010878581169770488\n",
      "Gradient Descent(45/99): loss=0.3903262329898036, gradient=0.0010653562283034037\n",
      "Gradient Descent(46/99): loss=0.3903261251333355, gradient=0.0010435655030783448\n",
      "Gradient Descent(47/99): loss=0.39032602158242463, gradient=0.0010224587637664376\n",
      "Gradient Descent(48/99): loss=0.3903259221193262, gradient=0.0010020103594966753\n",
      "Gradient Descent(49/99): loss=0.39032582653891934, gradient=0.0009821960189017127\n",
      "Gradient Descent(50/99): loss=0.39032573464785114, gradient=0.0009629927222094993\n",
      "Gradient Descent(51/99): loss=0.39032564626375627, gradient=0.0009443785897375076\n",
      "Gradient Descent(52/99): loss=0.3903255612145388, gradient=0.0009263327843975497\n",
      "Gradient Descent(53/99): loss=0.39032547933771256, gradient=0.0009088354261981199\n",
      "Gradient Descent(54/99): loss=0.39032540047979297, gradient=0.0008918675170457755\n",
      "Gradient Descent(55/99): loss=0.3903253244957345, gradient=0.0008754108744092635\n",
      "Gradient Descent(56/99): loss=0.39032525124841166, gradient=0.000859448072628845\n",
      "Gradient Descent(57/99): loss=0.3903251806081346, gradient=0.0008439623908354142\n",
      "Gradient Descent(58/99): loss=0.3903251124522029, gradient=0.00082893776659793\n",
      "Gradient Descent(59/99): loss=0.39032504666448836, gradient=0.0008143587545452817\n",
      "Gradient Descent(60/99): loss=0.39032498313504654, gradient=0.0008002104893181285\n",
      "Gradient Descent(61/99): loss=0.39032492175975486, gradient=0.0007864786522965707\n",
      "Gradient Descent(62/99): loss=0.39032486243997555, gradient=0.0007731494416273344\n",
      "Gradient Descent(63/99): loss=0.3903248050822383, gradient=0.0007602095451397742\n",
      "Gradient Descent(64/99): loss=0.3903247495979457, gradient=0.0007476461157952697\n",
      "Gradient Descent(65/99): loss=0.39032469590309626, gradient=0.000735446749362351\n",
      "Gradient Descent(66/99): loss=0.39032464391802507, gradient=0.0007235994640506088\n",
      "Gradient Descent(67/99): loss=0.39032459356715976, gradient=0.000712092681870507\n",
      "Gradient Descent(68/99): loss=0.39032454477879364, gradient=0.0007009152115169214\n",
      "Gradient Descent(69/99): loss=0.3903244974848695, gradient=0.0006900562325992379\n",
      "Gradient Descent(70/99): loss=0.3903244516207793, gradient=0.0006795052810632387\n",
      "Gradient Descent(71/99): loss=0.39032440712517397, gradient=0.0006692522356691038\n",
      "Gradient Descent(72/99): loss=0.39032436393978515, gradient=0.0006592873054067332\n",
      "Gradient Descent(73/99): loss=0.39032432200925743, gradient=0.000649601017743693\n",
      "Gradient Descent(74/99): loss=0.3903242812809901, gradient=0.0006401842076139743\n",
      "Gradient Descent(75/99): loss=0.3903242417049886, gradient=0.0006310280070666026\n",
      "Gradient Descent(76/99): loss=0.3903242032337238, gradient=0.0006221238355030435\n",
      "Gradient Descent(77/99): loss=0.39032416582199964, gradient=0.0006134633904404476\n",
      "Gradient Descent(78/99): loss=0.390324129430209, gradient=0.0006049811128782486\n",
      "Gradient Descent(79/99): loss=0.3903240940137799, gradient=0.0005967868250423737\n",
      "Gradient Descent(80/99): loss=0.3903240595338179, gradient=0.000588812855488509\n",
      "Gradient Descent(81/99): loss=0.3903240259532978, gradient=0.0005810519244291021\n",
      "Gradient Descent(82/99): loss=0.3903239932369706, gradient=0.0005734969979996157\n",
      "Gradient Descent(83/99): loss=0.390323961351268, gradient=0.0005661412739431328\n",
      "Gradient Descent(84/99): loss=0.390323930264216, gradient=0.0005589781743864225\n",
      "Gradient Descent(85/99): loss=0.39032389994535005, gradient=0.0005520013392265763\n",
      "Gradient Descent(86/99): loss=0.3903238703656371, gradient=0.0005452046197623487\n",
      "Gradient Descent(87/99): loss=0.3903238414974011, gradient=0.0005385820725298901\n",
      "Gradient Descent(88/99): loss=0.39032381331425237, gradient=0.0005321279533233867\n",
      "Gradient Descent(89/99): loss=0.3903237857910213, gradient=0.00052583671138587\n",
      "Gradient Descent(90/99): loss=0.3903237589036954, gradient=0.000519702983757118\n",
      "Gradient Descent(91/99): loss=0.39032373262936026, gradient=0.000513721589768772\n",
      "Gradient Descent(92/99): loss=0.3903237069461426, gradient=0.000507887525677476\n",
      "Gradient Descent(93/99): loss=0.39032368183315747, gradient=0.0005021959594286253\n",
      "Gradient Descent(94/99): loss=0.39032365727045815, gradient=0.0004966422255446935\n",
      "Gradient Descent(95/99): loss=0.390323633238988, gradient=0.0004912218201323843\n",
      "Gradient Descent(96/99): loss=0.39032360972053604, gradient=0.00048593039600441907\n",
      "Gradient Descent(97/99): loss=0.39032358669769385, gradient=0.0004807637579119904\n",
      "Gradient Descent(98/99): loss=0.3903235641538149, gradient=0.00047571785788494436\n",
      "Gradient Descent(99/99): loss=0.3903235420729777, gradient=0.00047078879067636784\n",
      "Gradient Descent(0/99): loss=0.3899756532969337, gradient=0.010007368847394277\n",
      "Gradient Descent(1/99): loss=0.3899694209449819, gradient=0.008243151465933971\n",
      "Gradient Descent(2/99): loss=0.3899650092044462, gradient=0.006907967127967761\n",
      "Gradient Descent(3/99): loss=0.3899617747064292, gradient=0.005890770929867972\n",
      "Gradient Descent(4/99): loss=0.3899593181354976, gradient=0.0051130415829800865\n",
      "Gradient Descent(5/99): loss=0.38995738856496737, gradient=0.004514325371968504\n",
      "Gradient Descent(6/99): loss=0.38995582582199445, gradient=0.004048899042364057\n",
      "Gradient Descent(7/99): loss=0.38995452604746994, gradient=0.0036818359447390774\n",
      "Gradient Descent(8/99): loss=0.38995342053788395, gradient=0.0033872872411856592\n",
      "Gradient Descent(9/99): loss=0.3899524627473445, gradient=0.003146593257574075\n",
      "Gradient Descent(10/99): loss=0.3899516204198888, gradient=0.0029460677489761187\n",
      "Gradient Descent(11/99): loss=0.38995087061067646, gradient=0.002775920592644948\n",
      "Gradient Descent(12/99): loss=0.3899501965694331, gradient=0.002629113802701778\n",
      "Gradient Descent(13/99): loss=0.3899495857538307, gradient=0.0025005628068237532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(14/99): loss=0.389949028544175, gradient=0.0023865594317013265\n",
      "Gradient Descent(15/99): loss=0.3899485173977618, gradient=0.002284363867681825\n",
      "Gradient Descent(16/99): loss=0.38994804628214585, gradient=0.002191920060802944\n",
      "Gradient Descent(17/99): loss=0.389947610287731, gradient=0.002107658607382861\n",
      "Gradient Descent(18/99): loss=0.3899472053574247, gradient=0.0020303604570926772\n",
      "Gradient Descent(19/99): loss=0.38994682809399983, gradient=0.0019590623609588185\n",
      "Gradient Descent(20/99): loss=0.38994647563925605, gradient=0.0018928823335722688\n",
      "Gradient Descent(21/99): loss=0.3899461455088655, gradient=0.00183141640668458\n",
      "Gradient Descent(22/99): loss=0.38994583557900725, gradient=0.0017740247407908483\n",
      "Gradient Descent(23/99): loss=0.3899455440026682, gradient=0.00172027229324681\n",
      "Gradient Descent(24/99): loss=0.38994526916258676, gradient=0.0016697915570371025\n",
      "Gradient Descent(25/99): loss=0.38994500963402856, gradient=0.0016222691648505165\n",
      "Gradient Descent(26/99): loss=0.3899447641659136, gradient=0.0015773637862541204\n",
      "Gradient Descent(27/99): loss=0.3899445316226819, gradient=0.0015349890588788639\n",
      "Gradient Descent(28/99): loss=0.38994431099982196, gradient=0.0014948663912607144\n",
      "Gradient Descent(29/99): loss=0.38994410139545216, gradient=0.0014568169986489752\n",
      "Gradient Descent(30/99): loss=0.3899439019967983, gradient=0.0014206830594991164\n",
      "Gradient Descent(31/99): loss=0.38994371206887785, gradient=0.0013863244943008897\n",
      "Gradient Descent(32/99): loss=0.38994353094496426, gradient=0.0013536163294763036\n",
      "Gradient Descent(33/99): loss=0.3899433580185018, gradient=0.0013224465240089037\n",
      "Gradient Descent(34/99): loss=0.3899431927362123, gradient=0.0012927141659015969\n",
      "Gradient Descent(35/99): loss=0.389943034592183, gradient=0.0012643279669352367\n",
      "Gradient Descent(36/99): loss=0.3899428831227722, gradient=0.001237204999961939\n",
      "Gradient Descent(37/99): loss=0.38994273790219713, gradient=0.001211269634765407\n",
      "Gradient Descent(38/99): loss=0.3899425985386932, gradient=0.0011864526374764299\n",
      "Gradient Descent(39/99): loss=0.389942464671159, gradient=0.0011626904054197436\n",
      "Gradient Descent(40/99): loss=0.38994233596621136, gradient=0.0011399243146291375\n",
      "Gradient Descent(41/99): loss=0.38994221211558855, gradient=0.001118100161483909\n",
      "Gradient Descent(42/99): loss=0.3899420928338568, gradient=0.001097167683265738\n",
      "Gradient Descent(43/99): loss=0.3899419778563714, gradient=0.0010770801451155882\n",
      "Gradient Descent(44/99): loss=0.3899418669374623, gradient=0.0010577939830285512\n",
      "Gradient Descent(45/99): loss=0.38994175984881146, gradient=0.0010392684942790783\n",
      "Gradient Descent(46/99): loss=0.38994165637800027, gradient=0.00102146556809677\n",
      "Gradient Descent(47/99): loss=0.38994155632720234, gradient=0.001004349450587961\n",
      "Gradient Descent(48/99): loss=0.38994145951200593, gradient=0.000987886538862622\n",
      "Gradient Descent(49/99): loss=0.38994136576035004, gradient=0.0009720452001264649\n",
      "Gradient Descent(50/99): loss=0.3899412749115606, gradient=0.0009567956121600193\n",
      "Gradient Descent(51/99): loss=0.38994118681547474, gradient=0.0009421096221606158\n",
      "Gradient Descent(52/99): loss=0.3899411013316465, gradient=0.0009279606213834117\n",
      "Gradient Descent(53/99): loss=0.38994101832862005, gradient=0.0009143234334057341\n",
      "Gradient Descent(54/99): loss=0.3899409376832674, gradient=0.0009011742141630613\n",
      "Gradient Descent(55/99): loss=0.38994085928018285, gradient=0.0008884903621798657\n",
      "Gradient Descent(56/99): loss=0.3899407830111279, gradient=0.0008762504376489443\n",
      "Gradient Descent(57/99): loss=0.389940708774521, gradient=0.0008644340892093969\n",
      "Gradient Descent(58/99): loss=0.38994063647497024, gradient=0.0008530219874383354\n",
      "Gradient Descent(59/99): loss=0.3899405660228417, gradient=0.000841995764213282\n",
      "Gradient Descent(60/99): loss=0.3899404973338637, gradient=0.0008313379572218606\n",
      "Gradient Descent(61/99): loss=0.38994043032875986, gradient=0.0008210319589971131\n",
      "Gradient Descent(62/99): loss=0.3899403649329116, gradient=0.0008110619699444077\n",
      "Gradient Descent(63/99): loss=0.38994030107604527, gradient=0.0008014129549005186\n",
      "Gradient Descent(64/99): loss=0.3899402386919438, gradient=0.0007920706028285558\n",
      "Gradient Descent(65/99): loss=0.38994017771817846, gradient=0.0007830212893072981\n",
      "Gradient Descent(66/99): loss=0.38994011809586165, gradient=0.000774252041519789\n",
      "Gradient Descent(67/99): loss=0.38994005976941565, gradient=0.0007657505054867702\n",
      "Gradient Descent(68/99): loss=0.38994000268635987, gradient=0.0007575049153233926\n",
      "Gradient Descent(69/99): loss=0.3899399467971125, gradient=0.0007495040643282523\n",
      "Gradient Descent(70/99): loss=0.38993989205480656, gradient=0.0007417372777383715\n",
      "Gradient Descent(71/99): loss=0.3899398384151166, gradient=0.0007341943870051462\n",
      "Gradient Descent(72/99): loss=0.38993978583610145, gradient=0.000726865705465085\n",
      "Gradient Descent(73/99): loss=0.38993973427805406, gradient=0.0007197420052945148\n",
      "Gradient Descent(74/99): loss=0.3899396837033635, gradient=0.0007128144956518312\n",
      "Gradient Descent(75/99): loss=0.3899396340763866, gradient=0.0007060748019211707\n",
      "Gradient Descent(76/99): loss=0.38993958536332607, gradient=0.0006995149459824214\n",
      "Gradient Descent(77/99): loss=0.3899395375321195, gradient=0.0006931273274408382\n",
      "Gradient Descent(78/99): loss=0.3899394905523344, gradient=0.0006869047057558729\n",
      "Gradient Descent(79/99): loss=0.38993944439506956, gradient=0.0006808401832164558\n",
      "Gradient Descent(80/99): loss=0.38993939903286423, gradient=0.0006749271887143081\n",
      "Gradient Descent(81/99): loss=0.38993935443961275, gradient=0.0006691594622713814\n",
      "Gradient Descent(82/99): loss=0.389939310590484, gradient=0.0006635310402829558\n",
      "Gradient Descent(83/99): loss=0.3899392674618473, gradient=0.0006580362414386104\n",
      "Gradient Descent(84/99): loss=0.38993922503120215, gradient=0.0006526696532890957\n",
      "Gradient Descent(85/99): loss=0.3899391832771127, gradient=0.0006474261194276858\n",
      "Gradient Descent(86/99): loss=0.38993914217914666, gradient=0.0006423007272574822\n",
      "Gradient Descent(87/99): loss=0.3899391017178175, gradient=0.0006372887963180472\n",
      "Gradient Descent(88/99): loss=0.38993906187453176, gradient=0.0006323858671463564\n",
      "Gradient Descent(89/99): loss=0.389939022631537, gradient=0.0006275876906483558\n",
      "Gradient Descent(90/99): loss=0.38993898397187576, gradient=0.0006228902179590441\n",
      "Gradient Descent(91/99): loss=0.38993894587934075, gradient=0.000618289590769672\n",
      "Gradient Descent(92/99): loss=0.3899389083384336, gradient=0.0006137821321023195\n",
      "Gradient Descent(93/99): loss=0.3899388713343251, gradient=0.0006093643375124071\n",
      "Gradient Descent(94/99): loss=0.38993883485281966, gradient=0.0006050328667008582\n",
      "Gradient Descent(95/99): loss=0.38993879888032007, gradient=0.0006007845355186642\n",
      "Gradient Descent(96/99): loss=0.3899387634037955, gradient=0.0005966163083466124\n",
      "Gradient Descent(97/99): loss=0.38993872841075106, gradient=0.0005925252908344725\n",
      "Gradient Descent(98/99): loss=0.3899386938891996, gradient=0.0005885087229837997\n",
      "Gradient Descent(99/99): loss=0.38993865982763426, gradient=0.0005845639725598579\n",
      "Gradient Descent(0/99): loss=0.38967521156259793, gradient=0.011088233554680556\n",
      "Gradient Descent(1/99): loss=0.3896702276191644, gradient=0.007445421355287883\n",
      "Gradient Descent(2/99): loss=0.389666692963901, gradient=0.006181414416967132\n",
      "Gradient Descent(3/99): loss=0.3896640967776971, gradient=0.0052814009312898\n",
      "Gradient Descent(4/99): loss=0.3896621509993292, gradient=0.0045617848619043996\n",
      "Gradient Descent(5/99): loss=0.3896606642997972, gradient=0.003978521338045236\n",
      "Gradient Descent(6/99): loss=0.3896595065493239, gradient=0.0035032037620661444\n",
      "Gradient Descent(7/99): loss=0.3896585882825832, gradient=0.003113360375074607\n",
      "Gradient Descent(8/99): loss=0.3896578472302313, gradient=0.0027913305424112646\n",
      "Gradient Descent(9/99): loss=0.38965723949211384, gradient=0.0025231998003984566\n",
      "Gradient Descent(10/99): loss=0.3896567336910588, gradient=0.002298046971971242\n",
      "Gradient Descent(11/99): loss=0.3896563070747846, gradient=0.002107327562995009\n",
      "Gradient Descent(12/99): loss=0.3896559428977626, gradient=0.0019443694696559327\n",
      "Gradient Descent(13/99): loss=0.3896556286485017, gradient=0.0018039639839813417\n",
      "Gradient Descent(14/99): loss=0.38965535483853936, gradient=0.0016820379539309255\n",
      "Gradient Descent(15/99): loss=0.38965511416704796, gradient=0.0015753943674932476\n",
      "Gradient Descent(16/99): loss=0.3896549009383983, gradient=0.0014815096373583224\n",
      "Gradient Descent(17/99): loss=0.3896547106514295, gradient=0.001398376946752082\n",
      "Gradient Descent(18/99): loss=0.38965453970626346, gradient=0.0013243862932563475\n",
      "Gradient Descent(19/99): loss=0.3896543851923597, gradient=0.0012582332659349257\n",
      "Gradient Descent(20/99): loss=0.3896542447332784, gradient=0.001198849990249784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(21/99): loss=0.38965411637147673, gradient=0.0011453529721453211\n",
      "Gradient Descent(22/99): loss=0.38965399848167903, gradient=0.0010970037057306797\n",
      "Gradient Descent(23/99): loss=0.3896538897049043, gradient=0.0010531788553562542\n",
      "Gradient Descent(24/99): loss=0.38965378889760105, gradient=0.0010133475868098164\n",
      "Gradient Descent(25/99): loss=0.38965369509197767, gradient=0.0009770542232381415\n",
      "Gradient Descent(26/99): loss=0.38965360746472766, gradient=0.0009439048647770158\n",
      "Gradient Descent(27/99): loss=0.38965352531213254, gradient=0.0009135569628446658\n",
      "Gradient Descent(28/99): loss=0.3896534480300577, gradient=0.0008857111042947068\n",
      "Gradient Descent(29/99): loss=0.38965337509775483, gradient=0.0008601044572073657\n",
      "Gradient Descent(30/99): loss=0.3896533060646438, gradient=0.0008365054753040866\n",
      "Gradient Descent(31/99): loss=0.38965324053946027, gradient=0.0008147095646249621\n",
      "Gradient Descent(32/99): loss=0.3896531781812914, gradient=0.0007945354940988472\n",
      "Gradient Descent(33/99): loss=0.3896531186921282, gradient=0.0007758223884783297\n",
      "Gradient Descent(34/99): loss=0.38965306181065396, gradient=0.0007584271834458979\n",
      "Gradient Descent(35/99): loss=0.3896530073070369, gradient=0.0007422224527230372\n",
      "Gradient Descent(36/99): loss=0.3896529549785476, gradient=0.0007270945388326625\n",
      "Gradient Descent(37/99): loss=0.3896529046458584, gradient=0.0007129419350532072\n",
      "Gradient Descent(38/99): loss=0.3896528561499059, gradient=0.0006996738777175386\n",
      "Gradient Descent(39/99): loss=0.3896528093492238, gradient=0.0006872091165605264\n",
      "Gradient Descent(40/99): loss=0.3896527641176682, gradient=0.0006754748371653088\n",
      "Gradient Descent(41/99): loss=0.3896527203424706, gradient=0.0006644057143317258\n",
      "Gradient Descent(42/99): loss=0.38965267792256875, gradient=0.0006539430788272298\n",
      "Gradient Descent(43/99): loss=0.38965263676717143, gradient=0.0006440341828021325\n",
      "Gradient Descent(44/99): loss=0.3896525967945188, gradient=0.0006346315513767515\n",
      "Gradient Descent(45/99): loss=0.3896525579308127, gradient=0.0006256924096983963\n",
      "Gradient Descent(46/99): loss=0.3896525201092883, gradient=0.0006171781762306165\n",
      "Gradient Descent(47/99): loss=0.3896524832694078, gradient=0.0006090540142554799\n",
      "Gradient Descent(48/99): loss=0.3896524473561595, gradient=0.0006012884345971039\n",
      "Gradient Descent(49/99): loss=0.38965241231944353, gradient=0.0005938529434518615\n",
      "Gradient Descent(50/99): loss=0.38965237811353715, gradient=0.0005867217299658536\n",
      "Gradient Descent(51/99): loss=0.3896523446966246, gradient=0.0005798713888557133\n",
      "Gradient Descent(52/99): loss=0.3896523120303842, gradient=0.0005732806739403738\n",
      "Gradient Descent(53/99): loss=0.38965228007962527, gradient=0.0005669302789508701\n",
      "Gradient Descent(54/99): loss=0.3896522488119678, gradient=0.0005608026424243214\n",
      "Gradient Descent(55/99): loss=0.3896522181975587, gradient=0.0005548817738729958\n",
      "Gradient Descent(56/99): loss=0.38965218820882197, gradient=0.0005491530987575871\n",
      "Gradient Descent(57/99): loss=0.38965215882023563, gradient=0.0005436033200916881\n",
      "Gradient Descent(58/99): loss=0.38965213000813403, gradient=0.0005382202947656354\n",
      "Gradient Descent(59/99): loss=0.38965210175053194, gradient=0.0005329929229074914\n",
      "Gradient Descent(60/99): loss=0.38965207402696783, gradient=0.0005279110488009279\n",
      "Gradient Descent(61/99): loss=0.3896520468183638, gradient=0.0005229653720565765\n",
      "Gradient Descent(62/99): loss=0.3896520201068993, gradient=0.0005181473678895315\n",
      "Gradient Descent(63/99): loss=0.3896519938759006, gradient=0.000513449215491194\n",
      "Gradient Descent(64/99): loss=0.38965196810973773, gradient=0.0005088637336043047\n",
      "Gradient Descent(65/99): loss=0.3896519427937361, gradient=0.0005043843225143199\n",
      "Gradient Descent(66/99): loss=0.38965191791409304, gradient=0.0005000049117626356\n",
      "Gradient Descent(67/99): loss=0.38965189345780615, gradient=0.0004957199129681883\n",
      "Gradient Descent(68/99): loss=0.3896518694126056, gradient=0.0004915241772145125\n",
      "Gradient Descent(69/99): loss=0.3896518457668957, gradient=0.0004874129565218968\n",
      "Gradient Descent(70/99): loss=0.3896518225096993, gradient=0.000483381868978965\n",
      "Gradient Descent(71/99): loss=0.38965179963060975, gradient=0.0004794268671558664\n",
      "Gradient Descent(72/99): loss=0.3896517771197466, gradient=0.00047554420946379803\n",
      "Gradient Descent(73/99): loss=0.3896517549677142, gradient=0.0004717304341627264\n",
      "Gradient Descent(74/99): loss=0.3896517331655666, gradient=0.0004679823357517333\n",
      "Gradient Descent(75/99): loss=0.389651711704773, gradient=0.0004642969435058686\n",
      "Gradient Descent(76/99): loss=0.38965169057718824, gradient=0.0004606715019481737\n",
      "Gradient Descent(77/99): loss=0.38965166977502447, gradient=0.0004571034530686582\n",
      "Gradient Descent(78/99): loss=0.3896516492908266, gradient=0.00045359042012169543\n",
      "Gradient Descent(79/99): loss=0.3896516291174489, gradient=0.0004501301928510737\n",
      "Gradient Descent(80/99): loss=0.38965160924803405, gradient=0.00044672071400727237\n",
      "Gradient Descent(81/99): loss=0.38965158967599367, gradient=0.00044336006703594495\n",
      "Gradient Descent(82/99): loss=0.38965157039499126, gradient=0.00044004646482844834\n",
      "Gradient Descent(83/99): loss=0.38965155139892527, gradient=0.0004367782394362868\n",
      "Gradient Descent(84/99): loss=0.38965153268191516, gradient=0.0004335538326616024\n",
      "Gradient Descent(85/99): loss=0.3896515142382871, gradient=0.0004303717874437284\n",
      "Gradient Descent(86/99): loss=0.3896514960625615, gradient=0.0004272307399706243\n",
      "Gradient Descent(87/99): loss=0.3896514781494416, gradient=0.0004241294124497306\n",
      "Gradient Descent(88/99): loss=0.3896514604938033, gradient=0.00042106660648009547\n",
      "Gradient Descent(89/99): loss=0.3896514430906848, gradient=0.0004180411969727761\n",
      "Gradient Descent(90/99): loss=0.3896514259352777, gradient=0.0004150521265714821\n",
      "Gradient Descent(91/99): loss=0.38965140902291895, gradient=0.0004120984005296536\n",
      "Gradient Descent(92/99): loss=0.38965139234908275, gradient=0.0004091790820048757\n",
      "Gradient Descent(93/99): loss=0.38965137590937354, gradient=0.0004062932877347464\n",
      "Gradient Descent(94/99): loss=0.38965135969951964, gradient=0.0004034401840614685\n",
      "Gradient Descent(95/99): loss=0.38965134371536725, gradient=0.00040061898327555797\n",
      "Gradient Descent(96/99): loss=0.38965132795287366, gradient=0.0003978289402518171\n",
      "Gradient Descent(97/99): loss=0.38965131240810347, gradient=0.0003950693493531974\n",
      "Gradient Descent(98/99): loss=0.38965129707722296, gradient=0.0003923395415795308\n",
      "Gradient Descent(99/99): loss=0.38965128195649534, gradient=0.00038963888194164904\n",
      "Gradient Descent(0/99): loss=0.38896145089323436, gradient=0.008180723907555053\n",
      "Gradient Descent(1/99): loss=0.3889586890510996, gradient=0.00555104768308527\n",
      "Gradient Descent(2/99): loss=0.3889567706770744, gradient=0.004564746599215383\n",
      "Gradient Descent(3/99): loss=0.3889553891529435, gradient=0.003860452782499827\n",
      "Gradient Descent(4/99): loss=0.38895436925018395, gradient=0.0033071578288285694\n",
      "Gradient Descent(5/99): loss=0.38895359628257514, gradient=0.0028699921654904118\n",
      "Gradient Descent(6/99): loss=0.3889529938934201, gradient=0.002525194530821346\n",
      "Gradient Descent(7/99): loss=0.388952510860495, gradient=0.0022536711694312165\n",
      "Gradient Descent(8/99): loss=0.3889521125701987, gradient=0.002039859787913983\n",
      "Gradient Descent(9/99): loss=0.3889517754422672, gradient=0.0018711222589184433\n",
      "Gradient Descent(10/99): loss=0.3889514832674747, gradient=0.0017372933585236728\n",
      "Gradient Descent(11/99): loss=0.38895122479089084, gradient=0.0016303049426861795\n",
      "Gradient Descent(12/99): loss=0.3889509921096549, gradient=0.0015438476445942045\n",
      "Gradient Descent(13/99): loss=0.3889507796054387, gradient=0.001473057626924383\n",
      "Gradient Descent(14/99): loss=0.388950583229371, gradient=0.0014142310477920496\n",
      "Gradient Descent(15/99): loss=0.3889504000202929, gradient=0.0013645734165858515\n",
      "Gradient Descent(16/99): loss=0.3889502277781888, gradient=0.0013219887585688278\n",
      "Gradient Descent(17/99): loss=0.3889500648413, gradient=0.0012849090475835364\n",
      "Gradient Descent(18/99): loss=0.38894990993288453, gradient=0.0012521605955525337\n",
      "Gradient Descent(19/99): loss=0.38894976205500725, gradient=0.001222861983918651\n",
      "Gradient Descent(20/99): loss=0.38894962041428144, gradient=0.001196347525120347\n",
      "Gradient Descent(21/99): loss=0.38894948436945737, gradient=0.0011721106112638797\n",
      "Gradient Descent(22/99): loss=0.38894935339405023, gradient=0.001149762135993406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(23/99): loss=0.38894922704940327, gradient=0.0011290001233271096\n",
      "Gradient Descent(24/99): loss=0.38894910496504476, gradient=0.0011095875804894714\n",
      "Gradient Descent(25/99): loss=0.38894898682419465, gradient=0.0010913363350602513\n",
      "Gradient Descent(26/99): loss=0.3889488723529351, gradient=0.0010740952057153555\n",
      "Gradient Descent(27/99): loss=0.38894876131201755, gradient=0.0010577413049285053\n",
      "Gradient Descent(28/99): loss=0.3889486534905859, gradient=0.001042173605858943\n",
      "Gradient Descent(29/99): loss=0.38894854870130746, gradient=0.0010273081496460917\n",
      "Gradient Descent(30/99): loss=0.3889484467765482, gradient=0.001013074445642084\n",
      "Gradient Descent(31/99): loss=0.3889483475653357, gradient=0.0009994127435874738\n",
      "Gradient Descent(32/99): loss=0.3889482509309177, gradient=0.0009862719470750863\n",
      "Gradient Descent(33/99): loss=0.38894815674877964, gradient=0.0009736080020416812\n",
      "Gradient Descent(34/99): loss=0.3889480649050195, gradient=0.0009613826399252099\n",
      "Gradient Descent(35/99): loss=0.388947975295003, gradient=0.000949562387878944\n",
      "Gradient Descent(36/99): loss=0.388947887822239, gradient=0.000938117781870996\n",
      "Gradient Descent(37/99): loss=0.38894780239743515, gradient=0.0009270227353286375\n",
      "Gradient Descent(38/99): loss=0.3889477189376949, gradient=0.0009162540281327845\n",
      "Gradient Descent(39/99): loss=0.3889476373658353, gradient=0.00090579088957985\n",
      "Gradient Descent(40/99): loss=0.3889475576189087, gradient=0.0008955081062678764\n",
      "Gradient Descent(41/99): loss=0.3889474796191504, gradient=0.0008856102433483208\n",
      "Gradient Descent(42/99): loss=0.38894740330342054, gradient=0.0008759665289576866\n",
      "Gradient Descent(43/99): loss=0.38894732861235487, gradient=0.0008665631495302999\n",
      "Gradient Descent(44/99): loss=0.3889472554900281, gradient=0.0008573875023836996\n",
      "Gradient Descent(45/99): loss=0.3889471838836563, gradient=0.0008484280541578393\n",
      "Gradient Descent(46/99): loss=0.3889471137433337, gradient=0.0008396742197406348\n",
      "Gradient Descent(47/99): loss=0.38894704502179905, gradient=0.0008311162582038175\n",
      "Gradient Descent(48/99): loss=0.3889469776742266, gradient=0.0008227451829387846\n",
      "Gradient Descent(49/99): loss=0.3889469116580395, gradient=0.0008145526837045192\n",
      "Gradient Descent(50/99): loss=0.3889468469327412, gradient=0.0008065310587156862\n",
      "Gradient Descent(51/99): loss=0.388946783459765, gradient=0.000798673155231786\n",
      "Gradient Descent(52/99): loss=0.38894672120233675, gradient=0.0007909723173747433\n",
      "Gradient Descent(53/99): loss=0.3889466601253509, gradient=0.0007834223401195611\n",
      "Gradient Descent(54/99): loss=0.38894660019525856, gradient=0.0007760174285779074\n",
      "Gradient Descent(55/99): loss=0.38894654137996426, gradient=0.0007687521618387213\n",
      "Gradient Descent(56/99): loss=0.38894648364873374, gradient=0.0007616214607478045\n",
      "Gradient Descent(57/99): loss=0.38894642697210685, gradient=0.0007546205591050895\n",
      "Gradient Descent(58/99): loss=0.3889463713218209, gradient=0.0007477449778391852\n",
      "Gradient Descent(59/99): loss=0.3889463166707367, gradient=0.0007409905017850079\n",
      "Gradient Descent(60/99): loss=0.3889462629927752, gradient=0.0007343531587456693\n",
      "Gradient Descent(61/99): loss=0.3889462102628541, gradient=0.0007278292005662309\n",
      "Gradient Descent(62/99): loss=0.38894615845683295, gradient=0.0007214150859858837\n",
      "Gradient Descent(63/99): loss=0.3889461075514611, gradient=0.0007151074650665415\n",
      "Gradient Descent(64/99): loss=0.3889460575243292, gradient=0.0007089031650246661\n",
      "Gradient Descent(65/99): loss=0.38894600835382515, gradient=0.0007027991773150759\n",
      "Gradient Descent(66/99): loss=0.38894596001909243, gradient=0.000696792645835308\n",
      "Gradient Descent(67/99): loss=0.3889459124999919, gradient=0.0006908808561364883\n",
      "Gradient Descent(68/99): loss=0.388945865777066, gradient=0.0006850612255391074\n",
      "Gradient Descent(69/99): loss=0.3889458198315051, gradient=0.0006793312940664127\n",
      "Gradient Descent(70/99): loss=0.38894577464511687, gradient=0.0006736887161164053\n",
      "Gradient Descent(71/99): loss=0.3889457302002976, gradient=0.0006681312528036556\n",
      "Gradient Descent(72/99): loss=0.3889456864800034, gradient=0.0006626567649097078\n",
      "Gradient Descent(73/99): loss=0.3889456434677272, gradient=0.0006572632063866228\n",
      "Gradient Descent(74/99): loss=0.3889456011474729, gradient=0.0006519486183653705\n",
      "Gradient Descent(75/99): loss=0.38894555950373405, gradient=0.0006467111236245823\n",
      "Gradient Descent(76/99): loss=0.38894551852147247, gradient=0.0006415489214806175\n",
      "Gradient Descent(77/99): loss=0.38894547818609904, gradient=0.0006364602830629357\n",
      "Gradient Descent(78/99): loss=0.38894543848345436, gradient=0.0006314435469427789\n",
      "Gradient Descent(79/99): loss=0.38894539939979156, gradient=0.000626497115086303\n",
      "Gradient Descent(80/99): loss=0.3889453609217603, gradient=0.0006216194491049719\n",
      "Gradient Descent(81/99): loss=0.3889453230363901, gradient=0.0006168090667800677\n",
      "Gradient Descent(82/99): loss=0.38894528573107656, gradient=0.0006120645388387648\n",
      "Gradient Descent(83/99): loss=0.3889452489935672, gradient=0.0006073844859620266\n",
      "Gradient Descent(84/99): loss=0.38894521281194777, gradient=0.0006027675760062728\n",
      "Gradient Descent(85/99): loss=0.3889451771746305, gradient=0.0005982125214215706\n",
      "Gradient Descent(86/99): loss=0.38894514207034125, gradient=0.0005937180768517206\n",
      "Gradient Descent(87/99): loss=0.38894510748810934, gradient=0.0005892830369019107\n",
      "Gradient Descent(88/99): loss=0.38894507341725554, gradient=0.0005849062340607047\n",
      "Gradient Descent(89/99): loss=0.3889450398473832, gradient=0.0005805865367655525\n",
      "Gradient Descent(90/99): loss=0.38894500676836835, gradient=0.0005763228475995697\n",
      "Gradient Descent(91/99): loss=0.38894497417034857, gradient=0.0005721141016110314\n",
      "Gradient Descent(92/99): loss=0.3889449420437167, gradient=0.0005679592647453176\n",
      "Gradient Descent(93/99): loss=0.3889449103791113, gradient=0.0005638573323812679\n",
      "Gradient Descent(94/99): loss=0.388944879167408, gradient=0.0005598073279642884\n",
      "Gradient Descent(95/99): loss=0.388944848399713, gradient=0.0005558083017283817\n",
      "Gradient Descent(96/99): loss=0.3889448180673549, gradient=0.0005518593295014895\n",
      "Gradient Descent(97/99): loss=0.3889447881618779, gradient=0.0005479595115869107\n",
      "Gradient Descent(98/99): loss=0.38894475867503475, gradient=0.0005441079717160674\n",
      "Gradient Descent(99/99): loss=0.38894472959878107, gradient=0.0005403038560665211\n",
      "Gradient Descent(0/99): loss=0.3901437091633169, gradient=0.015401843761509645\n",
      "Gradient Descent(1/99): loss=0.39013978539932453, gradient=0.006919898630951174\n",
      "Gradient Descent(2/99): loss=0.3901371324671371, gradient=0.005360598988253468\n",
      "Gradient Descent(3/99): loss=0.390135136638207, gradient=0.004615818592878631\n",
      "Gradient Descent(4/99): loss=0.3901335924545878, gradient=0.004048590110228713\n",
      "Gradient Descent(5/99): loss=0.390132371645839, gradient=0.0035909894388984104\n",
      "Gradient Descent(6/99): loss=0.3901313867276373, gradient=0.003217987641585954\n",
      "Gradient Descent(7/99): loss=0.3901305768426949, gradient=0.002911744290632067\n",
      "Gradient Descent(8/99): loss=0.39012989903047657, gradient=0.002658429077808042\n",
      "Gradient Descent(9/99): loss=0.39012932252576327, gradient=0.0024472407473404663\n",
      "Gradient Descent(10/99): loss=0.3901288249776416, gradient=0.0022697313241468297\n",
      "Gradient Descent(11/99): loss=0.3901283899146394, gradient=0.002119283715168352\n",
      "Gradient Descent(12/99): loss=0.3901280050261482, gradient=0.0019907041703787496\n",
      "Gradient Descent(13/99): loss=0.39012766098406526, gradient=0.0018799046229634195\n",
      "Gradient Descent(14/99): loss=0.3901273506255347, gradient=0.001783655683716966\n",
      "Gradient Descent(15/99): loss=0.39012706837956346, gradient=0.0016993951094355065\n",
      "Gradient Descent(16/99): loss=0.39012680986006404, gradient=0.0016250796718368784\n",
      "Gradient Descent(17/99): loss=0.3901265715736673, gradient=0.0015590708222941677\n",
      "Gradient Descent(18/99): loss=0.39012635070752427, gradient=0.0015000465438874393\n",
      "Gradient Descent(19/99): loss=0.39012614497343934, gradient=0.001446933403704096\n",
      "Gradient Descent(20/99): loss=0.39012595249210413, gradient=0.001398854129511489\n",
      "Gradient Descent(21/99): loss=0.3901257717061656, gradient=0.0013550870855145122\n",
      "Gradient Descent(22/99): loss=0.39012560131425444, gradient=0.0013150348545310252\n",
      "Gradient Descent(23/99): loss=0.3901254402204021, gradient=0.0012781997866775266\n",
      "Gradient Descent(24/99): loss=0.3901252874948689, gradient=0.0012441648814450668\n",
      "Gradient Descent(25/99): loss=0.3901251423435265, gradient=0.0012125787602476317\n",
      "Gradient Descent(26/99): loss=0.39012500408370404, gradient=0.0011831437849521447\n",
      "Gradient Descent(27/99): loss=0.390124872124978, gradient=0.001155606604941285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(28/99): loss=0.39012474595376667, gradient=0.00112975058733594\n",
      "Gradient Descent(29/99): loss=0.3901246251208853, gradient=0.001105389715085753\n",
      "Gradient Descent(30/99): loss=0.3901245092314214, gradient=0.001082363635840105\n",
      "Gradient Descent(31/99): loss=0.3901243979364448, gradient=0.0010605336186232832\n",
      "Gradient Descent(32/99): loss=0.3901242909261789, gradient=0.0010397792313141174\n",
      "Gradient Descent(33/99): loss=0.3901241879243428, gradient=0.001019995594274601\n",
      "Gradient Descent(34/99): loss=0.3901240886834397, gradient=0.0010010910975927485\n",
      "Gradient Descent(35/99): loss=0.3901239929808145, gradient=0.0009829854938512086\n",
      "Gradient Descent(36/99): loss=0.39012390061533947, gradient=0.0009656082970196017\n",
      "Gradient Descent(37/99): loss=0.39012381140461505, gradient=0.0009488974324225259\n",
      "Gradient Descent(38/99): loss=0.390123725182598, gradient=0.0009327980938244204\n",
      "Gradient Descent(39/99): loss=0.3901236417975833, gradient=0.0009172617722895132\n",
      "Gradient Descent(40/99): loss=0.39012356111047963, gradient=0.000902245428217265\n",
      "Gradient Descent(41/99): loss=0.3901234829933327, gradient=0.0008877107832626867\n",
      "Gradient Descent(42/99): loss=0.3901234073280556, gradient=0.0008736237130621101\n",
      "Gradient Descent(43/99): loss=0.39012333400533233, gradient=0.0008599537250452994\n",
      "Gradient Descent(44/99): loss=0.3901232629236711, gradient=0.0008466735083182359\n",
      "Gradient Descent(45/99): loss=0.39012319398858103, gradient=0.0008337585447858963\n",
      "Gradient Descent(46/99): loss=0.39012312711185765, gradient=0.0008211867724652067\n",
      "Gradient Descent(47/99): loss=0.39012306221095694, gradient=0.0008089382933936347\n",
      "Gradient Descent(48/99): loss=0.39012299920844984, gradient=0.0007969951197396408\n",
      "Gradient Descent(49/99): loss=0.3901229380315417, gradient=0.0007853409527132031\n",
      "Gradient Descent(50/99): loss=0.3901228786116503, gradient=0.0007739609896976307\n",
      "Gradient Descent(51/99): loss=0.39012282088403394, gradient=0.0007628417557143668\n",
      "Gradient Descent(52/99): loss=0.3901227647874609, gradient=0.0007519709559058563\n",
      "Gradient Descent(53/99): loss=0.39012271026391815, gradient=0.0007413373462078884\n",
      "Gradient Descent(54/99): loss=0.3901226572583506, gradient=0.0007309306197904783\n",
      "Gradient Descent(55/99): loss=0.39012260571843, gradient=0.0007207413071904008\n",
      "Gradient Descent(56/99): loss=0.3901225555943474, gradient=0.0007107606883526915\n",
      "Gradient Descent(57/99): loss=0.3901225068386278, gradient=0.0007009807150456641\n",
      "Gradient Descent(58/99): loss=0.3901224594059634, gradient=0.0006913939423258018\n",
      "Gradient Descent(59/99): loss=0.3901224132530628, gradient=0.0006819934679100049\n",
      "Gradient Descent(60/99): loss=0.3901223683385163, gradient=0.0006727728784661973\n",
      "Gradient Descent(61/99): loss=0.39012232462267254, gradient=0.0006637262019656516\n",
      "Gradient Descent(62/99): loss=0.3901222820675274, gradient=0.0006548478653545714\n",
      "Gradient Descent(63/99): loss=0.3901222406366227, gradient=0.0006461326568980872\n",
      "Gradient Descent(64/99): loss=0.3901222002949538, gradient=0.0006375756926361671\n",
      "Gradient Descent(65/99): loss=0.390122161008885, gradient=0.0006291723864604554\n",
      "Gradient Descent(66/99): loss=0.390122122746073, gradient=0.0006209184233860001\n",
      "Gradient Descent(67/99): loss=0.39012208547539545, gradient=0.0006128097356437874\n",
      "Gradient Descent(68/99): loss=0.39012204916688525, gradient=0.0006048424812682164\n",
      "Gradient Descent(69/99): loss=0.3901220137916719, gradient=0.000597013024892679\n",
      "Gradient Descent(70/99): loss=0.3901219793219235, gradient=0.0005893179205034388\n",
      "Gradient Descent(71/99): loss=0.39012194573079795, gradient=0.0005817538959303471\n",
      "Gradient Descent(72/99): loss=0.3901219129923933, gradient=0.000574317838881575\n",
      "Gradient Descent(73/99): loss=0.39012188108170387, gradient=0.0005670067843516078\n",
      "Gradient Descent(74/99): loss=0.39012184997457944, gradient=0.0005598179032517911\n",
      "Gradient Descent(75/99): loss=0.3901218196476854, gradient=0.0005527484921317953\n",
      "Gradient Descent(76/99): loss=0.39012179007846864, gradient=0.0005457959638739618\n",
      "Gradient Descent(77/99): loss=0.3901217612451214, gradient=0.0005389578392575468\n",
      "Gradient Descent(78/99): loss=0.39012173312655135, gradient=0.000532231739301243\n",
      "Gradient Descent(79/99): loss=0.39012170570235133, gradient=0.0005256153783022937\n",
      "Gradient Descent(80/99): loss=0.39012167895277067, gradient=0.0005191065575008165\n",
      "Gradient Descent(81/99): loss=0.3901216528586896, gradient=0.0005127031593046261\n",
      "Gradient Descent(82/99): loss=0.3901216274015945, gradient=0.0005064031420186757\n",
      "Gradient Descent(83/99): loss=0.3901216025635527, gradient=0.0005002045350271973\n",
      "Gradient Descent(84/99): loss=0.3901215783271923, gradient=0.0004941054343849519\n",
      "Gradient Descent(85/99): loss=0.3901215546756799, gradient=0.0004881039987760815\n",
      "Gradient Descent(86/99): loss=0.3901215315927001, gradient=0.0004821984458058772\n",
      "Gradient Descent(87/99): loss=0.3901215090624377, gradient=0.0004763870485922059\n",
      "Gradient Descent(88/99): loss=0.39012148706955857, gradient=0.0004706681326292888\n",
      "Gradient Descent(89/99): loss=0.3901214655991928, gradient=0.0004650400728970303\n",
      "Gradient Descent(90/99): loss=0.39012144463691845, gradient=0.0004595012911936285\n",
      "Gradient Descent(91/99): loss=0.3901214241687453, gradient=0.00045405025367062916\n",
      "Gradient Descent(92/99): loss=0.3901214041811, gradient=0.0004486854685519738\n",
      "Gradient Descent(93/99): loss=0.3901213846608128, gradient=0.00044340548402038347\n",
      "Gradient Descent(94/99): loss=0.39012136559510263, gradient=0.0004382088862562209\n",
      "Gradient Descent(95/99): loss=0.39012134697156453, gradient=0.0004330942976153949\n",
      "Gradient Descent(96/99): loss=0.39012132877815653, gradient=0.0004280603749339305\n",
      "Gradient Descent(97/99): loss=0.39012131100318903, gradient=0.000423105807948811\n",
      "Gradient Descent(98/99): loss=0.3901212936353118, gradient=0.00041822931782446103\n",
      "Gradient Descent(99/99): loss=0.3901212766635033, gradient=0.0004134296557768888\n",
      "Gradient Descent(0/99): loss=0.39035357028848067, gradient=0.007270522273562954\n",
      "Gradient Descent(1/99): loss=0.3903498896526128, gradient=0.006259621831992108\n",
      "Gradient Descent(2/99): loss=0.39034699864081024, gradient=0.005527863726946914\n",
      "Gradient Descent(3/99): loss=0.39034466854553984, gradient=0.004948576004079702\n",
      "Gradient Descent(4/99): loss=0.39034274754829124, gradient=0.00448188061552698\n",
      "Gradient Descent(5/99): loss=0.3903411319668247, gradient=0.004101077541477194\n",
      "Gradient Descent(6/99): loss=0.3903397495381426, gradient=0.003786329411921523\n",
      "Gradient Descent(7/99): loss=0.39033854887912106, gradient=0.0035227836166049115\n",
      "Gradient Descent(8/99): loss=0.390337492808544, gradient=0.0032990075635818653\n",
      "Gradient Descent(9/99): loss=0.3903365536646579, gradient=0.0031072219150536234\n",
      "Gradient Descent(10/99): loss=0.39033571072490736, gradient=0.0029406628610915055\n",
      "Gradient Descent(11/99): loss=0.3903349480861899, gradient=0.002794570019190511\n",
      "Gradient Descent(12/99): loss=0.39033425338311717, gradient=0.002665127185393443\n",
      "Gradient Descent(13/99): loss=0.3903336168465481, gradient=0.002549403386272059\n",
      "Gradient Descent(14/99): loss=0.39033303065180247, gradient=0.0024450916899853253\n",
      "Gradient Descent(15/99): loss=0.39033248845151675, gradient=0.0023503591854506854\n",
      "Gradient Descent(16/99): loss=0.3903319851176504, gradient=0.002263356275363884\n",
      "Gradient Descent(17/99): loss=0.3903315162332179, gradient=0.00218367901365219\n",
      "Gradient Descent(18/99): loss=0.3903310781747576, gradient=0.002109955497735716\n",
      "Gradient Descent(19/99): loss=0.3903306678749972, gradient=0.002041392881020098\n",
      "Gradient Descent(20/99): loss=0.3903302827139817, gradient=0.0019773367563599544\n",
      "Gradient Descent(21/99): loss=0.39032992043484044, gradient=0.0019172436732686328\n",
      "Gradient Descent(22/99): loss=0.39032957907799837, gradient=0.0018606596926793618\n",
      "Gradient Descent(23/99): loss=0.390329256929342, gradient=0.0018072035105994426\n",
      "Gradient Descent(24/99): loss=0.3903289524906933, gradient=0.0017564800006826435\n",
      "Gradient Descent(25/99): loss=0.39032866440880243, gradient=0.0017083730978392732\n",
      "Gradient Descent(26/99): loss=0.390328391490662, gradient=0.0016625632088128103\n",
      "Gradient Descent(27/99): loss=0.39032813267411953, gradient=0.0016188000552634248\n",
      "Gradient Descent(28/99): loss=0.39032788698599946, gradient=0.001577014353747279\n",
      "Gradient Descent(29/99): loss=0.39032765355303906, gradient=0.0015370028138399759\n",
      "Gradient Descent(30/99): loss=0.3903274315810314, gradient=0.0014986347294232243\n",
      "Gradient Descent(31/99): loss=0.3903272203448536, gradient=0.0014617954602193094\n",
      "Gradient Descent(32/99): loss=0.3903270191801105, gradient=0.0014263839130434356\n",
      "Gradient Descent(33/99): loss=0.39032682747608966, gradient=0.0013923104616866103\n",
      "Gradient Descent(34/99): loss=0.3903266446697861, gradient=0.0013594952236138476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(35/99): loss=0.39032647024080447, gradient=0.0013278666278655306\n",
      "Gradient Descent(36/99): loss=0.39032630370698523, gradient=0.0012973602213387989\n",
      "Gradient Descent(37/99): loss=0.3903261446206312, gradient=0.001267917670776213\n",
      "Gradient Descent(38/99): loss=0.39032599256523115, gradient=0.0012394859258787433\n",
      "Gradient Descent(39/99): loss=0.3903258471526054, gradient=0.0012120165154356837\n",
      "Gradient Descent(40/99): loss=0.3903257080204006, gradient=0.0011854649535622474\n",
      "Gradient Descent(41/99): loss=0.3903255748298816, gradient=0.001159790237324488\n",
      "Gradient Descent(42/99): loss=0.3903254472639795, gradient=0.0011349544204140147\n",
      "Gradient Descent(43/99): loss=0.3903253250255519, gradient=0.0011109222502761929\n",
      "Gradient Descent(44/99): loss=0.39032520783583136, gradient=0.0010876608583210374\n",
      "Gradient Descent(45/99): loss=0.390325095433031, gradient=0.0010651394946573948\n",
      "Gradient Descent(46/99): loss=0.39032498757109174, gradient=0.0010433293002691476\n",
      "Gradient Descent(47/99): loss=0.39032488401854704, gradient=0.0010222031107618528\n",
      "Gradient Descent(48/99): loss=0.3903247845574954, gradient=0.0010017352867973744\n",
      "Gradient Descent(49/99): loss=0.3903246889826638, gradient=0.0009819015671490518\n",
      "Gradient Descent(50/99): loss=0.39032459710055395, gradient=0.0009626789409793252\n",
      "Gradient Descent(51/99): loss=0.3903245087286602, gradient=0.0009440455364940445\n",
      "Gradient Descent(52/99): loss=0.39032442369475195, gradient=0.0009259805235852452\n",
      "Gradient Descent(53/99): loss=0.3903243418362145, gradient=0.0009084640284519977\n",
      "Gradient Descent(54/99): loss=0.39032426299943923, gradient=0.0008914770585037356\n",
      "Gradient Descent(55/99): loss=0.39032418703926286, gradient=0.0008750014361120561\n",
      "Gradient Descent(56/99): loss=0.3903241138184455, gradient=0.0008590197399952044\n",
      "Gradient Descent(57/99): loss=0.39032404320718983, gradient=0.000843515253201595\n",
      "Gradient Descent(58/99): loss=0.3903239750826914, gradient=0.0008284719168123498\n",
      "Gradient Descent(59/99): loss=0.39032390932872285, gradient=0.0008138742886103068\n",
      "Gradient Descent(60/99): loss=0.390323845835245, gradient=0.0007997075060719443\n",
      "Gradient Descent(61/99): loss=0.3903237844980454, gradient=0.0007859572531291026\n",
      "Gradient Descent(62/99): loss=0.3903237252183989, gradient=0.0007726097302252734\n",
      "Gradient Descent(63/99): loss=0.39032366790275325, gradient=0.0007596516272559878\n",
      "Gradient Descent(64/99): loss=0.39032361246243197, gradient=0.0007470700990387741\n",
      "Gradient Descent(65/99): loss=0.3903235588133586, gradient=0.000734852743005915\n",
      "Gradient Descent(66/99): loss=0.3903235068757961, gradient=0.0007229875788526754\n",
      "Gradient Descent(67/99): loss=0.3903234565741041, gradient=0.0007114630299095636\n",
      "Gradient Descent(68/99): loss=0.39032340783650993, gradient=0.0007002679060359536\n",
      "Gradient Descent(69/99): loss=0.39032336059489453, gradient=0.0006893913878587994\n",
      "Gradient Descent(70/99): loss=0.3903233147845898, gradient=0.0006788230122016087\n",
      "Gradient Descent(71/99): loss=0.3903232703441901, gradient=0.0006685526585687386\n",
      "Gradient Descent(72/99): loss=0.3903232272153727, gradient=0.0006585705365654662\n",
      "Gradient Descent(73/99): loss=0.39032318534273025, gradient=0.0006488671741505811\n",
      "Gradient Descent(74/99): loss=0.39032314467361323, gradient=0.0006394334066289787\n",
      "Gradient Descent(75/99): loss=0.3903231051579792, gradient=0.0006302603663034429\n",
      "Gradient Descent(76/99): loss=0.39032306674825434, gradient=0.0006213394727153951\n",
      "Gradient Descent(77/99): loss=0.3903230293991998, gradient=0.0006126624234108902\n",
      "Gradient Descent(78/99): loss=0.39032299306778695, gradient=0.0006042211851775413\n",
      "Gradient Descent(79/99): loss=0.39032295771308045, gradient=0.0005960079857029998\n",
      "Gradient Descent(80/99): loss=0.3903229232961256, gradient=0.000588015305613013\n",
      "Gradient Descent(81/99): loss=0.39032288977984514, gradient=0.0005802358708505425\n",
      "Gradient Descent(82/99): loss=0.39032285712893894, gradient=0.0005726626453631402\n",
      "Gradient Descent(83/99): loss=0.39032282530979084, gradient=0.0005652888240695598\n",
      "Gradient Descent(84/99): loss=0.39032279429038064, gradient=0.0005581078260797214\n",
      "Gradient Descent(85/99): loss=0.39032276404020033, gradient=0.0005511132881458159\n",
      "Gradient Descent(86/99): loss=0.3903227345301753, gradient=0.0005442990583248059\n",
      "Gradient Descent(87/99): loss=0.3903227057325897, gradient=0.0005376591898353521\n",
      "Gradient Descent(88/99): loss=0.3903226776210162, gradient=0.0005311879350940786\n",
      "Gradient Descent(89/99): loss=0.39032265017024953, gradient=0.0005248797399182649\n",
      "Gradient Descent(90/99): loss=0.3903226233562429, gradient=0.0005187292378840588\n",
      "Gradient Descent(91/99): loss=0.39032259715604956, gradient=0.0005127312448297467\n",
      "Gradient Descent(92/99): loss=0.3903225715477651, gradient=0.000506880753496733\n",
      "Gradient Descent(93/99): loss=0.3903225465104755, gradient=0.000501172928300218\n",
      "Gradient Descent(94/99): loss=0.3903225220242056, gradient=0.0004956031002237859\n",
      "Gradient Descent(95/99): loss=0.3903224980698724, gradient=0.0004901667618328792\n",
      "Gradient Descent(96/99): loss=0.3903224746292392, gradient=0.0004848595624022216\n",
      "Gradient Descent(97/99): loss=0.39032245168487334, gradient=0.000479677303153937\n",
      "Gradient Descent(98/99): loss=0.39032242922010546, gradient=0.00047461593260272986\n",
      "Gradient Descent(99/99): loss=0.39032240722251427, gradient=0.0004695942779566652\n",
      "Gradient Descent(0/99): loss=0.3899747635497063, gradient=0.010006951315772955\n",
      "Gradient Descent(1/99): loss=0.38996853157993794, gradient=0.008242757729885915\n",
      "Gradient Descent(2/99): loss=0.38996411977991846, gradient=0.006907889479096669\n",
      "Gradient Descent(3/99): loss=0.3899608849756808, gradient=0.0058909476445623025\n",
      "Gradient Descent(4/99): loss=0.389958427972431, gradient=0.0051134145796384685\n",
      "Gradient Descent(5/99): loss=0.38995649798052345, gradient=0.004514678983234482\n",
      "Gradient Descent(6/99): loss=0.38995493483542715, gradient=0.004049251130308449\n",
      "Gradient Descent(7/99): loss=0.3899536346582155, gradient=0.00368224497161266\n",
      "Gradient Descent(8/99): loss=0.38995252872235264, gradient=0.003387828393868171\n",
      "Gradient Descent(9/99): loss=0.3899515705168606, gradient=0.003147177436678838\n",
      "Gradient Descent(10/99): loss=0.3899507277907535, gradient=0.002946680679024197\n",
      "Gradient Descent(11/99): loss=0.38994997760182337, gradient=0.0027765511720964143\n",
      "Gradient Descent(12/99): loss=0.3899493032010029, gradient=0.002629753396040793\n",
      "Gradient Descent(13/99): loss=0.3899486920463066, gradient=0.002501204670450075\n",
      "Gradient Descent(14/99): loss=0.38994813451785576, gradient=0.0023871982666456093\n",
      "Gradient Descent(15/99): loss=0.3899476230724498, gradient=0.002284995480521408\n",
      "Gradient Descent(16/99): loss=0.3899471516769638, gradient=0.0021925411105719406\n",
      "Gradient Descent(17/99): loss=0.3899467154210212, gradient=0.002108266417777888\n",
      "Gradient Descent(18/99): loss=0.38994631024670107, gradient=0.0020309528768720723\n",
      "Gradient Descent(19/99): loss=0.3899459327961323, gradient=0.0019594188468596953\n",
      "Gradient Descent(20/99): loss=0.3899455801436109, gradient=0.0018933482401782915\n",
      "Gradient Descent(21/99): loss=0.3899452498278987, gradient=0.0018318702536084866\n",
      "Gradient Descent(22/99): loss=0.38994493972478206, gradient=0.0017744648585015746\n",
      "Gradient Descent(23/99): loss=0.3899446479868099, gradient=0.0017206973381748531\n",
      "Gradient Descent(24/99): loss=0.3899443729962548, gradient=0.0016702004500189934\n",
      "Gradient Descent(25/99): loss=0.38994411332790313, gradient=0.0016226610443889635\n",
      "Gradient Descent(26/99): loss=0.3899438677406404, gradient=0.0015776693335047092\n",
      "Gradient Descent(27/99): loss=0.3899436350868413, gradient=0.0015352794871711477\n",
      "Gradient Descent(28/99): loss=0.38994341436156643, gradient=0.0014951414505359128\n",
      "Gradient Descent(29/99): loss=0.3899432046625376, gradient=0.0014570764556863737\n",
      "Gradient Descent(30/99): loss=0.38994300517660974, gradient=0.0014209267079481377\n",
      "Gradient Descent(31/99): loss=0.3899428151684516, gradient=0.0013865521606773541\n",
      "Gradient Descent(32/99): loss=0.38994263397100437, gradient=0.0013538278762330166\n",
      "Gradient Descent(33/99): loss=0.3899424609773973, gradient=0.001322641850531283\n",
      "Gradient Descent(34/99): loss=0.3899422956340505, gradient=0.0012928932080706335\n",
      "Gradient Descent(35/99): loss=0.3899421374347635, gradient=0.001264490695734577\n",
      "Gradient Descent(36/99): loss=0.3899419859156202, gradient=0.0012373514194788451\n",
      "Gradient Descent(37/99): loss=0.38994184065057386, gradient=0.0012113997798374175\n",
      "Gradient Descent(38/99): loss=0.3899417012476093, gradient=0.0011865665711626953\n",
      "Gradient Descent(39/99): loss=0.38994156734538415, gradient=0.0011627882164209945\n",
      "Gradient Descent(40/99): loss=0.3899414386102854, gradient=0.0011400061147405166\n",
      "Gradient Descent(41/99): loss=0.3899413147338329, gradient=0.001118166083135068\n",
      "Gradient Descent(42/99): loss=0.3899411954303837, gradient=0.0010972178771828504\n",
      "Gradient Descent(43/99): loss=0.38994108043509357, gradient=0.001077114778123561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(44/99): loss=0.38994096950210255, gradient=0.0010578132360029378\n",
      "Gradient Descent(45/99): loss=0.38994086240291226, gradient=0.00103927256024882\n",
      "Gradient Descent(46/99): loss=0.38994075892493174, gradient=0.0010214546504957824\n",
      "Gradient Descent(47/99): loss=0.38994065887017115, gradient=0.0010043237616491345\n",
      "Gradient Descent(48/99): loss=0.38994056205406336, gradient=0.0009878462981475203\n",
      "Gradient Descent(49/99): loss=0.3899404683043992, gradient=0.0009719906331818524\n",
      "Gradient Descent(50/99): loss=0.3899403774603644, gradient=0.0009567269492931319\n",
      "Gradient Descent(51/99): loss=0.38994028937166314, gradient=0.0009420270973244841\n",
      "Gradient Descent(52/99): loss=0.38994020389772244, gradient=0.0009278644711639941\n",
      "Gradient Descent(53/99): loss=0.38994012090696656, gradient=0.0009142138961031972\n",
      "Gradient Descent(54/99): loss=0.38994004027615353, gradient=0.0009010515289598658\n",
      "Gradient Descent(55/99): loss=0.3899399618897702, gradient=0.0008883547683890122\n",
      "Gradient Descent(56/99): loss=0.3899398856394745, gradient=0.0008761021740360873\n",
      "Gradient Descent(57/99): loss=0.38993981142358836, gradient=0.0008642733933830132\n",
      "Gradient Descent(58/99): loss=0.38993973914662733, gradient=0.0008528490953028373\n",
      "Gradient Descent(59/99): loss=0.38993966871887065, gradient=0.0008418109094803833\n",
      "Gradient Descent(60/99): loss=0.38993960005596345, gradient=0.0008311413709754194\n",
      "Gradient Descent(61/99): loss=0.38993953307855084, gradient=0.0008208238693075894\n",
      "Gradient Descent(62/99): loss=0.38993946771193966, gradient=0.0008108426015294084\n",
      "Gradient Descent(63/99): loss=0.3899394038857858, gradient=0.000801182528827311\n",
      "Gradient Descent(64/99): loss=0.38993934153380533, gradient=0.0007918293362562067\n",
      "Gradient Descent(65/99): loss=0.3899392805935062, gradient=0.0007827693952645053\n",
      "Gradient Descent(66/99): loss=0.3899392210059399, gradient=0.0007739897287165144\n",
      "Gradient Descent(67/99): loss=0.3899391627154726, gradient=0.0007654779781563945\n",
      "Gradient Descent(68/99): loss=0.389939105669569, gradient=0.0007572223730931915\n",
      "Gradient Descent(69/99): loss=0.3899390498185965, gradient=0.0007492117021162349\n",
      "Gradient Descent(70/99): loss=0.3899389951156388, gradient=0.0007414352856736592\n",
      "Gradient Descent(71/99): loss=0.38993894151632497, gradient=0.0007338829503705299\n",
      "Gradient Descent(72/99): loss=0.38993888897866946, gradient=0.0007265450046594129\n",
      "Gradient Descent(73/99): loss=0.3899388374629241, gradient=0.0007194122158135012\n",
      "Gradient Descent(74/99): loss=0.3899387869314382, gradient=0.0007124757880852575\n",
      "Gradient Descent(75/99): loss=0.38993873734853113, gradient=0.0007057273419651005\n",
      "Gradient Descent(76/99): loss=0.38993868868037035, gradient=0.0006991588944651106\n",
      "Gradient Descent(77/99): loss=0.38993864089485936, gradient=0.0006927628403605547\n",
      "Gradient Descent(78/99): loss=0.38993859396153346, gradient=0.0006865319343296721\n",
      "Gradient Descent(79/99): loss=0.3899385478514615, gradient=0.000680459273938605\n",
      "Gradient Descent(80/99): loss=0.3899385025371533, gradient=0.0006745382834230496\n",
      "Gradient Descent(81/99): loss=0.3899384579924759, gradient=0.000668762698223468\n",
      "Gradient Descent(82/99): loss=0.38993841419257197, gradient=0.0006631265502342585\n",
      "Gradient Descent(83/99): loss=0.38993837111378576, gradient=0.0006576241537305605\n",
      "Gradient Descent(84/99): loss=0.389938328733593, gradient=0.0006522500919398132\n",
      "Gradient Descent(85/99): loss=0.38993828703053557, gradient=0.0006469992042269658\n",
      "Gradient Descent(86/99): loss=0.38993824598415966, gradient=0.0006418665738649717\n",
      "Gradient Descent(87/99): loss=0.38993820557495834, gradient=0.000636847516364311\n",
      "Gradient Descent(88/99): loss=0.38993816578431834, gradient=0.0006319375683355058\n",
      "Gradient Descent(89/99): loss=0.389938126594469, gradient=0.0006271324768623804\n",
      "Gradient Descent(90/99): loss=0.389938087988435, gradient=0.0006224281893630052\n",
      "Gradient Descent(91/99): loss=0.3899380499499924, gradient=0.0006178208439174041\n",
      "Gradient Descent(92/99): loss=0.38993801246362636, gradient=0.0006133067600421503\n",
      "Gradient Descent(93/99): loss=0.3899379755144926, gradient=0.0006088824298927049\n",
      "Gradient Descent(94/99): loss=0.3899379390883809, gradient=0.0006045445098748854\n",
      "Gradient Descent(95/99): loss=0.38993790317168, gradient=0.0006002898126485714\n",
      "Gradient Descent(96/99): loss=0.3899378677513455, gradient=0.0005961152995062986\n",
      "Gradient Descent(97/99): loss=0.3899378328148701, gradient=0.0005920180731109553\n",
      "Gradient Descent(98/99): loss=0.389937798350254, gradient=0.0005879953705772092\n",
      "Gradient Descent(99/99): loss=0.3899377643459792, gradient=0.0005840445568815422\n",
      "Gradient Descent(0/99): loss=0.38967466948433244, gradient=0.011083519127351376\n",
      "Gradient Descent(1/99): loss=0.3896696870143942, gradient=0.007444121388120328\n",
      "Gradient Descent(2/99): loss=0.3896661532021378, gradient=0.0061806424421039985\n",
      "Gradient Descent(3/99): loss=0.3896635575148223, gradient=0.005280868190409333\n",
      "Gradient Descent(4/99): loss=0.38966161203280625, gradient=0.004561425200068916\n",
      "Gradient Descent(5/99): loss=0.3896601255418471, gradient=0.003978184234446271\n",
      "Gradient Descent(6/99): loss=0.3896589679130183, gradient=0.003502969898295725\n",
      "Gradient Descent(7/99): loss=0.38965804971407086, gradient=0.0031132037948236667\n",
      "Gradient Descent(8/99): loss=0.38965730869682264, gradient=0.002791231068215741\n",
      "Gradient Descent(9/99): loss=0.38965670097452515, gradient=0.00252314188086803\n",
      "Gradient Descent(10/99): loss=0.38965619517847966, gradient=0.002298018739578796\n",
      "Gradient Descent(11/99): loss=0.38965576856171075, gradient=0.0021073200565859297\n",
      "Gradient Descent(12/99): loss=0.3896554043819679, gradient=0.0019443759948470222\n",
      "Gradient Descent(13/99): loss=0.38965509012974076, gradient=0.0018039795925604753\n",
      "Gradient Descent(14/99): loss=0.3896548163177219, gradient=0.0016820590263137705\n",
      "Gradient Descent(15/99): loss=0.38965457564571404, gradient=0.0015754182831853813\n",
      "Gradient Descent(16/99): loss=0.38965436241839196, gradient=0.0014815345189261744\n",
      "Gradient Descent(17/99): loss=0.389654172134696, gradient=0.001398401463738855\n",
      "Gradient Descent(18/99): loss=0.3896540011947295, gradient=0.0013244095138955122\n",
      "Gradient Descent(19/99): loss=0.3896538466878637, gradient=0.0012582545462348526\n",
      "Gradient Descent(20/99): loss=0.3896537062375356, gradient=0.0011988688918211177\n",
      "Gradient Descent(21/99): loss=0.3896535778860608, gradient=0.0011453692018408266\n",
      "Gradient Descent(22/99): loss=0.3896534600080219, gradient=0.001097017071659641\n",
      "Gradient Descent(23/99): loss=0.38965335124429873, gradient=0.001053189235084775\n",
      "Gradient Descent(24/99): loss=0.3896532504512095, gradient=0.001013354904565275\n",
      "Gradient Descent(25/99): loss=0.38965315666084255, gradient=0.0009770584337258856\n",
      "Gradient Descent(26/99): loss=0.38965306904978264, gradient=0.0009439059418259682\n",
      "Gradient Descent(27/99): loss=0.3896529869142135, gradient=0.0009135548915600951\n",
      "Gradient Descent(28/99): loss=0.3896529096499138, gradient=0.0008857058757425414\n",
      "Gradient Descent(29/99): loss=0.38965283673605766, gradient=0.0008600960649132466\n",
      "Gradient Descent(30/99): loss=0.3896527677219973, gradient=0.0008364939130414876\n",
      "Gradient Descent(31/99): loss=0.38965270221640835, gradient=0.0008146948251082807\n",
      "Gradient Descent(32/99): loss=0.38965263987832516, gradient=0.0007945175683024285\n",
      "Gradient Descent(33/99): loss=0.3896525804096924, gradient=0.0007758012653787307\n",
      "Gradient Descent(34/99): loss=0.38965252354915353, gradient=0.0007584028500413948\n",
      "Gradient Descent(35/99): loss=0.38965246906684076, gradient=0.0007421948942264714\n",
      "Gradient Descent(36/99): loss=0.3896524167599937, gradient=0.0007270637389637297\n",
      "Gradient Descent(37/99): loss=0.38965236644925694, gradient=0.0007129078763787684\n",
      "Gradient Descent(38/99): loss=0.3896523179755436, gradient=0.0006996365420054181\n",
      "Gradient Descent(39/99): loss=0.3896522711973662, gradient=0.0006871684851230049\n",
      "Gradient Descent(40/99): loss=0.3896522259885621, gradient=0.0006754308911782578\n",
      "Gradient Descent(41/99): loss=0.38965218223634646, gradient=0.0006643584351205384\n",
      "Gradient Descent(42/99): loss=0.3896521398396428, gradient=0.0006538924481155776\n",
      "Gradient Descent(43/99): loss=0.38965209870764667, gradient=0.0006439801829220967\n",
      "Gradient Descent(44/99): loss=0.38965205875858705, gradient=0.0006345741654413668\n",
      "Gradient Descent(45/99): loss=0.38965201991865533, gradient=0.0006256316217384846\n",
      "Gradient Descent(46/99): loss=0.3896519821210777, gradient=0.0006171139712989131\n",
      "Gradient Descent(47/99): loss=0.38965194530530833, gradient=0.0006089863785011203\n",
      "Gradient Descent(48/99): loss=0.3896519094163277, gradient=0.0006012173553142907\n",
      "Gradient Descent(49/99): loss=0.3896518744040294, gradient=0.0005937784091056287\n",
      "Gradient Descent(50/99): loss=0.3896518402226844, gradient=0.0005866437301986895\n",
      "Gradient Descent(51/99): loss=0.3896518068304713, gradient=0.0005797899144778487\n",
      "Gradient Descent(52/99): loss=0.38965177418906327, gradient=0.0005731957169065886\n",
      "Gradient Descent(53/99): loss=0.38965174226326477, gradient=0.0005668418323264753\n",
      "Gradient Descent(54/99): loss=0.3896517110206913, gradient=0.000560710700342544\n",
      "Gradient Descent(55/99): loss=0.3896516804314858, gradient=0.0005547863314856045\n",
      "Gradient Descent(56/99): loss=0.38965165046806816, gradient=0.0005490541521808574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(57/99): loss=0.3896516211049125, gradient=0.0005435008663488463\n",
      "Gradient Descent(58/99): loss=0.38965159231834934, gradient=0.000538114331727335\n",
      "Gradient Descent(59/99): loss=0.3896515640863909, gradient=0.0005328834492311933\n",
      "Gradient Descent(60/99): loss=0.38965153638857186, gradient=0.0005277980638700691\n",
      "Gradient Descent(61/99): loss=0.3896515092058114, gradient=0.0005228488759206682\n",
      "Gradient Descent(62/99): loss=0.3896514825202861, gradient=0.0005180273612051242\n",
      "Gradient Descent(63/99): loss=0.3896514563153191, gradient=0.0005133256994647861\n",
      "Gradient Descent(64/99): loss=0.38965143057527796, gradient=0.0005087367099371074\n",
      "Gradient Descent(65/99): loss=0.38965140528548486, gradient=0.0005042537933495486\n",
      "Gradient Descent(66/99): loss=0.3896513804321352, gradient=0.0004998708796351392\n",
      "Gradient Descent(67/99): loss=0.3896513560022237, gradient=0.0004955823807568143\n",
      "Gradient Descent(68/99): loss=0.3896513319834783, gradient=0.0004913831480972575\n",
      "Gradient Descent(69/99): loss=0.38965130836430056, gradient=0.0004872684339337208\n",
      "Gradient Descent(70/99): loss=0.38965128513371117, gradient=0.0004832338565723864\n",
      "Gradient Descent(71/99): loss=0.3896512622813008, gradient=0.00047927536876430385\n",
      "Gradient Descent(72/99): loss=0.3896512397971873, gradient=0.00047538922906757654\n",
      "Gradient Descent(73/99): loss=0.3896512176719728, gradient=0.0004715719758575692\n",
      "Gradient Descent(74/99): loss=0.38965119589670916, gradient=0.0004678204037198872\n",
      "Gradient Descent(75/99): loss=0.3896511744628633, gradient=0.0004641315419894141\n",
      "Gradient Descent(76/99): loss=0.38965115336228806, gradient=0.00046050263522479524\n",
      "Gradient Descent(77/99): loss=0.38965113258719375, gradient=0.00045693112542941614\n",
      "Gradient Descent(78/99): loss=0.3896511121301231, gradient=0.0004534146358509437\n",
      "Gradient Descent(79/99): loss=0.3896510919839287, gradient=0.00044995095620814696\n",
      "Gradient Descent(80/99): loss=0.38965107214175126, gradient=0.0004465380292101591\n",
      "Gradient Descent(81/99): loss=0.3896510525970005, gradient=0.00044317393824653796\n",
      "Gradient Descent(82/99): loss=0.3896510333433381, gradient=0.0004398568961391612\n",
      "Gradient Descent(83/99): loss=0.3896510143746611, gradient=0.000436585234858499\n",
      "Gradient Descent(84/99): loss=0.3896509956850869, gradient=0.0004333573961151943\n",
      "Gradient Descent(85/99): loss=0.38965097726893977, gradient=0.0004301719227480708\n",
      "Gradient Descent(86/99): loss=0.3896509591207391, gradient=0.00042702745083640246\n",
      "Gradient Descent(87/99): loss=0.3896509412351863, gradient=0.0004239227024720947\n",
      "Gradient Descent(88/99): loss=0.38965092360715553, gradient=0.000420856479132705\n",
      "Gradient Descent(89/99): loss=0.3896509062316838, gradient=0.00041782765560284215\n",
      "Gradient Descent(90/99): loss=0.389650889103961, gradient=0.00041483517439537597\n",
      "Gradient Descent(91/99): loss=0.3896508722193226, gradient=0.00041187804062948574\n",
      "Gradient Descent(92/99): loss=0.3896508555732413, gradient=0.0004089553173257246\n",
      "Gradient Descent(93/99): loss=0.38965083916132043, gradient=0.0004060661210824522\n",
      "Gradient Descent(94/99): loss=0.3896508229792868, gradient=0.000403209618100995\n",
      "Gradient Descent(95/99): loss=0.3896508070229852, gradient=0.000400385020529921\n",
      "Gradient Descent(96/99): loss=0.3896507912883721, gradient=0.0003975915831014344\n",
      "Gradient Descent(97/99): loss=0.3896507757715106, gradient=0.00039482860003556963\n",
      "Gradient Descent(98/99): loss=0.38965076046856545, gradient=0.000392095402189543\n",
      "Gradient Descent(99/99): loss=0.3896507453757991, gradient=0.0003893913544319021\n",
      "Gradient Descent(0/99): loss=0.38896069345375944, gradient=0.008182386185523693\n",
      "Gradient Descent(1/99): loss=0.38895793126644723, gradient=0.005551277041945703\n",
      "Gradient Descent(2/99): loss=0.3889560127391919, gradient=0.004564831884052332\n",
      "Gradient Descent(3/99): loss=0.3889546311147416, gradient=0.003860495464195756\n",
      "Gradient Descent(4/99): loss=0.3889536111638604, gradient=0.0033071627444580215\n",
      "Gradient Descent(5/99): loss=0.3889528381848793, gradient=0.0028699583715951194\n",
      "Gradient Descent(6/99): loss=0.38895223581313615, gradient=0.0025251215714507825\n",
      "Gradient Descent(7/99): loss=0.38895175281935634, gradient=0.0022535596785191645\n",
      "Gradient Descent(8/99): loss=0.38895135458466923, gradient=0.002039711536689426\n",
      "Gradient Descent(9/99): loss=0.38895101752467004, gradient=0.0018709399887905061\n",
      "Gradient Descent(10/99): loss=0.38895072542692993, gradient=0.0017370804843357141\n",
      "Gradient Descent(11/99): loss=0.38895046703403363, gradient=0.0016300652149171993\n",
      "Gradient Descent(12/99): loss=0.38895023444119975, gradient=0.001543584846329111\n",
      "Gradient Descent(13/99): loss=0.3889500220286195, gradient=0.0014727753486792603\n",
      "Gradient Descent(14/99): loss=0.38894982574628445, gradient=0.0014139325496410264\n",
      "Gradient Descent(15/99): loss=0.3889496426321662, gradient=0.0013642615649753517\n",
      "Gradient Descent(16/99): loss=0.38894947048558703, gradient=0.0013216660165430473\n",
      "Gradient Descent(17/99): loss=0.3889493076442895, gradient=0.0012845774976648152\n",
      "Gradient Descent(18/99): loss=0.38894915283115716, gradient=0.0012518219791642928\n",
      "Gradient Descent(19/99): loss=0.3889490050479784, gradient=0.0012225177465664456\n",
      "Gradient Descent(20/99): loss=0.3889488635011652, gradient=0.0011959988609669208\n",
      "Gradient Descent(21/99): loss=0.3889487275493245, gradient=0.0011717585038402256\n",
      "Gradient Descent(22/99): loss=0.38894859666587345, gradient=0.0011494073937411168\n",
      "Gradient Descent(23/99): loss=0.38894847041209063, gradient=0.0011286434097776347\n",
      "Gradient Descent(24/99): loss=0.38894834841746695, gradient=0.0011092294394492793\n",
      "Gradient Descent(25/99): loss=0.3889482303652046, gradient=0.0010909772114082819\n",
      "Gradient Descent(26/99): loss=0.38894811598138246, gradient=0.0010737354624746843\n",
      "Gradient Descent(27/99): loss=0.38894800502676113, gradient=0.0010573812372403537\n",
      "Gradient Descent(28/99): loss=0.38894789729050167, gradient=0.0010418134524140842\n",
      "Gradient Descent(29/99): loss=0.3889477925852942, gradient=0.0010269481020451067\n",
      "Gradient Descent(30/99): loss=0.38894769074353297, gradient=0.00101271465607447\n",
      "Gradient Descent(31/99): loss=0.3889475916142757, gradient=0.000999053331149276\n",
      "Gradient Descent(32/99): loss=0.38894749506080306, gradient=0.0009859130029824363\n",
      "Gradient Descent(33/99): loss=0.38894740095863445, gradient=0.000973249593948574\n",
      "Gradient Descent(34/99): loss=0.38894730919390275, gradient=0.000961024815512709\n",
      "Gradient Descent(35/99): loss=0.3889472196620081, gradient=0.0009492051778497665\n",
      "Gradient Descent(36/99): loss=0.38894713226649424, gradient=0.0009377612024578438\n",
      "Gradient Descent(37/99): loss=0.38894704691810256, gradient=0.000926666790403089\n",
      "Gradient Descent(38/99): loss=0.3889469635339705, gradient=0.0009158987109854059\n",
      "Gradient Descent(39/99): loss=0.38894688205557676, gradient=0.0009052207012653944\n",
      "Gradient Descent(40/99): loss=0.388946802389796, gradient=0.0008950617981684933\n",
      "Gradient Descent(41/99): loss=0.38894672446947615, gradient=0.0008851716146349855\n",
      "Gradient Descent(42/99): loss=0.38894664823161296, gradient=0.000875535000667154\n",
      "Gradient Descent(43/99): loss=0.3889465736169655, gradient=0.000866138179846445\n",
      "Gradient Descent(44/99): loss=0.3889465005697218, gradient=0.0008569685835504553\n",
      "Gradient Descent(45/99): loss=0.3889464290372016, gradient=0.0008480147098274293\n",
      "Gradient Descent(46/99): loss=0.3889463589695951, gradient=0.0008392660026480043\n",
      "Gradient Descent(47/99): loss=0.388946290319729, gradient=0.0008307127480924718\n",
      "Gradient Descent(48/99): loss=0.3889462230428587, gradient=0.0008223459846853284\n",
      "Gradient Descent(49/99): loss=0.38894615709648145, gradient=0.0008141574256054772\n",
      "Gradient Descent(50/99): loss=0.38894609244017014, gradient=0.000806139390910545\n",
      "Gradient Descent(51/99): loss=0.38894602903542047, gradient=0.0007982847482431462\n",
      "Gradient Descent(52/99): loss=0.38894596684551686, gradient=0.0007905868607520128\n",
      "Gradient Descent(53/99): loss=0.3889459058354078, gradient=0.0007830395411756055\n",
      "Gradient Descent(54/99): loss=0.38894584597159376, gradient=0.0007756370112107268\n",
      "Gradient Descent(55/99): loss=0.3889457872220254, gradient=0.0007683738654312113\n",
      "Gradient Descent(56/99): loss=0.38894572955601014, gradient=0.0007612450391402681\n",
      "Gradient Descent(57/99): loss=0.3889456729441277, gradient=0.0007542457796357433\n",
      "Gradient Descent(58/99): loss=0.3889456173581506, gradient=0.0007473716204481583\n",
      "Gradient Descent(59/99): loss=0.3889455627709736, gradient=0.0007406183581781495\n",
      "Gradient Descent(60/99): loss=0.3889455091565486, gradient=0.0007339820316143587\n",
      "Gradient Descent(61/99): loss=0.3889454564898214, gradient=0.0007274589028600703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(62/99): loss=0.38894540474667827, gradient=0.0007210454402344699\n",
      "Gradient Descent(63/99): loss=0.388945353903893, gradient=0.0007147383027480119\n",
      "Gradient Descent(64/99): loss=0.388945303939079, gradient=0.000708534325977644\n",
      "Gradient Descent(65/99): loss=0.38894525483064524, gradient=0.0007024305091912936\n",
      "Gradient Descent(66/99): loss=0.38894520655775444, gradient=0.0006964240035904827\n",
      "Gradient Descent(67/99): loss=0.3889451591002859, gradient=0.0006905121015566634\n",
      "Gradient Descent(68/99): loss=0.38894511243879865, gradient=0.0006846922268006014\n",
      "Gradient Descent(69/99): loss=0.3889450665544988, gradient=0.0006789619253265214\n",
      "Gradient Descent(70/99): loss=0.3889450214292085, gradient=0.0006733188571331967\n",
      "Gradient Descent(71/99): loss=0.38894497704533754, gradient=0.0006677607885828699\n",
      "Gradient Descent(72/99): loss=0.38894493338585445, gradient=0.0006622855853765411\n",
      "Gradient Descent(73/99): loss=0.388944890434264, gradient=0.0006568912060811013\n",
      "Gradient Descent(74/99): loss=0.3889448481745808, gradient=0.0006515756961592392\n",
      "Gradient Descent(75/99): loss=0.388944806591309, gradient=0.0006463371824585408\n",
      "Gradient Descent(76/99): loss=0.38894476566941927, gradient=0.0006411738681202328\n",
      "Gradient Descent(77/99): loss=0.38894472539433156, gradient=0.0006360840278719019\n",
      "Gradient Descent(78/99): loss=0.38894468575189445, gradient=0.0006310660036724923\n",
      "Gradient Descent(79/99): loss=0.38894464672836915, gradient=0.0006261182006801169\n",
      "Gradient Descent(80/99): loss=0.38894460831041194, gradient=0.0006212390835166064\n",
      "Gradient Descent(81/99): loss=0.3889445704850595, gradient=0.0006164271728046404\n",
      "Gradient Descent(82/99): loss=0.3889445332397135, gradient=0.0006116810419559273\n",
      "Gradient Descent(83/99): loss=0.38894449656212693, gradient=0.0006069993141900697\n",
      "Gradient Descent(84/99): loss=0.38894446044039155, gradient=0.0006023806597663127\n",
      "Gradient Descent(85/99): loss=0.3889444248629239, gradient=0.0005978237934114131\n",
      "Gradient Descent(86/99): loss=0.3889443898184552, gradient=0.0005933274719283571\n",
      "Gradient Descent(87/99): loss=0.3889443552960188, gradient=0.0005888904919720296\n",
      "Gradient Descent(88/99): loss=0.38894432128494, gradient=0.000584511687978944\n",
      "Gradient Descent(89/99): loss=0.38894428777482576, gradient=0.0005801899302393824\n",
      "Gradient Descent(90/99): loss=0.38894425475555533, gradient=0.0005759241231009163\n",
      "Gradient Descent(91/99): loss=0.3889442222172705, gradient=0.0005717132032935773\n",
      "Gradient Descent(92/99): loss=0.3889441901503668, gradient=0.0005675561383674383\n",
      "Gradient Descent(93/99): loss=0.38894415854548536, gradient=0.0005634519252340181\n",
      "Gradient Descent(94/99): loss=0.38894412739350537, gradient=0.0005593995888038973\n",
      "Gradient Descent(95/99): loss=0.3889440966855347, gradient=0.0005553981807135654\n",
      "Gradient Descent(96/99): loss=0.3889440664129051, gradient=0.0005514467781342736\n",
      "Gradient Descent(97/99): loss=0.38894403656716237, gradient=0.000547544482657539\n",
      "Gradient Descent(98/99): loss=0.38894400714006155, gradient=0.0005436904192511041\n",
      "Gradient Descent(99/99): loss=0.3889439781235601, gradient=0.0005398837352803585\n",
      "Gradient Descent(0/99): loss=0.39014283499419794, gradient=0.015400764359707273\n",
      "Gradient Descent(1/99): loss=0.3901389122837985, gradient=0.006919555148791423\n",
      "Gradient Descent(2/99): loss=0.3901362600461939, gradient=0.0053603401812753575\n",
      "Gradient Descent(3/99): loss=0.39013426483648517, gradient=0.004615568428945834\n",
      "Gradient Descent(4/99): loss=0.3901327211782619, gradient=0.00404834130781925\n",
      "Gradient Descent(5/99): loss=0.390131500829745, gradient=0.003590739724736285\n",
      "Gradient Descent(6/99): loss=0.3901305163181326, gradient=0.0032177354772664744\n",
      "Gradient Descent(7/99): loss=0.3901297067973587, gradient=0.0029114885147698763\n",
      "Gradient Descent(8/99): loss=0.39012902931505705, gradient=0.002658168803158153\n",
      "Gradient Descent(9/99): loss=0.3901284531125164, gradient=0.002446975294074338\n",
      "Gradient Descent(10/99): loss=0.3901279558439122, gradient=0.0022694601710722033\n",
      "Gradient Descent(11/99): loss=0.39012752104180193, gradient=0.0021190064635409674\n",
      "Gradient Descent(12/99): loss=0.39012713639878066, gradient=0.00199042051684887\n",
      "Gradient Descent(13/99): loss=0.3901267925893058, gradient=0.0018796143391646147\n",
      "Gradient Descent(14/99): loss=0.39012648245257714, gradient=0.0017833586006917785\n",
      "Gradient Descent(15/99): loss=0.39012620041926216, gradient=0.0016990911055944746\n",
      "Gradient Descent(16/99): loss=0.39012594210461976, gradient=0.0016247686635170657\n",
      "Gradient Descent(17/99): loss=0.3901257040163788, gradient=0.0015587527562508105\n",
      "Gradient Descent(18/99): loss=0.39012548334258923, gradient=0.0014997213912428262\n",
      "Gradient Descent(19/99): loss=0.3901252777957964, gradient=0.0014466011550146151\n",
      "Gradient Descent(20/99): loss=0.39012508549730357, gradient=0.0013985147907068687\n",
      "Gradient Descent(21/99): loss=0.39012490489026674, gradient=0.0013547406745307831\n",
      "Gradient Descent(22/99): loss=0.39012473467374165, gradient=0.0013146813985039168\n",
      "Gradient Descent(23/99): loss=0.3901245737521153, gradient=0.0012778393195954543\n",
      "Gradient Descent(24/99): loss=0.3901244211959484, gradient=0.0012437974421837614\n",
      "Gradient Descent(25/99): loss=0.39012427621136564, gradient=0.0012122043909244507\n",
      "Gradient Descent(26/99): loss=0.39012413811591173, gradient=0.0011827625295506503\n",
      "Gradient Descent(27/99): loss=0.3901240063193468, gradient=0.001155218508164797\n",
      "Gradient Descent(28/99): loss=0.39012388030824646, gradient=0.0011293556936559617\n",
      "Gradient Descent(29/99): loss=0.39012375963356055, gradient=0.001104988067957041\n",
      "Gradient Descent(30/99): loss=0.39012364390049326, gradient=0.0010819552770574635\n",
      "Gradient Descent(31/99): loss=0.3901235327602155, gradient=0.001060118587799511\n",
      "Gradient Descent(32/99): loss=0.3901234259030384, gradient=0.001039357565461186\n",
      "Gradient Descent(33/99): loss=0.39012332305275765, gradient=0.0010195673274724956\n",
      "Gradient Descent(34/99): loss=0.3901232239619436, gradient=0.0010006562607331776\n",
      "Gradient Descent(35/99): loss=0.3901231284080008, gradient=0.00098254411444465\n",
      "Gradient Descent(36/99): loss=0.3901230361898533, gradient=0.0009651603990555865\n",
      "Gradient Descent(37/99): loss=0.39012294712514833, gradient=0.0009484430362754302\n",
      "Gradient Descent(38/99): loss=0.3901228610478844, gradient=0.0009323372161972298\n",
      "Gradient Descent(39/99): loss=0.39012277780639376, gradient=0.0009167944261892922\n",
      "Gradient Descent(40/99): loss=0.3901226972616191, gradient=0.0009017716229573478\n",
      "Gradient Descent(41/99): loss=0.3901226192856371, gradient=0.0008872305244869921\n",
      "Gradient Descent(42/99): loss=0.39012254376038796, gradient=0.0008731370027876041\n",
      "Gradient Descent(43/99): loss=0.3901224705765823, gradient=0.000859460561719439\n",
      "Gradient Descent(44/99): loss=0.3901223996327513, gradient=0.0008461738868885229\n",
      "Gradient Descent(45/99): loss=0.39012233083442704, gradient=0.0008332524567793905\n",
      "Gradient Descent(46/99): loss=0.3901222640934251, gradient=0.0008206742060752641\n",
      "Gradient Descent(47/99): loss=0.39012219932722114, gradient=0.0008084192335728901\n",
      "Gradient Descent(48/99): loss=0.3901221364584041, gradient=0.0007964695482975761\n",
      "Gradient Descent(49/99): loss=0.39012207541419713, gradient=0.0007848088484164661\n",
      "Gradient Descent(50/99): loss=0.39012201612603437, gradient=0.0007734223283729281\n",
      "Gradient Descent(51/99): loss=0.39012195852919007, gradient=0.0007622965103522276\n",
      "Gradient Descent(52/99): loss=0.3901219025624479, gradient=0.0007514190967650577\n",
      "Gradient Descent(53/99): loss=0.39012184816780937, gradient=0.0007407788409198051\n",
      "Gradient Descent(54/99): loss=0.3901217952902339, gradient=0.0007303654334624422\n",
      "Gradient Descent(55/99): loss=0.39012174387740717, gradient=0.000720169402508251\n",
      "Gradient Descent(56/99): loss=0.39012169387953355, gradient=0.000710182025681759\n",
      "Gradient Descent(57/99): loss=0.39012164524915177, gradient=0.0007003952525299078\n",
      "Gradient Descent(58/99): loss=0.39012159794096624, gradient=0.0006908016359851621\n",
      "Gradient Descent(59/99): loss=0.39012155191169867, gradient=0.000681394271735666\n",
      "Gradient Descent(60/99): loss=0.3901215071199521, gradient=0.000672166744513319\n",
      "Gradient Descent(61/99): loss=0.3901214635260869, gradient=0.0006631130804442061\n",
      "Gradient Descent(62/99): loss=0.39012142109211145, gradient=0.0006542277047174567\n",
      "Gradient Descent(63/99): loss=0.3901213797815793, gradient=0.0006455054039271183\n",
      "Gradient Descent(64/99): loss=0.390121339559497, gradient=0.0006369412925254809\n",
      "Gradient Descent(65/99): loss=0.3901213003922412, gradient=0.0006285307828977247\n",
      "Gradient Descent(66/99): loss=0.3901212622474798, gradient=0.0006202695586312754\n",
      "Gradient Descent(67/99): loss=0.3901212250941015, gradient=0.0006121535506060697\n",
      "Gradient Descent(68/99): loss=0.39012118890215036, gradient=0.0006041789155798169\n",
      "Gradient Descent(69/99): loss=0.3901211536427668, gradient=0.0005963420169815549\n",
      "Gradient Descent(70/99): loss=0.39012111928813015, gradient=0.0005886394076634118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(71/99): loss=0.39012108581140853, gradient=0.0005810678143894972\n",
      "Gradient Descent(72/99): loss=0.3901210531867106, gradient=0.0005736241238686072\n",
      "Gradient Descent(73/99): loss=0.3901210213890418, gradient=0.0005663053701605652\n",
      "Gradient Descent(74/99): loss=0.39012099039426135, gradient=0.0005591087233052043\n",
      "Gradient Descent(75/99): loss=0.39012096017904574, gradient=0.0005520314790420083\n",
      "Gradient Descent(76/99): loss=0.39012093072085113, gradient=0.0005450710495033242\n",
      "Gradient Descent(77/99): loss=0.39012090199788, gradient=0.0005382249547769458\n",
      "Gradient Descent(78/99): loss=0.3901208739890501, gradient=0.0005314908152475299\n",
      "Gradient Descent(79/99): loss=0.39012084667396346, gradient=0.0005248663446344313\n",
      "Gradient Descent(80/99): loss=0.3901208200328793, gradient=0.0005183493436549345\n",
      "Gradient Descent(81/99): loss=0.39012079404668765, gradient=0.0005119376942482809\n",
      "Gradient Descent(82/99): loss=0.3901207686968834, gradient=0.0005056293543037583\n",
      "Gradient Descent(83/99): loss=0.39012074396554414, gradient=0.000499422352842459\n",
      "Gradient Descent(84/99): loss=0.39012071983530605, gradient=0.0004933147856074183\n",
      "Gradient Descent(85/99): loss=0.39012069628934504, gradient=0.00048730481102196853\n",
      "Gradient Descent(86/99): loss=0.3901206733113546, gradient=0.00048139064648070534\n",
      "Gradient Descent(87/99): loss=0.3901206508855283, gradient=0.0004755705649406703\n",
      "Gradient Descent(88/99): loss=0.3901206289965404, gradient=0.0004698428917842271\n",
      "Gradient Descent(89/99): loss=0.39012060762952966, gradient=0.000464206001928221\n",
      "Gradient Descent(90/99): loss=0.39012058677008216, gradient=0.0004586583171561107\n",
      "Gradient Descent(91/99): loss=0.39012056640421605, gradient=0.0004531983036528063\n",
      "Gradient Descent(92/99): loss=0.3901205465183664, gradient=0.00044782446972316356\n",
      "Gradient Descent(93/99): loss=0.3901205270993711, gradient=0.0004425353636784497\n",
      "Gradient Descent(94/99): loss=0.39012050813445726, gradient=0.000437329571874743\n",
      "Gradient Descent(95/99): loss=0.3901204896112268, gradient=0.00043220571689074497\n",
      "Gradient Descent(96/99): loss=0.3901204715176465, gradient=0.00042716245583236597\n",
      "Gradient Descent(97/99): loss=0.3901204538420336, gradient=0.00042219847875306837\n",
      "Gradient Descent(98/99): loss=0.39012043657304535, gradient=0.0004173125071807699\n",
      "Gradient Descent(99/99): loss=0.3901204196996678, gradient=0.00041250329274147276\n",
      "Gradient Descent(0/99): loss=0.39035249096151137, gradient=0.007270965547354143\n",
      "Gradient Descent(1/99): loss=0.3903488097964638, gradient=0.006260319122516879\n",
      "Gradient Descent(2/99): loss=0.3903459182709517, gradient=0.005528601739275532\n",
      "Gradient Descent(3/99): loss=0.39034358767637284, gradient=0.004949327880172028\n",
      "Gradient Descent(4/99): loss=0.3903416662030018, gradient=0.0044826342210785505\n",
      "Gradient Descent(5/99): loss=0.3903400501691498, gradient=0.0041018244104378074\n",
      "Gradient Descent(6/99): loss=0.3903386673117751, gradient=0.0037870635084866038\n",
      "Gradient Descent(7/99): loss=0.39033746624666926, gradient=0.0035235007361081455\n",
      "Gradient Descent(8/99): loss=0.3903364098722082, gradient=0.0032994299104719222\n",
      "Gradient Descent(9/99): loss=0.3903354704191168, gradient=0.003107665139144109\n",
      "Gradient Descent(10/99): loss=0.3903346271844915, gradient=0.002941067521727238\n",
      "Gradient Descent(11/99): loss=0.3903338642522805, gradient=0.0027949836011059884\n",
      "Gradient Descent(12/99): loss=0.39033316925973965, gradient=0.002665544807812119\n",
      "Gradient Descent(13/99): loss=0.3903325324396605, gradient=0.0025498208983736415\n",
      "Gradient Descent(14/99): loss=0.3903319459687582, gradient=0.002445505571649242\n",
      "Gradient Descent(15/99): loss=0.390331403500657, gradient=0.0023507664587692732\n",
      "Gradient Descent(16/99): loss=0.39033089998651116, gradient=0.002263390158352047\n",
      "Gradient Descent(17/99): loss=0.39033043092036707, gradient=0.0021837234506330325\n",
      "Gradient Descent(18/99): loss=0.3903299926805054, gradient=0.002110006985446409\n",
      "Gradient Descent(19/99): loss=0.3903295822010563, gradient=0.0020414483241821342\n",
      "Gradient Descent(20/99): loss=0.3903291968631746, gradient=0.0019773934215876966\n",
      "Gradient Descent(21/99): loss=0.39032883441086547, gradient=0.0019172991482608664\n",
      "Gradient Descent(22/99): loss=0.39032849288523525, gradient=0.0018607118510340142\n",
      "Gradient Descent(23/99): loss=0.3903281705726947, gradient=0.0018072504807492716\n",
      "Gradient Descent(24/99): loss=0.3903278659865418, gradient=0.001756450563839816\n",
      "Gradient Descent(25/99): loss=0.3903275777597438, gradient=0.001708346101143673\n",
      "Gradient Descent(26/99): loss=0.39032730470013316, gradient=0.0016625356961226486\n",
      "Gradient Descent(27/99): loss=0.39032704575354016, gradient=0.0016187220570512003\n",
      "Gradient Descent(28/99): loss=0.3903267999389792, gradient=0.0015769340437962365\n",
      "Gradient Descent(29/99): loss=0.3903265663836246, gradient=0.0015369181800176143\n",
      "Gradient Descent(30/99): loss=0.39032634429359236, gradient=0.0014985440367640901\n",
      "Gradient Descent(31/99): loss=0.39032613294398844, gradient=0.001461697212627407\n",
      "Gradient Descent(32/99): loss=0.39032593167057056, gradient=0.0014262768203274086\n",
      "Gradient Descent(33/99): loss=0.3903257398627184, gradient=0.0013921934112148958\n",
      "Gradient Descent(34/99): loss=0.39032555695746884, gradient=0.0013593672559576696\n",
      "Gradient Descent(35/99): loss=0.39032538243442966, gradient=0.0013277269158761436\n",
      "Gradient Descent(36/99): loss=0.39032521581141255, gradient=0.001297208052178618\n",
      "Gradient Descent(37/99): loss=0.39032505664066597, gradient=0.0012677524304869558\n",
      "Gradient Descent(38/99): loss=0.39032490450560675, gradient=0.0012393070861274672\n",
      "Gradient Descent(39/99): loss=0.390324759017966, gradient=0.0012118236221280978\n",
      "Gradient Descent(40/99): loss=0.3903246198152906, gradient=0.0011852576170572266\n",
      "Gradient Descent(41/99): loss=0.39032448655873775, gradient=0.0011595681240204009\n",
      "Gradient Descent(42/99): loss=0.39032435893112455, gradient=0.0011347172455108462\n",
      "Gradient Descent(43/99): loss=0.3903242366351901, gradient=0.0011106697715449615\n",
      "Gradient Descent(44/99): loss=0.39032411939204614, gradient=0.0010873928707365419\n",
      "Gradient Descent(45/99): loss=0.3903240069397848, gradient=0.0010648558257706784\n",
      "Gradient Descent(46/99): loss=0.3903238990322249, gradient=0.0010430298062147647\n",
      "Gradient Descent(47/99): loss=0.3903237954377792, gradient=0.0010218876728086393\n",
      "Gradient Descent(48/99): loss=0.3903236959384265, gradient=0.001001403808366205\n",
      "Gradient Descent(49/99): loss=0.39032360032877655, gradient=0.0009815539712312129\n",
      "Gradient Descent(50/99): loss=0.39032350841521585, gradient=0.0009623151678984715\n",
      "Gradient Descent(51/99): loss=0.3903234200151265, gradient=0.0009436655419637192\n",
      "Gradient Descent(52/99): loss=0.39032333495616883, gradient=0.0009255842770198497\n",
      "Gradient Descent(53/99): loss=0.3903232530756215, gradient=0.0009080515114960158\n",
      "Gradient Descent(54/99): loss=0.3903231742197734, gradient=0.0008910482637480846\n",
      "Gradient Descent(55/99): loss=0.39032309824336026, gradient=0.0008745563659716769\n",
      "Gradient Descent(56/99): loss=0.3903230250090471, gradient=0.0008585584057249712\n",
      "Gradient Descent(57/99): loss=0.39032295438694276, gradient=0.0008430376740314033\n",
      "Gradient Descent(58/99): loss=0.3903228862541532, gradient=0.0008279781191845249\n",
      "Gradient Descent(59/99): loss=0.3903228204943643, gradient=0.0008133643055049911\n",
      "Gradient Descent(60/99): loss=0.3903227569974539, gradient=0.000799181376408133\n",
      "Gradient Descent(61/99): loss=0.39032269565912864, gradient=0.0007854150212307462\n",
      "Gradient Descent(62/99): loss=0.3903226363805871, gradient=0.0007720514453431258\n",
      "Gradient Descent(63/99): loss=0.3903225790682024, gradient=0.0007590773431376092\n",
      "Gradient Descent(64/99): loss=0.39032252363322684, gradient=0.000746479873539748\n",
      "Gradient Descent(65/99): loss=0.3903224699915156, gradient=0.0007342466377365156\n",
      "Gradient Descent(66/99): loss=0.3903224180632656, gradient=0.0007223656588550902\n",
      "Gradient Descent(67/99): loss=0.3903223677727736, gradient=0.0007108253633613327\n",
      "Gradient Descent(68/99): loss=0.39032231904820613, gradient=0.0006996145639761446\n",
      "Gradient Descent(69/99): loss=0.3903222718213862, gradient=0.0006887224439337129\n",
      "Gradient Descent(70/99): loss=0.3903222260275896, gradient=0.0006781385424274458\n",
      "Gradient Descent(71/99): loss=0.390322181605357, gradient=0.000667852741108932\n",
      "Gradient Descent(72/99): loss=0.39032213849631453, gradient=0.000657855251521106\n",
      "Gradient Descent(73/99): loss=0.3903220966450054, gradient=0.0006481366033619956\n",
      "Gradient Descent(74/99): loss=0.3903220559987324, gradient=0.0006386876334874073\n",
      "Gradient Descent(75/99): loss=0.390322016507408, gradient=0.0006294994755718434\n",
      "Gradient Descent(76/99): loss=0.3903219781234145, gradient=0.0006205635503571645\n",
      "Gradient Descent(77/99): loss=0.39032194080147103, gradient=0.0006118715564261782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(78/99): loss=0.390321904498509, gradient=0.0006034154614462447\n",
      "Gradient Descent(79/99): loss=0.39032186917355427, gradient=0.0005951874938343961\n",
      "Gradient Descent(80/99): loss=0.3903218347876151, gradient=0.0005871801348012821\n",
      "Gradient Descent(81/99): loss=0.39032180130357835, gradient=0.0005793861107361066\n",
      "Gradient Descent(82/99): loss=0.39032176868610985, gradient=0.000571798385899772\n",
      "Gradient Descent(83/99): loss=0.39032173690156063, gradient=0.0005644101553968314\n",
      "Gradient Descent(84/99): loss=0.39032170591787874, gradient=0.0005572148384009039\n",
      "Gradient Descent(85/99): loss=0.3903216757045254, gradient=0.0005502060716111242\n",
      "Gradient Descent(86/99): loss=0.39032164623239723, gradient=0.0005433777029198871\n",
      "Gradient Descent(87/99): loss=0.3903216174737504, gradient=0.0005367237852750994\n",
      "Gradient Descent(88/99): loss=0.3903215894021302, gradient=0.0005302385707217032\n",
      "Gradient Descent(89/99): loss=0.3903215619923053, gradient=0.0005239165046097269\n",
      "Gradient Descent(90/99): loss=0.39032153522020424, gradient=0.0005177522199575783\n",
      "Gradient Descent(91/99): loss=0.3903215090628557, gradient=0.0005117405319608969\n",
      "Gradient Descent(92/99): loss=0.39032148349833257, gradient=0.000505876432638548\n",
      "Gradient Descent(93/99): loss=0.39032145850569777, gradient=0.0005001550856086592\n",
      "Gradient Descent(94/99): loss=0.3903214340649553, gradient=0.0004945718209885714\n",
      "Gradient Descent(95/99): loss=0.39032141015700095, gradient=0.000489122130413278\n",
      "Gradient Descent(96/99): loss=0.3903213867635781, gradient=0.0004838016621682706\n",
      "Gradient Descent(97/99): loss=0.39032136386723504, gradient=0.0004786062164325844\n",
      "Gradient Descent(98/99): loss=0.3903213414512835, gradient=0.0004735317406291282\n",
      "Gradient Descent(99/99): loss=0.3903213194997615, gradient=0.00046857432487954447\n",
      "Gradient Descent(0/99): loss=0.38997390627505135, gradient=0.010006540519616537\n",
      "Gradient Descent(1/99): loss=0.38996767508915275, gradient=0.008242078894112739\n",
      "Gradient Descent(2/99): loss=0.38996326362407074, gradient=0.0069074647332945\n",
      "Gradient Descent(3/99): loss=0.3899600288869671, gradient=0.0058907283887458676\n",
      "Gradient Descent(4/99): loss=0.3899575717964645, gradient=0.005113354531785144\n",
      "Gradient Descent(5/99): loss=0.38995564176352593, gradient=0.004514412760289363\n",
      "Gradient Descent(6/99): loss=0.38995407843658175, gradient=0.004049233681270339\n",
      "Gradient Descent(7/99): loss=0.3899527780166253, gradient=0.003682361800884908\n",
      "Gradient Descent(8/99): loss=0.38995167180550494, gradient=0.0033880487908644445\n",
      "Gradient Descent(9/99): loss=0.3899507133100353, gradient=0.0031474769205512075\n",
      "Gradient Descent(10/99): loss=0.389949870290437, gradient=0.0029470398286122928\n",
      "Gradient Descent(11/99): loss=0.38994911981155683, gradient=0.002776954598933279\n",
      "Gradient Descent(12/99): loss=0.38994844512873905, gradient=0.0026301888791626345\n",
      "Gradient Descent(13/99): loss=0.3899478337027043, gradient=0.002501662456325746\n",
      "Gradient Descent(14/99): loss=0.3899472759151689, gradient=0.002387670520159903\n",
      "Gradient Descent(15/99): loss=0.3899467642237946, gradient=0.0022854758632845093\n",
      "Gradient Descent(16/99): loss=0.3899462925958315, gradient=0.0021930244606179162\n",
      "Gradient Descent(17/99): loss=0.38994585612095434, gradient=0.002108748507009977\n",
      "Gradient Descent(18/99): loss=0.3899454507410748, gradient=0.002031430226753581\n",
      "Gradient Descent(19/99): loss=0.38994507313648896, gradient=0.0019596792635434047\n",
      "Gradient Descent(20/99): loss=0.3899447203357315, gradient=0.0018936172739398623\n",
      "Gradient Descent(21/99): loss=0.3899443898783097, gradient=0.001832143679426114\n",
      "Gradient Descent(22/99): loss=0.3899440796404417, gradient=0.0017747392355418727\n",
      "Gradient Descent(23/99): loss=0.38994378777490774, gradient=0.0017209698412390317\n",
      "Gradient Descent(24/99): loss=0.3899435126640641, gradient=0.0016704687439917764\n",
      "Gradient Descent(25/99): loss=0.38994325288267595, gradient=0.0016229231908573684\n",
      "Gradient Descent(26/99): loss=0.3899430072099205, gradient=0.001577789788968965\n",
      "Gradient Descent(27/99): loss=0.3899427744754176, gradient=0.0015353968573321077\n",
      "Gradient Descent(28/99): loss=0.38994255367411373, gradient=0.0014952549244174008\n",
      "Gradient Descent(29/99): loss=0.38994234390363114, gradient=0.001457185230282375\n",
      "Gradient Descent(30/99): loss=0.38994214435072994, gradient=0.0014210300113668398\n",
      "Gradient Descent(31/99): loss=0.3899419542799842, gradient=0.0013866492671082218\n",
      "Gradient Descent(32/99): loss=0.3899417730242374, gradient=0.0013539181148939966\n",
      "Gradient Descent(33/99): loss=0.3899415999765175, gradient=0.0013227246102677506\n",
      "Gradient Descent(34/99): loss=0.3899414345831396, gradient=0.0012929679388795873\n",
      "Gradient Descent(35/99): loss=0.38994127633779385, gradient=0.00126455690817094\n",
      "Gradient Descent(36/99): loss=0.3899411247764513, gradient=0.001237408682651709\n",
      "Gradient Descent(37/99): loss=0.3899409794729503, gradient=0.0012114477185129196\n",
      "Gradient Descent(38/99): loss=0.38994084003515805, gradient=0.0011866048623449194\n",
      "Gradient Descent(39/99): loss=0.38994070610161485, gradient=0.0011628165856737535\n",
      "Gradient Descent(40/99): loss=0.3899405773385891, gradient=0.0011400243324319013\n",
      "Gradient Descent(41/99): loss=0.3899404534374826, gradient=0.0011181739607273153\n",
      "Gradient Descent(42/99): loss=0.3899403341125362, gradient=0.0010972152636473645\n",
      "Gradient Descent(43/99): loss=0.3899402190987908, gradient=0.0010771015565303758\n",
      "Gradient Descent(44/99): loss=0.38994010815027286, gradient=0.0010577893203119013\n",
      "Gradient Descent(45/99): loss=0.38994000103837373, gradient=0.0010392378923149963\n",
      "Gradient Descent(46/99): loss=0.3899398975503944, gradient=0.001021409197291361\n",
      "Gradient Descent(47/99): loss=0.3899397974882402, gradient=0.0010042675126974206\n",
      "Gradient Descent(48/99): loss=0.3899397006672417, gradient=0.0009877792631603217\n",
      "Gradient Descent(49/99): loss=0.3899396069150914, gradient=0.0009719128398894302\n",
      "Gradient Descent(50/99): loss=0.3899395160708793, gradient=0.0009566384414543283\n",
      "Gradient Descent(51/99): loss=0.38993942798421727, gradient=0.000941927932904179\n",
      "Gradient Descent(52/99): loss=0.38993934251444384, gradient=0.0009277547206659757\n",
      "Gradient Descent(53/99): loss=0.3899392595298978, gradient=0.0009140936410454511\n",
      "Gradient Descent(54/99): loss=0.389939178907255, gradient=0.0009009208604817161\n",
      "Gradient Descent(55/99): loss=0.3899391005309225, gradient=0.0008882137859789631\n",
      "Gradient Descent(56/99): loss=0.3899390242924838, gradient=0.0008759509843708226\n",
      "Gradient Descent(57/99): loss=0.3899389500901876, gradient=0.0008641121092682858\n",
      "Gradient Descent(58/99): loss=0.3899388778284801, gradient=0.0008526778347082856\n",
      "Gradient Descent(59/99): loss=0.38993880741757336, gradient=0.000841629794660392\n",
      "Gradient Descent(60/99): loss=0.3899387387730491, gradient=0.0008309505276693966\n",
      "Gradient Descent(61/99): loss=0.3899386718154911, gradient=0.0008206234260132742\n",
      "Gradient Descent(62/99): loss=0.3899386064701484, gradient=0.0008106326888434563\n",
      "Gradient Descent(63/99): loss=0.3899385426666208, gradient=0.0008009632788482024\n",
      "Gradient Descent(64/99): loss=0.3899384803385708, gradient=0.0007916008820440241\n",
      "Gradient Descent(65/99): loss=0.38993841942345553, gradient=0.0007825318703539834\n",
      "Gradient Descent(66/99): loss=0.38993835986227837, gradient=0.000773743266678847\n",
      "Gradient Descent(67/99): loss=0.38993830159935805, gradient=0.0007652227122058783\n",
      "Gradient Descent(68/99): loss=0.38993824458211596, gradient=0.0007569584357359544\n",
      "Gradient Descent(69/99): loss=0.38993818876087605, gradient=0.0007489392248369121\n",
      "Gradient Descent(70/99): loss=0.3899381340886824, gradient=0.0007411543986579888\n",
      "Gradient Descent(71/99): loss=0.38993808052112455, gradient=0.0007335937822603566\n",
      "Gradient Descent(72/99): loss=0.389938028016181, gradient=0.0007262476823379306\n",
      "Gradient Descent(73/99): loss=0.3899379765340676, gradient=0.0007191068642183337\n",
      "Gradient Descent(74/99): loss=0.38993792603710037, gradient=0.0007121625300467479\n",
      "Gradient Descent(75/99): loss=0.3899378764895664, gradient=0.0007054062980684339\n",
      "Gradient Descent(76/99): loss=0.3899378278576025, gradient=0.0006988301829334509\n",
      "Gradient Descent(77/99): loss=0.38993778010908314, gradient=0.0006924265769580321\n",
      "Gradient Descent(78/99): loss=0.38993773321351505, gradient=0.0006861882322824331\n",
      "Gradient Descent(79/99): loss=0.38993768714194066, gradient=0.0006801082438718586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(80/99): loss=0.38993764186684393, gradient=0.0006741800333132166\n",
      "Gradient Descent(81/99): loss=0.389937597362068, gradient=0.0006683973333637277\n",
      "Gradient Descent(82/99): loss=0.38993755360273147, gradient=0.0006627541732121619\n",
      "Gradient Descent(83/99): loss=0.38993751056515674, gradient=0.0006572448644168527\n",
      "Gradient Descent(84/99): loss=0.3899374682267983, gradient=0.0006518639874868226\n",
      "Gradient Descent(85/99): loss=0.3899374265661771, gradient=0.0006466063790759982\n",
      "Gradient Descent(86/99): loss=0.38993738556281976, gradient=0.0006414671197616381\n",
      "Gradient Descent(87/99): loss=0.38993734519720114, gradient=0.0006364415223804389\n",
      "Gradient Descent(88/99): loss=0.3899373054506897, gradient=0.0006315251208977323\n",
      "Gradient Descent(89/99): loss=0.3899372663054976, gradient=0.0006267136597857105\n",
      "Gradient Descent(90/99): loss=0.3899372277446331, gradient=0.0006220030838892609\n",
      "Gradient Descent(91/99): loss=0.3899371897518564, gradient=0.0006173895287576918\n",
      "Gradient Descent(92/99): loss=0.3899371523116378, gradient=0.000612869311422908\n",
      "Gradient Descent(93/99): loss=0.38993711540911846, gradient=0.0006084389216043379\n",
      "Gradient Descent(94/99): loss=0.389937079030074, gradient=0.0006040950133233303\n",
      "Gradient Descent(95/99): loss=0.3899370431608799, gradient=0.0005998343969084356\n",
      "Gradient Descent(96/99): loss=0.3899370077884793, gradient=0.0005956540313757699\n",
      "Gradient Descent(97/99): loss=0.3899369729003519, gradient=0.0005915510171680976\n",
      "Gradient Descent(98/99): loss=0.3899369384844872, gradient=0.0005875225892368627\n",
      "Gradient Descent(99/99): loss=0.3899369045293546, gradient=0.0005835661104529106\n",
      "Gradient Descent(0/99): loss=0.3896741485301931, gradient=0.01107896318029038\n",
      "Gradient Descent(1/99): loss=0.3896691674264632, gradient=0.007442874499058552\n",
      "Gradient Descent(2/99): loss=0.3896656344234333, gradient=0.006179905512096388\n",
      "Gradient Descent(3/99): loss=0.38966303921175716, gradient=0.00528036177565367\n",
      "Gradient Descent(4/99): loss=0.3896610940182032, gradient=0.004561085600417601\n",
      "Gradient Descent(5/99): loss=0.38965960777464603, gradient=0.003977758619585912\n",
      "Gradient Descent(6/99): loss=0.38965845030434565, gradient=0.0035026482527044265\n",
      "Gradient Descent(7/99): loss=0.3896575322083768, gradient=0.003112960480956085\n",
      "Gradient Descent(8/99): loss=0.3896567912600544, gradient=0.0027910461961622337\n",
      "Gradient Descent(9/99): loss=0.3896561835862608, gradient=0.0025230001662082304\n",
      "Gradient Descent(10/99): loss=0.38965567782694366, gradient=0.0022979085648747134\n",
      "Gradient Descent(11/99): loss=0.3896552512405554, gradient=0.0021072326938235317\n",
      "Gradient Descent(12/99): loss=0.3896548870882098, gradient=0.0019443049643083142\n",
      "Gradient Descent(13/99): loss=0.3896545728624363, gradient=0.0018039201404848797\n",
      "Gradient Descent(14/99): loss=0.3896542990771226, gradient=0.001682007705947482\n",
      "Gradient Descent(15/99): loss=0.38965405843273, gradient=0.0015753726243255982\n",
      "Gradient Descent(16/99): loss=0.3896538452342559, gradient=0.0014814927714473373\n",
      "Gradient Descent(17/99): loss=0.38965365498075655, gradient=0.001398362401469528\n",
      "Gradient Descent(18/99): loss=0.38965348407232797, gradient=0.0013243722866319414\n",
      "Gradient Descent(19/99): loss=0.3896533295982621, gradient=0.0012582185694133177\n",
      "Gradient Descent(20/99): loss=0.3896531891818812, gradient=0.001198833765186077\n",
      "Gradient Descent(21/99): loss=0.38965306086537027, gradient=0.001145334650150647\n",
      "Gradient Descent(22/99): loss=0.38965294302317793, gradient=0.0010969829019668578\n",
      "Gradient Descent(23/99): loss=0.3896528342960554, gradient=0.001053155306373065\n",
      "Gradient Descent(24/99): loss=0.38965273354020086, gradient=0.001013321106528218\n",
      "Gradient Descent(25/99): loss=0.38965263978759346, gradient=0.0009770246722580105\n",
      "Gradient Descent(26/99): loss=0.3896525522147213, gradient=0.0009438721294032208\n",
      "Gradient Descent(27/99): loss=0.38965247011768184, gradient=0.0009135209411472381\n",
      "Gradient Descent(28/99): loss=0.38965239289217934, gradient=0.000885671697213762\n",
      "Gradient Descent(29/99): loss=0.3896523200173232, gradient=0.0008600615632340787\n",
      "Gradient Descent(30/99): loss=0.38965225104240997, gradient=0.0008364589876575884\n",
      "Gradient Descent(31/99): loss=0.38965218557606873, gradient=0.0008146593701333846\n",
      "Gradient Descent(32/99): loss=0.3896521232772928, gradient=0.0007944814732091125\n",
      "Gradient Descent(33/99): loss=0.38965206384799406, gradient=0.0007757644159783783\n",
      "Gradient Descent(34/99): loss=0.38965200702678704, gradient=0.0007583651296023555\n",
      "Gradient Descent(35/99): loss=0.38965195258378094, gradient=0.0007421561846249704\n",
      "Gradient Descent(36/99): loss=0.3896519003161957, gradient=0.0007270239217977123\n",
      "Gradient Descent(37/99): loss=0.3896518500446606, gradient=0.0007128668339993799\n",
      "Gradient Descent(38/99): loss=0.38965180161007557, gradient=0.0006995941584395879\n",
      "Gradient Descent(39/99): loss=0.3896517548709431, gradient=0.0006871246468743987\n",
      "Gradient Descent(40/99): loss=0.38965170970109225, gradient=0.0006753854879023096\n",
      "Gradient Descent(41/99): loss=0.3896516659877321, gradient=0.000664311360176694\n",
      "Gradient Descent(42/99): loss=0.38965162362978145, gradient=0.0006538435990033961\n",
      "Gradient Descent(43/99): loss=0.3896515825364319, gradient=0.0006439294616105673\n",
      "Gradient Descent(44/99): loss=0.38965154262591056, gradient=0.0006345214786021314\n",
      "Gradient Descent(45/99): loss=0.38965150382440683, gradient=0.0006255768808944631\n",
      "Gradient Descent(46/99): loss=0.3896514660651461, gradient=0.0006170570928993606\n",
      "Gradient Descent(47/99): loss=0.389651429287582, gradient=0.0006089272839340249\n",
      "Gradient Descent(48/99): loss=0.3896513934366953, gradient=0.0006011559708659132\n",
      "Gradient Descent(49/99): loss=0.38965135846238014, gradient=0.0005937146658767393\n",
      "Gradient Descent(50/99): loss=0.38965132431890803, gradient=0.0005865775639854053\n",
      "Gradient Descent(51/99): loss=0.3896512909644586, gradient=0.0005797212656247804\n",
      "Gradient Descent(52/99): loss=0.38965125836070574, gradient=0.0005731245301384835\n",
      "Gradient Descent(53/99): loss=0.3896512264724556, gradient=0.0005667680565640697\n",
      "Gradient Descent(54/99): loss=0.3896511952673244, gradient=0.0005606342885074376\n",
      "Gradient Descent(55/99): loss=0.3896511647154566, gradient=0.0005547072402982019\n",
      "Gradient Descent(56/99): loss=0.38965113478927316, gradient=0.000548972341954758\n",
      "Gradient Descent(57/99): loss=0.3896511054632496, gradient=0.0005434163007846315\n",
      "Gradient Descent(58/99): loss=0.38965107671371807, gradient=0.0005380269777079089\n",
      "Gradient Descent(59/99): loss=0.38965104851869076, gradient=0.0005327932766206949\n",
      "Gradient Descent(60/99): loss=0.38965102085770464, gradient=0.0005277050453179242\n",
      "Gradient Descent(61/99): loss=0.3896509937116793, gradient=0.0005227529866715655\n",
      "Gradient Descent(62/99): loss=0.3896509670627927, gradient=0.0005179285789161836\n",
      "Gradient Descent(63/99): loss=0.38965094089436847, gradient=0.0005132240040303914\n",
      "Gradient Descent(64/99): loss=0.38965091519077555, gradient=0.000508632083321829\n",
      "Gradient Descent(65/99): loss=0.38965088993733676, gradient=0.0005041462194292528\n",
      "Gradient Descent(66/99): loss=0.38965086512024827, gradient=0.0004997603440468324\n",
      "Gradient Descent(67/99): loss=0.3896508407265054, gradient=0.0004954688707566689\n",
      "Gradient Descent(68/99): loss=0.3896508167438367, gradient=0.0004912666524271797\n",
      "Gradient Descent(69/99): loss=0.3896507931606442, gradient=0.0004871489426960292\n",
      "Gradient Descent(70/99): loss=0.3896507699659494, gradient=0.0004831113611126003\n",
      "Gradient Descent(71/99): loss=0.3896507471493439, gradient=0.00047914986156136696\n",
      "Gradient Descent(72/99): loss=0.38965072470094486, gradient=0.0004752607036318327\n",
      "Gradient Descent(73/99): loss=0.3896507026113557, gradient=0.00047144042663561925\n",
      "Gradient Descent(74/99): loss=0.3896506808716283, gradient=0.00046768582600627864\n",
      "Gradient Descent(75/99): loss=0.3896506594732298, gradient=0.00046399393184473153\n",
      "Gradient Descent(76/99): loss=0.3896506384080139, gradient=0.0004603619893998544\n",
      "Gradient Descent(77/99): loss=0.38965061766819054, gradient=0.00045678744129517456\n",
      "Gradient Descent(78/99): loss=0.3896505972463032, gradient=0.0004532679113337911\n",
      "Gradient Descent(79/99): loss=0.3896505771352043, gradient=0.0004498011897303244\n",
      "Gradient Descent(80/99): loss=0.3896505573280349, gradient=0.000446385219634745\n",
      "Gradient Descent(81/99): loss=0.38965053781820513, gradient=0.000443018084827079\n",
      "Gradient Descent(82/99): loss=0.3896505185993767, gradient=0.0004396979984734089\n",
      "Gradient Descent(83/99): loss=0.38965049966544685, gradient=0.00043642329284589734\n",
      "Gradient Descent(84/99): loss=0.3896504810105331, gradient=0.00043319240991793266\n",
      "Gradient Descent(85/99): loss=0.38965046262896014, gradient=0.0004300038927556539\n",
      "Gradient Descent(86/99): loss=0.38965044451524705, gradient=0.0004268563776331834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(87/99): loss=0.3896504266640957, gradient=0.00042374858680758646\n",
      "Gradient Descent(88/99): loss=0.3896504090703806, gradient=0.000420679321894828\n",
      "Gradient Descent(89/99): loss=0.3896503917291384, gradient=0.00041764745779323696\n",
      "Gradient Descent(90/99): loss=0.38965037463555946, gradient=0.0004146519371072637\n",
      "Gradient Descent(91/99): loss=0.38965035778497936, gradient=0.0004116917650275334\n",
      "Gradient Descent(92/99): loss=0.38965034117287134, gradient=0.0004087660046277962\n",
      "Gradient Descent(93/99): loss=0.38965032479483835, gradient=0.0004058737725432566\n",
      "Gradient Descent(94/99): loss=0.3896503086466078, gradient=0.00040301423499704053\n",
      "Gradient Descent(95/99): loss=0.3896502927240242, gradient=0.00040018660414639553\n",
      "Gradient Descent(96/99): loss=0.3896502770230447, gradient=0.00039739013472016446\n",
      "Gradient Descent(97/99): loss=0.38965026153973215, gradient=0.00039462412092436297\n",
      "Gradient Descent(98/99): loss=0.3896502462702522, gradient=0.00039188789359262773\n",
      "Gradient Descent(99/99): loss=0.38965023121086667, gradient=0.0003891808175614234\n",
      "Gradient Descent(0/99): loss=0.38895996457726695, gradient=0.008184027434691928\n",
      "Gradient Descent(1/99): loss=0.38895720194877975, gradient=0.005551424998774576\n",
      "Gradient Descent(2/99): loss=0.3889552832223241, gradient=0.004564865685462051\n",
      "Gradient Descent(3/99): loss=0.3889539014394965, gradient=0.003860515033188339\n",
      "Gradient Descent(4/99): loss=0.38895288139098805, gradient=0.003307166243652531\n",
      "Gradient Descent(5/99): loss=0.3889521083575949, gradient=0.0028699395186918128\n",
      "Gradient Descent(6/99): loss=0.3889515059675544, gradient=0.0025250758808304293\n",
      "Gradient Descent(7/99): loss=0.3889510229836284, gradient=0.0022534847586266915\n",
      "Gradient Descent(8/99): loss=0.3889506247807303, gradient=0.002039606901092418\n",
      "Gradient Descent(9/99): loss=0.38895028776928325, gradient=0.0018708067150035844\n",
      "Gradient Descent(10/99): loss=0.3889499957327177, gradient=0.0017369207844576393\n",
      "Gradient Descent(11/99): loss=0.38894973741032174, gradient=0.0016298819921170163\n",
      "Gradient Descent(12/99): loss=0.38894950489471963, gradient=0.0015433813051776667\n",
      "Gradient Descent(13/99): loss=0.38894929256407706, gradient=0.0014725546990292553\n",
      "Gradient Descent(14/99): loss=0.38894909636681707, gradient=0.0014136978119959655\n",
      "Gradient Descent(15/99): loss=0.38894891333970705, gradient=0.001364015463429529\n",
      "Gradient Descent(16/99): loss=0.38894874128115126, gradient=0.0013214109363209727\n",
      "Gradient Descent(17/99): loss=0.3889485785281996, gradient=0.0012843154844946784\n",
      "Gradient Descent(18/99): loss=0.38894842380321937, gradient=0.0012515547623317019\n",
      "Gradient Descent(19/99): loss=0.38894827610761873, gradient=0.001222246773037429\n",
      "Gradient Descent(20/99): loss=0.388948134647536, gradient=0.0011957253327130428\n",
      "Gradient Descent(21/99): loss=0.3889479987813854, gradient=0.001171483413952826\n",
      "Gradient Descent(22/99): loss=0.3889478679824545, gradient=0.0011491315591786106\n",
      "Gradient Descent(23/99): loss=0.3889477418119403, gradient=0.0011283674999472207\n",
      "Gradient Descent(24/99): loss=0.3889476198992884, gradient=0.0011089540005732087\n",
      "Gradient Descent(25/99): loss=0.38894750192768207, gradient=0.0010907026869961482\n",
      "Gradient Descent(26/99): loss=0.3889473876232037, gradient=0.0010734622103740648\n",
      "Gradient Descent(27/99): loss=0.388947276746631, gradient=0.0010571095437690065\n",
      "Gradient Descent(28/99): loss=0.3889471690871534, gradient=0.0010415435440479875\n",
      "Gradient Descent(29/99): loss=0.38894706445749816, gradient=0.001026680155082114\n",
      "Gradient Descent(30/99): loss=0.3889469626901005, gradient=0.0010124488046322987\n",
      "Gradient Descent(31/99): loss=0.3889468636340646, gradient=0.0009987896737975825\n",
      "Gradient Descent(32/99): loss=0.3889467671527179, gradient=0.0009856516082562305\n",
      "Gradient Descent(33/99): loss=0.38894667312162884, gradient=0.0009729905049453499\n",
      "Gradient Descent(34/99): loss=0.388946581426979, gradient=0.0009607680537373972\n",
      "Gradient Descent(35/99): loss=0.38894649196421727, gradient=0.0009489507464421996\n",
      "Gradient Descent(36/99): loss=0.3889464046369351, gradient=0.0009375090889108007\n",
      "Gradient Descent(37/99): loss=0.38894631935592106, gradient=0.0009264169688599022\n",
      "Gradient Descent(38/99): loss=0.388946236038358, gradient=0.0009156511441888214\n",
      "Gradient Descent(39/99): loss=0.38894615464338195, gradient=0.0009047716071521338\n",
      "Gradient Descent(40/99): loss=0.38894607505801, gradient=0.0008946304081731649\n",
      "Gradient Descent(41/99): loss=0.38894599721535866, gradient=0.0008847567250266795\n",
      "Gradient Descent(42/99): loss=0.38894592105267145, gradient=0.0008751354788787257\n",
      "Gradient Descent(43/99): loss=0.3889458465109346, gradient=0.0008657529611939975\n",
      "Gradient Descent(44/99): loss=0.38894577353454474, gradient=0.000856596667885189\n",
      "Gradient Descent(45/99): loss=0.3889457020710134, gradient=0.0008476551581734763\n",
      "Gradient Descent(46/99): loss=0.38894563207070504, gradient=0.0008389179338630093\n",
      "Gradient Descent(47/99): loss=0.38894556348660764, gradient=0.000830375335582632\n",
      "Gradient Descent(48/99): loss=0.3889454962741231, gradient=0.0008220184532018241\n",
      "Gradient Descent(49/99): loss=0.3889454303908836, gradient=0.000813839048142509\n",
      "Gradient Descent(50/99): loss=0.38894536579658506, gradient=0.0008058294857210301\n",
      "Gradient Descent(51/99): loss=0.38894530245283604, gradient=0.0007979826759828406\n",
      "Gradient Descent(52/99): loss=0.38894524032302447, gradient=0.0007902920217588713\n",
      "Gradient Descent(53/99): loss=0.3889451793721929, gradient=0.0007827513728880824\n",
      "Gradient Descent(54/99): loss=0.38894511956692823, gradient=0.0007753549857258192\n",
      "Gradient Descent(55/99): loss=0.3889450608752605, gradient=0.0007680974872016506\n",
      "Gradient Descent(56/99): loss=0.38894500326656917, gradient=0.0007609738428075487\n",
      "Gradient Descent(57/99): loss=0.3889449467114998, gradient=0.0007539793279957198\n",
      "Gradient Descent(58/99): loss=0.38894489118188563, gradient=0.0007471095025442754\n",
      "Gradient Descent(59/99): loss=0.3889448366506771, gradient=0.0007403601875169911\n",
      "Gradient Descent(60/99): loss=0.3889447830918756, gradient=0.0007337274444980516\n",
      "Gradient Descent(61/99): loss=0.388944730480474, gradient=0.0007272075568295182\n",
      "Gradient Descent(62/99): loss=0.3889446787924004, gradient=0.0007207970126176842\n",
      "Gradient Descent(63/99): loss=0.3889446280044667, gradient=0.0007144924893074946\n",
      "Gradient Descent(64/99): loss=0.3889445780943223, gradient=0.0007082908396509524\n",
      "Gradient Descent(65/99): loss=0.38894452904040755, gradient=0.0007021890789193132\n",
      "Gradient Descent(66/99): loss=0.3889444808219151, gradient=0.0006961843732278594\n",
      "Gradient Descent(67/99): loss=0.3889444334187505, gradient=0.0006902740288591323\n",
      "Gradient Descent(68/99): loss=0.38894438681149734, gradient=0.0006844554824840417\n",
      "Gradient Descent(69/99): loss=0.38894434098138414, gradient=0.0006787262921933223\n",
      "Gradient Descent(70/99): loss=0.3889442959102532, gradient=0.0006730841292610811\n",
      "Gradient Descent(71/99): loss=0.3889442515805325, gradient=0.0006675267705719633\n",
      "Gradient Descent(72/99): loss=0.38894420797520807, gradient=0.0006620520916507624\n",
      "Gradient Descent(73/99): loss=0.3889441650777993, gradient=0.0006566580602399227\n",
      "Gradient Descent(74/99): loss=0.3889441228723348, gradient=0.0006513427303765517\n",
      "Gradient Descent(75/99): loss=0.38894408134333125, gradient=0.000646104236924845\n",
      "Gradient Descent(76/99): loss=0.3889440404757713, gradient=0.0006409407905254043\n",
      "Gradient Descent(77/99): loss=0.38894400025508447, gradient=0.0006358506729254832\n",
      "Gradient Descent(78/99): loss=0.3889439606671291, gradient=0.0006308322326587248\n",
      "Gradient Descent(79/99): loss=0.38894392169817477, gradient=0.0006258838810453167\n",
      "Gradient Descent(80/99): loss=0.3889438833348854, gradient=0.0006210040884864397\n",
      "Gradient Descent(81/99): loss=0.3889438455643043, gradient=0.000616191381029061\n",
      "Gradient Descent(82/99): loss=0.3889438083738394, gradient=0.0006114443371798185\n",
      "Gradient Descent(83/99): loss=0.3889437717512492, gradient=0.0006067615849476403\n",
      "Gradient Descent(84/99): loss=0.3889437356846302, gradient=0.0006021417990974583\n",
      "Gradient Descent(85/99): loss=0.38894370016240354, gradient=0.0005975836985985345\n",
      "Gradient Descent(86/99): loss=0.3889436651733038, gradient=0.0005930860442518399\n",
      "Gradient Descent(87/99): loss=0.38894363070636806, gradient=0.0005886476364833597\n",
      "Gradient Descent(88/99): loss=0.3889435967509245, gradient=0.0005842673132898576\n",
      "Gradient Descent(89/99): loss=0.3889435632965826, gradient=0.0005799439483258765\n",
      "Gradient Descent(90/99): loss=0.38894353033322365, gradient=0.0005756764491210694\n",
      "Gradient Descent(91/99): loss=0.3889434978509912, gradient=0.0005714637554182101\n",
      "Gradient Descent(92/99): loss=0.3889434658402822, gradient=0.0005673048376224489\n",
      "Gradient Descent(93/99): loss=0.3889434342917398, gradient=0.0005631986953537234\n",
      "Gradient Descent(94/99): loss=0.38894340319624315, gradient=0.0005591443560944405\n",
      "Gradient Descent(95/99): loss=0.38894337254490163, gradient=0.00055514087392554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(96/99): loss=0.3889433423290468, gradient=0.0005511873283440669\n",
      "Gradient Descent(97/99): loss=0.3889433125402252, gradient=0.0005472828231566721\n",
      "Gradient Descent(98/99): loss=0.3889432831701921, gradient=0.0005434264854427289\n",
      "Gradient Descent(99/99): loss=0.3889432542109047, gradient=0.0005396174645829688\n",
      "Gradient Descent(0/99): loss=0.3901419870163888, gradient=0.015399757359299382\n",
      "Gradient Descent(1/99): loss=0.3901380659732101, gradient=0.006919248066820864\n",
      "Gradient Descent(2/99): loss=0.39013541480812836, gradient=0.005360116349694952\n",
      "Gradient Descent(3/99): loss=0.390133420564092, gradient=0.0046153549908787865\n",
      "Gradient Descent(4/99): loss=0.3901318777163904, gradient=0.004048131340607501\n",
      "Gradient Descent(5/99): loss=0.39013065807256486, gradient=0.003590531079220683\n",
      "Gradient Descent(6/99): loss=0.3901296741775617, gradient=0.003217526643577072\n",
      "Gradient Descent(7/99): loss=0.3901288652037449, gradient=0.002911278286010087\n",
      "Gradient Descent(8/99): loss=0.3901281882120839, gradient=0.002657956189991655\n",
      "Gradient Descent(9/99): loss=0.39012761245458594, gradient=0.0024467594745337575\n",
      "Gradient Descent(10/99): loss=0.39012711559382557, gradient=0.002269240456461437\n",
      "Gradient Descent(11/99): loss=0.3901266811690306, gradient=0.002118782274463076\n",
      "Gradient Descent(12/99): loss=0.3901262968781012, gradient=0.0019901913656984608\n",
      "Gradient Descent(13/99): loss=0.3901259533997361, gradient=0.0018793798167209786\n",
      "Gradient Descent(14/99): loss=0.3901256435765414, gradient=0.001783118365338328\n",
      "Gradient Descent(15/99): loss=0.3901253618419289, gradient=0.0016988448742823155\n",
      "Gradient Descent(16/99): loss=0.39012510381338206, gradient=0.0016245162039627227\n",
      "Gradient Descent(17/99): loss=0.3901248660004386, gradient=0.0015584938800773586\n",
      "Gradient Descent(18/99): loss=0.3901246455926267, gradient=0.0014994559478987706\n",
      "Gradient Descent(19/99): loss=0.39012444030370513, gradient=0.001446329026367793\n",
      "Gradient Descent(20/99): loss=0.39012424825597747, gradient=0.0013982358862436035\n",
      "Gradient Descent(21/99): loss=0.39012406789342835, gradient=0.001354454927105163\n",
      "Gradient Descent(22/99): loss=0.39012389791580276, gradient=0.0013143887605875284\n",
      "Gradient Descent(23/99): loss=0.39012373722806376, gradient=0.0012775397599827936\n",
      "Gradient Descent(24/99): loss=0.3901235849012551, gradient=0.0012434909431090699\n",
      "Gradient Descent(25/99): loss=0.39012344014190875, gradient=0.0012118909455492626\n",
      "Gradient Descent(26/99): loss=0.3901233022679136, gradient=0.001182442139782542\n",
      "Gradient Descent(27/99): loss=0.39012317068932256, gradient=0.001154891182771494\n",
      "Gradient Descent(28/99): loss=0.3901230448929611, gradient=0.0011290214466408285\n",
      "Gradient Descent(29/99): loss=0.39012292442999275, gradient=0.0011046469171657651\n",
      "Gradient Descent(30/99): loss=0.3901228089058054, gradient=0.0010816072429875162\n",
      "Gradient Descent(31/99): loss=0.3901226979717286, gradient=0.0010597636925881596\n",
      "Gradient Descent(32/99): loss=0.390122591318211, gradient=0.0010389958320287619\n",
      "Gradient Descent(33/99): loss=0.3901224886691681, gradient=0.001019198778801938\n",
      "Gradient Descent(34/99): loss=0.3901223897772749, gradient=0.001000280919267323\n",
      "Gradient Descent(35/99): loss=0.3901222944200276, gradient=0.0009821620015856537\n",
      "Gradient Descent(36/99): loss=0.3901222023964313, gradient=0.0009647715347534842\n",
      "Gradient Descent(37/99): loss=0.3901221135242052, gradient=0.0009480474386928744\n",
      "Gradient Descent(38/99): loss=0.39012202763741155, gradient=0.0009319349014400668\n",
      "Gradient Descent(39/99): loss=0.39012194458443994, gradient=0.0009163854080940245\n",
      "Gradient Descent(40/99): loss=0.390121864226284, gradient=0.0009013559129269269\n",
      "Gradient Descent(41/99): loss=0.39012178643506745, gradient=0.0008868081313681792\n",
      "Gradient Descent(42/99): loss=0.3901217110927733, gradient=0.0008727079327837527\n",
      "Gradient Descent(43/99): loss=0.3901216380901503, gradient=0.0008590248183333736\n",
      "Gradient Descent(44/99): loss=0.39012156732576586, gradient=0.000845731470890743\n",
      "Gradient Descent(45/99): loss=0.3901214987051847, gradient=0.0008328033661977451\n",
      "Gradient Descent(46/99): loss=0.3901214321402535, gradient=0.0008202184362028941\n",
      "Gradient Descent(47/99): loss=0.39012136754847704, gradient=0.0008079567769908485\n",
      "Gradient Descent(48/99): loss=0.39012130485247143, gradient=0.0007960003949100919\n",
      "Gradient Descent(49/99): loss=0.39012124397948555, gradient=0.0007843329854962793\n",
      "Gradient Descent(50/99): loss=0.39012118486097797, gradient=0.000772939740615136\n",
      "Gradient Descent(51/99): loss=0.39012112743224675, gradient=0.0007618071799344821\n",
      "Gradient Descent(52/99): loss=0.39012107163209797, gradient=0.0007509230034133974\n",
      "Gradient Descent(53/99): loss=0.3901210174025551, gradient=0.0007402759619780804\n",
      "Gradient Descent(54/99): loss=0.3901209646885987, gradient=0.000729855743965197\n",
      "Gradient Descent(55/99): loss=0.3901209134379348, gradient=0.0007196528752555002\n",
      "Gradient Descent(56/99): loss=0.3901208636007882, gradient=0.0007096586313152677\n",
      "Gradient Descent(57/99): loss=0.3901208151297166, gradient=0.0006998649596101929\n",
      "Gradient Descent(58/99): loss=0.3901207679794441, gradient=0.0006902644110690888\n",
      "Gradient Descent(59/99): loss=0.390120722106711, gradient=0.0006808500794534001\n",
      "Gradient Descent(60/99): loss=0.3901206774701384, gradient=0.0006716155476453028\n",
      "Gradient Descent(61/99): loss=0.39012063403010555, gradient=0.0006625548399970332\n",
      "Gradient Descent(62/99): loss=0.39012059174863817, gradient=0.0006536623799989327\n",
      "Gradient Descent(63/99): loss=0.3901205505893077, gradient=0.0006449329526200673\n",
      "Gradient Descent(64/99): loss=0.39012051051713864, gradient=0.0006363616707603151\n",
      "Gradient Descent(65/99): loss=0.3901204714985244, gradient=0.0006279439453236908\n",
      "Gradient Descent(66/99): loss=0.3901204335011498, gradient=0.0006196754584859236\n",
      "Gradient Descent(67/99): loss=0.39012039649392066, gradient=0.0006115521397836505\n",
      "Gradient Descent(68/99): loss=0.39012036044689813, gradient=0.0006035701446979047\n",
      "Gradient Descent(69/99): loss=0.39012032533123836, gradient=0.000595725835446164\n",
      "Gradient Descent(70/99): loss=0.39012029111913754, gradient=0.0005880157637326877\n",
      "Gradient Descent(71/99): loss=0.39012025778378, gradient=0.0005804366552358415\n",
      "Gradient Descent(72/99): loss=0.3901202252992906, gradient=0.0005729853956395834\n",
      "Gradient Descent(73/99): loss=0.39012019364069, gradient=0.0005656590180380826\n",
      "Gradient Descent(74/99): loss=0.39012016278385386, gradient=0.0005584546915637206\n",
      "Gradient Descent(75/99): loss=0.39012013270547385, gradient=0.0005513697111052147\n",
      "Gradient Descent(76/99): loss=0.39012010338302133, gradient=0.0005444014879996451\n",
      "Gradient Descent(77/99): loss=0.39012007479471483, gradient=0.000537547541594118\n",
      "Gradient Descent(78/99): loss=0.3901200469194864, gradient=0.0005308054915858206\n",
      "Gradient Descent(79/99): loss=0.3901200197369533, gradient=0.0005241730510590069\n",
      "Gradient Descent(80/99): loss=0.3901199932273898, gradient=0.0005176480201473558\n",
      "Gradient Descent(81/99): loss=0.3901199673717, gradient=0.0005112282802569075\n",
      "Gradient Descent(82/99): loss=0.3901199421513935, gradient=0.0005049117887935731\n",
      "Gradient Descent(83/99): loss=0.390119917548562, gradient=0.0004986965743439927\n",
      "Gradient Descent(84/99): loss=0.3901198935458564, gradient=0.0004925807322650539\n",
      "Gradient Descent(85/99): loss=0.3901198701264658, gradient=0.0004865624206418056\n",
      "Gradient Descent(86/99): loss=0.39011984727409815, gradient=0.00048063985657750325\n",
      "Gradient Descent(87/99): loss=0.39011982497296016, gradient=0.0004748113127845746\n",
      "Gradient Descent(88/99): loss=0.39011980320773976, gradient=0.00046907511444701475\n",
      "Gradient Descent(89/99): loss=0.39011978196358915, gradient=0.0004634296363290852\n",
      "Gradient Descent(90/99): loss=0.3901197612261074, gradient=0.0004578733001070962\n",
      "Gradient Descent(91/99): loss=0.39011974098132574, gradient=0.0004524045719039521\n",
      "Gradient Descent(92/99): loss=0.3901197212156919, gradient=0.00044702196000749155\n",
      "Gradient Descent(93/99): loss=0.39011970191605655, gradient=0.0004417240127565236\n",
      "Gradient Descent(94/99): loss=0.39011968306965883, gradient=0.0004365093165792844\n",
      "Gradient Descent(95/99): loss=0.39011966466411396, gradient=0.00043137649417084864\n",
      "Gradient Descent(96/99): loss=0.39011964668739996, gradient=0.0004263242027976965\n",
      "Gradient Descent(97/99): loss=0.39011962912784653, gradient=0.0004213511327181278\n",
      "Gradient Descent(98/99): loss=0.3901196119741227, gradient=0.000416456005708803\n",
      "Gradient Descent(99/99): loss=0.39011959521522616, gradient=0.0004116375736883532\n",
      "Gradient Descent(0/99): loss=0.39035146073945465, gradient=0.007271374570252987\n",
      "Gradient Descent(1/99): loss=0.3903477794308801, gradient=0.00626096522819835\n",
      "Gradient Descent(2/99): loss=0.3903448877260615, gradient=0.005529283122629158\n",
      "Gradient Descent(3/99): loss=0.39034255690908226, gradient=0.004950019293736038\n",
      "Gradient Descent(4/99): loss=0.39034063519122625, gradient=0.004483324447216626\n",
      "Gradient Descent(5/99): loss=0.3903390188975432, gradient=0.004102505753460645\n",
      "Gradient Descent(6/99): loss=0.39033763577094277, gradient=0.003787730585655575\n",
      "Gradient Descent(7/99): loss=0.3903364344308976, gradient=0.0035241498857158065\n",
      "Gradient Descent(8/99): loss=0.39033537793669815, gradient=0.0032995211601973594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(9/99): loss=0.390334438313532, gradient=0.003107817822407203\n",
      "Gradient Descent(10/99): loss=0.390333594900877, gradient=0.0029411674017212268\n",
      "Gradient Descent(11/99): loss=0.39033283176108774, gradient=0.002795123833687075\n",
      "Gradient Descent(12/99): loss=0.3903321365396415, gradient=0.0026657164777025137\n",
      "Gradient Descent(13/99): loss=0.39033149947574003, gradient=0.0025500162860497945\n",
      "Gradient Descent(14/99): loss=0.3903309127511045, gradient=0.0024457180361497476\n",
      "Gradient Descent(15/99): loss=0.39033037002326804, gradient=0.0023509903166905382\n",
      "Gradient Descent(16/99): loss=0.3903298660859494, gradient=0.0022643651944983722\n",
      "Gradient Descent(17/99): loss=0.3903293969175409, gradient=0.0021832184241009954\n",
      "Gradient Descent(18/99): loss=0.39032895855502514, gradient=0.002109545954767774\n",
      "Gradient Descent(19/99): loss=0.39032854793682886, gradient=0.0020410246494304697\n",
      "Gradient Descent(20/99): loss=0.3903281624477395, gradient=0.0019770011997116963\n",
      "Gradient Descent(21/99): loss=0.3903277998347533, gradient=0.0019169331437703763\n",
      "Gradient Descent(22/99): loss=0.39032745814146036, gradient=0.0018603674229189006\n",
      "Gradient Descent(23/99): loss=0.3903271356563297, gradient=0.0018069235184318272\n",
      "Gradient Descent(24/99): loss=0.39032683091588416, gradient=0.0017560022864750338\n",
      "Gradient Descent(25/99): loss=0.3903265425271882, gradient=0.001707929242956455\n",
      "Gradient Descent(26/99): loss=0.3903262693004848, gradient=0.001662144131425752\n",
      "Gradient Descent(27/99): loss=0.390326010197687, gradient=0.0016182592565452173\n",
      "Gradient Descent(28/99): loss=0.3903257642234811, gradient=0.0015764921909844313\n",
      "Gradient Descent(29/99): loss=0.39032553050648244, gradient=0.0015364930880864033\n",
      "Gradient Descent(30/99): loss=0.3903253082539852, gradient=0.0014981320652403798\n",
      "Gradient Descent(31/99): loss=0.39032509674205407, gradient=0.0014612951935583504\n",
      "Gradient Descent(32/99): loss=0.39032489530722464, gradient=0.0014258819942966276\n",
      "Gradient Descent(33/99): loss=0.39032470333950375, gradient=0.001391803372016432\n",
      "Gradient Descent(34/99): loss=0.3903245202764324, gradient=0.0013589799028295546\n",
      "Gradient Descent(35/99): loss=0.39032434559801915, gradient=0.0013273404123093854\n",
      "Gradient Descent(36/99): loss=0.39032417882239157, gradient=0.0012968207904282768\n",
      "Gradient Descent(37/99): loss=0.3903240195020445, gradient=0.0012673630010152843\n",
      "Gradient Descent(38/99): loss=0.39032386722058154, gradient=0.0012389142513037153\n",
      "Gradient Descent(39/99): loss=0.3903237215898747, gradient=0.0012114262935966095\n",
      "Gradient Descent(40/99): loss=0.3903235822475709, gradient=0.001184854836261533\n",
      "Gradient Descent(41/99): loss=0.3903234488548952, gradient=0.0011591590454401244\n",
      "Gradient Descent(42/99): loss=0.3903233210947053, gradient=0.0011343011222286123\n",
      "Gradient Descent(43/99): loss=0.39032319866975984, gradient=0.001110245942813041\n",
      "Gradient Descent(44/99): loss=0.3903230813011716, gradient=0.0010869607512592405\n",
      "Gradient Descent(45/99): loss=0.39032296872701905, gradient=0.0010644148964587018\n",
      "Gradient Descent(46/99): loss=0.39032286070109584, gradient=0.0010425796062016755\n",
      "Gradient Descent(47/99): loss=0.3903227569917801, gradient=0.0010214277925506365\n",
      "Gradient Descent(48/99): loss=0.39032265738100785, gradient=0.0010009338836707185\n",
      "Gradient Descent(49/99): loss=0.3903225616633409, gradient=0.0009810736780827528\n",
      "Gradient Descent(50/99): loss=0.3903224696451123, gradient=0.0009618242179692475\n",
      "Gradient Descent(51/99): loss=0.3903223811436483, gradient=0.0009431636787119124\n",
      "Gradient Descent(52/99): loss=0.3903222959865494, gradient=0.0009250712722932513\n",
      "Gradient Descent(53/99): loss=0.3903222140110333, gradient=0.0009075271625697079\n",
      "Gradient Descent(54/99): loss=0.3903221350633266, gradient=0.0008905123907361829\n",
      "Gradient Descent(55/99): loss=0.3903220589981031, gradient=0.0008740088095601817\n",
      "Gradient Descent(56/99): loss=0.3903219856779634, gradient=0.0008579990251821572\n",
      "Gradient Descent(57/99): loss=0.3903219149729538, gradient=0.0008424663454566756\n",
      "Gradient Descent(58/99): loss=0.39032184676011733, gradient=0.0008273947339633699\n",
      "Gradient Descent(59/99): loss=0.39032178092307823, gradient=0.0008127687689417515\n",
      "Gradient Descent(60/99): loss=0.39032171735165244, gradient=0.0007985736065124057\n",
      "Gradient Descent(61/99): loss=0.3903216559414867, gradient=0.0007847949476366042\n",
      "Gradient Descent(62/99): loss=0.39032159659371984, gradient=0.0007714190083433388\n",
      "Gradient Descent(63/99): loss=0.3903215392146671, gradient=0.0007584324928173583\n",
      "Gradient Descent(64/99): loss=0.39032148371552383, gradient=0.0007458225689969814\n",
      "Gradient Descent(65/99): loss=0.3903214300120893, gradient=0.000733576846377113\n",
      "Gradient Descent(66/99): loss=0.39032137802450645, gradient=0.0007216833557534609\n",
      "Gradient Descent(67/99): loss=0.3903213276770184, gradient=0.000710130530677849\n",
      "Gradient Descent(68/99): loss=0.3903212788977409, gradient=0.0006989071904242146\n",
      "Gradient Descent(69/99): loss=0.3903212316184454, gradient=0.0006880025242903705\n",
      "Gradient Descent(70/99): loss=0.39032118577435976, gradient=0.0006774060770820305\n",
      "Gradient Descent(71/99): loss=0.3903211413039768, gradient=0.0006671077356453402\n",
      "Gradient Descent(72/99): loss=0.3903210981488758, gradient=0.000657097716329644\n",
      "Gradient Descent(73/99): loss=0.3903210562535551, gradient=0.0006473665532777506\n",
      "Gradient Descent(74/99): loss=0.3903210155652736, gradient=0.0006379050874516743\n",
      "Gradient Descent(75/99): loss=0.3903209760339012, gradient=0.0006287044563148248\n",
      "Gradient Descent(76/99): loss=0.3903209376117787, gradient=0.0006197560840994392\n",
      "Gradient Descent(77/99): loss=0.39032090025358546, gradient=0.0006110516725972954\n",
      "Gradient Descent(78/99): loss=0.3903208639162133, gradient=0.000602583192418863\n",
      "Gradient Descent(79/99): loss=0.39032082855865013, gradient=0.0005943428746728601\n",
      "Gradient Descent(80/99): loss=0.39032079414186777, gradient=0.0005863232030231945\n",
      "Gradient Descent(81/99): loss=0.3903207606287173, gradient=0.0005785169060864311\n",
      "Gradient Descent(82/99): loss=0.3903207279838293, gradient=0.0005709169501362071\n",
      "Gradient Descent(83/99): loss=0.39032069617352133, gradient=0.0005635165320861413\n",
      "Gradient Descent(84/99): loss=0.39032066516570846, gradient=0.000556309072725487\n",
      "Gradient Descent(85/99): loss=0.39032063492982044, gradient=0.0005492882101853612\n",
      "Gradient Descent(86/99): loss=0.39032060543672265, gradient=0.0005424477936158338\n",
      "Gradient Descent(87/99): loss=0.3903205766586407, gradient=0.0005357818770571263\n",
      "Gradient Descent(88/99): loss=0.3903205485690911, gradient=0.0005292847134899567\n",
      "Gradient Descent(89/99): loss=0.39032052114281435, gradient=0.0005229507490519073\n",
      "Gradient Descent(90/99): loss=0.39032049435571076, gradient=0.0005167746174087233\n",
      "Gradient Descent(91/99): loss=0.39032046818478255, gradient=0.0005107511342713473\n",
      "Gradient Descent(92/99): loss=0.39032044260807597, gradient=0.000504875292049526\n",
      "Gradient Descent(93/99): loss=0.39032041760462916, gradient=0.0004991422546352111\n",
      "Gradient Descent(94/99): loss=0.3903203931544209, gradient=0.0004935473523100305\n",
      "Gradient Descent(95/99): loss=0.39032036923832286, gradient=0.0004880860767707633\n",
      "Gradient Descent(96/99): loss=0.3903203458380553, gradient=0.0004827540762691332\n",
      "Gradient Descent(97/99): loss=0.3903203229361433, gradient=0.0004775471508616145\n",
      "Gradient Descent(98/99): loss=0.39032030051587674, gradient=0.0004724612477663447\n",
      "Gradient Descent(99/99): loss=0.390320278561272, gradient=0.0004674924568243761\n",
      "Gradient Descent(0/99): loss=0.38997307801007247, gradient=0.010005997875251598\n",
      "Gradient Descent(1/99): loss=0.3899668481185545, gradient=0.008240919783173683\n",
      "Gradient Descent(2/99): loss=0.3899624373862387, gradient=0.0069066193081946635\n",
      "Gradient Descent(3/99): loss=0.38995920303110726, gradient=0.00589014281661093\n",
      "Gradient Descent(4/99): loss=0.38995674611222425, gradient=0.005112978556835879\n",
      "Gradient Descent(5/99): loss=0.3899548163840658, gradient=0.004513564047123811\n",
      "Gradient Descent(6/99): loss=0.38995325321113755, gradient=0.004048580817789846\n",
      "Gradient Descent(7/99): loss=0.38995195284344725, gradient=0.003681876385078061\n",
      "Gradient Descent(8/99): loss=0.38995084661588236, gradient=0.0033877054510237418\n",
      "Gradient Descent(9/99): loss=0.3899498880578491, gradient=0.0031472531588818894\n",
      "Gradient Descent(10/99): loss=0.3899490449452417, gradient=0.0029469159103923085\n",
      "Gradient Descent(11/99): loss=0.38994829435391504, gradient=0.002776913389564186\n",
      "Gradient Descent(12/99): loss=0.38994761954701823, gradient=0.0026302156201712095\n",
      "Gradient Descent(13/99): loss=0.38994700799084475, gradient=0.0025017445056231403\n",
      "Gradient Descent(14/99): loss=0.3899464500711018, gradient=0.002387797085173184\n",
      "Gradient Descent(15/99): loss=0.3899459382483067, gradient=0.0022856377473724036\n",
      "Gradient Descent(16/99): loss=0.3899454664917405, gradient=0.0021932138337506842\n",
      "Gradient Descent(17/99): loss=0.3899450298925029, gradient=0.0021089587046720015\n",
      "Gradient Descent(18/99): loss=0.38994462455761, gradient=0.0020307925823614945\n",
      "Gradient Descent(19/99): loss=0.3899442468952673, gradient=0.0019595578702206505\n",
      "Gradient Descent(20/99): loss=0.38994389403260066, gradient=0.0018935319723712598\n",
      "Gradient Descent(21/99): loss=0.3899435635110115, gradient=0.0018320877549610108\n",
      "Gradient Descent(22/99): loss=0.38994325320818324, gradient=0.0017747070568601513\n",
      "Gradient Descent(23/99): loss=0.38994296127799033, gradient=0.0017209566457567454\n",
      "Gradient Descent(24/99): loss=0.3899426861036117, gradient=0.001670470479476353\n",
      "Gradient Descent(25/99): loss=0.3899424262604278, gradient=0.0016229363935793013\n",
      "Gradient Descent(26/99): loss=0.38994218056774793, gradient=0.0015775508394437953\n",
      "Gradient Descent(27/99): loss=0.3899419478115646, gradient=0.0015351754155267323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(28/99): loss=0.389941726987182, gradient=0.0014950494278457645\n",
      "Gradient Descent(29/99): loss=0.38994151719258635, gradient=0.0014569940472341416\n",
      "Gradient Descent(30/99): loss=0.38994131761488815, gradient=0.0014208515025784293\n",
      "Gradient Descent(31/99): loss=0.38994112751898563, gradient=0.0013864818278468665\n",
      "Gradient Descent(32/99): loss=0.3899409462380193, gradient=0.0013537602026331937\n",
      "Gradient Descent(33/99): loss=0.38994077316528064, gradient=0.0013225747618901625\n",
      "Gradient Descent(34/99): loss=0.389940607747317, gradient=0.0012928247803720427\n",
      "Gradient Descent(35/99): loss=0.38994044947801987, gradient=0.0012644191590221105\n",
      "Gradient Descent(36/99): loss=0.38994029789353246, gradient=0.0012372751565857113\n",
      "Gradient Descent(37/99): loss=0.3899401525678397, gradient=0.0012113173217531807\n",
      "Gradient Descent(38/99): loss=0.3899400131089307, gradient=0.0011864765902738785\n",
      "Gradient Descent(39/99): loss=0.389939879155447, gradient=0.001162689518508243\n",
      "Gradient Descent(40/99): loss=0.3899397503737393, gradient=0.0011398976303518703\n",
      "Gradient Descent(41/99): loss=0.3899396264552753, gradient=0.00111804685876306\n",
      "Gradient Descent(42/99): loss=0.389939507114346, gradient=0.0010970870665327054\n",
      "Gradient Descent(43/99): loss=0.38993939208603146, gradient=0.0010769716336593755\n",
      "Gradient Descent(44/99): loss=0.3899392811243851, gradient=0.0010576571008866368\n",
      "Gradient Descent(45/99): loss=0.38993917400081646, gradient=0.0010391028607365993\n",
      "Gradient Descent(46/99): loss=0.3899390705026368, gradient=0.0010212708888213899\n",
      "Gradient Descent(47/99): loss=0.38993897043175385, gradient=0.0010041255094007748\n",
      "Gradient Descent(48/99): loss=0.38993887360349544, gradient=0.0009876331901284713\n",
      "Gradient Descent(49/99): loss=0.3899387798455466, gradient=0.0009717623617364309\n",
      "Gradient Descent(50/99): loss=0.38993868899698475, gradient=0.0009564832590731689\n",
      "Gradient Descent(51/99): loss=0.3899386009074066, gradient=0.0009417677804684635\n",
      "Gradient Descent(52/99): loss=0.3899385154361319, gradient=0.0009275893628611121\n",
      "Gradient Descent(53/99): loss=0.38993843245147847, gradient=0.0009139228705134111\n",
      "Gradient Descent(54/99): loss=0.38993835183009934, gradient=0.0009007444954629295\n",
      "Gradient Descent(55/99): loss=0.3899382734563779, gradient=0.0008880316681368021\n",
      "Gradient Descent(56/99): loss=0.38993819722187106, gradient=0.0008757629767838668\n",
      "Gradient Descent(57/99): loss=0.3899381230248013, gradient=0.0008639180945776735\n",
      "Gradient Descent(58/99): loss=0.3899380507695877, gradient=0.0008524777134075088\n",
      "Gradient Descent(59/99): loss=0.38993798036641497, gradient=0.0008414234835164296\n",
      "Gradient Descent(60/99): loss=0.3899379117308366, gradient=0.0008307379582650548\n",
      "Gradient Descent(61/99): loss=0.3899378447834088, gradient=0.0008204045434012393\n",
      "Gradient Descent(62/99): loss=0.38993777944935193, gradient=0.0008104074503035676\n",
      "Gradient Descent(63/99): loss=0.38993771565823837, gradient=0.0008007316527400381\n",
      "Gradient Descent(64/99): loss=0.3899376533437033, gradient=0.0007913628467479106\n",
      "Gradient Descent(65/99): loss=0.38993759244317605, gradient=0.000782287413293701\n",
      "Gradient Descent(66/99): loss=0.38993753289763344, gradient=0.0007734923834200983\n",
      "Gradient Descent(67/99): loss=0.3899374746513675, gradient=0.0007649654056252522\n",
      "Gradient Descent(68/99): loss=0.3899374176517738, gradient=0.0007566947152549808\n",
      "Gradient Descent(69/99): loss=0.38993736184915095, gradient=0.0007486691057170107\n",
      "Gradient Descent(70/99): loss=0.38993730719651754, gradient=0.0007408779013515909\n",
      "Gradient Descent(71/99): loss=0.38993725364943954, gradient=0.0007333109318145085\n",
      "Gradient Descent(72/99): loss=0.389937201165871, gradient=0.0007259585078466314\n",
      "Gradient Descent(73/99): loss=0.38993714970600485, gradient=0.0007188113983199358\n",
      "Gradient Descent(74/99): loss=0.3899370992321348, gradient=0.0007118608084638362\n",
      "Gradient Descent(75/99): loss=0.3899370497085254, gradient=0.0007050983591863032\n",
      "Gradient Descent(76/99): loss=0.3899370011012922, gradient=0.0006985160674154211\n",
      "Gradient Descent(77/99): loss=0.38993695337828893, gradient=0.0006921063273941396\n",
      "Gradient Descent(78/99): loss=0.38993690650900265, gradient=0.000685861892869415\n",
      "Gradient Descent(79/99): loss=0.3899368604644552, gradient=0.0006797758601223922\n",
      "Gradient Descent(80/99): loss=0.3899368152171124, gradient=0.0006738416517918316\n",
      "Gradient Descent(81/99): loss=0.3899367707407979, gradient=0.0006680530014476994\n",
      "Gradient Descent(82/99): loss=0.38993672701061344, gradient=0.0006624039388754952\n",
      "Gradient Descent(83/99): loss=0.38993668400286335, gradient=0.0006568887760353248\n",
      "Gradient Descent(84/99): loss=0.38993664169498504, gradient=0.0006515020936630312\n",
      "Gradient Descent(85/99): loss=0.38993660006548353, gradient=0.0006462387284821671\n",
      "Gradient Descent(86/99): loss=0.38993655909386965, gradient=0.0006410937609994024\n",
      "Gradient Descent(87/99): loss=0.3899365187606025, gradient=0.00063606250385583\n",
      "Gradient Descent(88/99): loss=0.3899364790470359, gradient=0.0006311404907102369\n",
      "Gradient Descent(89/99): loss=0.3899364399353674, gradient=0.00062632346563054\n",
      "Gradient Descent(90/99): loss=0.38993640140859126, gradient=0.0006216073729711727\n",
      "Gradient Descent(91/99): loss=0.3899363634504542, gradient=0.000616988347715869\n",
      "Gradient Descent(92/99): loss=0.38993632604541306, gradient=0.0006124627062656714\n",
      "Gradient Descent(93/99): loss=0.3899362891785967, gradient=0.0006080269376531149\n",
      "Gradient Descent(94/99): loss=0.38993625283576766, gradient=0.0006036776951644676\n",
      "Gradient Descent(95/99): loss=0.38993621700329023, gradient=0.000599411788352954\n",
      "Gradient Descent(96/99): loss=0.3899361816680952, gradient=0.0005952261754256691\n",
      "Gradient Descent(97/99): loss=0.3899361468176515, gradient=0.0005911179559888165\n",
      "Gradient Descent(98/99): loss=0.38993611243993703, gradient=0.0005870843641354192\n",
      "Gradient Descent(99/99): loss=0.3899360785234113, gradient=0.0005831227618610445\n",
      "Gradient Descent(0/99): loss=0.38967364452375086, gradient=0.01107454400806699\n",
      "Gradient Descent(1/99): loss=0.3896686646379754, gradient=0.007441679879409083\n",
      "Gradient Descent(2/99): loss=0.3896651324137648, gradient=0.006179204795226177\n",
      "Gradient Descent(3/99): loss=0.3896625376547087, gradient=0.005279883297971585\n",
      "Gradient Descent(4/99): loss=0.38966059274845016, gradient=0.004560767978219808\n",
      "Gradient Descent(5/99): loss=0.3896591068372844, gradient=0.003977147468496423\n",
      "Gradient Descent(6/99): loss=0.38965794960347383, gradient=0.00350214593776152\n",
      "Gradient Descent(7/99): loss=0.3896570316831202, gradient=0.0031125410194394174\n",
      "Gradient Descent(8/99): loss=0.3896562908724668, gradient=0.002790689503271872\n",
      "Gradient Descent(9/99): loss=0.3896556833129646, gradient=0.0025226908916757464\n",
      "Gradient Descent(10/99): loss=0.38965517765386043, gradient=0.002297635127493634\n",
      "Gradient Descent(11/99): loss=0.3896547511594736, gradient=0.0021069864756298276\n",
      "Gradient Descent(12/99): loss=0.3896543870945751, gradient=0.0019440796433823342\n",
      "Gradient Descent(13/99): loss=0.3896540729539275, gradient=0.0018037111464570467\n",
      "Gradient Descent(14/99): loss=0.3896537992527386, gradient=0.001681811783275238\n",
      "Gradient Descent(15/99): loss=0.3896535586922095, gradient=0.0015751874882000308\n",
      "Gradient Descent(16/99): loss=0.38965334557771253, gradient=0.0014813168412541524\n",
      "Gradient Descent(17/99): loss=0.389653155408454, gradient=0.0013981945975428203\n",
      "Gradient Descent(18/99): loss=0.38965298458454517, gradient=0.0013242118775085537\n",
      "Gradient Descent(19/99): loss=0.3896528301952172, gradient=0.0012580650586301576\n",
      "Gradient Descent(20/99): loss=0.3896526898636911, gradient=0.0011986868085731312\n",
      "Gradient Descent(21/99): loss=0.38965256163203515, gradient=0.00114519399635766\n",
      "Gradient Descent(22/99): loss=0.389652443874578, gradient=0.0010968483505152094\n",
      "Gradient Descent(23/99): loss=0.3896523352319563, gradient=0.0010530266788115721\n",
      "Gradient Descent(24/99): loss=0.3896522345602648, gradient=0.001013198227302923\n",
      "Gradient Descent(25/99): loss=0.38965214089139094, gradient=0.0009769073567130855\n",
      "Gradient Descent(26/99): loss=0.3896520534017441, gradient=0.0009437601769530864\n",
      "Gradient Descent(27/99): loss=0.3896519713873561, gradient=0.0009134141321371197\n",
      "Gradient Descent(28/99): loss=0.3896518942438765, gradient=0.0008855697923501227\n",
      "Gradient Descent(29/99): loss=0.38965182145037164, gradient=0.0008599643047442462\n",
      "Gradient Descent(30/99): loss=0.38965175255610435, gradient=0.0008363661015510247\n",
      "Gradient Descent(31/99): loss=0.3896516871696785, gradient=0.0008145705691008181\n",
      "Gradient Descent(32/99): loss=0.38965162495006994, gradient=0.0007943964598212583\n",
      "Gradient Descent(33/99): loss=0.38965156559917885, gradient=0.0007756828859431656\n",
      "Gradient Descent(34/99): loss=0.38965150885561367, gradient=0.0007582867749135818\n",
      "Gradient Descent(35/99): loss=0.38965145448948224, gradient=0.0007420806964927466\n",
      "Gradient Descent(36/99): loss=0.3896514022980065, gradient=0.0007269509932930374\n",
      "Gradient Descent(37/99): loss=0.38965135210182134, gradient=0.0007127961623775124\n",
      "Gradient Descent(38/99): loss=0.389651303741834, gradient=0.0006995254471304154\n",
      "Gradient Descent(39/99): loss=0.3896512570765567, gradient=0.0006870576071436509\n",
      "Gradient Descent(40/99): loss=0.3896512119798298, gradient=0.0006753198401998643\n",
      "Gradient Descent(41/99): loss=0.38965116833887514, gradient=0.0006642468351941604\n",
      "Gradient Descent(42/99): loss=0.3896511260526244, gradient=0.0006537799384683452\n",
      "Gradient Descent(43/99): loss=0.3896510850302837, gradient=0.0006438664188462497\n",
      "Gradient Descent(44/99): loss=0.38965104519009364, gradient=0.0006344588188817133\n",
      "Gradient Descent(45/99): loss=0.38965100645825856, gradient=0.0006255143816182447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(46/99): loss=0.3896509687680181, gradient=0.0006169945436218112\n",
      "Gradient Descent(47/99): loss=0.38965093205884077, gradient=0.0006088644862650028\n",
      "Gradient Descent(48/99): loss=0.38965089627572097, gradient=0.0006010927382690458\n",
      "Gradient Descent(49/99): loss=0.389650861368567, gradient=0.0005936508233843477\n",
      "Gradient Descent(50/99): loss=0.3896508272916643, gradient=0.0005865129478482568\n",
      "Gradient Descent(51/99): loss=0.3896507940032052, gradient=0.0005796557229115501\n",
      "Gradient Descent(52/99): loss=0.38965076146487726, gradient=0.0005730579182986412\n",
      "Gradient Descent(53/99): loss=0.3896507296414983, gradient=0.0005667002429653402\n",
      "Gradient Descent(54/99): loss=0.3896506985006967, gradient=0.0005605651499572754\n",
      "Gradient Descent(55/99): loss=0.38965066801262876, gradient=0.0005546366625575955\n",
      "Gradient Descent(56/99): loss=0.38965063814972606, gradient=0.0005489002192507106\n",
      "Gradient Descent(57/99): loss=0.38965060888647507, gradient=0.0005433425353268668\n",
      "Gradient Descent(58/99): loss=0.3896505801992178, gradient=0.0005379514792139687\n",
      "Gradient Descent(59/99): loss=0.3896505520659767, gradient=0.0005327159618530237\n",
      "Gradient Descent(60/99): loss=0.38965052446629733, gradient=0.0005276258376353545\n",
      "Gradient Descent(61/99): loss=0.38965049738110863, gradient=0.000522671815597322\n",
      "Gradient Descent(62/99): loss=0.38965047079259707, gradient=0.0005178453797237905\n",
      "Gradient Descent(63/99): loss=0.38965044468409427, gradient=0.0005131387173483935\n",
      "Gradient Descent(64/99): loss=0.3896504190399768, gradient=0.0005085446547577157\n",
      "Gradient Descent(65/99): loss=0.3896503938455752, gradient=0.0005040565992132086\n",
      "Gradient Descent(66/99): loss=0.38965036908709233, gradient=0.0004996684866947616\n",
      "Gradient Descent(67/99): loss=0.38965034475153076, gradient=0.000495374734752914\n",
      "Gradient Descent(68/99): loss=0.3896503208266252, gradient=0.00049117019992564\n",
      "Gradient Descent(69/99): loss=0.38965029730078415, gradient=0.0004870501392402319\n",
      "Gradient Descent(70/99): loss=0.3896502741630348, gradient=0.0004830101753730202\n",
      "Gradient Descent(71/99): loss=0.3896502514029747, gradient=0.0004790462650903763\n",
      "Gradient Descent(72/99): loss=0.3896502290107265, gradient=0.00047515467063447\n",
      "Gradient Descent(73/99): loss=0.38965020697689867, gradient=0.00047133193375615477\n",
      "Gradient Descent(74/99): loss=0.3896501852925483, gradient=0.0004675748521297284\n",
      "Gradient Descent(75/99): loss=0.3896501639491476, gradient=0.00046388045791212067\n",
      "Gradient Descent(76/99): loss=0.38965014293855454, gradient=0.00046024599823673066\n",
      "Gradient Descent(77/99): loss=0.38965012225298423, gradient=0.0004566689174527438\n",
      "Gradient Descent(78/99): loss=0.3896501018849843, gradient=0.0004531468409415178\n",
      "Gradient Descent(79/99): loss=0.3896500818274113, gradient=0.0004496775603597194\n",
      "Gradient Descent(80/99): loss=0.38965006207341074, gradient=0.00044625902017329024\n",
      "Gradient Descent(81/99): loss=0.38965004261639624, gradient=0.0004428893053617334\n",
      "Gradient Descent(82/99): loss=0.38965002345003413, gradient=0.0004395666301831224\n",
      "Gradient Descent(83/99): loss=0.38965000456822485, gradient=0.00043628932790250503\n",
      "Gradient Descent(84/99): loss=0.38964998596509, gradient=0.0004330558413948349\n",
      "Gradient Descent(85/99): loss=0.38964996763495785, gradient=0.00042986471454351037\n",
      "Gradient Descent(86/99): loss=0.38964994957235083, gradient=0.000426714584362558\n",
      "Gradient Descent(87/99): loss=0.38964993177197466, gradient=0.0004236041737777971\n",
      "Gradient Descent(88/99): loss=0.3896499142287067, gradient=0.0004205322850083853\n",
      "Gradient Descent(89/99): loss=0.3896498969375874, gradient=0.00041749779349579947\n",
      "Gradient Descent(90/99): loss=0.38964987989381006, gradient=0.0004144996423325354\n",
      "Gradient Descent(91/99): loss=0.38964986309271377, gradient=0.00041153683714663093\n",
      "Gradient Descent(92/99): loss=0.3896498465297745, gradient=0.0004086084414028785\n",
      "Gradient Descent(93/99): loss=0.3896498302005988, gradient=0.00040571357208506013\n",
      "Gradient Descent(94/99): loss=0.3896498141009167, gradient=0.00040285139572624265\n",
      "Gradient Descent(95/99): loss=0.3896497982265762, gradient=0.0004000211247578216\n",
      "Gradient Descent(96/99): loss=0.3896497825735367, gradient=0.00039722201415055494\n",
      "Gradient Descent(97/99): loss=0.38964976713786476, gradient=0.00039445335832264495\n",
      "Gradient Descent(98/99): loss=0.38964975191572826, gradient=0.0003917144882929251\n",
      "Gradient Descent(99/99): loss=0.3896497369033926, gradient=0.00038900476905848316\n",
      "Gradient Descent(0/99): loss=0.3889592602050856, gradient=0.008185684309810398\n",
      "Gradient Descent(1/99): loss=0.38895649694753204, gradient=0.005551419937999095\n",
      "Gradient Descent(2/99): loss=0.3889545779353781, gradient=0.004564799179171347\n",
      "Gradient Descent(3/99): loss=0.3889531958851904, gradient=0.0038604876645784054\n",
      "Gradient Descent(4/99): loss=0.3889521756475567, gradient=0.003307163400891248\n",
      "Gradient Descent(5/99): loss=0.38895140248141025, gradient=0.0028699445161943352\n",
      "Gradient Descent(6/99): loss=0.38895080000942284, gradient=0.00252507638251106\n",
      "Gradient Descent(7/99): loss=0.3889503169845008, gradient=0.0022534725103951748\n",
      "Gradient Descent(8/99): loss=0.38894991877341784, gradient=0.002039577087972964\n",
      "Gradient Descent(9/99): loss=0.3889495817793502, gradient=0.0018707572772638514\n",
      "Gradient Descent(10/99): loss=0.3889492897797136, gradient=0.0017368517199768238\n",
      "Gradient Descent(11/99): loss=0.3889490315088801, gradient=0.0016297946946494178\n",
      "Gradient Descent(12/99): loss=0.38894879905553686, gradient=0.0015432779945818167\n",
      "Gradient Descent(13/99): loss=0.3889485867947421, gradient=0.001472437978459703\n",
      "Gradient Descent(14/99): loss=0.3889483906724935, gradient=0.001413570357524349\n",
      "Gradient Descent(15/99): loss=0.388948207723688, gradient=0.0013638798291798983\n",
      "Gradient Descent(16/99): loss=0.3889480357453029, gradient=0.001321269448030963\n",
      "Gradient Descent(17/99): loss=0.3889478730733121, gradient=0.0012841701940189002\n",
      "Gradient Descent(18/99): loss=0.3889477184292829, gradient=0.0012514074403451448\n",
      "Gradient Descent(19/99): loss=0.3889475708140389, gradient=0.0012220989233538454\n",
      "Gradient Descent(20/99): loss=0.3889474294332999, gradient=0.0011955782171884496\n",
      "Gradient Descent(21/99): loss=0.38894729364519054, gradient=0.0011713380811876975\n",
      "Gradient Descent(22/99): loss=0.38894716292280684, gradient=0.001148988873160664\n",
      "Gradient Descent(23/99): loss=0.3889470368272293, gradient=0.0011282281666530588\n",
      "Gradient Descent(24/99): loss=0.38894691498784245, gradient=0.0011088185916794317\n",
      "Gradient Descent(25/99): loss=0.3889467970878128, gradient=0.0010905716604910748\n",
      "Gradient Descent(26/99): loss=0.3889466828532347, gradient=0.001073335928199457\n",
      "Gradient Descent(27/99): loss=0.38894657204492217, gradient=0.0010569882867750768\n",
      "Gradient Descent(28/99): loss=0.3889464644521164, gradient=0.0010414275246006823\n",
      "Gradient Descent(29/99): loss=0.38894635988760784, gradient=0.0010265695276600502\n",
      "Gradient Descent(30/99): loss=0.38894625818390227, gradient=0.0010123436747251073\n",
      "Gradient Descent(31/99): loss=0.38894615919017866, gradient=0.0009986901053799751\n",
      "Gradient Descent(32/99): loss=0.38894606276984267, gradient=0.0009855576300736636\n",
      "Gradient Descent(33/99): loss=0.38894596879854065, gradient=0.000972902115809875\n",
      "Gradient Descent(34/99): loss=0.38894587716253215, gradient=0.0009606852270003058\n",
      "Gradient Descent(35/99): loss=0.3889457877573429, gradient=0.0009488734337812745\n",
      "Gradient Descent(36/99): loss=0.3889457004866388, gradient=0.0009374372235464571\n",
      "Gradient Descent(37/99): loss=0.38894561526128135, gradient=0.0009263504682945132\n",
      "Gradient Descent(38/99): loss=0.38894553199852355, gradient=0.0009155899125470606\n",
      "Gradient Descent(39/99): loss=0.3889454506914005, gradient=0.0009043245953520273\n",
      "Gradient Descent(40/99): loss=0.3889453711889055, gradient=0.0008942174873811452\n",
      "Gradient Descent(41/99): loss=0.3889452934246007, gradient=0.0008843758657674785\n",
      "Gradient Descent(42/99): loss=0.3889452173361553, gradient=0.0008747847084616281\n",
      "Gradient Descent(43/99): loss=0.38894514286495196, gradient=0.0008654303807105112\n",
      "Gradient Descent(44/99): loss=0.3889450699557552, gradient=0.0008563004624757585\n",
      "Gradient Descent(45/99): loss=0.38894499855641623, gradient=0.00084738360253002\n",
      "Gradient Descent(46/99): loss=0.38894492861761415, gradient=0.0008386693943626465\n",
      "Gradient Descent(47/99): loss=0.3889448600926244, gradient=0.0008301482700703483\n",
      "Gradient Descent(48/99): loss=0.3889447929371141, gradient=0.0008218114091624271\n",
      "Gradient Descent(49/99): loss=0.38894472710895767, gradient=0.0008136506598000364\n",
      "Gradient Descent(50/99): loss=0.3889446625680726, gradient=0.0008056584704542549\n",
      "Gradient Descent(51/99): loss=0.3889445992762707, gradient=0.000797827830335951\n",
      "Gradient Descent(52/99): loss=0.38894453719712385, gradient=0.0007901522172473722\n",
      "Gradient Descent(53/99): loss=0.38894447629584417, gradient=0.000782625551739917\n",
      "Gradient Descent(54/99): loss=0.38894441653917206, gradient=0.000775242156656712\n",
      "Gradient Descent(55/99): loss=0.3889443578952771, gradient=0.0007679967212908119\n",
      "Gradient Descent(56/99): loss=0.38894430033366656, gradient=0.0007608842695187175\n",
      "Gradient Descent(57/99): loss=0.38894424382510157, gradient=0.0007539001313711099\n",
      "Gradient Descent(58/99): loss=0.38894418834152084, gradient=0.0007470399175881358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(59/99): loss=0.3889441338559698, gradient=0.000740299496776073\n",
      "Gradient Descent(60/99): loss=0.38894408034253714, gradient=0.0007336749748415567\n",
      "Gradient Descent(61/99): loss=0.388944027776294, gradient=0.0007271626764256831\n",
      "Gradient Descent(62/99): loss=0.38894397613323967, gradient=0.0007207591281023572\n",
      "Gradient Descent(63/99): loss=0.3889439253902512, gradient=0.0007144610431374491\n",
      "Gradient Descent(64/99): loss=0.38894387552503507, gradient=0.0007082653076341682\n",
      "Gradient Descent(65/99): loss=0.3889438265160848, gradient=0.0007021689679129308\n",
      "Gradient Descent(66/99): loss=0.38894377834264066, gradient=0.0006961692189952502\n",
      "Gradient Descent(67/99): loss=0.38894373098465085, gradient=0.0006902633940761989\n",
      "Gradient Descent(68/99): loss=0.3889436844227372, gradient=0.0006844489548863793\n",
      "Gradient Descent(69/99): loss=0.38894363863816267, gradient=0.0006787234828545813\n",
      "Gradient Descent(70/99): loss=0.38894359361280095, gradient=0.0006730846709947594\n",
      "Gradient Descent(71/99): loss=0.38894354932910724, gradient=0.0006675303164477514\n",
      "Gradient Descent(72/99): loss=0.3889435057700925, gradient=0.0006620583136182111\n",
      "Gradient Descent(73/99): loss=0.3889434629192978, gradient=0.0006566666478516026\n",
      "Gradient Descent(74/99): loss=0.38894342076077154, gradient=0.0006513533896039472\n",
      "Gradient Descent(75/99): loss=0.3889433792790471, gradient=0.0006461166890607289\n",
      "Gradient Descent(76/99): loss=0.3889433384591226, gradient=0.0006409547711657447\n",
      "Gradient Descent(77/99): loss=0.38894329828644053, gradient=0.0006358659310260961\n",
      "Gradient Descent(78/99): loss=0.38894325874687086, gradient=0.0006308485296606098\n",
      "Gradient Descent(79/99): loss=0.38894321982669283, gradient=0.0006259009900638949\n",
      "Gradient Descent(80/99): loss=0.3889431815125791, gradient=0.0006210217935602175\n",
      "Gradient Descent(81/99): loss=0.38894314379157974, gradient=0.0006162094764228387\n",
      "Gradient Descent(82/99): loss=0.38894310665110887, gradient=0.000611462626738984\n",
      "Gradient Descent(83/99): loss=0.38894307007893, gradient=0.0006067798814993967\n",
      "Gradient Descent(84/99): loss=0.38894303406314296, gradient=0.000602159923895626\n",
      "Gradient Descent(85/99): loss=0.3889429985921722, gradient=0.0005976014808086884\n",
      "Gradient Descent(86/99): loss=0.38894296365475456, gradient=0.000593103320473714\n",
      "Gradient Descent(87/99): loss=0.3889429292399281, gradient=0.0005886642503077402\n",
      "Gradient Descent(88/99): loss=0.3889428953370215, gradient=0.0005842831148873303\n",
      "Gradient Descent(89/99): loss=0.388942861935645, gradient=0.0005799587940652004\n",
      "Gradient Descent(90/99): loss=0.38894282902567856, gradient=0.0005756902012149348\n",
      "Gradient Descent(91/99): loss=0.38894279659726516, gradient=0.0005714762815938294\n",
      "Gradient Descent(92/99): loss=0.38894276464080046, gradient=0.0005673160108158334\n",
      "Gradient Descent(93/99): loss=0.38894273314692485, gradient=0.0005632083934253708\n",
      "Gradient Descent(94/99): loss=0.3889427021065159, gradient=0.0005591524615649934\n",
      "Gradient Descent(95/99): loss=0.3889426715106801, gradient=0.000555147273729847\n",
      "Gradient Descent(96/99): loss=0.38894264135074574, gradient=0.0005511919136022981\n",
      "Gradient Descent(97/99): loss=0.3889426116182566, gradient=0.0005472854889610885\n",
      "Gradient Descent(98/99): loss=0.3889425823049639, gradient=0.0005434271306590751\n",
      "Gradient Descent(99/99): loss=0.3889425534028213, gradient=0.0005396159916652854\n",
      "Gradient Descent(0/99): loss=0.39014115554512724, gradient=0.01539885269778825\n",
      "Gradient Descent(1/99): loss=0.39013723738930495, gradient=0.006918996281220078\n",
      "Gradient Descent(2/99): loss=0.3901345880512961, gradient=0.005359947395234253\n",
      "Gradient Descent(3/99): loss=0.39013259546399, gradient=0.004615199981909804\n",
      "Gradient Descent(4/99): loss=0.3901310539965003, gradient=0.004047983814344271\n",
      "Gradient Descent(5/99): loss=0.3901298360464917, gradient=0.003588745968611817\n",
      "Gradient Descent(6/99): loss=0.3901288534795052, gradient=0.003216065486326285\n",
      "Gradient Descent(7/99): loss=0.39012804558587927, gradient=0.00291005686159819\n",
      "Gradient Descent(8/99): loss=0.39012736950088706, gradient=0.0026569091521952914\n",
      "Gradient Descent(9/99): loss=0.39012679452727006, gradient=0.0024458379437758758\n",
      "Gradient Descent(10/99): loss=0.3901262983617261, gradient=0.0022684081454263086\n",
      "Gradient Descent(11/99): loss=0.39012586456691967, gradient=0.002118012374424545\n",
      "Gradient Descent(12/99): loss=0.39012548085700377, gradient=0.0019894641491536792\n",
      "Gradient Descent(13/99): loss=0.39012513792210696, gradient=0.0018786808123461983\n",
      "Gradient Descent(14/99): loss=0.3901248286129803, gradient=0.0017824369813041964\n",
      "Gradient Descent(15/99): loss=0.39012454736892416, gradient=0.0016981733685788331\n",
      "Gradient Descent(16/99): loss=0.3901242898117381, gradient=0.0016238489200927821\n",
      "Gradient Descent(17/99): loss=0.3901240524541736, gradient=0.0015578266833906533\n",
      "Gradient Descent(18/99): loss=0.3901238324881874, gradient=0.0014987858120892221\n",
      "Gradient Descent(19/99): loss=0.39012362762940167, gradient=0.0014456537313818647\n",
      "Gradient Descent(20/99): loss=0.39012343600157257, gradient=0.00139755379825486\n",
      "Gradient Descent(21/99): loss=0.3901232560498326, gradient=0.0013537648385918094\n",
      "Gradient Descent(22/99): loss=0.3901230864748487, gradient=0.0013136897742320416\n",
      "Gradient Descent(23/99): loss=0.39012292618233346, gradient=0.0012768312044154856\n",
      "Gradient Descent(24/99): loss=0.3901227742439484, gradient=0.0012427723117377398\n",
      "Gradient Descent(25/99): loss=0.3901226298667397, gradient=0.0012111618521005126\n",
      "Gradient Descent(26/99): loss=0.39012249236903057, gradient=0.0011817022859315503\n",
      "Gradient Descent(27/99): loss=0.3901223611612429, gradient=0.001154140334516023\n",
      "Gradient Descent(28/99): loss=0.3901222357305184, gradient=0.001128259417004162\n",
      "Gradient Descent(29/99): loss=0.39012211562829496, gradient=0.0011038735534866146\n",
      "Gradient Descent(30/99): loss=0.39012200046019885, gradient=0.0010808224175406621\n",
      "Gradient Descent(31/99): loss=0.39012188987776825, gradient=0.0010589672956302584\n",
      "Gradient Descent(32/99): loss=0.39012178357163635, gradient=0.0010381877666157302\n",
      "Gradient Descent(33/99): loss=0.3901216812658813, gradient=0.0010183789569057705\n",
      "Gradient Descent(34/99): loss=0.3901215827133242, gradient=0.0009994492588506834\n",
      "Gradient Descent(35/99): loss=0.39012148769159066, gradient=0.0009813184243867865\n",
      "Gradient Descent(36/99): loss=0.3901213959998029, gradient=0.0009639159646006179\n",
      "Gradient Descent(37/99): loss=0.3901213074557857, gradient=0.0009471798002165836\n",
      "Gradient Descent(38/99): loss=0.390121221893697, gradient=0.0009310551190872194\n",
      "Gradient Descent(39/99): loss=0.39012113916201363, gradient=0.0009154934053713514\n",
      "Gradient Descent(40/99): loss=0.39012105912180967, gradient=0.0009004516118220603\n",
      "Gradient Descent(41/99): loss=0.39012098164528186, gradient=0.000885891451907858\n",
      "Gradient Descent(42/99): loss=0.39012090661448134, gradient=0.0008717787926997417\n",
      "Gradient Descent(43/99): loss=0.3901208339202203, gradient=0.0008580831328130946\n",
      "Gradient Descent(44/99): loss=0.3901207634611253, gradient=0.000844777152395555\n",
      "Gradient Descent(45/99): loss=0.39012069514281633, gradient=0.0008318363243350171\n",
      "Gradient Descent(46/99): loss=0.3901206288771922, gradient=0.0008192385776413224\n",
      "Gradient Descent(47/99): loss=0.39012056458180716, gradient=0.0008069640054109521\n",
      "Gradient Descent(48/99): loss=0.39012050217932454, gradient=0.0007949946109826039\n",
      "Gradient Descent(49/99): loss=0.39012044159703774, gradient=0.0007833140868839318\n",
      "Gradient Descent(50/99): loss=0.3901203827664492, gradient=0.0007719076219925271\n",
      "Gradient Descent(51/99): loss=0.39012032562289795, gradient=0.0007607617330234288\n",
      "Gradient Descent(52/99): loss=0.3901202701052302, gradient=0.0007498641170296798\n",
      "Gradient Descent(53/99): loss=0.39012021615550857, gradient=0.0007392035220885281\n",
      "Gradient Descent(54/99): loss=0.3901201637187511, gradient=0.000728769633751896\n",
      "Gradient Descent(55/99): loss=0.3901201127427008, gradient=0.0007185529751862224\n",
      "Gradient Descent(56/99): loss=0.39012006317761855, gradient=0.0007085448192184479\n",
      "Gradient Descent(57/99): loss=0.3901200149760974, gradient=0.000698737110753508\n",
      "Gradient Descent(58/99): loss=0.39011996809289556, gradient=0.0006891223982401955\n",
      "Gradient Descent(59/99): loss=0.39011992248478805, gradient=0.0006796937730429541\n",
      "Gradient Descent(60/99): loss=0.39011987811042903, gradient=0.0006704448157305514\n",
      "Gradient Descent(61/99): loss=0.3901198349302306, gradient=0.0006613695484264305\n",
      "Gradient Descent(62/99): loss=0.3901197929062511, gradient=0.0006524623924765979\n",
      "Gradient Descent(63/99): loss=0.39011975200209414, gradient=0.0006437181307904371\n",
      "Gradient Descent(64/99): loss=0.39011971218281566, gradient=0.0006351318742920298\n",
      "Gradient Descent(65/99): loss=0.3901196734148405, gradient=0.000626699031993063\n",
      "Gradient Descent(66/99): loss=0.39011963566588465, gradient=0.0006184152842597248\n",
      "Gradient Descent(67/99): loss=0.390119598904884, gradient=0.0006102765589009373\n",
      "Gradient Descent(68/99): loss=0.3901195631019303, gradient=0.0006022790097508616\n",
      "Gradient Descent(69/99): loss=0.39011952822820983, gradient=0.0005944189974603117\n",
      "Gradient Descent(70/99): loss=0.39011949425594833, gradient=0.0005866930722458729\n",
      "Gradient Descent(71/99): loss=0.3901194611583594, gradient=0.0005790979583764096\n",
      "Gradient Descent(72/99): loss=0.3901194289095968, gradient=0.0005716305402036628\n",
      "Gradient Descent(73/99): loss=0.3901193974847099, gradient=0.0005642878495661647\n",
      "Gradient Descent(74/99): loss=0.3901193668596032, gradient=0.0005570670544162185\n",
      "Gradient Descent(75/99): loss=0.39011933701099605, gradient=0.0005499654485375887\n",
      "Gradient Descent(76/99): loss=0.3901193079163882, gradient=0.0005429804422368012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(77/99): loss=0.3901192795540251, gradient=0.0005361095539042764\n",
      "Gradient Descent(78/99): loss=0.3901192519028666, gradient=0.0005293504023538476\n",
      "Gradient Descent(79/99): loss=0.39011922494255663, gradient=0.00052270069985975\n",
      "Gradient Descent(80/99): loss=0.39011919865339606, gradient=0.000516158245818199\n",
      "Gradient Descent(81/99): loss=0.3901191730163155, gradient=0.0005097209209704951\n",
      "Gradient Descent(82/99): loss=0.3901191480128505, gradient=0.0005033866821303321\n",
      "Gradient Descent(83/99): loss=0.3901191236251186, gradient=0.0004971535573647436\n",
      "Gradient Descent(84/99): loss=0.3901190998357956, gradient=0.0004910196415836233\n",
      "Gradient Descent(85/99): loss=0.39011907662809675, gradient=0.0004849830924975942\n",
      "Gradient Descent(86/99): loss=0.39011905398575397, gradient=0.0004790421269087334\n",
      "Gradient Descent(87/99): loss=0.39011903189299896, gradient=0.00047319501730147957\n",
      "Gradient Descent(88/99): loss=0.3901190103345438, gradient=0.000467440088705523\n",
      "Gradient Descent(89/99): loss=0.39011898929556404, gradient=0.0004617757158049159\n",
      "Gradient Descent(90/99): loss=0.3901189687616832, gradient=0.00045620032027043444\n",
      "Gradient Descent(91/99): loss=0.3901189487189554, gradient=0.00045071236829453737\n",
      "Gradient Descent(92/99): loss=0.39011892915385166, gradient=0.0004453103683104125\n",
      "Gradient Descent(93/99): loss=0.39011891005324517, gradient=0.000439992868878663\n",
      "Gradient Descent(94/99): loss=0.3901188914043974, gradient=0.00043475845672630303\n",
      "Gradient Descent(95/99): loss=0.39011887319494565, gradient=0.00042960575492511494\n",
      "Gradient Descent(96/99): loss=0.3901188554128899, gradient=0.00042453342119685327\n",
      "Gradient Descent(97/99): loss=0.3901188380465812, gradient=0.00041954014633433206\n",
      "Gradient Descent(98/99): loss=0.39011882108470985, gradient=0.00041462465272893974\n",
      "Gradient Descent(99/99): loss=0.3901188045162946, gradient=0.0004097856929950925\n",
      "Gradient Descent(0/99): loss=0.3903504838161578, gradient=0.007271679071625734\n",
      "Gradient Descent(1/99): loss=0.3903468035705794, gradient=0.006261463251001653\n",
      "Gradient Descent(2/99): loss=0.3903439127988694, gradient=0.005529792147521626\n",
      "Gradient Descent(3/99): loss=0.3903415827117125, gradient=0.004950517346814528\n",
      "Gradient Descent(4/99): loss=0.3903396615807977, gradient=0.004483802324938583\n",
      "Gradient Descent(5/99): loss=0.39033804575018927, gradient=0.004102957450161313\n",
      "Gradient Descent(6/99): loss=0.3903366629842323, gradient=0.0037881521586598937\n",
      "Gradient Descent(7/99): loss=0.3903354619182511, gradient=0.003524538889673168\n",
      "Gradient Descent(8/99): loss=0.3903344059339859, gradient=0.0032988259695820106\n",
      "Gradient Descent(9/99): loss=0.39033346678171726, gradient=0.0031068524318484175\n",
      "Gradient Descent(10/99): loss=0.390332623642239, gradient=0.0029405234390099295\n",
      "Gradient Descent(11/99): loss=0.3903318606704029, gradient=0.0027945696197041645\n",
      "Gradient Descent(12/99): loss=0.3903311655327717, gradient=0.002665235052084053\n",
      "Gradient Descent(13/99): loss=0.3903305284854287, gradient=0.002549592847441392\n",
      "Gradient Descent(14/99): loss=0.3903299417236454, gradient=0.002445339780108711\n",
      "Gradient Descent(15/99): loss=0.3903293989158623, gradient=0.002350646240060878\n",
      "Gradient Descent(16/99): loss=0.39032889486460304, gradient=0.002264045889006862\n",
      "Gradient Descent(17/99): loss=0.3903284258413265, gradient=0.0021815406405498684\n",
      "Gradient Descent(18/99): loss=0.39032798756408366, gradient=0.0021079601325090353\n",
      "Gradient Descent(19/99): loss=0.3903275769809481, gradient=0.002039518436034322\n",
      "Gradient Descent(20/99): loss=0.3903271914851436, gradient=0.0019755635703179535\n",
      "Gradient Descent(21/99): loss=0.39032682883066694, gradient=0.0019155543009586374\n",
      "Gradient Descent(22/99): loss=0.3903264870670154, gradient=0.0018590386645117533\n",
      "Gradient Descent(23/99): loss=0.3903261644876311, gradient=0.001805637118515246\n",
      "Gradient Descent(24/99): loss=0.39032585967468963, gradient=0.001754490714282819\n",
      "Gradient Descent(25/99): loss=0.3903255711862012, gradient=0.00170648718050452\n",
      "Gradient Descent(26/99): loss=0.3903252978964771, gradient=0.0016603907692636709\n",
      "Gradient Descent(27/99): loss=0.3903250386766626, gradient=0.001616764218223772\n",
      "Gradient Descent(28/99): loss=0.39032479256875846, gradient=0.001575045367273276\n",
      "Gradient Descent(29/99): loss=0.390324558704699, gradient=0.0015350864987449843\n",
      "Gradient Descent(30/99): loss=0.3903243362945447, gradient=0.0014967587360224719\n",
      "Gradient Descent(31/99): loss=0.3903241246166762, gradient=0.0014599490208477522\n",
      "Gradient Descent(32/99): loss=0.3903239230095668, gradient=0.001424557627558092\n",
      "Gradient Descent(33/99): loss=0.39032373086484645, gradient=0.0013904961119635716\n",
      "Gradient Descent(34/99): loss=0.3903235476214157, gradient=0.0013576856134033181\n",
      "Gradient Descent(35/99): loss=0.39032337276042234, gradient=0.0013260554447330618\n",
      "Gradient Descent(36/99): loss=0.39032320580094987, gradient=0.0012955419177759838\n",
      "Gradient Descent(37/99): loss=0.390323046296293, gradient=0.0012660873619010655\n",
      "Gradient Descent(38/99): loss=0.39032289383072644, gradient=0.0012376393014592148\n",
      "Gradient Descent(39/99): loss=0.3903227480166841, gradient=0.001210149764254703\n",
      "Gradient Descent(40/99): loss=0.3903226084922824, gradient=0.0011835746984001517\n",
      "Gradient Descent(41/99): loss=0.39032247491913924, gradient=0.0011578734790639244\n",
      "Gradient Descent(42/99): loss=0.39032234698043905, gradient=0.0011330084899753823\n",
      "Gradient Descent(43/99): loss=0.3903222243792125, gradient=0.0011089447672693825\n",
      "Gradient Descent(44/99): loss=0.3903221068367978, gradient=0.001085649695455124\n",
      "Gradient Descent(45/99): loss=0.3903219940914593, gradient=0.001063092747084399\n",
      "Gradient Descent(46/99): loss=0.39032188589714206, gradient=0.0010412452591560403\n",
      "Gradient Descent(47/99): loss=0.3903217820223479, gradient=0.0010200802404844121\n",
      "Gradient Descent(48/99): loss=0.3903216822491122, gradient=0.0009995722052378844\n",
      "Gradient Descent(49/99): loss=0.3903215863720756, gradient=0.0009796970286535573\n",
      "Gradient Descent(50/99): loss=0.3903214941976321, gradient=0.0009604318215944745\n",
      "Gradient Descent(51/99): loss=0.3903214055431533, gradient=0.0009417548211585237\n",
      "Gradient Descent(52/99): loss=0.39032132023627375, gradient=0.000923645294997428\n",
      "Gradient Descent(53/99): loss=0.3903212381142334, gradient=0.0009060834573757681\n",
      "Gradient Descent(54/99): loss=0.39032115902327164, gradient=0.0008890503953088621\n",
      "Gradient Descent(55/99): loss=0.3903210828180672, gradient=0.0008725280033743106\n",
      "Gradient Descent(56/99): loss=0.39032100936121916, gradient=0.0008564989260070047\n",
      "Gradient Descent(57/99): loss=0.39032093852276617, gradient=0.0008409465062652117\n",
      "Gradient Descent(58/99): loss=0.3903208701797389, gradient=0.0008258547402056962\n",
      "Gradient Descent(59/99): loss=0.3903208042157447, gradient=0.0008112082361314567\n",
      "Gradient Descent(60/99): loss=0.3903207405205797, gradient=0.0007969921780814481\n",
      "Gradient Descent(61/99): loss=0.3903206789898667, gradient=0.0007831922930211829\n",
      "Gradient Descent(62/99): loss=0.39032061952471947, gradient=0.000769794821268191\n",
      "Gradient Descent(63/99): loss=0.3903205620314248, gradient=0.0007567864897508731\n",
      "Gradient Descent(64/99): loss=0.3903205064211486, gradient=0.000744154487753157\n",
      "Gradient Descent(65/99): loss=0.3903204526096588, gradient=0.000731886444844258\n",
      "Gradient Descent(66/99): loss=0.39032040051706607, gradient=0.0007199704107322624\n",
      "Gradient Descent(67/99): loss=0.3903203500675803, gradient=0.0007083948368134972\n",
      "Gradient Descent(68/99): loss=0.39032030118928224, gradient=0.0006971485592205254\n",
      "Gradient Descent(69/99): loss=0.3903202538139096, gradient=0.0006862207831944509\n",
      "Gradient Descent(70/99): loss=0.3903202078766544, gradient=0.0006756010686305472\n",
      "Gradient Descent(71/99): loss=0.3903201633159735, gradient=0.0006652793166644945\n",
      "Gradient Descent(72/99): loss=0.39032012007341094, gradient=0.0006552457571822513\n",
      "Gradient Descent(73/99): loss=0.39032007809342917, gradient=0.0006454909371512453\n",
      "Gradient Descent(74/99): loss=0.39032003732325077, gradient=0.0006360057096833072\n",
      "Gradient Descent(75/99): loss=0.39031999771271, gradient=0.0006267812237488273\n",
      "Gradient Descent(76/99): loss=0.39031995921411156, gradient=0.0006178089144737788\n",
      "Gradient Descent(77/99): loss=0.3903199217820993, gradient=0.0006090804939565452\n",
      "Gradient Descent(78/99): loss=0.39031988537352935, gradient=0.0006005879425513479\n",
      "Gradient Descent(79/99): loss=0.39031984994735486, gradient=0.0005923235005697479\n",
      "Gradient Descent(80/99): loss=0.39031981546451233, gradient=0.0005842796603582043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(81/99): loss=0.39031978188781824, gradient=0.0005764491587148649\n",
      "Gradient Descent(82/99): loss=0.39031974918186935, gradient=0.0005688249696121524\n",
      "Gradient Descent(83/99): loss=0.3903197173129487, gradient=0.0005614002971970111\n",
      "Gradient Descent(84/99): loss=0.3903196862489382, gradient=0.0005541685690434035\n",
      "Gradient Descent(85/99): loss=0.3903196559592346, gradient=0.0005471234296344887\n",
      "Gradient Descent(86/99): loss=0.3903196264146702, gradient=0.0005402587340557597\n",
      "Gradient Descent(87/99): loss=0.39031959758743867, gradient=0.0005335685418815018\n",
      "Gradient Descent(88/99): loss=0.39031956945102464, gradient=0.0005270471112403278\n",
      "Gradient Descent(89/99): loss=0.39031954198013674, gradient=0.0005206888930464171\n",
      "Gradient Descent(90/99): loss=0.3903195151506449, gradient=0.000514488525385948\n",
      "Gradient Descent(91/99): loss=0.39031948893952045, gradient=0.0005084408280482178\n",
      "Gradient Descent(92/99): loss=0.39031946332477974, gradient=0.000502540797194202\n",
      "Gradient Descent(93/99): loss=0.3903194382854312, gradient=0.0004967836001540911\n",
      "Gradient Descent(94/99): loss=0.3903194138014245, gradient=0.0004911645703490545\n",
      "Gradient Descent(95/99): loss=0.3903193898536025, gradient=0.0004856792023308768\n",
      "Gradient Descent(96/99): loss=0.390319366423657, gradient=0.0004803231469354917\n",
      "Gradient Descent(97/99): loss=0.3903193434940853, gradient=0.0004750922065468293\n",
      "Gradient Descent(98/99): loss=0.3903193210481501, gradient=0.00046998233046712987\n",
      "Gradient Descent(99/99): loss=0.3903192990698406, gradient=0.0004649896103917098\n",
      "Gradient Descent(0/99): loss=0.38997227296038484, gradient=0.010005307329779828\n",
      "Gradient Descent(1/99): loss=0.3899660459370328, gradient=0.008238824327180403\n",
      "Gradient Descent(2/99): loss=0.3899616372618778, gradient=0.0069048675852880536\n",
      "Gradient Descent(3/99): loss=0.3899584043899424, gradient=0.005888684514016837\n",
      "Gradient Descent(4/99): loss=0.38995594857512594, gradient=0.005111767468106451\n",
      "Gradient Descent(5/99): loss=0.3899540201970616, gradient=0.004511308471120966\n",
      "Gradient Descent(6/99): loss=0.38995245804324463, gradient=0.0040466232611329495\n",
      "Gradient Descent(7/99): loss=0.389951158449971, gradient=0.003680188539347518\n",
      "Gradient Descent(8/99): loss=0.38995005281418027, gradient=0.0033862585560750515\n",
      "Gradient Descent(9/99): loss=0.38994909471048317, gradient=0.003146018815290945\n",
      "Gradient Descent(10/99): loss=0.38994825194818217, gradient=0.002945866969816092\n",
      "Gradient Descent(11/99): loss=0.38994750162810604, gradient=0.0027760246310297816\n",
      "Gradient Descent(12/99): loss=0.3899468270322597, gradient=0.0026294641365283158\n",
      "Gradient Descent(13/99): loss=0.3899462156412886, gradient=0.0025011098379171537\n",
      "Gradient Descent(14/99): loss=0.38994565785189617, gradient=0.002387261179220789\n",
      "Gradient Descent(15/99): loss=0.3899451461330656, gradient=0.002285184803353506\n",
      "Gradient Descent(16/99): loss=0.3899446744606224, gradient=0.002192830102276632\n",
      "Gradient Descent(17/99): loss=0.38994423793073696, gradient=0.0021086322646625306\n",
      "Gradient Descent(18/99): loss=0.3899438328106957, gradient=0.0020296907215989423\n",
      "Gradient Descent(19/99): loss=0.38994345532675295, gradient=0.001958567952143828\n",
      "Gradient Descent(20/99): loss=0.38994310261386667, gradient=0.001892637011743392\n",
      "Gradient Descent(21/99): loss=0.3899427722191282, gradient=0.0018312736633042874\n",
      "Gradient Descent(22/99): loss=0.3899424620247767, gradient=0.0017739620387106883\n",
      "Gradient Descent(23/99): loss=0.3899421701882793, gradient=0.0017202707198645968\n",
      "Gradient Descent(24/99): loss=0.3899418950956956, gradient=0.001669835124262766\n",
      "Gradient Descent(25/99): loss=0.38994163532472575, gradient=0.0016223442804966864\n",
      "Gradient Descent(26/99): loss=0.3899413897733668, gradient=0.0015764909143729743\n",
      "Gradient Descent(27/99): loss=0.3899411571442183, gradient=0.0015341687254317295\n",
      "Gradient Descent(28/99): loss=0.3899409364340231, gradient=0.0014940925796297002\n",
      "Gradient Descent(29/99): loss=0.38994072674216457, gradient=0.0014560834725452516\n",
      "Gradient Descent(30/99): loss=0.389940527257068, gradient=0.0014199835880973352\n",
      "Gradient Descent(31/99): loss=0.3899403372448521, gradient=0.0013856530052897068\n",
      "Gradient Descent(32/99): loss=0.38994015603977417, gradient=0.0013529670089899076\n",
      "Gradient Descent(33/99): loss=0.3899399830361414, gradient=0.0013218138779216959\n",
      "Gradient Descent(34/99): loss=0.3899398176814168, gradient=0.0012920930534328285\n",
      "Gradient Descent(35/99): loss=0.38993965947031495, gradient=0.0012637136147477378\n",
      "Gradient Descent(36/99): loss=0.3899395079397147, gradient=0.0012365930028100409\n",
      "Gradient Descent(37/99): loss=0.3899393626642578, gradient=0.0012106559471256923\n",
      "Gradient Descent(38/99): loss=0.38993922325251823, gradient=0.0011858335593721552\n",
      "Gradient Descent(39/99): loss=0.38993908934365645, gradient=0.0011620625647364002\n",
      "Gradient Descent(40/99): loss=0.3899389606044843, gradient=0.001139284647542634\n",
      "Gradient Descent(41/99): loss=0.38993883672687796, gradient=0.001117445892124962\n",
      "Gradient Descent(42/99): loss=0.3899387174254903, gradient=0.0010964963033833813\n",
      "Gradient Descent(43/99): loss=0.3899386024357208, gradient=0.0010763893942408124\n",
      "Gradient Descent(44/99): loss=0.38993849151190674, gradient=0.0010570818294537954\n",
      "Gradient Descent(45/99): loss=0.389938384425706, gradient=0.0010385331170365845\n",
      "Gradient Descent(46/99): loss=0.3899382809646507, gradient=0.0010207053400293428\n",
      "Gradient Descent(47/99): loss=0.3899381809308425, gradient=0.0010035629225423445\n",
      "Gradient Descent(48/99): loss=0.38993808413978, gradient=0.0009870724249951741\n",
      "Gradient Descent(49/99): loss=0.3899379904192973, gradient=0.0009712023642842858\n",
      "Gradient Descent(50/99): loss=0.38993789960860364, gradient=0.0009559230552853414\n",
      "Gradient Descent(51/99): loss=0.38993781155741064, gradient=0.0009412064706573861\n",
      "Gradient Descent(52/99): loss=0.38993772612513755, gradient=0.0009270261163817061\n",
      "Gradient Descent(53/99): loss=0.38993764318019, gradient=0.0009133569208590885\n",
      "Gradient Descent(54/99): loss=0.38993756259929596, gradient=0.0009001751357163461\n",
      "Gradient Descent(55/99): loss=0.38993748426690367, gradient=0.0008874582467474259\n",
      "Gradient Descent(56/99): loss=0.38993740807462623, gradient=0.0008751848936474794\n",
      "Gradient Descent(57/99): loss=0.3899373339207335, gradient=0.0008633347973933712\n",
      "Gradient Descent(58/99): loss=0.38993726170968446, gradient=0.0008518886942905211\n",
      "Gradient Descent(59/99): loss=0.3899371913516974, gradient=0.0008408282758467387\n",
      "Gradient Descent(60/99): loss=0.3899371227623538, gradient=0.0008301361337533411\n",
      "Gradient Descent(61/99): loss=0.3899370558622322, gradient=0.000819795709355497\n",
      "Gradient Descent(62/99): loss=0.3899369905765708, gradient=0.0008097912470813872\n",
      "Gradient Descent(63/99): loss=0.3899369268349558, gradient=0.000800107751372993\n",
      "Gradient Descent(64/99): loss=0.3899368645710324, gradient=0.0007907309467253335\n",
      "Gradient Descent(65/99): loss=0.3899368037222369, gradient=0.0007816472404947004\n",
      "Gradient Descent(66/99): loss=0.38993674422954994, gradient=0.000772843688183582\n",
      "Gradient Descent(67/99): loss=0.38993668603726506, gradient=0.0007643079609484313\n",
      "Gradient Descent(68/99): loss=0.38993662909277677, gradient=0.0007560283151118481\n",
      "Gradient Descent(69/99): loss=0.38993657334638104, gradient=0.0007479935634888014\n",
      "Gradient Descent(70/99): loss=0.38993651875109175, gradient=0.0007401930483620857\n",
      "Gradient Descent(71/99): loss=0.38993646526246856, gradient=0.0007326166159632876\n",
      "Gradient Descent(72/99): loss=0.389936412838458, gradient=0.0007252545923342473\n",
      "Gradient Descent(73/99): loss=0.3899363614392447, gradient=0.0007180977604591242\n",
      "Gradient Descent(74/99): loss=0.3899363110271121, gradient=0.0007111373385712348\n",
      "Gradient Descent(75/99): loss=0.3899362616199414, gradient=0.0007036055662891191\n",
      "Gradient Descent(76/99): loss=0.38993621313105853, gradient=0.0006970044137392217\n",
      "Gradient Descent(77/99): loss=0.3899361655273475, gradient=0.0006905781125144796\n",
      "Gradient Descent(78/99): loss=0.3899361187783741, gradient=0.0006843176153964944\n",
      "Gradient Descent(79/99): loss=0.38993607285505233, gradient=0.0006782158539881735\n",
      "Gradient Descent(80/99): loss=0.3899360277297992, gradient=0.0006722661980367464\n",
      "Gradient Descent(81/99): loss=0.3899359833763872, gradient=0.0006664623504388036\n",
      "Gradient Descent(82/99): loss=0.3899359397698754, gradient=0.0006607983210832183\n",
      "Gradient Descent(83/99): loss=0.3899358968865294, gradient=0.0006552684081996865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(84/99): loss=0.38993585470375214, gradient=0.0006498671823689668\n",
      "Gradient Descent(85/99): loss=0.38993581320001636, gradient=0.0006445894722293003\n",
      "Gradient Descent(86/99): loss=0.38993577235480303, gradient=0.0006394303513971791\n",
      "Gradient Descent(87/99): loss=0.3899357321485436, gradient=0.0006343851263151581\n",
      "Gradient Descent(88/99): loss=0.38993569256256605, gradient=0.0006294493248473461\n",
      "Gradient Descent(89/99): loss=0.38993565357904286, gradient=0.0006246186855097379\n",
      "Gradient Descent(90/99): loss=0.38993561518094544, gradient=0.0006198891472599728\n",
      "Gradient Descent(91/99): loss=0.389935577351998, gradient=0.0006152568397943285\n",
      "Gradient Descent(92/99): loss=0.38993554007663694, gradient=0.0006107180743143613\n",
      "Gradient Descent(93/99): loss=0.38993550333997073, gradient=0.0006062693347328228\n",
      "Gradient Descent(94/99): loss=0.38993546712774313, gradient=0.0006019072692953154\n",
      "Gradient Descent(95/99): loss=0.3899354314262997, gradient=0.0005976286825960892\n",
      "Gradient Descent(96/99): loss=0.38993539622255424, gradient=0.0005934305279695314\n",
      "Gradient Descent(97/99): loss=0.38993536150395897, gradient=0.0005893099002402243\n",
      "Gradient Descent(98/99): loss=0.38993532725847563, gradient=0.0005852640288156675\n",
      "Gradient Descent(99/99): loss=0.3899352934745487, gradient=0.000581290271106065\n",
      "Gradient Descent(0/99): loss=0.3896731696251952, gradient=0.011070371076060187\n",
      "Gradient Descent(1/99): loss=0.38966819018735716, gradient=0.007440534397198941\n",
      "Gradient Descent(2/99): loss=0.38966465820184665, gradient=0.00617851542593857\n",
      "Gradient Descent(3/99): loss=0.38966206343061127, gradient=0.005279395079443537\n",
      "Gradient Descent(4/99): loss=0.3896601184337025, gradient=0.004560423475702313\n",
      "Gradient Descent(5/99): loss=0.38965863268907497, gradient=0.003976098323885972\n",
      "Gradient Descent(6/99): loss=0.38965747555151564, gradient=0.0035012084809633117\n",
      "Gradient Descent(7/99): loss=0.3896565576916035, gradient=0.003111688230004371\n",
      "Gradient Descent(8/99): loss=0.3896558169273157, gradient=0.0027899006501798276\n",
      "Gradient Descent(9/99): loss=0.38965520941277937, gradient=0.0025219503133323416\n",
      "Gradient Descent(10/99): loss=0.3896547038043714, gradient=0.0022969311984467793\n",
      "Gradient Descent(11/99): loss=0.389654277370013, gradient=0.0021063107318411082\n",
      "Gradient Descent(12/99): loss=0.3896539133759088, gradient=0.0019434260527989597\n",
      "Gradient Descent(13/99): loss=0.3896535993169545, gradient=0.0018030755126781585\n",
      "Gradient Descent(14/99): loss=0.38965332570774447, gradient=0.001681191268124344\n",
      "Gradient Descent(15/99): loss=0.38965308524847375, gradient=0.0015745802370926542\n",
      "Gradient Descent(16/99): loss=0.3896528722433361, gradient=0.0014807216945828927\n",
      "Gradient Descent(17/99): loss=0.3896526821903212, gradient=0.001397610872005325\n",
      "Gradient Descent(18/99): loss=0.3896525114883646, gradient=0.0013236392029791118\n",
      "Gradient Descent(19/99): loss=0.38965235722560737, gradient=0.001257503258329502\n",
      "Gradient Descent(20/99): loss=0.3896522170242845, gradient=0.0011981358125171995\n",
      "Gradient Descent(21/99): loss=0.38965208892559206, gradient=0.0011446537801156223\n",
      "Gradient Descent(22/99): loss=0.3896519713030972, gradient=0.0010963188930580175\n",
      "Gradient Descent(23/99): loss=0.38965186279678066, gradient=0.0010525079346587558\n",
      "Gradient Descent(24/99): loss=0.38965176226217674, gradient=0.0010126901092981013\n",
      "Gradient Descent(25/99): loss=0.38965166873070023, gradient=0.000976409726625423\n",
      "Gradient Descent(26/99): loss=0.38965158137836375, gradient=0.000943272841763335\n",
      "Gradient Descent(27/99): loss=0.3896514995008697, gradient=0.0009129368443692802\n",
      "Gradient Descent(28/99): loss=0.3896514224935979, gradient=0.0008851022531940701\n",
      "Gradient Descent(29/99): loss=0.38965134983539373, gradient=0.0008595061690095444\n",
      "Gradient Descent(30/99): loss=0.38965128107534364, gradient=0.0008359169837215016\n",
      "Gradient Descent(31/99): loss=0.3896512158219106, gradient=0.0008141300499393397\n",
      "Gradient Descent(32/99): loss=0.38965115373396186, gradient=0.0007939640931162581\n",
      "Gradient Descent(33/99): loss=0.38965109451331426, gradient=0.0007752582051018553\n",
      "Gradient Descent(34/99): loss=0.3896510378985161, gradient=0.0007578692991954335\n",
      "Gradient Descent(35/99): loss=0.3896509836596324, gradient=0.0007416699367479455\n",
      "Gradient Descent(36/99): loss=0.3896509315938587, gradient=0.0007265464571251914\n",
      "Gradient Descent(37/99): loss=0.38965088152181526, gradient=0.0007123973586925221\n",
      "Gradient Descent(38/99): loss=0.38965083328440536, gradient=0.0006991318900641996\n",
      "Gradient Descent(39/99): loss=0.38965078674014575, gradient=0.0006866688193850031\n",
      "Gradient Descent(40/99): loss=0.3896507417628883, gradient=0.000674935355740406\n",
      "Gradient Descent(41/99): loss=0.3896506982398709, gradient=0.0006638662015476645\n",
      "Gradient Descent(42/99): loss=0.3896506560700465, gradient=0.0006534027184078124\n",
      "Gradient Descent(43/99): loss=0.389650615162645, gradient=0.0006434921917098885\n",
      "Gradient Descent(44/99): loss=0.3896505754359343, gradient=0.0006340871814992924\n",
      "Gradient Descent(45/99): loss=0.38965053681614664, gradient=0.0006251449489077205\n",
      "Gradient Descent(46/99): loss=0.3896504992365516, gradient=0.00061662694890325\n",
      "Gradient Descent(47/99): loss=0.3896504626366474, gradient=0.0006084983813360871\n",
      "Gradient Descent(48/99): loss=0.38965042696145974, gradient=0.0006007277932813616\n",
      "Gradient Descent(49/99): loss=0.38965039216092734, gradient=0.0005932867265571208\n",
      "Gradient Descent(50/99): loss=0.3896503581893663, gradient=0.0005861494050507066\n",
      "Gradient Descent(51/99): loss=0.38965032500499913, gradient=0.0005792924571423385\n",
      "Gradient Descent(52/99): loss=0.38965029256954303, gradient=0.0005726946690860768\n",
      "Gradient Descent(53/99): loss=0.3896502608478449, gradient=0.000566336765709699\n",
      "Gradient Descent(54/99): loss=0.3896502298075613, gradient=0.0005602012152328986\n",
      "Gradient Descent(55/99): loss=0.3896501994188758, gradient=0.0005542720553894077\n",
      "Gradient Descent(56/99): loss=0.38965016965424687, gradient=0.0005485347383780523\n",
      "Gradient Descent(57/99): loss=0.3896501404881864, gradient=0.000542975992464582\n",
      "Gradient Descent(58/99): loss=0.3896501118970613, gradient=0.0005375836983194407\n",
      "Gradient Descent(59/99): loss=0.389650083858918, gradient=0.0005323467784056344\n",
      "Gradient Descent(60/99): loss=0.3896500563533251, gradient=0.0005272550979338575\n",
      "Gradient Descent(61/99): loss=0.38965002936123344, gradient=0.0005222993760789183\n",
      "Gradient Descent(62/99): loss=0.3896500028648509, gradient=0.000517471106308498\n",
      "Gradient Descent(63/99): loss=0.38964997684752967, gradient=0.0005127624848102247\n",
      "Gradient Descent(64/99): loss=0.3896499512936658, gradient=0.0005081663461245995\n",
      "Gradient Descent(65/99): loss=0.3896499261886091, gradient=0.0005036761051963294\n",
      "Gradient Descent(66/99): loss=0.38964990151858026, gradient=0.0004992857051477987\n",
      "Gradient Descent(67/99): loss=0.38964987727059913, gradient=0.0004949895701607447\n",
      "Gradient Descent(68/99): loss=0.3896498534324175, gradient=0.0004907825629230365\n",
      "Gradient Descent(69/99): loss=0.38964982999245995, gradient=0.0004866599461586855\n",
      "Gradient Descent(70/99): loss=0.3896498069397692, gradient=0.0004826173478162123\n",
      "Gradient Descent(71/99): loss=0.38964978426395736, gradient=0.0004786507295361468\n",
      "Gradient Descent(72/99): loss=0.389649761955162, gradient=0.0004747563580631854\n",
      "Gradient Descent(73/99): loss=0.3896497400040051, gradient=0.0004709307793039218\n",
      "Gradient Descent(74/99): loss=0.3896497184015572, gradient=0.00046717079476495506\n",
      "Gradient Descent(75/99): loss=0.3896496971393037, gradient=0.00046347344013496303\n",
      "Gradient Descent(76/99): loss=0.38964967620911467, gradient=0.00045983596579967804\n",
      "Gradient Descent(77/99): loss=0.38964965560321735, gradient=0.0004562558191011029\n",
      "Gradient Descent(78/99): loss=0.389649635314171, gradient=0.0004527306281731946\n",
      "Gradient Descent(79/99): loss=0.3896496153348437, gradient=0.00044925818720238067\n",
      "Gradient Descent(80/99): loss=0.38964959565839163, gradient=0.00044583644297839975\n",
      "Gradient Descent(81/99): loss=0.3896495762782399, gradient=0.00044246348261413587\n",
      "Gradient Descent(82/99): loss=0.3896495571880638, gradient=0.0004391375223251034\n",
      "Gradient Descent(83/99): loss=0.3896495383817748, gradient=0.0004358568971714127\n",
      "Gradient Descent(84/99): loss=0.389649519853504, gradient=0.00043262005167352135\n",
      "Gradient Descent(85/99): loss=0.3896495015975892, gradient=0.00042942553122231504\n",
      "Gradient Descent(86/99): loss=0.38964948360856216, gradient=0.00042627197421235097\n",
      "Gradient Descent(87/99): loss=0.3896494658811376, gradient=0.00042315810483320676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(88/99): loss=0.38964944841020177, gradient=0.0004200827264601812\n",
      "Gradient Descent(89/99): loss=0.3896494311908039, gradient=0.00041704471559245355\n",
      "Gradient Descent(90/99): loss=0.38964941421814586, gradient=0.00041404301628929853\n",
      "Gradient Descent(91/99): loss=0.38964939748757477, gradient=0.00041107663506252125\n",
      "Gradient Descent(92/99): loss=0.3896493809945745, gradient=0.0004081446361844183\n",
      "Gradient Descent(93/99): loss=0.38964936473476003, gradient=0.00040524613737624444\n",
      "Gradient Descent(94/99): loss=0.38964934870386914, gradient=0.0004023803058445992\n",
      "Gradient Descent(95/99): loss=0.3896493328977574, gradient=0.00039954635463577395\n",
      "Gradient Descent(96/99): loss=0.38964931731239166, gradient=0.00039674353928189276\n",
      "Gradient Descent(97/99): loss=0.38964930194384595, gradient=0.00039397115471345116\n",
      "Gradient Descent(98/99): loss=0.3896492867882954, gradient=0.0003912285324168421\n",
      "Gradient Descent(99/99): loss=0.3896492718420125, gradient=0.0003885150378159465\n",
      "Gradient Descent(0/99): loss=0.3889585858306488, gradient=0.008188849812201734\n",
      "Gradient Descent(1/99): loss=0.3889558223907663, gradient=0.005552674436252812\n",
      "Gradient Descent(2/99): loss=0.38895390378804623, gradient=0.004565737741783269\n",
      "Gradient Descent(3/99): loss=0.38895252215045195, gradient=0.0038612471639201274\n",
      "Gradient Descent(4/99): loss=0.38895150238682413, gradient=0.003307763001305893\n",
      "Gradient Descent(5/99): loss=0.3889507299044368, gradient=0.0028695506919990724\n",
      "Gradient Descent(6/99): loss=0.38895012802025625, gradient=0.0025247198657268564\n",
      "Gradient Descent(7/99): loss=0.3889496455194705, gradient=0.0022531085124647423\n",
      "Gradient Descent(8/99): loss=0.3889492477855687, gradient=0.0020391794602885645\n",
      "Gradient Descent(9/99): loss=0.3889489112317569, gradient=0.0018703125116203108\n",
      "Gradient Descent(10/99): loss=0.3889486196415084, gradient=0.0017363550127617085\n",
      "Gradient Descent(11/99): loss=0.38894836175321723, gradient=0.001629246831705514\n",
      "Gradient Descent(12/99): loss=0.3889481296584895, gradient=0.001542682978247595\n",
      "Gradient Descent(13/99): loss=0.38894791773470994, gradient=0.0014718013463045786\n",
      "Gradient Descent(14/99): loss=0.3889477219298655, gradient=0.001412898088342744\n",
      "Gradient Descent(15/99): loss=0.3889475392806309, gradient=0.001363177699135369\n",
      "Gradient Descent(16/99): loss=0.38894736758560633, gradient=0.0013205427004959651\n",
      "Gradient Descent(17/99): loss=0.3889472051822598, gradient=0.001283423409336462\n",
      "Gradient Descent(18/99): loss=0.38894705079353603, gradient=0.0012506445218253238\n",
      "Gradient Descent(19/99): loss=0.38894690342152294, gradient=0.0012213231436041998\n",
      "Gradient Descent(20/99): loss=0.38894676227309727, gradient=0.0011947922903745513\n",
      "Gradient Descent(21/99): loss=0.3889466267074373, gradient=0.0011705442416795218\n",
      "Gradient Descent(22/99): loss=0.38894649619859556, gradient=0.0011481889504814376\n",
      "Gradient Descent(23/99): loss=0.3889463703085162, gradient=0.0011274236522693163\n",
      "Gradient Descent(24/99): loss=0.38894624866736455, gradient=0.0011080106962916788\n",
      "Gradient Descent(25/99): loss=0.3889461309590081, gradient=0.0010897613620771043\n",
      "Gradient Descent(26/99): loss=0.388946016910173, gradient=0.0010725240117728562\n",
      "Gradient Descent(27/99): loss=0.38894590628224096, gradient=0.0010561753770537794\n",
      "Gradient Descent(28/99): loss=0.38894579886496305, gradient=0.0010406141127678788\n",
      "Gradient Descent(29/99): loss=0.388945694471588, gradient=0.0010257559932820726\n",
      "Gradient Descent(30/99): loss=0.3889455929350352, gradient=0.001011530303734413\n",
      "Gradient Descent(31/99): loss=0.38894549410485474, gradient=0.0009978771048758496\n",
      "Gradient Descent(32/99): loss=0.38894539784478804, gradient=0.0009847451405524956\n",
      "Gradient Descent(33/99): loss=0.38894530403078464, gradient=0.0009720902213175285\n",
      "Gradient Descent(34/99): loss=0.38894521254937836, gradient=0.0009598739636004333\n",
      "Gradient Descent(35/99): loss=0.388945123296344, gradient=0.0009480627966538256\n",
      "Gradient Descent(36/99): loss=0.3889450361755737, gradient=0.0009366271729674028\n",
      "Gradient Descent(37/99): loss=0.3889449510981345, gradient=0.0009255409346974543\n",
      "Gradient Descent(38/99): loss=0.3889448679814674, gradient=0.000914780800827576\n",
      "Gradient Descent(39/99): loss=0.3889447868825197, gradient=0.0009027792697172631\n",
      "Gradient Descent(40/99): loss=0.38894470757923005, gradient=0.0008927239021510466\n",
      "Gradient Descent(41/99): loss=0.38894463000581087, gradient=0.0008829314057657472\n",
      "Gradient Descent(42/99): loss=0.38894455410062445, gradient=0.0008733865328995654\n",
      "Gradient Descent(43/99): loss=0.388944479805721, gradient=0.000864075548927974\n",
      "Gradient Descent(44/99): loss=0.38894440706650707, gradient=0.0008549860207808467\n",
      "Gradient Descent(45/99): loss=0.38894433583144133, gradient=0.0008461066432058783\n",
      "Gradient Descent(46/99): loss=0.3889442660517715, gradient=0.0008374270947148069\n",
      "Gradient Descent(47/99): loss=0.3889441976813023, gradient=0.0008289379172165506\n",
      "Gradient Descent(48/99): loss=0.38894413067619005, gradient=0.0008206304146969213\n",
      "Gradient Descent(49/99): loss=0.38894406499476075, gradient=0.0008124965673225877\n",
      "Gradient Descent(50/99): loss=0.38894400059734513, gradient=0.0008045289581236738\n",
      "Gradient Descent(51/99): loss=0.38894393744613465, gradient=0.0007967207100062958\n",
      "Gradient Descent(52/99): loss=0.3889438755050476, gradient=0.0007890654313059505\n",
      "Gradient Descent(53/99): loss=0.3889438147396119, gradient=0.000781557168450124\n",
      "Gradient Descent(54/99): loss=0.38894375511685586, gradient=0.0007741903645768112\n",
      "Gradient Descent(55/99): loss=0.38894369660521055, gradient=0.0007669598231750742\n",
      "Gradient Descent(56/99): loss=0.3889436391744208, gradient=0.0007598606759848379\n",
      "Gradient Descent(57/99): loss=0.3889435827954637, gradient=0.0007528883545320319\n",
      "Gradient Descent(58/99): loss=0.3889435274404731, gradient=0.0007460385647828678\n",
      "Gradient Descent(59/99): loss=0.3889434730826715, gradient=0.000739307264489415\n",
      "Gradient Descent(60/99): loss=0.38894341969630786, gradient=0.0007326906428698237\n",
      "Gradient Descent(61/99): loss=0.3889433672565973, gradient=0.0007261851023231715\n",
      "Gradient Descent(62/99): loss=0.3889433157396702, gradient=0.0007197872419263304\n",
      "Gradient Descent(63/99): loss=0.38894326512252086, gradient=0.000713493842498342\n",
      "Gradient Descent(64/99): loss=0.38894321538296184, gradient=0.0007073018530497113\n",
      "Gradient Descent(65/99): loss=0.3889431664995817, gradient=0.000701208378459513\n",
      "Gradient Descent(66/99): loss=0.38894311845170604, gradient=0.000695210668246338\n",
      "Gradient Descent(67/99): loss=0.3889430712193594, gradient=0.0006893061063154338\n",
      "Gradient Descent(68/99): loss=0.38894302478323234, gradient=0.0006834922015811145\n",
      "Gradient Descent(69/99): loss=0.38894297912464876, gradient=0.0006777665793765406\n",
      "Gradient Descent(70/99): loss=0.38894293422553644, gradient=0.0006721269735719238\n",
      "Gradient Descent(71/99): loss=0.38894289006839877, gradient=0.000666571219334225\n",
      "Gradient Descent(72/99): loss=0.38894284663628936, gradient=0.0006610972464672139\n",
      "Gradient Descent(73/99): loss=0.38894280391278696, gradient=0.0006557030732786814\n",
      "Gradient Descent(74/99): loss=0.38894276188197274, gradient=0.0006503868009270173\n",
      "Gradient Descent(75/99): loss=0.3889427205284087, gradient=0.0006451466082048142\n",
      "Gradient Descent(76/99): loss=0.38894267983711805, gradient=0.0006399807467211427\n",
      "Gradient Descent(77/99): loss=0.388942639793565, gradient=0.0006348875364485211\n",
      "Gradient Descent(78/99): loss=0.3889426003836375, gradient=0.000629865361603522\n",
      "Gradient Descent(79/99): loss=0.38894256159363017, gradient=0.0006249126668331512\n",
      "Gradient Descent(80/99): loss=0.38894252341022856, gradient=0.000620027953682239\n",
      "Gradient Descent(81/99): loss=0.38894248582049346, gradient=0.0006152097773181298\n",
      "Gradient Descent(82/99): loss=0.38894244881184753, gradient=0.0006104567434926237\n",
      "Gradient Descent(83/99): loss=0.38894241237206006, gradient=0.0006057675057220456\n",
      "Gradient Descent(84/99): loss=0.38894237648923613, gradient=0.0006011407626677286\n",
      "Gradient Descent(85/99): loss=0.38894234115180304, gradient=0.0005965752557018878\n",
      "Gradient Descent(86/99): loss=0.38894230634849936, gradient=0.0005920697666438278\n",
      "Gradient Descent(87/99): loss=0.3889422720683635, gradient=0.0005876231156534841\n",
      "Gradient Descent(88/99): loss=0.3889422383007237, gradient=0.000583234159270299\n",
      "Gradient Descent(89/99): loss=0.3889422050351875, gradient=0.0005789017885862483\n",
      "Gradient Descent(90/99): loss=0.38894217226163286, gradient=0.0005746249275424859\n",
      "Gradient Descent(91/99): loss=0.38894213997019855, gradient=0.000570402531340801\n",
      "Gradient Descent(92/99): loss=0.38894210815127583, gradient=0.0005662335849608389\n",
      "Gradient Descent(93/99): loss=0.38894207679550047, gradient=0.0005621171017751036\n",
      "Gradient Descent(94/99): loss=0.3889420458937434, gradient=0.0005580521222547753\n",
      "Gradient Descent(95/99): loss=0.3889420154371055, gradient=0.0005540377127592007\n",
      "Gradient Descent(96/99): loss=0.3889419854169081, gradient=0.000550072964403186\n",
      "Gradient Descent(97/99): loss=0.38894195582468716, gradient=0.0005461569919958724\n",
      "Gradient Descent(98/99): loss=0.38894192665218724, gradient=0.0005422889330465115\n",
      "Gradient Descent(99/99): loss=0.3889418978913529, gradient=0.0005384679468319908\n",
      "Gradient Descent(0/99): loss=0.39014036984723327, gradient=0.01539811860044767\n",
      "Gradient Descent(1/99): loss=0.3901364555292294, gradient=0.0069187678918690735\n",
      "Gradient Descent(2/99): loss=0.39013380745094217, gradient=0.005359768528549623\n",
      "Gradient Descent(3/99): loss=0.39013181641058836, gradient=0.004615016537700862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(4/99): loss=0.39013027620000523, gradient=0.004047789840222204\n",
      "Gradient Descent(5/99): loss=0.39012905889933275, gradient=0.0035901819708165634\n",
      "Gradient Descent(6/99): loss=0.39012807708020314, gradient=0.003217166319772363\n",
      "Gradient Descent(7/99): loss=0.3901272699775388, gradient=0.0029109035829347615\n",
      "Gradient Descent(8/99): loss=0.39012659469127875, gradient=0.0026575642967355323\n",
      "Gradient Descent(9/99): loss=0.3901260205059872, gradient=0.002446347894485293\n",
      "Gradient Descent(10/99): loss=0.39012552510919546, gradient=0.002268806985273571\n",
      "Gradient Descent(11/99): loss=0.3901250920598341, gradient=0.0021183249837916597\n",
      "Gradient Descent(12/99): loss=0.3901247090713037, gradient=0.0019897085874564092\n",
      "Gradient Descent(13/99): loss=0.390124366834575, gradient=0.0018788701257059191\n",
      "Gradient Descent(14/99): loss=0.39012405820200574, gradient=0.0017825805598823974\n",
      "Gradient Descent(15/99): loss=0.3901237776147973, gradient=0.0016982779554448164\n",
      "Gradient Descent(16/99): loss=0.3901235206966824, gradient=0.0016239193540794686\n",
      "Gradient Descent(17/99): loss=0.3901232839622409, gradient=0.0015578664414623656\n",
      "Gradient Descent(18/99): loss=0.39012306460508894, gradient=0.0014987974023369527\n",
      "Gradient Descent(19/99): loss=0.39012286034231913, gradient=0.0014456389758476466\n",
      "Gradient Descent(20/99): loss=0.3901226692989672, gradient=0.0013975140352030606\n",
      "Gradient Descent(21/99): loss=0.3901224899212705, gradient=0.0013537010663246557\n",
      "Gradient Descent(22/99): loss=0.39012232091084015, gradient=0.0013136027527620164\n",
      "Gradient Descent(23/99): loss=0.39012216117419435, gradient=0.001276721526934054\n",
      "Gradient Descent(24/99): loss=0.39012200978367884, gradient=0.0012426404545546204\n",
      "Gradient Descent(25/99): loss=0.3901218659469209, gradient=0.0012110082093184581\n",
      "Gradient Descent(26/99): loss=0.39012172898273617, gradient=0.001181527193356413\n",
      "Gradient Descent(27/99): loss=0.39012159830196413, gradient=0.0011539440860189286\n",
      "Gradient Descent(28/99): loss=0.39012147339210107, gradient=0.0011280422756244357\n",
      "Gradient Descent(29/99): loss=0.39012135380488533, gradient=0.0011036357588960963\n",
      "Gradient Descent(30/99): loss=0.39012123914619945, gradient=0.0010805641910123162\n",
      "Gradient Descent(31/99): loss=0.3901211290677996, gradient=0.001058688843310505\n",
      "Gradient Descent(32/99): loss=0.3901210232605044, gradient=0.0010378892816589123\n",
      "Gradient Descent(33/99): loss=0.3901209215284401, gradient=0.001017276138590478\n",
      "Gradient Descent(34/99): loss=0.3901208235463812, gradient=0.0009983186584784935\n",
      "Gradient Descent(35/99): loss=0.39012072908869005, gradient=0.0009801649597600377\n",
      "Gradient Descent(36/99): loss=0.3901206379552159, gradient=0.0009627399714138453\n",
      "Gradient Descent(37/99): loss=0.3901205499637238, gradient=0.0009459812827056821\n",
      "Gradient Descent(38/99): loss=0.39012046494850866, gradient=0.000929834020654421\n",
      "Gradient Descent(39/99): loss=0.39012038275814154, gradient=0.0009142496474741837\n",
      "Gradient Descent(40/99): loss=0.39012030325379676, gradient=0.0008991851088820947\n",
      "Gradient Descent(41/99): loss=0.3901202263077648, gradient=0.00088460211837607\n",
      "Gradient Descent(42/99): loss=0.3901201518021846, gradient=0.0008704665460848701\n",
      "Gradient Descent(43/99): loss=0.39012007962795126, gradient=0.0008567478945642828\n",
      "Gradient Descent(44/99): loss=0.39012000968376753, gradient=0.0008434188476924016\n",
      "Gradient Descent(45/99): loss=0.3901199418753262, gradient=0.0008304548813712905\n",
      "Gradient Descent(46/99): loss=0.3901198761145935, gradient=0.0008178339267198146\n",
      "Gradient Descent(47/99): loss=0.3901198123191865, gradient=0.0008055360780155749\n",
      "Gradient Descent(48/99): loss=0.390119750411829, gradient=0.0007935433389093498\n",
      "Gradient Descent(49/99): loss=0.3901196903198713, gradient=0.0007818394014649854\n",
      "Gradient Descent(50/99): loss=0.39011963197487026, gradient=0.00077040945342433\n",
      "Gradient Descent(51/99): loss=0.39011957531221675, gradient=0.0007592400097959939\n",
      "Gradient Descent(52/99): loss=0.3901195202708076, gradient=0.0007483187654497585\n",
      "Gradient Descent(53/99): loss=0.3901194667927536, gradient=0.0007376344658861282\n",
      "Gradient Descent(54/99): loss=0.39011941482312096, gradient=0.0007271767937600003\n",
      "Gradient Descent(55/99): loss=0.39011936430969874, gradient=0.0007169362690841667\n",
      "Gradient Descent(56/99): loss=0.3901193152027934, gradient=0.0007069041613296822\n",
      "Gradient Descent(57/99): loss=0.3901192674550429, gradient=0.0006970724118905928\n",
      "Gradient Descent(58/99): loss=0.3901192210212499, gradient=0.0006874335655899189\n",
      "Gradient Descent(59/99): loss=0.39011917585823325, gradient=0.0006779807100858905\n",
      "Gradient Descent(60/99): loss=0.3901191319246909, gradient=0.0006687074221902825\n",
      "Gradient Descent(61/99): loss=0.3901190891810781, gradient=0.0006596077202438725\n",
      "Gradient Descent(62/99): loss=0.3901190475894969, gradient=0.0006506760218060018\n",
      "Gradient Descent(63/99): loss=0.3901190071135935, gradient=0.0006419071060139489\n",
      "Gradient Descent(64/99): loss=0.39011896771846727, gradient=0.0006332960800499395\n",
      "Gradient Descent(65/99): loss=0.39011892937058595, gradient=0.000624838349227621\n",
      "Gradient Descent(66/99): loss=0.3901188920377082, gradient=0.000616529590270592\n",
      "Gradient Descent(67/99): loss=0.39011885568881377, gradient=0.000608365727410322\n",
      "Gradient Descent(68/99): loss=0.39011882029403666, gradient=0.0006003429109771116\n",
      "Gradient Descent(69/99): loss=0.39011878582460624, gradient=0.0005924574981987308\n",
      "Gradient Descent(70/99): loss=0.39011875225279097, gradient=0.0005847060359554702\n",
      "Gradient Descent(71/99): loss=0.39011871955184774, gradient=0.0005770852452720113\n",
      "Gradient Descent(72/99): loss=0.3901186876959731, gradient=0.0005695920073522699\n",
      "Gradient Descent(73/99): loss=0.3901186566602594, gradient=0.0005622233509871003\n",
      "Gradient Descent(74/99): loss=0.3901186264206535, gradient=0.0005549764411844393\n",
      "Gradient Descent(75/99): loss=0.39011859695391793, gradient=0.0005478485688898406\n",
      "Gradient Descent(76/99): loss=0.39011856823759505, gradient=0.0005408371416797883\n",
      "Gradient Descent(77/99): loss=0.39011854024997256, gradient=0.0005339396753250451\n",
      "Gradient Descent(78/99): loss=0.3901185129700528, gradient=0.0005271537861319342\n",
      "Gradient Descent(79/99): loss=0.39011848637752217, gradient=0.0005204771839803999\n",
      "Gradient Descent(80/99): loss=0.3901184604527236, gradient=0.0005139076659874198\n",
      "Gradient Descent(81/99): loss=0.3901184351766295, gradient=0.0005074431107308176\n",
      "Gradient Descent(82/99): loss=0.39011841053081736, gradient=0.000501081472977724\n",
      "Gradient Descent(83/99): loss=0.39011838649744596, gradient=0.0004948207788665294\n",
      "Gradient Descent(84/99): loss=0.39011836305923314, gradient=0.0004886591214970768\n",
      "Gradient Descent(85/99): loss=0.3901183401994343, gradient=0.00048259465688953597\n",
      "Gradient Descent(86/99): loss=0.390118317901823, gradient=0.00047662560027567763\n",
      "Gradient Descent(87/99): loss=0.390118296150671, gradient=0.00047075022269064325\n",
      "Gradient Descent(88/99): loss=0.3901182749307308, gradient=0.00046496684783645125\n",
      "Gradient Descent(89/99): loss=0.39011825422721874, gradient=0.0004592738491918984\n",
      "Gradient Descent(90/99): loss=0.3901182340257967, gradient=0.0004536696473456176\n",
      "Gradient Descent(91/99): loss=0.390118214312559, gradient=0.00044815270753169616\n",
      "Gradient Descent(92/99): loss=0.39011819507401574, gradient=0.0004427215373493801\n",
      "Gradient Descent(93/99): loss=0.3901181762970786, gradient=0.0004373746846506606\n",
      "Gradient Descent(94/99): loss=0.3901181579690479, gradient=0.00043211073557991743\n",
      "Gradient Descent(95/99): loss=0.3901181400775988, gradient=0.0004269283127530226\n",
      "Gradient Descent(96/99): loss=0.3901181226107691, gradient=0.00042182607356322207\n",
      "Gradient Descent(97/99): loss=0.3901181055569468, gradient=0.00041680270860320755\n",
      "Gradient Descent(98/99): loss=0.3901180889048601, gradient=0.0004118569401931716\n",
      "Gradient Descent(99/99): loss=0.3901180726435636, gradient=0.0004069875210062415\n",
      "Gradient Descent(0/99): loss=0.3903496165397088, gradient=0.0072718360575102755\n",
      "Gradient Descent(1/99): loss=0.39034593900625336, gradient=0.006261742935002815\n",
      "Gradient Descent(2/99): loss=0.39034305050826174, gradient=0.005530050395406299\n",
      "Gradient Descent(3/99): loss=0.3903407222924891, gradient=0.004950738696385541\n",
      "Gradient Descent(4/99): loss=0.39033880270816007, gradient=0.004483981499073915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(5/99): loss=0.3903371881495405, gradient=0.004103091643052497\n",
      "Gradient Descent(6/99): loss=0.3903358064244387, gradient=0.0037882400713331464\n",
      "Gradient Descent(7/99): loss=0.3903346062031849, gradient=0.0035245802406649743\n",
      "Gradient Descent(8/99): loss=0.39033355149912247, gradient=0.0032967726622695273\n",
      "Gradient Descent(9/99): loss=0.39033261340763203, gradient=0.00310470177673605\n",
      "Gradient Descent(10/99): loss=0.3903317710533286, gradient=0.0029386069273764216\n",
      "Gradient Descent(11/99): loss=0.3903310086440911, gradient=0.0027928512318766093\n",
      "Gradient Descent(12/99): loss=0.390330313888873, gradient=0.00266368270906066\n",
      "Gradient Descent(13/99): loss=0.3903296770777638, gradient=0.002548178349992761\n",
      "Gradient Descent(14/99): loss=0.390329090433399, gradient=0.0024440385860046367\n",
      "Gradient Descent(15/99): loss=0.39032854764630903, gradient=0.0023494371507638383\n",
      "Gradient Descent(16/99): loss=0.39032804353690925, gradient=0.002262910696252537\n",
      "Gradient Descent(17/99): loss=0.3903275749484227, gradient=0.002177779579105544\n",
      "Gradient Descent(18/99): loss=0.3903271369782539, gradient=0.002104403354756754\n",
      "Gradient Descent(19/99): loss=0.39032672659384, gradient=0.0020361420561052727\n",
      "Gradient Descent(20/99): loss=0.39032634120524246, gradient=0.001972346114753458\n",
      "Gradient Descent(21/99): loss=0.39032597858052237, gradient=0.0019124766400905415\n",
      "Gradient Descent(22/99): loss=0.39032563678108717, gradient=0.0018560837712197954\n",
      "Gradient Descent(23/99): loss=0.3903253141104582, gradient=0.0018027898439478321\n",
      "Gradient Descent(24/99): loss=0.39032500923795027, gradient=0.0017512421990916627\n",
      "Gradient Descent(25/99): loss=0.3903247206292036, gradient=0.001703396876957996\n",
      "Gradient Descent(26/99): loss=0.39032444722056736, gradient=0.001657113196724342\n",
      "Gradient Descent(27/99): loss=0.3903241878350741, gradient=0.0016136195344495583\n",
      "Gradient Descent(28/99): loss=0.3903239415224823, gradient=0.0015720163939515652\n",
      "Gradient Descent(29/99): loss=0.3903237074213042, gradient=0.0015321581248350863\n",
      "Gradient Descent(30/99): loss=0.39032348474715806, gradient=0.0014939176617447096\n",
      "Gradient Descent(31/99): loss=0.39032327278313234, gradient=0.0014571835287953564\n",
      "Gradient Descent(32/99): loss=0.3903230708716925, gradient=0.0014218573800055545\n",
      "Gradient Descent(33/99): loss=0.390322878407859, gradient=0.0013878519724816226\n",
      "Gradient Descent(34/99): loss=0.3903226948334169, gradient=0.0013550894906360974\n",
      "Gradient Descent(35/99): loss=0.39032251963197123, gradient=0.0013235001561246905\n",
      "Gradient Descent(36/99): loss=0.390322352324702, gradient=0.0012930210710766228\n",
      "Gradient Descent(37/99): loss=0.39032219246669714, gradient=0.001263595252393195\n",
      "Gradient Descent(38/99): loss=0.3903220396437658, gradient=0.0012351708229952907\n",
      "Gradient Descent(39/99): loss=0.39032189346965757, gradient=0.0012077003323663182\n",
      "Gradient Descent(40/99): loss=0.3903217535836189, gradient=0.0011811401839146494\n",
      "Gradient Descent(41/99): loss=0.3903216196482397, gradient=0.0011554501508359038\n",
      "Gradient Descent(42/99): loss=0.39032149134754063, gradient=0.001130592965503122\n",
      "Gradient Descent(43/99): loss=0.39032136838527354, gradient=0.0011065339701182705\n",
      "Gradient Descent(44/99): loss=0.3903212504833983, gradient=0.0010832408185456493\n",
      "Gradient Descent(45/99): loss=0.39032113738071567, gradient=0.0010606832210276309\n",
      "Gradient Descent(46/99): loss=0.390321028831634, gradient=0.0010388327249270142\n",
      "Gradient Descent(47/99): loss=0.3903209246050552, gradient=0.001017662525822168\n",
      "Gradient Descent(48/99): loss=0.3903208244833597, gradient=0.0009971473042442033\n",
      "Gradient Descent(49/99): loss=0.3903207282614846, gradient=0.0009772630841368174\n",
      "Gradient Descent(50/99): loss=0.39032063574608067, gradient=0.0009579871097686943\n",
      "Gradient Descent(51/99): loss=0.3903205467547389, gradient=0.0009392977383631661\n",
      "Gradient Descent(52/99): loss=0.3903204611152817, gradient=0.0009211743461508208\n",
      "Gradient Descent(53/99): loss=0.39032037866510977, gradient=0.0009035972459163708\n",
      "Gradient Descent(54/99): loss=0.3903202992505986, gradient=0.0008865476144135368\n",
      "Gradient Descent(55/99): loss=0.3903202227265411, gradient=0.0008700074282734069\n",
      "Gradient Descent(56/99): loss=0.390320148955633, gradient=0.0008539594072413121\n",
      "Gradient Descent(57/99): loss=0.3903200778079915, gradient=0.0008383869637523811\n",
      "Gradient Descent(58/99): loss=0.3903200091607124, gradient=0.0008232741580029382\n",
      "Gradient Descent(59/99): loss=0.3903199428974537, gradient=0.0008086056577967916\n",
      "Gradient Descent(60/99): loss=0.39031987890805175, gradient=0.0007943667025507252\n",
      "Gradient Descent(61/99): loss=0.3903198170881589, gradient=0.0007805430709288949\n",
      "Gradient Descent(62/99): loss=0.3903197573389089, gradient=0.0007671210516510359\n",
      "Gradient Descent(63/99): loss=0.39031969956660084, gradient=0.0007540874170815255\n",
      "Gradient Descent(64/99): loss=0.39031964368240524, gradient=0.0007414293992591388\n",
      "Gradient Descent(65/99): loss=0.39031958960208907, gradient=0.0007291346680731857\n",
      "Gradient Descent(66/99): loss=0.39031953724575513, gradient=0.0007171913113301483\n",
      "Gradient Descent(67/99): loss=0.3903194865376006, gradient=0.0007055878164880875\n",
      "Gradient Descent(68/99): loss=0.39031943740568925, gradient=0.0006943130538647938\n",
      "Gradient Descent(69/99): loss=0.3903193897817375, gradient=0.0006833562611496482\n",
      "Gradient Descent(70/99): loss=0.3903193436009123, gradient=0.0006727070290708012\n",
      "Gradient Descent(71/99): loss=0.3903192988016426, gradient=0.0006623552880877068\n",
      "Gradient Descent(72/99): loss=0.3903192553254412, gradient=0.0006522912959941006\n",
      "Gradient Descent(73/99): loss=0.39031921311673684, gradient=0.0006425056263314433\n",
      "Gradient Descent(74/99): loss=0.3903191721227162, gradient=0.0006329891575240285\n",
      "Gradient Descent(75/99): loss=0.3903191322931756, gradient=0.0006237330626581382\n",
      "Gradient Descent(76/99): loss=0.3903190935803806, gradient=0.0006147287998361542\n",
      "Gradient Descent(77/99): loss=0.39031905593893285, gradient=0.0006059681030456444\n",
      "Gradient Descent(78/99): loss=0.3903190193256471, gradient=0.0005974429734895627\n",
      "Gradient Descent(79/99): loss=0.3903189836994319, gradient=0.0005891456713306423\n",
      "Gradient Descent(80/99): loss=0.39031894902117925, gradient=0.0005810687078085442\n",
      "Gradient Descent(81/99): loss=0.3903189152536602, gradient=0.0005732048376929511\n",
      "Gradient Descent(82/99): loss=0.3903188823614252, gradient=0.0005655470520406513\n",
      "Gradient Descent(83/99): loss=0.39031885031071023, gradient=0.0005580885712277722\n",
      "Gradient Descent(84/99): loss=0.39031881906935034, gradient=0.0005508228382330214\n",
      "Gradient Descent(85/99): loss=0.39031878860669383, gradient=0.0005437435121490091\n",
      "Gradient Descent(86/99): loss=0.3903187588935256, gradient=0.0005368444619036764\n",
      "Gradient Descent(87/99): loss=0.3903187299019905, gradient=0.0005301197601737793\n",
      "Gradient Descent(88/99): loss=0.3903187016055254, gradient=0.000523563677477087\n",
      "Gradient Descent(89/99): loss=0.39031867397879005, gradient=0.0005171706764294964\n",
      "Gradient Descent(90/99): loss=0.39031864699760627, gradient=0.0005109354061566206\n",
      "Gradient Descent(91/99): loss=0.3903186206388967, gradient=0.0005048526968500941\n",
      "Gradient Descent(92/99): loss=0.39031859488062975, gradient=0.0004989175544603436\n",
      "Gradient Descent(93/99): loss=0.3903185697017654, gradient=0.0004931251555184998\n",
      "Gradient Descent(94/99): loss=0.3903185450822054, gradient=0.0004874708420819749\n",
      "Gradient Descent(95/99): loss=0.3903185210027457, gradient=0.00048195011679742354\n",
      "Gradient Descent(96/99): loss=0.3903184974450297, gradient=0.00047655863807776237\n",
      "Gradient Descent(97/99): loss=0.3903184743915086, gradient=0.00047129221538868475\n",
      "Gradient Descent(98/99): loss=0.3903184518253974, gradient=0.0004661468046417336\n",
      "Gradient Descent(99/99): loss=0.39031842973063935, gradient=0.0004611185036911487\n",
      "Gradient Descent(0/99): loss=0.3899715231253537, gradient=0.010004084868156094\n",
      "Gradient Descent(1/99): loss=0.38996530095728427, gradient=0.008234673595156656\n",
      "Gradient Descent(2/99): loss=0.3899608960044209, gradient=0.006901125636300759\n",
      "Gradient Descent(3/99): loss=0.38995766607093535, gradient=0.005885301841576157\n",
      "Gradient Descent(4/99): loss=0.38995521264846555, gradient=0.005108701647349612\n",
      "Gradient Descent(5/99): loss=0.3899532872633658, gradient=0.004506074224363507\n",
      "Gradient Descent(6/99): loss=0.38995172752608775, gradient=0.004041878817711752\n",
      "Gradient Descent(7/99): loss=0.389950429909937, gradient=0.0036759052826642303\n",
      "Gradient Descent(8/99): loss=0.38994932591242626, gradient=0.0033824011457549894\n",
      "Gradient Descent(9/99): loss=0.3899483691830077, gradient=0.0031425476642691135\n",
      "Gradient Descent(10/99): loss=0.38994752758737916, gradient=0.0029427409376982638\n",
      "Gradient Descent(11/99): loss=0.3899467782693307, gradient=0.0027732032473187524\n",
      "Gradient Descent(12/99): loss=0.3899461045439091, gradient=0.0026269091289208685\n",
      "Gradient Descent(13/99): loss=0.3899454939173872, gradient=0.0024987860119052266\n",
      "Gradient Descent(14/99): loss=0.3899449368064884, gradient=0.0023851368012311348\n",
      "Gradient Descent(15/99): loss=0.38994442569594007, gradient=0.0022832316496087424\n",
      "Gradient Descent(16/99): loss=0.389943954574019, gradient=0.002191023305743915\n",
      "Gradient Descent(17/99): loss=0.3899435192137901, gradient=0.0021035615915335087\n",
      "Gradient Descent(18/99): loss=0.38994311477840904, gradient=0.002026720061085163\n",
      "Gradient Descent(19/99): loss=0.3899427379036723, gradient=0.0019558032705192093\n",
      "Gradient Descent(20/99): loss=0.38994238573610246, gradient=0.001890051007340547\n",
      "Gradient Descent(21/99): loss=0.38994205583279745, gradient=0.0018288433186660887\n",
      "Gradient Descent(22/99): loss=0.3899417460839275, gradient=0.0017716677179946412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(23/99): loss=0.38994145465348495, gradient=0.0017180955407732805\n",
      "Gradient Descent(24/99): loss=0.38994117993290583, gradient=0.0016677644791543542\n",
      "Gradient Descent(25/99): loss=0.3899409205043622, gradient=0.0016203654621726653\n",
      "Gradient Descent(26/99): loss=0.38994067541665656, gradient=0.00157362516544623\n",
      "Gradient Descent(27/99): loss=0.38994044321352583, gradient=0.001531413683720514\n",
      "Gradient Descent(28/99): loss=0.3899402228945215, gradient=0.001491443104421604\n",
      "Gradient Descent(29/99): loss=0.38994001356185976, gradient=0.0014535337481920811\n",
      "Gradient Descent(30/99): loss=0.38993981440671277, gradient=0.0014175274533102474\n",
      "Gradient Descent(31/99): loss=0.38993962469781057, gradient=0.0013832841853338037\n",
      "Gradient Descent(32/99): loss=0.38993944377185713, gradient=0.001350679276241\n",
      "Gradient Descent(33/99): loss=0.38993927102542675, gradient=0.001319601159960574\n",
      "Gradient Descent(34/99): loss=0.38993910590806985, gradient=0.0012899495030666457\n",
      "Gradient Descent(35/99): loss=0.38993894791641126, gradient=0.0012616336526814274\n",
      "Gradient Descent(36/99): loss=0.38993879658907243, gradient=0.0012345713409018003\n",
      "Gradient Descent(37/99): loss=0.38993865150227897, gradient=0.0012086875980575076\n",
      "Gradient Descent(38/99): loss=0.3899385122660437, gradient=0.0011839138369895278\n",
      "Gradient Descent(39/99): loss=0.38993837852083096, gradient=0.001160187078138829\n",
      "Gradient Descent(40/99): loss=0.3899382499346348, gradient=0.0011374492911365233\n",
      "Gradient Descent(41/99): loss=0.3899381262004011, gradient=0.0011156468332118605\n",
      "Gradient Descent(42/99): loss=0.38993800703375214, gradient=0.0010947299683887264\n",
      "Gradient Descent(43/99): loss=0.38993789217096475, gradient=0.0010746524543476556\n",
      "Gradient Descent(44/99): loss=0.38993778136717105, gradient=0.001055371186160836\n",
      "Gradient Descent(45/99): loss=0.389937674394749, gradient=0.0010368458879832058\n",
      "Gradient Descent(46/99): loss=0.3899375710418832, gradient=0.0010190388453058129\n",
      "Gradient Descent(47/99): loss=0.3899374711112665, gradient=0.0010019146716146975\n",
      "Gradient Descent(48/99): loss=0.3899373744189338, gradient=0.0009854401043141567\n",
      "Gradient Descent(49/99): loss=0.38993728079320594, gradient=0.0009695838256055893\n",
      "Gradient Descent(50/99): loss=0.389937190073733, gradient=0.0009543163047010145\n",
      "Gradient Descent(51/99): loss=0.38993710211062743, gradient=0.0009396096583203594\n",
      "Gradient Descent(52/99): loss=0.3899370167636722, gradient=0.0009254375268949561\n",
      "Gradient Descent(53/99): loss=0.38993693390160283, gradient=0.0009117749642941904\n",
      "Gradient Descent(54/99): loss=0.38993685340144707, gradient=0.0008985983392239111\n",
      "Gradient Descent(55/99): loss=0.38993677514792546, gradient=0.000885885246721215\n",
      "Gradient Descent(56/99): loss=0.3899366990328984, gradient=0.0008736144284045458\n",
      "Gradient Descent(57/99): loss=0.38993662495486, gradient=0.0008617657003344361\n",
      "Gradient Descent(58/99): loss=0.38993655281847317, gradient=0.0008503198875065012\n",
      "Gradient Descent(59/99): loss=0.38993648253414137, gradient=0.0008392587641403626\n",
      "Gradient Descent(60/99): loss=0.38993641401761375, gradient=0.0008285649990461791\n",
      "Gradient Descent(61/99): loss=0.3899363471896215, gradient=0.0008182221054543066\n",
      "Gradient Descent(62/99): loss=0.389936281975541, gradient=0.0008082143947783643\n",
      "Gradient Descent(63/99): loss=0.3899362183050838, gradient=0.0007985269338582095\n",
      "Gradient Descent(64/99): loss=0.38993615611200844, gradient=0.0007891455052900325\n",
      "Gradient Descent(65/99): loss=0.3899360953338543, gradient=0.0007800565705069592\n",
      "Gradient Descent(66/99): loss=0.38993603591169457, gradient=0.0007712472353187511\n",
      "Gradient Descent(67/99): loss=0.38993597778990724, gradient=0.0007627052176582517\n",
      "Gradient Descent(68/99): loss=0.38993592091596263, gradient=0.0007544188173176069\n",
      "Gradient Descent(69/99): loss=0.389935865240225, gradient=0.0007463768874846786\n",
      "Gradient Descent(70/99): loss=0.3899358107157698, gradient=0.0007385688079160621\n",
      "Gradient Descent(71/99): loss=0.3899357572982125, gradient=0.0007309844596037268\n",
      "Gradient Descent(72/99): loss=0.3899357049455493, gradient=0.0007236142008112048\n",
      "Gradient Descent(73/99): loss=0.38993565361800914, gradient=0.0007164488443698393\n",
      "Gradient Descent(74/99): loss=0.3899356032779154, gradient=0.0007094796361403704\n",
      "Gradient Descent(75/99): loss=0.389935553889558, gradient=0.000702698234555332\n",
      "Gradient Descent(76/99): loss=0.38993550541907246, gradient=0.0006960966911682936\n",
      "Gradient Descent(77/99): loss=0.3899354578343286, gradient=0.0006896674321440494\n",
      "Gradient Descent(78/99): loss=0.38993541110482527, gradient=0.0006834032406314469\n",
      "Gradient Descent(79/99): loss=0.3899353652015931, gradient=0.0006772972399658244\n",
      "Gradient Descent(80/99): loss=0.3899353200971034, gradient=0.000671342877654595\n",
      "Gradient Descent(81/99): loss=0.38993527576518244, gradient=0.0006655339101028932\n",
      "Gradient Descent(82/99): loss=0.3899352321809317, gradient=0.0006598643880404694\n",
      "Gradient Descent(83/99): loss=0.3899351893206538, gradient=0.0006543286426148453\n",
      "Gradient Descent(84/99): loss=0.38993514716178157, gradient=0.0006489212721176775\n",
      "Gradient Descent(85/99): loss=0.3899351056828142, gradient=0.0006436371293145888\n",
      "Gradient Descent(86/99): loss=0.3899350648632547, gradient=0.0006384713093503128\n",
      "Gradient Descent(87/99): loss=0.3899350246835528, gradient=0.0006334191382033831\n",
      "Gradient Descent(88/99): loss=0.38993498512505165, gradient=0.0006284761616654265\n",
      "Gradient Descent(89/99): loss=0.3899349461699377, gradient=0.0006236381348228243\n",
      "Gradient Descent(90/99): loss=0.3899349078011919, gradient=0.0006189010120180929\n",
      "Gradient Descent(91/99): loss=0.38993487000254773, gradient=0.0006142609372713003\n",
      "Gradient Descent(92/99): loss=0.38993483275844754, gradient=0.0006097142351409903\n",
      "Gradient Descent(93/99): loss=0.38993479605400444, gradient=0.0006052574020068862\n",
      "Gradient Descent(94/99): loss=0.38993475987496634, gradient=0.0006008870977555229\n",
      "Gradient Descent(95/99): loss=0.3899347242076804, gradient=0.00059660013785268\n",
      "Gradient Descent(96/99): loss=0.3899346890390607, gradient=0.000592393485785301\n",
      "Gradient Descent(97/99): loss=0.3899346543565592, gradient=0.0005882642458578622\n",
      "Gradient Descent(98/99): loss=0.3899346201481363, gradient=0.0005842096563275345\n",
      "Gradient Descent(99/99): loss=0.389934586402234, gradient=0.0005802270828640928\n",
      "Gradient Descent(0/99): loss=0.38967270422848815, gradient=0.011066702441235412\n",
      "Gradient Descent(1/99): loss=0.38966772465990224, gradient=0.007439621516637954\n",
      "Gradient Descent(2/99): loss=0.38966419277825604, gradient=0.006178012180535786\n",
      "Gradient Descent(3/99): loss=0.389661597883161, gradient=0.005279072156474526\n",
      "Gradient Descent(4/99): loss=0.3896596527650097, gradient=0.004560231540591558\n",
      "Gradient Descent(5/99): loss=0.38965816748220256, gradient=0.003974423232983253\n",
      "Gradient Descent(6/99): loss=0.38965701070899667, gradient=0.003499672454851641\n",
      "Gradient Descent(7/99): loss=0.3896560931572712, gradient=0.003110262689317482\n",
      "Gradient Descent(8/99): loss=0.3896553526724061, gradient=0.0027885638721943475\n",
      "Gradient Descent(9/99): loss=0.38965474542492307, gradient=0.002520686123742642\n",
      "Gradient Descent(10/99): loss=0.38965424008092303, gradient=0.002295727835471668\n",
      "Gradient Descent(11/99): loss=0.3896538139136772, gradient=0.002105159867161683\n",
      "Gradient Descent(12/99): loss=0.3896534501920012, gradient=0.0019423219666418845\n",
      "Gradient Descent(13/99): loss=0.38965313641170435, gradient=0.0018020144159944123\n",
      "Gradient Descent(14/99): loss=0.3896528630872826, gradient=0.0016801707590260752\n",
      "Gradient Descent(15/99): loss=0.38965262291826086, gradient=0.0015735988746464007\n",
      "Gradient Descent(16/99): loss=0.38965241020787433, gradient=0.0014797786714503035\n",
      "Gradient Descent(17/99): loss=0.3896522204530411, gradient=0.0013967057679022366\n",
      "Gradient Descent(18/99): loss=0.38965205005162046, gradient=0.0013227718027293063\n",
      "Gradient Descent(19/99): loss=0.3896518960907337, gradient=0.0012566734206903572\n",
      "Gradient Descent(20/99): loss=0.38965175619168874, gradient=0.0011973433784312239\n",
      "Gradient Descent(21/99): loss=0.3896516283948594, gradient=0.001143898511295351\n",
      "Gradient Descent(22/99): loss=0.3896515110731, gradient=0.0010956004337175508\n",
      "Gradient Descent(23/99): loss=0.389651402865786, gradient=0.0010518257907427438\n",
      "Gradient Descent(24/99): loss=0.3896513026279451, gradient=0.001012043640769634\n",
      "Gradient Descent(25/99): loss=0.3896512093905753, gradient=0.0009757981493151939\n",
      "Gradient Descent(26/99): loss=0.38965112232935323, gradient=0.0009426952360156619\n",
      "Gradient Descent(27/99): loss=0.3896510407397159, gradient=0.0009123921682974204\n",
      "Gradient Descent(28/99): loss=0.3896509640168385, gradient=0.000884589358819099\n",
      "Gradient Descent(29/99): loss=0.3896508916394149, gradient=0.0008590238199322742\n",
      "Gradient Descent(30/99): loss=0.3896508231564245, gradient=0.0008354638732863368\n",
      "Gradient Descent(31/99): loss=0.3896507581762609, gradient=0.0008137048191065285\n",
      "Gradient Descent(32/99): loss=0.3896506963577536, gradient=0.0007935653474727582\n",
      "Gradient Descent(33/99): loss=0.3896506374027086, gradient=0.0007748845306166063\n",
      "Gradient Descent(34/99): loss=0.3896505810496839, gradient=0.0007575192764703607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(35/99): loss=0.3896505270687725, gradient=0.0007413421536292814\n",
      "Gradient Descent(36/99): loss=0.38965047525721125, gradient=0.0007262395196307383\n",
      "Gradient Descent(37/99): loss=0.3896504254356733, gradient=0.0007121099002773919\n",
      "Gradient Descent(38/99): loss=0.3896503774451236, gradient=0.0006988625792978126\n",
      "Gradient Descent(39/99): loss=0.3896503311441467, gradient=0.0006864163661446473\n",
      "Gradient Descent(40/99): loss=0.3896502864066673, gradient=0.0006746985160488108\n",
      "Gradient Descent(41/99): loss=0.3896502431199992, gradient=0.000663643781192206\n",
      "Gradient Descent(42/99): loss=0.3896502011831732, gradient=0.0006531935754833005\n",
      "Gradient Descent(43/99): loss=0.3896501605054985, gradient=0.0006432952382241825\n",
      "Gradient Descent(44/99): loss=0.38965012100532154, gradient=0.00063390138417487\n",
      "Gradient Descent(45/99): loss=0.3896500826089538, gradient=0.0006249693293035553\n",
      "Gradient Descent(46/99): loss=0.3896500452497421, gradient=0.0006164605829710363\n",
      "Gradient Descent(47/99): loss=0.3896500088672612, gradient=0.0006083403985133523\n",
      "Gradient Descent(48/99): loss=0.38964997340661106, gradient=0.0006005773752130748\n",
      "Gradient Descent(49/99): loss=0.3896499388178029, gradient=0.0005931431055257227\n",
      "Gradient Descent(50/99): loss=0.38964990505522273, gradient=0.0005860118621845308\n",
      "Gradient Descent(51/99): loss=0.38964987207716156, gradient=0.0005791603204619211\n",
      "Gradient Descent(52/99): loss=0.38964983984540125, gradient=0.000572567311439608\n",
      "Gradient Descent(53/99): loss=0.38964980832485246, gradient=0.0005662136026402863\n",
      "Gradient Descent(54/99): loss=0.3896497774832319, gradient=0.000560081702813004\n",
      "Gradient Descent(55/99): loss=0.38964974729078145, gradient=0.0005541556880521903\n",
      "Gradient Descent(56/99): loss=0.3896497177200153, gradient=0.0005484210467688697\n",
      "Gradient Descent(57/99): loss=0.3896496887454986, gradient=0.0005428645413320567\n",
      "Gradient Descent(58/99): loss=0.3896496603436499, gradient=0.00053747408446058\n",
      "Gradient Descent(59/99): loss=0.389649632492564, gradient=0.0005322386286765255\n",
      "Gradient Descent(60/99): loss=0.38964960517185643, gradient=0.0005271480673344222\n",
      "Gradient Descent(61/99): loss=0.389649578362523, gradient=0.0005221931459177318\n",
      "Gradient Descent(62/99): loss=0.3896495520468141, gradient=0.0005173653824509462\n",
      "Gradient Descent(63/99): loss=0.38964952620812315, gradient=0.0005126569960130138\n",
      "Gradient Descent(64/99): loss=0.3896495008308854, gradient=0.0005080608424571106\n",
      "Gradient Descent(65/99): loss=0.38964947590048793, gradient=0.000503570356548172\n",
      "Gradient Descent(66/99): loss=0.38964945140318763, gradient=0.0004991794998216763\n",
      "Gradient Descent(67/99): loss=0.3896494273260385, gradient=0.000494882713548895\n",
      "Gradient Descent(68/99): loss=0.38964940365682577, gradient=0.0004906748762642638\n",
      "Gradient Descent(69/99): loss=0.3896493803840051, gradient=0.0004865512653737519\n",
      "Gradient Descent(70/99): loss=0.38964935749665, gradient=0.00048250752241790567\n",
      "Gradient Descent(71/99): loss=0.3896493349844015, gradient=0.0004785396216113235\n",
      "Gradient Descent(72/99): loss=0.3896493128374256, gradient=0.0004746438413226918\n",
      "Gradient Descent(73/99): loss=0.3896492910463712, gradient=0.0004708167381974062\n",
      "Gradient Descent(74/99): loss=0.38964926960233504, gradient=0.00046705512365691155\n",
      "Gradient Descent(75/99): loss=0.3896492484968271, gradient=0.0004633560425385098\n",
      "Gradient Descent(76/99): loss=0.38964922772174243, gradient=0.0004597167536647303\n",
      "Gradient Descent(77/99): loss=0.38964920726933155, gradient=0.0004561347121537707\n",
      "Gradient Descent(78/99): loss=0.3896491871321763, gradient=0.00045260755330288295\n",
      "Gradient Descent(79/99): loss=0.3896491673031669, gradient=0.0004491330778940043\n",
      "Gradient Descent(80/99): loss=0.3896491477754806, gradient=0.0004457092387863362\n",
      "Gradient Descent(81/99): loss=0.3896491285425634, gradient=0.0004423341286754062\n",
      "Gradient Descent(82/99): loss=0.38964910959811105, gradient=0.00043900596890914434\n",
      "Gradient Descent(83/99): loss=0.38964909093605427, gradient=0.00043572309926362434\n",
      "Gradient Descent(84/99): loss=0.3896490725505431, gradient=0.000432483968590613\n",
      "Gradient Descent(85/99): loss=0.3896490544359344, gradient=0.00042928712625698906\n",
      "Gradient Descent(86/99): loss=0.38964903658677774, gradient=0.00042613121430532804\n",
      "Gradient Descent(87/99): loss=0.38964901899780585, gradient=0.00042301496027036446\n",
      "Gradient Descent(88/99): loss=0.38964900166392225, gradient=0.0004199371705935457\n",
      "Gradient Descent(89/99): loss=0.38964898458019265, gradient=0.0004168967245824969\n",
      "Gradient Descent(90/99): loss=0.3896489677418361, gradient=0.00041389256886783604\n",
      "Gradient Descent(91/99): loss=0.3896489511442156, gradient=0.00041092371231392686\n",
      "Gradient Descent(92/99): loss=0.3896489347828317, gradient=0.0004079892213442251\n",
      "Gradient Descent(93/99): loss=0.38964891865331414, gradient=0.00040508821564571223\n",
      "Gradient Descent(94/99): loss=0.38964890275141667, gradient=0.00040221986421994695\n",
      "Gradient Descent(95/99): loss=0.3896488870730097, gradient=0.0003993833817511605\n",
      "Gradient Descent(96/99): loss=0.38964887161407563, gradient=0.00039657802526465405\n",
      "Gradient Descent(97/99): loss=0.38964885637070207, gradient=0.00039380309105147877\n",
      "Gradient Descent(98/99): loss=0.38964884133907957, gradient=0.0003910579118361111\n",
      "Gradient Descent(99/99): loss=0.38964882651549426, gradient=0.00038834185416834993\n",
      "Gradient Descent(0/99): loss=0.3889579458937976, gradient=0.008192067703282178\n",
      "Gradient Descent(1/99): loss=0.3889551815781205, gradient=0.005553324698348437\n",
      "Gradient Descent(2/99): loss=0.3889532630786614, gradient=0.004566293942465125\n",
      "Gradient Descent(3/99): loss=0.38895188145871484, gradient=0.003861848937023253\n",
      "Gradient Descent(4/99): loss=0.3889508618419935, gradient=0.0033083790757320736\n",
      "Gradient Descent(5/99): loss=0.38895008936667924, gradient=0.0028709954180379384\n",
      "Gradient Descent(6/99): loss=0.38894948763747583, gradient=0.0025259527788293254\n",
      "Gradient Descent(7/99): loss=0.3889490053760946, gradient=0.002254173847698675\n",
      "Gradient Descent(8/99): loss=0.3889486079302811, gradient=0.0020401105023200305\n",
      "Gradient Descent(9/99): loss=0.388948271688955, gradient=0.0018711357420183726\n",
      "Gradient Descent(10/99): loss=0.388947980420178, gradient=0.001737092012516301\n",
      "Gradient Descent(11/99): loss=0.38894772285243523, gradient=0.001629915443565693\n",
      "Gradient Descent(12/99): loss=0.38894749107115406, gradient=0.001543298052255136\n",
      "Gradient Descent(13/99): loss=0.38894727945000895, gradient=0.0014723752509711344\n",
      "Gradient Descent(14/99): loss=0.3889470839349358, gradient=0.001413441096555998\n",
      "Gradient Descent(15/99): loss=0.3889469015616608, gradient=0.0013636983054849609\n",
      "Gradient Descent(16/99): loss=0.3889467301285682, gradient=0.0013210478948707304\n",
      "Gradient Descent(17/99): loss=0.38894656797338617, gradient=0.0012839189159046397\n",
      "Gradient Descent(18/99): loss=0.38894641381962053, gradient=0.001251135006538164\n",
      "Gradient Descent(19/99): loss=0.38894626667010096, gradient=0.001221812392075436\n",
      "Gradient Descent(20/99): loss=0.38894612573254544, gradient=0.001195283358349882\n",
      "Gradient Descent(21/99): loss=0.3889459903670161, gradient=0.001171039580903634\n",
      "Gradient Descent(22/99): loss=0.38894586004845383, gradient=0.0011486905131097163\n",
      "Gradient Descent(23/99): loss=0.38894573433967344, gradient=0.0011279329769978777\n",
      "Gradient Descent(24/99): loss=0.3889456128716749, gradient=0.0011085289791780378\n",
      "Gradient Descent(25/99): loss=0.3889454953291181, gradient=0.0010902895146962229\n",
      "Gradient Descent(26/99): loss=0.3889453814394721, gradient=0.0010730627089825028\n",
      "Gradient Descent(27/99): loss=0.38894527096481135, gradient=0.0010567250962780208\n",
      "Gradient Descent(28/99): loss=0.3889451636955298, gradient=0.0010411751663748716\n",
      "Gradient Descent(29/99): loss=0.38894505944546964, gradient=0.001026328555343961\n",
      "Gradient Descent(30/99): loss=0.38894495804809764, gradient=0.0010121144322146711\n",
      "Gradient Descent(31/99): loss=0.3889448593534678, gradient=0.000998472760088946\n",
      "Gradient Descent(32/99): loss=0.38894476322578353, gradient=0.0009853522005771986\n",
      "Gradient Descent(33/99): loss=0.3889446695414197, gradient=0.0009727084949146638\n",
      "Gradient Descent(34/99): loss=0.38894457818729994, gradient=0.0009605032010783203\n",
      "Gradient Descent(35/99): loss=0.3889444890595567, gradient=0.0009487026990416483\n",
      "Gradient Descent(36/99): loss=0.388944402062411, gradient=0.0009372773997873573\n",
      "Gradient Descent(37/99): loss=0.38894431710723126, gradient=0.0009262011105732178\n",
      "Gradient Descent(38/99): loss=0.3889442341117354, gradient=0.000915450521122135\n",
      "Gradient Descent(39/99): loss=0.38894415324835263, gradient=0.000902135843954903\n",
      "Gradient Descent(40/99): loss=0.38894407416862287, gradient=0.000892169236911291\n",
      "Gradient Descent(41/99): loss=0.38894399680711333, gradient=0.0008824650970001188\n",
      "Gradient Descent(42/99): loss=0.3889439211028589, gradient=0.0008730064544676359\n",
      "Gradient Descent(43/99): loss=0.3889438469987083, gradient=0.0008637784531012575\n",
      "Gradient Descent(44/99): loss=0.38894377444094114, gradient=0.0008547679638686134\n",
      "Gradient Descent(45/99): loss=0.38894370337891115, gradient=0.0008459632856852887\n",
      "Gradient Descent(46/99): loss=0.3889436337647531, gradient=0.0008373539110113798\n",
      "Gradient Descent(47/99): loss=0.3889435655531295, gradient=0.000828930340548456\n",
      "Gradient Descent(48/99): loss=0.3889434987010125, gradient=0.0008206839353671888\n",
      "Gradient Descent(49/99): loss=0.3889434331674951, gradient=0.0008126067977490928\n",
      "Gradient Descent(50/99): loss=0.3889433689136231, gradient=0.0008046916742059281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(51/99): loss=0.38894330590224874, gradient=0.0007969318757534617\n",
      "Gradient Descent(52/99): loss=0.38894324409789965, gradient=0.000789321211715966\n",
      "Gradient Descent(53/99): loss=0.3889431834666618, gradient=0.0007818539342281972\n",
      "Gradient Descent(54/99): loss=0.38894312397607256, gradient=0.0007745246912696151\n",
      "Gradient Descent(55/99): loss=0.3889430655950287, gradient=0.0007673284865633749\n",
      "Gradient Descent(56/99): loss=0.38894300829369666, gradient=0.0007602606450500208\n",
      "Gradient Descent(57/99): loss=0.388942952043436, gradient=0.0007533167829286268\n",
      "Gradient Descent(58/99): loss=0.38894289681672684, gradient=0.0007464927814752509\n",
      "Gradient Descent(59/99): loss=0.3889428425871044, gradient=0.0007397847640119114\n",
      "Gradient Descent(60/99): loss=0.38894278932909865, gradient=0.0007331890755273446\n",
      "Gradient Descent(61/99): loss=0.38894273701817905, gradient=0.0007267022645471411\n",
      "Gradient Descent(62/99): loss=0.38894268563070283, gradient=0.0007203210669278259\n",
      "Gradient Descent(63/99): loss=0.38894263514386784, gradient=0.0007140423913084771\n",
      "Gradient Descent(64/99): loss=0.38894258553566885, gradient=0.0007078633059998215\n",
      "Gradient Descent(65/99): loss=0.3889425367848567, gradient=0.0007017810271289384\n",
      "Gradient Descent(66/99): loss=0.388942488870901, gradient=0.0006957929078861298\n",
      "Gradient Descent(67/99): loss=0.38894244177395415, gradient=0.0006898964287452901\n",
      "Gradient Descent(68/99): loss=0.38894239547481957, gradient=0.0006840891885480616\n",
      "Gradient Descent(69/99): loss=0.3889423499549198, gradient=0.0006783688963578332\n",
      "Gradient Descent(70/99): loss=0.3889423051962692, gradient=0.0006727333640025033\n",
      "Gradient Descent(71/99): loss=0.38894226118144704, gradient=0.0006671804992364876\n",
      "Gradient Descent(72/99): loss=0.38894221789357136, gradient=0.0006617082994598679\n",
      "Gradient Descent(73/99): loss=0.3889421753162768, gradient=0.0006563148459419473\n",
      "Gradient Descent(74/99): loss=0.38894213343369205, gradient=0.0006509982985011566\n",
      "Gradient Descent(75/99): loss=0.3889420922304183, gradient=0.0006457568906002059\n",
      "Gradient Descent(76/99): loss=0.388942051691511, gradient=0.0006405889248185749\n",
      "Gradient Descent(77/99): loss=0.3889420118024605, gradient=0.0006354927686696529\n",
      "Gradient Descent(78/99): loss=0.3889419725491754, gradient=0.0006304668507323267\n",
      "Gradient Descent(79/99): loss=0.3889419339179654, gradient=0.0006255096570706918\n",
      "Gradient Descent(80/99): loss=0.38894189589552625, gradient=0.0006206197279174394\n",
      "Gradient Descent(81/99): loss=0.3889418584689251, gradient=0.0006157956545991643\n",
      "Gradient Descent(82/99): loss=0.38894182162558605, gradient=0.0006110360766841673\n",
      "Gradient Descent(83/99): loss=0.38894178535327834, gradient=0.0006063396793343651\n",
      "Gradient Descent(84/99): loss=0.38894174964010153, gradient=0.0006017051908450823\n",
      "Gradient Descent(85/99): loss=0.38894171447447606, gradient=0.0005971313803584263\n",
      "Gradient Descent(86/99): loss=0.3889416798451306, gradient=0.0005926170557355615\n",
      "Gradient Descent(87/99): loss=0.3889416457410913, gradient=0.0005881610615764637\n",
      "Gradient Descent(88/99): loss=0.38894161215167206, gradient=0.0005837622773753407\n",
      "Gradient Descent(89/99): loss=0.3889415790664646, gradient=0.0005794196158014085\n",
      "Gradient Descent(90/99): loss=0.38894154647532897, gradient=0.0005751320210951429\n",
      "Gradient Descent(91/99): loss=0.3889415143683848, gradient=0.0005708984675718051\n",
      "Gradient Descent(92/99): loss=0.3889414827360025, gradient=0.0005667179582236734\n",
      "Gradient Descent(93/99): loss=0.38894145156879584, gradient=0.0005625895234137718\n",
      "Gradient Descent(94/99): loss=0.3889414208576135, gradient=0.0005585122196542263\n",
      "Gradient Descent(95/99): loss=0.38894139059353233, gradient=0.0005544851284626789\n",
      "Gradient Descent(96/99): loss=0.3889413607678489, gradient=0.000550507355291912\n",
      "Gradient Descent(97/99): loss=0.3889413313720745, gradient=0.0005465780285257774\n",
      "Gradient Descent(98/99): loss=0.388941302397927, gradient=0.0005426962985379094\n",
      "Gradient Descent(99/99): loss=0.3889412738373256, gradient=0.000538861336807941\n",
      "Gradient Descent(0/99): loss=0.39013961430442506, gradient=0.01539786323683256\n",
      "Gradient Descent(1/99): loss=0.390135705419462, gradient=0.006918653445017834\n",
      "Gradient Descent(2/99): loss=0.3901330611270416, gradient=0.005359655314602988\n",
      "Gradient Descent(3/99): loss=0.3901310735461629, gradient=0.004614889565122943\n",
      "Gradient Descent(4/99): loss=0.3901295363154862, gradient=0.004047646073553656\n",
      "Gradient Descent(5/99): loss=0.390128321663251, gradient=0.0035900188994701437\n",
      "Gradient Descent(6/99): loss=0.3901273422155556, gradient=0.003216981435657935\n",
      "Gradient Descent(7/99): loss=0.3901265372640108, gradient=0.0029106944324307775\n",
      "Gradient Descent(8/99): loss=0.3901258639498862, gradient=0.0026573285510805107\n",
      "Gradient Descent(9/99): loss=0.3901252915910431, gradient=0.0024460834070920085\n",
      "Gradient Descent(10/99): loss=0.39012479790116034, gradient=0.002268511830993679\n",
      "Gradient Descent(11/99): loss=0.39012436645994414, gradient=0.002117997479922318\n",
      "Gradient Descent(12/99): loss=0.3901239849973224, gradient=0.0019893472989409066\n",
      "Gradient Descent(13/99): loss=0.3901236442174749, gradient=0.0018784738581847072\n",
      "Gradient Descent(14/99): loss=0.39012333698336055, gradient=0.001782148344391143\n",
      "Gradient Descent(15/99): loss=0.3901230577447252, gradient=0.0016978090280796126\n",
      "Gradient Descent(16/99): loss=0.3901228021322224, gradient=0.0016234131332279612\n",
      "Gradient Descent(17/99): loss=0.3901225666660628, gradient=0.0015573225044652856\n",
      "Gradient Descent(18/99): loss=0.3901223485444669, gradient=0.0014982154628164468\n",
      "Gradient Descent(19/99): loss=0.39012214548831065, gradient=0.0014450188624543794\n",
      "Gradient Descent(20/99): loss=0.3901219556257537, gradient=0.0013968556721879319\n",
      "Gradient Descent(21/99): loss=0.39012177740562476, gradient=0.0013530044561059125\n",
      "Gradient Descent(22/99): loss=0.3901216095316955, gradient=0.001312867960508652\n",
      "Gradient Descent(23/99): loss=0.3901214509122926, gradient=0.0012759486670939945\n",
      "Gradient Descent(24/99): loss=0.39012130062128286, gradient=0.001241829679204731\n",
      "Gradient Descent(25/99): loss=0.390121157867578, gradient=0.001210159698185122\n",
      "Gradient Descent(26/99): loss=0.39012102197108234, gradient=0.0011806411453458717\n",
      "Gradient Descent(27/99): loss=0.3901208923435619, gradient=0.001153020712094486\n",
      "Gradient Descent(28/99): loss=0.3901207684733037, gradient=0.001127081792874151\n",
      "Gradient Descent(29/99): loss=0.3901206499127247, gradient=0.001102638385642161\n",
      "Gradient Descent(30/99): loss=0.3901205362682903, gradient=0.0010795301428248017\n",
      "Gradient Descent(31/99): loss=0.3901204271922594, gradient=0.0010576183298004716\n",
      "Gradient Descent(32/99): loss=0.39012032237588645, gradient=0.0010367825039375505\n",
      "Gradient Descent(33/99): loss=0.390120221543787, gradient=0.0010169177695585692\n",
      "Gradient Descent(34/99): loss=0.39012012444924715, gradient=0.0009979324963213098\n",
      "Gradient Descent(35/99): loss=0.390120030870299, gradient=0.0009797464129519121\n",
      "Gradient Descent(36/99): loss=0.39011994060641947, gradient=0.0009622890069498622\n",
      "Gradient Descent(37/99): loss=0.3901198534757443, gradient=0.0009454981752375623\n",
      "Gradient Descent(38/99): loss=0.3901197693127066, gradient=0.0009293190818124298\n",
      "Gradient Descent(39/99): loss=0.3901196879660278, gradient=0.0009137031870769691\n",
      "Gradient Descent(40/99): loss=0.3901196092969997, gradient=0.0008986074202609447\n",
      "Gradient Descent(41/99): loss=0.39011953317801507, gradient=0.0008839934716576283\n",
      "Gradient Descent(42/99): loss=0.3901194594913033, gradient=0.0008698271856056936\n",
      "Gradient Descent(43/99): loss=0.3901193881278388, gradient=0.0008560780385066433\n",
      "Gradient Descent(44/99): loss=0.3901193189863981, gradient=0.000842718688871075\n",
      "Gradient Descent(45/99): loss=0.3901192519727405, gradient=0.0008297245885690974\n",
      "Gradient Descent(46/99): loss=0.3901191869988953, gradient=0.0008170736462415184\n",
      "Gradient Descent(47/99): loss=0.3901191239825399, gradient=0.0008047459352824142\n",
      "Gradient Descent(48/99): loss=0.39011906284645576, gradient=0.0007927234400034799\n",
      "Gradient Descent(49/99): loss=0.3901190035180496, gradient=0.0007809898345814246\n",
      "Gradient Descent(50/99): loss=0.3901189459289336, gradient=0.0007695302902139153\n",
      "Gradient Descent(51/99): loss=0.39011889001455374, gradient=0.0007583313065968527\n",
      "Gradient Descent(52/99): loss=0.3901188357138614, gradient=0.0007473805644113465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(53/99): loss=0.3901187829690226, gradient=0.0007366667959930903\n",
      "Gradient Descent(54/99): loss=0.3901187317251583, gradient=0.0007261796717644997\n",
      "Gradient Descent(55/99): loss=0.3901186819301133, gradient=0.000715909700354641\n",
      "Gradient Descent(56/99): loss=0.39011863353425064, gradient=0.0007058481406245659\n",
      "Gradient Descent(57/99): loss=0.39011858649026454, gradient=0.0006959869240636851\n",
      "Gradient Descent(58/99): loss=0.3901185407530164, gradient=0.000686318586234738\n",
      "Gradient Descent(59/99): loss=0.39011849627938217, gradient=0.0006768362061246912\n",
      "Gradient Descent(60/99): loss=0.3901184530281204, gradient=0.0006675333524131611\n",
      "Gradient Descent(61/99): loss=0.3901184109597455, gradient=0.000658404035802926\n",
      "Gradient Descent(62/99): loss=0.39011837003642036, gradient=0.000649442666668986\n",
      "Gradient Descent(63/99): loss=0.3901183302218534, gradient=0.0006406440173810496\n",
      "Gradient Descent(64/99): loss=0.3901182914812053, gradient=0.00063200318873778\n",
      "Gradient Descent(65/99): loss=0.3901182537810074, gradient=0.0006235155800232054\n",
      "Gradient Descent(66/99): loss=0.39011821708908173, gradient=0.0006151768622583079\n",
      "Gradient Descent(67/99): loss=0.3901181813744719, gradient=0.0006069829542746605\n",
      "Gradient Descent(68/99): loss=0.39011814660737654, gradient=0.0005989300012835836\n",
      "Gradient Descent(69/99): loss=0.3901181127590904, gradient=0.0005910143556548893\n",
      "Gradient Descent(70/99): loss=0.39011807980194746, gradient=0.0005832325596541131\n",
      "Gradient Descent(71/99): loss=0.3901180477092706, gradient=0.00057558132991857\n",
      "Gradient Descent(72/99): loss=0.39011801645532235, gradient=0.0005680575434776585\n",
      "Gradient Descent(73/99): loss=0.3901179860152619, gradient=0.000560658225147694\n",
      "Gradient Descent(74/99): loss=0.3901179563651022, gradient=0.0005533805361506412\n",
      "Gradient Descent(75/99): loss=0.3901179274816731, gradient=0.0005462217638240647\n",
      "Gradient Descent(76/99): loss=0.3901178993425836, gradient=0.000539179312305464\n",
      "Gradient Descent(77/99): loss=0.3901178719261889, gradient=0.000532250694087211\n",
      "Gradient Descent(78/99): loss=0.3901178452115578, gradient=0.0005254335223506271\n",
      "Gradient Descent(79/99): loss=0.390117819178444, gradient=0.0005187255039976527\n",
      "Gradient Descent(80/99): loss=0.39011779380725703, gradient=0.0005121244333082466\n",
      "Gradient Descent(81/99): loss=0.39011776907903667, gradient=0.0005056281861596011\n",
      "Gradient Descent(82/99): loss=0.39011774497542656, gradient=0.0004992347147500328\n",
      "Gradient Descent(83/99): loss=0.39011772147865215, gradient=0.0004929420427769788\n",
      "Gradient Descent(84/99): loss=0.39011769857149764, gradient=0.0004867482610241173\n",
      "Gradient Descent(85/99): loss=0.3901176762372841, gradient=0.00048065152331745227\n",
      "Gradient Descent(86/99): loss=0.39011765445985097, gradient=0.00047465004281409154\n",
      "Gradient Descent(87/99): loss=0.39011763322353554, gradient=0.00046874208859201796\n",
      "Gradient Descent(88/99): loss=0.3901176125131552, gradient=0.00046292598251220017\n",
      "Gradient Descent(89/99): loss=0.3901175923139907, gradient=0.0004572000963270191\n",
      "Gradient Descent(90/99): loss=0.39011757261176877, gradient=0.00045156284901230265\n",
      "Gradient Descent(91/99): loss=0.39011755339264753, gradient=0.0004460127043024253\n",
      "Gradient Descent(92/99): loss=0.3901175346432001, gradient=0.00044054816840939995\n",
      "Gradient Descent(93/99): loss=0.3901175163504018, gradient=0.0004351677879102385\n",
      "Gradient Descent(94/99): loss=0.3901174985016151, gradient=0.000429870147786608\n",
      "Gradient Descent(95/99): loss=0.3901174810845772, gradient=0.0004246538696039445\n",
      "Gradient Descent(96/99): loss=0.39011746408738757, gradient=0.00041951760981781105\n",
      "Gradient Descent(97/99): loss=0.3901174474984959, gradient=0.0004144600581963111\n",
      "Gradient Descent(98/99): loss=0.39011743130668974, gradient=0.00040947993634873767\n",
      "Gradient Descent(99/99): loss=0.3901174155010849, gradient=0.00040457599635172975\n",
      "Gradient Descent(0/99): loss=0.390348908831972, gradient=0.0072716430537842726\n",
      "Gradient Descent(1/99): loss=0.3903452374235781, gradient=0.006261551463111539\n",
      "Gradient Descent(2/99): loss=0.39034235423040237, gradient=0.005529787607481508\n",
      "Gradient Descent(3/99): loss=0.3903400304379639, gradient=0.0049503977800553\n",
      "Gradient Descent(4/99): loss=0.39033811458238993, gradient=0.004483562162583228\n",
      "Gradient Descent(5/99): loss=0.39033650315554275, gradient=0.004102595718576958\n",
      "Gradient Descent(6/99): loss=0.3903351240576638, gradient=0.003787670549067132\n",
      "Gradient Descent(7/99): loss=0.3903339260325847, gradient=0.0035239406774181387\n",
      "Gradient Descent(8/99): loss=0.3903328747802253, gradient=0.003290655547016841\n",
      "Gradient Descent(9/99): loss=0.39033193918087467, gradient=0.0030999187115781555\n",
      "Gradient Descent(10/99): loss=0.3903310987484041, gradient=0.0029343072608654123\n",
      "Gradient Descent(11/99): loss=0.39033033779736837, gradient=0.00278896494091591\n",
      "Gradient Descent(12/99): loss=0.3903296441221959, gradient=0.0026601473653604705\n",
      "Gradient Descent(13/99): loss=0.3903290080816304, gradient=0.0025449390830112203\n",
      "Gradient Descent(14/99): loss=0.3903284219536367, gradient=0.0024410476876996837\n",
      "Gradient Descent(15/99): loss=0.3903278794734895, gradient=0.002346653479007939\n",
      "Gradient Descent(16/99): loss=0.39032737549791846, gradient=0.002260298986592505\n",
      "Gradient Descent(17/99): loss=0.3903269057573613, gradient=0.0021808070961914\n",
      "Gradient Descent(18/99): loss=0.39032646878924593, gradient=0.0020966550104273966\n",
      "Gradient Descent(19/99): loss=0.3903260591623629, gradient=0.002028809877109889\n",
      "Gradient Descent(20/99): loss=0.39032567431880694, gradient=0.0019653865573031515\n",
      "Gradient Descent(21/99): loss=0.39032531205788357, gradient=0.0019058497759138102\n",
      "Gradient Descent(22/99): loss=0.39032497046661113, gradient=0.0018497539547761477\n",
      "Gradient Descent(23/99): loss=0.39032464821320767, gradient=0.0017946210499286603\n",
      "Gradient Descent(24/99): loss=0.39032434338660416, gradient=0.001744667322136244\n",
      "Gradient Descent(25/99): loss=0.3903240549042412, gradient=0.0016958759976865134\n",
      "Gradient Descent(26/99): loss=0.390323781382968, gradient=0.0016506321153795906\n",
      "Gradient Descent(27/99): loss=0.39032352178661334, gradient=0.0016074129832886702\n",
      "Gradient Descent(28/99): loss=0.39032327517942633, gradient=0.0015660544905499994\n",
      "Gradient Descent(29/99): loss=0.39032304071221735, gradient=0.001526414376036638\n",
      "Gradient Descent(30/99): loss=0.39032281761114124, gradient=0.0014883685887955769\n",
      "Gradient Descent(31/99): loss=0.39032260516830575, gradient=0.0014518083213452132\n",
      "Gradient Descent(32/99): loss=0.390322402733914, gradient=0.0014166375809419656\n",
      "Gradient Descent(33/99): loss=0.39032220970963527, gradient=0.0013827711940667601\n",
      "Gradient Descent(34/99): loss=0.3903220255429793, gradient=0.0013501331610991068\n",
      "Gradient Descent(35/99): loss=0.3903218497224909, gradient=0.0013186552950248407\n",
      "Gradient Descent(36/99): loss=0.3903216817736212, gradient=0.0012882760912775154\n",
      "Gradient Descent(37/99): loss=0.3903215212551597, gradient=0.001258939786264786\n",
      "Gradient Descent(38/99): loss=0.3903213677561317, gradient=0.001230595570410716\n",
      "Gradient Descent(39/99): loss=0.39032122089308613, gradient=0.0012031969281218861\n",
      "Gradient Descent(40/99): loss=0.390321080307714, gradient=0.0011767010823319855\n",
      "Gradient Descent(41/99): loss=0.3903209456647425, gradient=0.0011510685254745724\n",
      "Gradient Descent(42/99): loss=0.3903208166500671, gradient=0.0011262626221004363\n",
      "Gradient Descent(43/99): loss=0.39032069296908656, gradient=0.0011022492710632318\n",
      "Gradient Descent(44/99): loss=0.39032057434521106, gradient=0.001078996617381799\n",
      "Gradient Descent(45/99): loss=0.39032046051852054, gradient=0.0010564748056541623\n",
      "Gradient Descent(46/99): loss=0.3903203512445547, gradient=0.0010346557683309885\n",
      "Gradient Descent(47/99): loss=0.39032024629321643, gradient=0.0010135130433211613\n",
      "Gradient Descent(48/99): loss=0.3903201454477737, gradient=0.000993021616351449\n",
      "Gradient Descent(49/99): loss=0.3903200485039519, gradient=0.0009731577842780369\n",
      "Gradient Descent(50/99): loss=0.3903199552691017, gradient=0.0009538990361833514\n",
      "Gradient Descent(51/99): loss=0.3903198655614374, gradient=0.0009352239496136073\n",
      "Gradient Descent(52/99): loss=0.39031977920933536, gradient=0.0009171120997416505\n",
      "Gradient Descent(53/99): loss=0.3903196960506899, gradient=0.0008995439795960032\n",
      "Gradient Descent(54/99): loss=0.39031961593231546, gradient=0.0008825009297879731\n",
      "Gradient Descent(55/99): loss=0.39031953870939645, gradient=0.0008659650764153623\n",
      "Gradient Descent(56/99): loss=0.39031946424497604, gradient=0.0008499192760208869\n",
      "Gradient Descent(57/99): loss=0.39031939240948027, gradient=0.0008343470666550018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(58/99): loss=0.390319323080279, gradient=0.0008192326242316555\n",
      "Gradient Descent(59/99): loss=0.390319256141273, gradient=0.0008045607234862552\n",
      "Gradient Descent(60/99): loss=0.39031919148251276, gradient=0.0007903167029423802\n",
      "Gradient Descent(61/99): loss=0.39031912899983967, gradient=0.0007764864333796556\n",
      "Gradient Descent(62/99): loss=0.3903190685945529, gradient=0.0007630562893647041\n",
      "Gradient Descent(63/99): loss=0.39031901017309617, gradient=0.0007500131234674034\n",
      "Gradient Descent(64/99): loss=0.3903189536467657, gradient=0.0007373442428364202\n",
      "Gradient Descent(65/99): loss=0.39031889893143557, gradient=0.0007250373878503698\n",
      "Gradient Descent(66/99): loss=0.39031884594730176, gradient=0.0007130807125988259\n",
      "Gradient Descent(67/99): loss=0.39031879461863894, gradient=0.0007014627669786291\n",
      "Gradient Descent(68/99): loss=0.39031874487357565, gradient=0.0006901724802189701\n",
      "Gradient Descent(69/99): loss=0.3903186966438809, gradient=0.0006791991456713237\n",
      "Gradient Descent(70/99): loss=0.39031864986476345, gradient=0.0006685324067212769\n",
      "Gradient Descent(71/99): loss=0.3903186044746839, gradient=0.0006581622436966778\n",
      "Gradient Descent(72/99): loss=0.39031856041517765, gradient=0.00064807896166158\n",
      "Gradient Descent(73/99): loss=0.39031851763068726, gradient=0.0006382731789990287\n",
      "Gradient Descent(74/99): loss=0.3903184760684064, gradient=0.0006287358166972468\n",
      "Gradient Descent(75/99): loss=0.3903184356781308, gradient=0.0006194580882638088\n",
      "Gradient Descent(76/99): loss=0.39031839641211874, gradient=0.0006104314902011999\n",
      "Gradient Descent(77/99): loss=0.39031835842086654, gradient=0.0005983338465349679\n",
      "Gradient Descent(78/99): loss=0.3903183214405232, gradient=0.0005898814445187085\n",
      "Gradient Descent(79/99): loss=0.3903182854427697, gradient=0.00058168399813415\n",
      "Gradient Descent(80/99): loss=0.39031825038587925, gradient=0.0005737064359716389\n",
      "Gradient Descent(81/99): loss=0.3903182162339689, gradient=0.0005659389685587163\n",
      "Gradient Descent(82/99): loss=0.390318182952115, gradient=0.0005583738232698165\n",
      "Gradient Descent(83/99): loss=0.3903181505072842, gradient=0.0005510038396926567\n",
      "Gradient Descent(84/99): loss=0.3903181188679831, gradient=0.00054382227866833\n",
      "Gradient Descent(85/99): loss=0.39031808800421985, gradient=0.0005368227379658524\n",
      "Gradient Descent(86/99): loss=0.3903180578874056, gradient=0.0005299990998795539\n",
      "Gradient Descent(87/99): loss=0.3903180284902802, gradient=0.0005233454966452296\n",
      "Gradient Descent(88/99): loss=0.3903179997868406, gradient=0.000516856286748915\n",
      "Gradient Descent(89/99): loss=0.39031797175227356, gradient=0.0005105260379984614\n",
      "Gradient Descent(90/99): loss=0.3903179443628939, gradient=0.0005043495148382268\n",
      "Gradient Descent(91/99): loss=0.39031791759608575, gradient=0.000498321668360517\n",
      "Gradient Descent(92/99): loss=0.3903178914302476, gradient=0.000492437628057889\n",
      "Gradient Descent(93/99): loss=0.3903178658447405, gradient=0.0004866926947229643\n",
      "Gradient Descent(94/99): loss=0.39031784081983883, gradient=0.00048108233412377056\n",
      "Gradient Descent(95/99): loss=0.39031781633668483, gradient=0.0004756021712199925\n",
      "Gradient Descent(96/99): loss=0.3903177923772433, gradient=0.0004702479847698891\n",
      "Gradient Descent(97/99): loss=0.39031776892426207, gradient=0.0004650157022319387\n",
      "Gradient Descent(98/99): loss=0.3903177459612315, gradient=0.0004599013948974415\n",
      "Gradient Descent(99/99): loss=0.3903177234723483, gradient=0.0004549012732131882\n",
      "Gradient Descent(0/99): loss=0.38997078892167236, gradient=0.010003300290005726\n",
      "Gradient Descent(1/99): loss=0.38996457319650873, gradient=0.008228394577647177\n",
      "Gradient Descent(2/99): loss=0.3899601734338433, gradient=0.006895781025957107\n",
      "Gradient Descent(3/99): loss=0.3899569477299225, gradient=0.005880824922539691\n",
      "Gradient Descent(4/99): loss=0.3899544979183686, gradient=0.005105029631341379\n",
      "Gradient Descent(5/99): loss=0.38995257763893154, gradient=0.004498364534735104\n",
      "Gradient Descent(6/99): loss=0.389951022130493, gradient=0.004035312374073241\n",
      "Gradient Descent(7/99): loss=0.3899497297884802, gradient=0.003666026577820362\n",
      "Gradient Descent(8/99): loss=0.3899486307385031, gradient=0.003372615097870074\n",
      "Gradient Descent(9/99): loss=0.3899476785816344, gradient=0.003133037968630992\n",
      "Gradient Descent(10/99): loss=0.38994684118770245, gradient=0.002933608712974968\n",
      "Gradient Descent(11/99): loss=0.3899460957222164, gradient=0.0027644959050144467\n",
      "Gradient Descent(12/99): loss=0.3899454255305344, gradient=0.002618639488888677\n",
      "Gradient Descent(13/99): loss=0.389944818150974, gradient=0.0024909457890637315\n",
      "Gradient Descent(14/99): loss=0.38994426403106214, gradient=0.002377705755218603\n",
      "Gradient Descent(15/99): loss=0.38994375568365763, gradient=0.002276183541437664\n",
      "Gradient Descent(16/99): loss=0.3899432885040385, gradient=0.0021775414749254525\n",
      "Gradient Descent(17/99): loss=0.38994285597280387, gradient=0.002094421801438659\n",
      "Gradient Descent(18/99): loss=0.38994245411856726, gradient=0.0020181121786105355\n",
      "Gradient Descent(19/99): loss=0.3899420796066199, gradient=0.0019476749873192944\n",
      "Gradient Descent(20/99): loss=0.38994172961012374, gradient=0.0018823574118132789\n",
      "Gradient Descent(21/99): loss=0.3899414017079082, gradient=0.0018215453609316807\n",
      "Gradient Descent(22/99): loss=0.38994109380860553, gradient=0.0017647312116445717\n",
      "Gradient Descent(23/99): loss=0.3899408040919054, gradient=0.0017114903851893838\n",
      "Gradient Descent(24/99): loss=0.3899405309627262, gradient=0.0016614640359376282\n",
      "Gradient Descent(25/99): loss=0.38994027361793, gradient=0.001610485077865121\n",
      "Gradient Descent(26/99): loss=0.38994003014995865, gradient=0.0015661541549897619\n",
      "Gradient Descent(27/99): loss=0.3899397994478943, gradient=0.0015242586916915247\n",
      "Gradient Descent(28/99): loss=0.3899395805199248, gradient=0.0014845916444212659\n",
      "Gradient Descent(29/99): loss=0.3899393724765083, gradient=0.0014469724088214493\n",
      "Gradient Descent(30/99): loss=0.38993917451663146, gradient=0.0014112424134553557\n",
      "Gradient Descent(31/99): loss=0.3899389859163463, gradient=0.0013772615748152577\n",
      "Gradient Descent(32/99): loss=0.3899388060191686, gradient=0.0013449054228796176\n",
      "Gradient Descent(33/99): loss=0.38993863422798103, gradient=0.001314062753958158\n",
      "Gradient Descent(34/99): loss=0.3899384699981539, gradient=0.0012846337017295905\n",
      "Gradient Descent(35/99): loss=0.3899383128316692, gradient=0.0012565281425350606\n",
      "Gradient Descent(36/99): loss=0.38993816227207384, gradient=0.0012296643697373702\n",
      "Gradient Descent(37/99): loss=0.3899380179001161, gradient=0.0012039679860799866\n",
      "Gradient Descent(38/99): loss=0.3899378793299611, gradient=0.0011793709737226063\n",
      "Gradient Descent(39/99): loss=0.38993774620588545, gradient=0.0011558109098835797\n",
      "Gradient Descent(40/99): loss=0.38993761819938194, gradient=0.0011332303024108314\n",
      "Gradient Descent(41/99): loss=0.3899374950066113, gradient=0.0011115760245944267\n",
      "Gradient Descent(42/99): loss=0.38993737634614845, gradient=0.0010907988324602225\n",
      "Gradient Descent(43/99): loss=0.38993726195698536, gradient=0.0010708529508925888\n",
      "Gradient Descent(44/99): loss=0.38993715159675163, gradient=0.0010516957174126146\n",
      "Gradient Descent(45/99): loss=0.38993704504012394, gradient=0.0010332872744236163\n",
      "Gradient Descent(46/99): loss=0.3899369420774037, gradient=0.001015590302337479\n",
      "Gradient Descent(47/99): loss=0.3899368425132348, gradient=0.000998569787291464\n",
      "Gradient Descent(48/99): loss=0.38993674616545126, gradient=0.000982192818221033\n",
      "Gradient Descent(49/99): loss=0.3899366528640334, gradient=0.0009664284089181227\n",
      "Gradient Descent(50/99): loss=0.3899365624501639, gradient=0.0009512473414124095\n",
      "Gradient Descent(51/99): loss=0.3899364747753703, gradient=0.000936622027598911\n",
      "Gradient Descent(52/99): loss=0.38993638970074396, gradient=0.0009225263865183949\n",
      "Gradient Descent(53/99): loss=0.38993630709622906, gradient=0.0009089357351003211\n",
      "Gradient Descent(54/99): loss=0.3899362268399726, gradient=0.0008958266905125594\n",
      "Gradient Descent(55/99): loss=0.38993614881773003, gradient=0.0008831770825433248\n",
      "Gradient Descent(56/99): loss=0.38993607292231997, gradient=0.0008709658746757887\n",
      "Gradient Descent(57/99): loss=0.3899359990531247, gradient=0.0008591730927139851\n",
      "Gradient Descent(58/99): loss=0.38993592711562886, gradient=0.0008477797599864361\n",
      "Gradient Descent(59/99): loss=0.38993585702099876, gradient=0.0008367678382935419\n",
      "Gradient Descent(60/99): loss=0.3899357886856897, gradient=0.0008261201738867561\n",
      "Gradient Descent(61/99): loss=0.3899357220310883, gradient=0.000815820447867361\n",
      "Gradient Descent(62/99): loss=0.38993565698317895, gradient=0.0008058531304804188\n",
      "Gradient Descent(63/99): loss=0.3899355934722372, gradient=0.0007962034388524918\n",
      "Gradient Descent(64/99): loss=0.3899355314325453, gradient=0.0007868572977850962\n",
      "Gradient Descent(65/99): loss=0.38993547080212826, gradient=0.0007778013032688951\n",
      "Gradient Descent(66/99): loss=0.38993541152251104, gradient=0.0007690226884305466\n",
      "Gradient Descent(67/99): loss=0.3899353535384905, gradient=0.0007605092916622283\n",
      "Gradient Descent(68/99): loss=0.38993529679792605, gradient=0.0007522495267183811\n",
      "Gradient Descent(69/99): loss=0.38993524125154394, gradient=0.0007442323545924577\n",
      "Gradient Descent(70/99): loss=0.3899351868527559, gradient=0.0007364472570114641\n",
      "Gradient Descent(71/99): loss=0.38993513355748916, gradient=0.000728884211406836\n",
      "Gradient Descent(72/99): loss=0.3899350813240303, gradient=0.0007215336672385615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(73/99): loss=0.3899350301128785, gradient=0.0007143865235647367\n",
      "Gradient Descent(74/99): loss=0.38993497988660775, gradient=0.0007074341077621066\n",
      "Gradient Descent(75/99): loss=0.38993493060974094, gradient=0.0007006681553145487\n",
      "Gradient Descent(76/99): loss=0.38993488224863043, gradient=0.0006940807905960983\n",
      "Gradient Descent(77/99): loss=0.38993483477134755, gradient=0.0006876645085830831\n",
      "Gradient Descent(78/99): loss=0.3899347881475787, gradient=0.0006814121574381074\n",
      "Gradient Descent(79/99): loss=0.3899347423485287, gradient=0.0006753169219133587\n",
      "Gradient Descent(80/99): loss=0.3899346973468306, gradient=0.0006693723075268396\n",
      "Gradient Descent(81/99): loss=0.38993465311646147, gradient=0.0006635721254695024\n",
      "Gradient Descent(82/99): loss=0.3899346096326628, gradient=0.000657910478204898\n",
      "Gradient Descent(83/99): loss=0.3899345668718671, gradient=0.0006523817457260565\n",
      "Gradient Descent(84/99): loss=0.38993452481162866, gradient=0.0006469805724380023\n",
      "Gradient Descent(85/99): loss=0.38993448343055825, gradient=0.0006417018546354818\n",
      "Gradient Descent(86/99): loss=0.3899344427082637, gradient=0.0006365407285491665\n",
      "Gradient Descent(87/99): loss=0.3899344026252917, gradient=0.000631492558933527\n",
      "Gradient Descent(88/99): loss=0.3899343631630757, gradient=0.0006265529281736095\n",
      "Gradient Descent(89/99): loss=0.38993432430388514, gradient=0.000621717625886545\n",
      "Gradient Descent(90/99): loss=0.38993428603077945, gradient=0.0006169826389977152\n",
      "Gradient Descent(91/99): loss=0.3899342483275631, gradient=0.0006123441422699927\n",
      "Gradient Descent(92/99): loss=0.3899342111787459, gradient=0.0006077984892678753\n",
      "Gradient Descent(93/99): loss=0.38993417456950297, gradient=0.0006033422037372042\n",
      "Gradient Descent(94/99): loss=0.3899341384856389, gradient=0.0005989719713832433\n",
      "Gradient Descent(95/99): loss=0.3899341029135542, gradient=0.0005946846320301939\n",
      "Gradient Descent(96/99): loss=0.38993406784021245, gradient=0.0005904771721459496\n",
      "Gradient Descent(97/99): loss=0.38993403325311093, gradient=0.0005863467177166162\n",
      "Gradient Descent(98/99): loss=0.3899339991402516, gradient=0.0005822905274556592\n",
      "Gradient Descent(99/99): loss=0.38993396549011544, gradient=0.0005783059863338553\n",
      "Gradient Descent(0/99): loss=0.3896722401072705, gradient=0.011063736065316552\n",
      "Gradient Descent(1/99): loss=0.3896672589792343, gradient=0.00743898932262693\n",
      "Gradient Descent(2/99): loss=0.3896637266069818, gradient=0.006177705972421125\n",
      "Gradient Descent(3/99): loss=0.3896611310810174, gradient=0.005278902143372399\n",
      "Gradient Descent(4/99): loss=0.38965918552434564, gradient=0.004560160933841668\n",
      "Gradient Descent(5/99): loss=0.3896577010516199, gradient=0.003971354270647044\n",
      "Gradient Descent(6/99): loss=0.3896565449714304, gradient=0.0034967839108742574\n",
      "Gradient Descent(7/99): loss=0.389655628047624, gradient=0.0031075219515891315\n",
      "Gradient Descent(8/99): loss=0.3896548881608786, gradient=0.002785946684427456\n",
      "Gradient Descent(9/99): loss=0.38965428150222187, gradient=0.0025181750318283664\n",
      "Gradient Descent(10/99): loss=0.3896537767495068, gradient=0.0022933107826450193\n",
      "Gradient Descent(11/99): loss=0.38965335118194655, gradient=0.002102828951027192\n",
      "Gradient Descent(12/99): loss=0.3896529880706962, gradient=0.0019400723782919608\n",
      "Gradient Descent(13/99): loss=0.3896526749117011, gradient=0.0017998435646925813\n",
      "Gradient Descent(14/99): loss=0.3896524022183242, gradient=0.001678077571250393\n",
      "Gradient Descent(15/99): loss=0.38965216268828823, gradient=0.0015715832472029741\n",
      "Gradient Descent(16/99): loss=0.3896519506227384, gradient=0.0014778410546701852\n",
      "Gradient Descent(17/99): loss=0.3896517615164536, gradient=0.0013948468551317217\n",
      "Gradient Descent(18/99): loss=0.38965159176524566, gradient=0.0013209923047387798\n",
      "Gradient Descent(19/99): loss=0.3896514384543564, gradient=0.001254973906932662\n",
      "Gradient Descent(20/99): loss=0.38965129920342234, gradient=0.001195724170436785\n",
      "Gradient Descent(21/99): loss=0.3896511720513643, gradient=0.0011423596164671453\n",
      "Gradient Descent(22/99): loss=0.38965105536979944, gradient=0.0010941415102313891\n",
      "Gradient Descent(23/99): loss=0.38965094779706877, gradient=0.0010504461361801934\n",
      "Gradient Descent(24/99): loss=0.3896508481873511, gradient=0.0010107421985880004\n",
      "Gradient Descent(25/99): loss=0.38965075557096207, gradient=0.0009745735283966661\n",
      "Gradient Descent(26/99): loss=0.38965066912304225, gradient=0.0009415457394261333\n",
      "Gradient Descent(27/99): loss=0.3896505881386204, gradient=0.0009113158280998457\n",
      "Gradient Descent(28/99): loss=0.3896505120125721, gradient=0.0008835839743790425\n",
      "Gradient Descent(29/99): loss=0.38965044022338635, gradient=0.0008580869976587363\n",
      "Gradient Descent(30/99): loss=0.38965037231991506, gradient=0.0008345930661898464\n",
      "Gradient Descent(31/99): loss=0.38965030791049243, gradient=0.0008128973649394038\n",
      "Gradient Descent(32/99): loss=0.3896502466539416, gradient=0.0007928185045479112\n",
      "Gradient Descent(33/99): loss=0.38965018825210807, gradient=0.0007741955106875126\n",
      "Gradient Descent(34/99): loss=0.3896501324436254, gradient=0.000756885274291844\n",
      "Gradient Descent(35/99): loss=0.3896500789986916, gradient=0.0007407603730151721\n",
      "Gradient Descent(36/99): loss=0.3896500277146723, gradient=0.0007257071959791186\n",
      "Gradient Descent(37/99): loss=0.3896499784123857, gradient=0.0007116243196523279\n",
      "Gradient Descent(38/99): loss=0.38964993093295647, gradient=0.0006984210942424575\n",
      "Gradient Descent(39/99): loss=0.3896498851351379, gradient=0.0006860164084601452\n",
      "Gradient Descent(40/99): loss=0.38964984089302984, gradient=0.0006743376068079354\n",
      "Gradient Descent(41/99): loss=0.38964979809412553, gradient=0.0006633195382766541\n",
      "Gradient Descent(42/99): loss=0.38964975663763646, gradient=0.0006529037189351507\n",
      "Gradient Descent(43/99): loss=0.38964971643305285, gradient=0.0006430375936971478\n",
      "Gradient Descent(44/99): loss=0.38964967739890083, gradient=0.0006336738847560141\n",
      "Gradient Descent(45/99): loss=0.38964963946166886, gradient=0.0006247700159578167\n",
      "Gradient Descent(46/99): loss=0.3896496025548774, gradient=0.0006162876038388278\n",
      "Gradient Descent(47/99): loss=0.3896495666182706, gradient=0.0006081920072686322\n",
      "Gradient Descent(48/99): loss=0.3896495315971133, gradient=0.0006004519286660669\n",
      "Gradient Descent(49/99): loss=0.3896494974415766, gradient=0.0005930390606319284\n",
      "Gradient Descent(50/99): loss=0.3896494641062017, gradient=0.0005859277726000911\n",
      "Gradient Descent(51/99): loss=0.38964943154942844, gradient=0.0005790948327658711\n",
      "Gradient Descent(52/99): loss=0.38964939973318263, gradient=0.0005725191611254285\n",
      "Gradient Descent(53/99): loss=0.38964936862251304, gradient=0.0005661816099623173\n",
      "Gradient Descent(54/99): loss=0.38964933818526964, gradient=0.0005600647685600478\n",
      "Gradient Descent(55/99): loss=0.38964930839182177, gradient=0.0005541527893065754\n",
      "Gradient Descent(56/99): loss=0.38964927921480613, gradient=0.0005484312326993148\n",
      "Gradient Descent(57/99): loss=0.38964925062890465, gradient=0.0005428869290580973\n",
      "Gradient Descent(58/99): loss=0.3896492226106483, gradient=0.0005375078550187288\n",
      "Gradient Descent(59/99): loss=0.3896491951382399, gradient=0.0005322830231115879\n",
      "Gradient Descent(60/99): loss=0.38964916819139783, gradient=0.000527202382932334\n",
      "Gradient Descent(61/99): loss=0.38964914175121657, gradient=0.0005222567325926569\n",
      "Gradient Descent(62/99): loss=0.3896491158000413, gradient=0.0005174376392947234\n",
      "Gradient Descent(63/99): loss=0.3896490903213557, gradient=0.0005127373680112042\n",
      "Gradient Descent(64/99): loss=0.3896490652996818, gradient=0.0005081488173737359\n",
      "Gradient Descent(65/99): loss=0.38964904072049006, gradient=0.0005036654619789838\n",
      "Gradient Descent(66/99): loss=0.3896490165701175, gradient=0.0004992813004136463\n",
      "Gradient Descent(67/99): loss=0.38964899283569454, gradient=0.0004949908083822812\n",
      "Gradient Descent(68/99): loss=0.3896489695050802, gradient=0.0004907888963929599\n",
      "Gradient Descent(69/99): loss=0.38964894656680105, gradient=0.0004866708715182929\n",
      "Gradient Descent(70/99): loss=0.3896489240099986, gradient=0.0004826324028051846\n",
      "Gradient Descent(71/99): loss=0.38964890182437983, gradient=0.0004786694899546013\n",
      "Gradient Descent(72/99): loss=0.3896488800001736, gradient=0.0004747784349355403\n",
      "Gradient Descent(73/99): loss=0.38964885852808995, gradient=0.0004709558162343076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(74/99): loss=0.38964883739928446, gradient=0.0004671984654743165\n",
      "Gradient Descent(75/99): loss=0.3896488166053247, gradient=0.0004635034461693374\n",
      "Gradient Descent(76/99): loss=0.3896487961381601, gradient=0.0004598680343997861\n",
      "Gradient Descent(77/99): loss=0.3896487759900948, gradient=0.0004562897012237287\n",
      "Gradient Descent(78/99): loss=0.38964875615376227, gradient=0.00045276609665461254\n",
      "Gradient Descent(79/99): loss=0.389648736622103, gradient=0.000449295035055258\n",
      "Gradient Descent(80/99): loss=0.389648717388343, gradient=0.00044587448181307915\n",
      "Gradient Descent(81/99): loss=0.3896486984459755, gradient=0.0004425025411764498\n",
      "Gradient Descent(82/99): loss=0.3896486797887426, gradient=0.0004391774451427849\n",
      "Gradient Descent(83/99): loss=0.3896486614106197, gradient=0.00043589754330143485\n",
      "Gradient Descent(84/99): loss=0.3896486433058011, gradient=0.0004326612935437243\n",
      "Gradient Descent(85/99): loss=0.38964862546868567, gradient=0.00042946725356057873\n",
      "Gradient Descent(86/99): loss=0.3896486078938653, gradient=0.00042631407305705824\n",
      "Gradient Descent(87/99): loss=0.38964859057611295, gradient=0.000423200486619151\n",
      "Gradient Descent(88/99): loss=0.3896485735103725, gradient=0.0004201253071747442\n",
      "Gradient Descent(89/99): loss=0.3896485566917487, gradient=0.00041708741999641353\n",
      "Gradient Descent(90/99): loss=0.3896485401154986, gradient=0.0004140857771981716\n",
      "Gradient Descent(91/99): loss=0.389648523777023, gradient=0.00041111939268338575\n",
      "Gradient Descent(92/99): loss=0.3896485076718589, gradient=0.00040818733750440204\n",
      "Gradient Descent(93/99): loss=0.3896484917956726, gradient=0.0004052887355988349\n",
      "Gradient Descent(94/99): loss=0.3896484761442531, gradient=0.0004024227598699377\n",
      "Gradient Descent(95/99): loss=0.3896484607135057, gradient=0.00039958862858187837\n",
      "Gradient Descent(96/99): loss=0.38964844549944666, gradient=0.00039678560204345955\n",
      "Gradient Descent(97/99): loss=0.3896484304981986, gradient=0.0003940129795556488\n",
      "Gradient Descent(98/99): loss=0.38964841570598385, gradient=0.000391270096601162\n",
      "Gradient Descent(99/99): loss=0.3896484011191222, gradient=0.0003885563222557367\n",
      "Gradient Descent(0/99): loss=0.38895734209846544, gradient=0.00819706543454855\n",
      "Gradient Descent(1/99): loss=0.3889545763822749, gradient=0.005554547095128174\n",
      "Gradient Descent(2/99): loss=0.3889526582772065, gradient=0.004567518830053175\n",
      "Gradient Descent(3/99): loss=0.38895127682221303, gradient=0.0038632608006459544\n",
      "Gradient Descent(4/99): loss=0.3889502575816961, gradient=0.0033099099800928916\n",
      "Gradient Descent(5/99): loss=0.3889494855716731, gradient=0.0028725850502892706\n",
      "Gradient Descent(6/99): loss=0.38894888439348735, gradient=0.0025275592086816175\n",
      "Gradient Descent(7/99): loss=0.3889484027368358, gradient=0.0022557701121131086\n",
      "Gradient Descent(8/99): loss=0.38894800592751494, gradient=0.00204168123086709\n",
      "Gradient Descent(9/99): loss=0.3889476703354473, gradient=0.001872674324577174\n",
      "Gradient Descent(10/99): loss=0.3889473797144707, gradient=0.0017385981597927852\n",
      "Gradient Descent(11/99): loss=0.38894712278266846, gradient=0.0016313930952250768\n",
      "Gradient Descent(12/99): loss=0.3889468916183059, gradient=0.0015447536600065753\n",
      "Gradient Descent(13/99): loss=0.3889466805904421, gradient=0.0014738164626127293\n",
      "Gradient Descent(14/99): loss=0.3889464856423461, gradient=0.0014148758268575582\n",
      "Gradient Descent(15/99): loss=0.3889463038085291, gradient=0.0013651341396601463\n",
      "Gradient Descent(16/99): loss=0.38894613288720675, gradient=0.001322491749658103\n",
      "Gradient Descent(17/99): loss=0.38894597121667285, gradient=0.0012853768826264563\n",
      "Gradient Descent(18/99): loss=0.38894581752149243, gradient=0.0012526123121353139\n",
      "Gradient Descent(19/99): loss=0.3889456708058698, gradient=0.0012233134299682472\n",
      "Gradient Descent(20/99): loss=0.3889455302790797, gradient=0.0011968117559542214\n",
      "Gradient Descent(21/99): loss=0.3889453953028287, gradient=0.0011725982825738336\n",
      "Gradient Descent(22/99): loss=0.3889452653537231, gradient=0.0011502818658600423\n",
      "Gradient Descent(23/99): loss=0.3889451399962169, gradient=0.0011295588120791285\n",
      "Gradient Descent(24/99): loss=0.3889450188628938, gradient=0.0011101906861812891\n",
      "Gradient Descent(25/99): loss=0.38894490163992096, gradient=0.0010919881069941934\n",
      "Gradient Descent(26/99): loss=0.3889447880561899, gradient=0.0010747988805072564\n",
      "Gradient Descent(27/99): loss=0.38894467787510417, gradient=0.0010584992702401532\n",
      "Gradient Descent(28/99): loss=0.388944570888296, gradient=0.0010429875367893716\n",
      "Gradient Descent(29/99): loss=0.38894446691075435, gradient=0.0010281791222939473\n",
      "Gradient Descent(30/99): loss=0.3889443657770052, gradient=0.0010140030317486056\n",
      "Gradient Descent(31/99): loss=0.388944267338079, gradient=0.0010003990895628442\n",
      "Gradient Descent(32/99): loss=0.3889441714590781, gradient=0.0009873158401548555\n",
      "Gradient Descent(33/99): loss=0.38894407801720343, gradient=0.0009747089258348547\n",
      "Gradient Descent(34/99): loss=0.38894398690013904, gradient=0.0009625398212080359\n",
      "Gradient Descent(35/99): loss=0.3889438980047143, gradient=0.0009507748361481977\n",
      "Gradient Descent(36/99): loss=0.38894381123579186, gradient=0.0009393843228931157\n",
      "Gradient Descent(37/99): loss=0.3889437265053299, gradient=0.0009283420396948291\n",
      "Gradient Descent(38/99): loss=0.38894364419527216, gradient=0.0009123848987479148\n",
      "Gradient Descent(39/99): loss=0.3889435637244594, gradient=0.0009022621641205186\n",
      "Gradient Descent(40/99): loss=0.38894348502062237, gradient=0.0008924343446835603\n",
      "Gradient Descent(41/99): loss=0.38894340801805294, gradient=0.0008828749964680298\n",
      "Gradient Descent(42/99): loss=0.3889433326560237, gradient=0.0008735624144490143\n",
      "Gradient Descent(43/99): loss=0.3889432588781488, gradient=0.000864478526449448\n",
      "Gradient Descent(44/99): loss=0.3889431866317497, gradient=0.0008556080729271415\n",
      "Gradient Descent(45/99): loss=0.38894311586738484, gradient=0.0008469379938285878\n",
      "Gradient Descent(46/99): loss=0.38894304653846495, gradient=0.0008384569678617946\n",
      "Gradient Descent(47/99): loss=0.3889429786009417, gradient=0.0008301550643091568\n",
      "Gradient Descent(48/99): loss=0.38894291201305026, gradient=0.0008220234781554603\n",
      "Gradient Descent(49/99): loss=0.38894284673509644, gradient=0.0008140543270863384\n",
      "Gradient Descent(50/99): loss=0.38894278272927374, gradient=0.000806240494603428\n",
      "Gradient Descent(51/99): loss=0.38894271995950974, gradient=0.0007985755076628039\n",
      "Gradient Descent(52/99): loss=0.3889426583913298, gradient=0.000791053440286207\n",
      "Gradient Descent(53/99): loss=0.38894259799174147, gradient=0.0007836688368253457\n",
      "Gradient Descent(54/99): loss=0.3889425387291299, gradient=0.0007764166501916315\n",
      "Gradient Descent(55/99): loss=0.38894248057316516, gradient=0.0007692921915662088\n",
      "Gradient Descent(56/99): loss=0.38894242349472097, gradient=0.0007622910889841505\n",
      "Gradient Descent(57/99): loss=0.38894236746579947, gradient=0.000755409252840234\n",
      "Gradient Descent(58/99): loss=0.38894231245946354, gradient=0.0007486428468414698\n",
      "Gradient Descent(59/99): loss=0.38894225844977626, gradient=0.0007419882632896605\n",
      "Gradient Descent(60/99): loss=0.38894220541174446, gradient=0.0007354421018406728\n",
      "Gradient Descent(61/99): loss=0.38894215332126636, gradient=0.0007290011510849022\n",
      "Gradient Descent(62/99): loss=0.3889421021550857, gradient=0.000722662372441173\n",
      "Gradient Descent(63/99): loss=0.38894205189074676, gradient=0.0007164228859683164\n",
      "Gradient Descent(64/99): loss=0.3889420025065546, gradient=0.0007102799577817739\n",
      "Gradient Descent(65/99): loss=0.388941953981537, gradient=0.0007042309888275855\n",
      "Gradient Descent(66/99): loss=0.3889419062954098, gradient=0.0006982735048149522\n",
      "Gradient Descent(67/99): loss=0.3889418594285443, gradient=0.000692405147146016\n",
      "Gradient Descent(68/99): loss=0.388941813361937, gradient=0.0006866236647111877\n",
      "Gradient Descent(69/99): loss=0.38894176807718134, gradient=0.0006809269064414755\n",
      "Gradient Descent(70/99): loss=0.38894172355644063, gradient=0.0006753128145267944\n",
      "Gradient Descent(71/99): loss=0.38894167978242405, gradient=0.0006697794182241973\n",
      "Gradient Descent(72/99): loss=0.3889416367383624, gradient=0.0006643248281911234\n",
      "Gradient Descent(73/99): loss=0.388941594407987, gradient=0.0006589472312882901\n",
      "Gradient Descent(74/99): loss=0.38894155277550807, gradient=0.0006536448858047105\n",
      "Gradient Descent(75/99): loss=0.38894151182559655, gradient=0.0006484161170625185\n",
      "Gradient Descent(76/99): loss=0.3889414715433645, gradient=0.0006432593133666042\n",
      "Gradient Descent(77/99): loss=0.3889414319143488, gradient=0.0006381729222655318\n",
      "Gradient Descent(78/99): loss=0.3889413929244937, gradient=0.0006331554470969295\n",
      "Gradient Descent(79/99): loss=0.3889413545601368, gradient=0.0006282054437912187\n",
      "Gradient Descent(80/99): loss=0.3889413168079922, gradient=0.000623321517911901\n",
      "Gradient Descent(81/99): loss=0.3889412796551391, gradient=0.0006185023219116258\n",
      "Gradient Descent(82/99): loss=0.3889412430890071, gradient=0.0006137465525867944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(83/99): loss=0.3889412070973634, gradient=0.0006090529487135048\n",
      "Gradient Descent(84/99): loss=0.38894117166830183, gradient=0.0006044202888505817\n",
      "Gradient Descent(85/99): loss=0.38894113679023096, gradient=0.0005998473892962913\n",
      "Gradient Descent(86/99): loss=0.38894110245186364, gradient=0.0005953331021861036\n",
      "Gradient Descent(87/99): loss=0.3889410686422059, gradient=0.0005908763137208966\n",
      "Gradient Descent(88/99): loss=0.3889410353505483, gradient=0.0005864759425149458\n",
      "Gradient Descent(89/99): loss=0.38894100256645514, gradient=0.0005821309380551375\n",
      "Gradient Descent(90/99): loss=0.388940970279757, gradient=0.0005778402792618552\n",
      "Gradient Descent(91/99): loss=0.38894093848054057, gradient=0.0005736029731450328\n",
      "Gradient Descent(92/99): loss=0.388940907159142, gradient=0.0005694180535469103\n",
      "Gradient Descent(93/99): loss=0.38894087630613794, gradient=0.0005652845799661612\n",
      "Gradient Descent(94/99): loss=0.3889408459123385, gradient=0.0005612016364562327\n",
      "Gradient Descent(95/99): loss=0.38894081596877955, gradient=0.0005571683305932319\n",
      "Gradient Descent(96/99): loss=0.3889407864667163, gradient=0.0005531837925078134\n",
      "Gradient Descent(97/99): loss=0.38894075739761674, gradient=0.0005492471739764757\n",
      "Gradient Descent(98/99): loss=0.3889407287531546, gradient=0.0005453576475676183\n",
      "Gradient Descent(99/99): loss=0.3889407005252034, gradient=0.000541514405839163\n",
      "Gradient Descent(0/99): loss=0.3901388622769348, gradient=0.015398346063783647\n",
      "Gradient Descent(1/99): loss=0.39013496685959825, gradient=0.00691900933512835\n",
      "Gradient Descent(2/99): loss=0.39013233021745647, gradient=0.0053600311176545705\n",
      "Gradient Descent(3/99): loss=0.39013035002882573, gradient=0.00461528860431507\n",
      "Gradient Descent(4/99): loss=0.39012881899106794, gradient=0.004048068257357264\n",
      "Gradient Descent(5/99): loss=0.3901276098063916, gradient=0.003590463128052511\n",
      "Gradient Descent(6/99): loss=0.39012663519540325, gradient=0.0032174453693697576\n",
      "Gradient Descent(7/99): loss=0.39012583458677674, gradient=0.0029111747125813684\n",
      "Gradient Descent(8/99): loss=0.390125165214897, gradient=0.002657821097670176\n",
      "Gradient Descent(9/99): loss=0.39012459647414177, gradient=0.0024465837097651932\n",
      "Gradient Descent(10/99): loss=0.39012410613776444, gradient=0.0022690152040568136\n",
      "Gradient Descent(11/99): loss=0.3901236778327547, gradient=0.002118499267780127\n",
      "Gradient Descent(12/99): loss=0.3901232993265144, gradient=0.0019898430285353818\n",
      "Gradient Descent(13/99): loss=0.39012296135307156, gradient=0.0018789593421327125\n",
      "Gradient Descent(14/99): loss=0.3901226567992423, gradient=0.001782619742498491\n",
      "Gradient Descent(15/99): loss=0.39012238013392203, gradient=0.0016982628763148608\n",
      "Gradient Descent(16/99): loss=0.39012212700320303, gradient=0.001623846348917046\n",
      "Gradient Descent(17/99): loss=0.39012189393979974, gradient=0.001557732375232492\n",
      "Gradient Descent(18/99): loss=0.39012167815210763, gradient=0.001498599625241856\n",
      "Gradient Descent(19/99): loss=0.39012147736932196, gradient=0.0014453752749048038\n",
      "Gradient Descent(20/99): loss=0.3901212897264384, gradient=0.0013971825849843345\n",
      "Gradient Descent(21/99): loss=0.39012111367792923, gradient=0.0013533003811661305\n",
      "Gradient Descent(22/99): loss=0.39012094793224844, gradient=0.0013131316418398503\n",
      "Gradient Descent(23/99): loss=0.3901207914016259, gradient=0.0012761790529621652\n",
      "Gradient Descent(24/99): loss=0.39012064316319617, gradient=0.0012420258964297983\n",
      "Gradient Descent(25/99): loss=0.39012050242861956, gradient=0.001210321028757939\n",
      "Gradient Descent(26/99): loss=0.3901203685201222, gradient=0.001180767005398519\n",
      "Gradient Descent(27/99): loss=0.39012024085143954, gradient=0.001153110633154479\n",
      "Gradient Descent(28/99): loss=0.39012011891253556, gradient=0.001127135405271526\n",
      "Gradient Descent(29/99): loss=0.39012000225726007, gradient=0.001102655403907741\n",
      "Gradient Descent(30/99): loss=0.3901198904933086, gradient=0.0010795103529029767\n",
      "Gradient Descent(31/99): loss=0.3901197832739991, gradient=0.0010575615778979888\n",
      "Gradient Descent(32/99): loss=0.3901196802915014, gradient=0.0010366886868348147\n",
      "Gradient Descent(33/99): loss=0.390119581271225, gradient=0.0010167868262193638\n",
      "Gradient Descent(34/99): loss=0.39011948596714874, gradient=0.0009977644006488354\n",
      "Gradient Descent(35/99): loss=0.3901193941579093, gradient=0.000979541167551679\n",
      "Gradient Descent(36/99): loss=0.3901193056435151, gradient=0.0009620466377736025\n",
      "Gradient Descent(37/99): loss=0.3901192202425711, gradient=0.000945218726994944\n",
      "Gradient Descent(38/99): loss=0.3901191377899269, gradient=0.0009290026140501443\n",
      "Gradient Descent(39/99): loss=0.39011905813467396, gradient=0.0009133497708359088\n",
      "Gradient Descent(40/99): loss=0.3901189811384375, gradient=0.0008982171352329161\n",
      "Gradient Descent(41/99): loss=0.390118906673911, gradient=0.0008835664037733221\n",
      "Gradient Descent(42/99): loss=0.39011883462359703, gradient=0.0008693634249928729\n",
      "Gradient Descent(43/99): loss=0.39011876487871944, gradient=0.0008555776777674491\n",
      "Gradient Descent(44/99): loss=0.3901186973382853, gradient=0.0008421818216319263\n",
      "Gradient Descent(45/99): loss=0.3901186319082683, gradient=0.000829151308264762\n",
      "Gradient Descent(46/99): loss=0.39011856850089793, gradient=0.0008164640450990326\n",
      "Gradient Descent(47/99): loss=0.39011850703404083, gradient=0.0008041001034758525\n",
      "Gradient Descent(48/99): loss=0.3901184474306577, gradient=0.0007920414649539799\n",
      "Gradient Descent(49/99): loss=0.39011838961832745, gradient=0.0007802718003816541\n",
      "Gradient Descent(50/99): loss=0.3901183335288272, gradient=0.0007687762771577844\n",
      "Gradient Descent(51/99): loss=0.3901182790977634, gradient=0.0007575413907988224\n",
      "Gradient Descent(52/99): loss=0.3901182262642438, gradient=0.0007465548175019562\n",
      "Gradient Descent(53/99): loss=0.390118174970587, gradient=0.00073580528487905\n",
      "Gradient Descent(54/99): loss=0.3901181251620645, gradient=0.0007252824584431269\n",
      "Gradient Descent(55/99): loss=0.39011807678666965, gradient=0.0007149768417747184\n",
      "Gradient Descent(56/99): loss=0.3901180297949121, gradient=0.0007048796885859995\n",
      "Gradient Descent(57/99): loss=0.3901179841396327, gradient=0.0006949829251501806\n",
      "Gradient Descent(58/99): loss=0.39011793977583753, gradient=0.0006852790817740894\n",
      "Gradient Descent(59/99): loss=0.3901178966605481, gradient=0.0006757612321721635\n",
      "Gradient Descent(60/99): loss=0.3901178547526664, gradient=0.000666422939754622\n",
      "Gradient Descent(61/99): loss=0.39011781401285284, gradient=0.0006572582099741512\n",
      "Gradient Descent(62/99): loss=0.39011777440341455, gradient=0.0006482614479885355\n",
      "Gradient Descent(63/99): loss=0.3901177358882039, gradient=0.0006394274209947473\n",
      "Gradient Descent(64/99): loss=0.3901176984325281, gradient=0.0006307512246723095\n",
      "Gradient Descent(65/99): loss=0.3901176620030625, gradient=0.0006222282532478025\n",
      "Gradient Descent(66/99): loss=0.3901176265677755, gradient=0.0006138541727528649\n",
      "Gradient Descent(67/99): loss=0.39011759209585567, gradient=0.0006056248971031651\n",
      "Gradient Descent(68/99): loss=0.3901175585576485, gradient=0.0005975365666718891\n",
      "Gradient Descent(69/99): loss=0.3901175259245939, gradient=0.000589585529072246\n",
      "Gradient Descent(70/99): loss=0.3901174941691728, gradient=0.000581768321897695\n",
      "Gradient Descent(71/99): loss=0.3901174632648539, gradient=0.0005740816572002388\n",
      "Gradient Descent(72/99): loss=0.3901174331860464, gradient=0.0005665224075129627\n",
      "Gradient Descent(73/99): loss=0.3901174039080555, gradient=0.0005590875932464126\n",
      "Gradient Descent(74/99): loss=0.39011737540704067, gradient=0.0005517743713086722\n",
      "Gradient Descent(75/99): loss=0.3901173476599777, gradient=0.0005445800248165603\n",
      "Gradient Descent(76/99): loss=0.3901173206446213, gradient=0.000537501953780887\n",
      "Gradient Descent(77/99): loss=0.39011729433947184, gradient=0.0005305376666622658\n",
      "Gradient Descent(78/99): loss=0.39011726872374386, gradient=0.0005236847727058243\n",
      "Gradient Descent(79/99): loss=0.3901172437773353, gradient=0.000516940974973758\n",
      "Gradient Descent(80/99): loss=0.3901172194808003, gradient=0.0005103040640031868\n",
      "Gradient Descent(81/99): loss=0.3901171958153217, gradient=0.0005037719120261043\n",
      "Gradient Descent(82/99): loss=0.39011717276268676, gradient=0.0004973424676937638\n",
      "Gradient Descent(83/99): loss=0.390117150305263, gradient=0.0004910137512554066\n",
      "Gradient Descent(84/99): loss=0.39011712842597623, gradient=0.0004847838501460863\n",
      "Gradient Descent(85/99): loss=0.3901171071082888, gradient=0.00047865091494309243\n",
      "Gradient Descent(86/99): loss=0.39011708633617964, gradient=0.0004726131556557282\n",
      "Gradient Descent(87/99): loss=0.3901170660941258, gradient=0.0004666688383157767\n",
      "Gradient Descent(88/99): loss=0.3901170463670829, gradient=0.00046081628184022516\n",
      "Gradient Descent(89/99): loss=0.3901170271404686, gradient=0.0004550538551407022\n",
      "Gradient Descent(90/99): loss=0.39011700840014674, gradient=0.00044937997445633905\n",
      "Gradient Descent(91/99): loss=0.39011699013241014, gradient=0.0004437931008895345\n",
      "Gradient Descent(92/99): loss=0.39011697232396686, gradient=0.0004382917381264946\n",
      "Gradient Descent(93/99): loss=0.3901169549619249, gradient=0.00043287443032514865\n",
      "Gradient Descent(94/99): loss=0.3901169380337793, gradient=0.00042753976015597587\n",
      "Gradient Descent(95/99): loss=0.39011692152739813, gradient=0.0004222863469825166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(96/99): loss=0.39011690543101085, gradient=0.00041711284516846313\n",
      "Gradient Descent(97/99): loss=0.39011688973319575, gradient=0.00041201794250134805\n",
      "Gradient Descent(98/99): loss=0.39011687442286835, gradient=0.000407000358722593\n",
      "Gradient Descent(99/99): loss=0.39011685948926994, gradient=0.00040205884415479376\n",
      "Gradient Descent(0/99): loss=0.39034846119801697, gradient=0.007271069005226155\n",
      "Gradient Descent(1/99): loss=0.3903448024185886, gradient=0.006260786947843535\n",
      "Gradient Descent(2/99): loss=0.3903419302008154, gradient=0.005528867176709831\n",
      "Gradient Descent(3/99): loss=0.39033961560268116, gradient=0.004949332587159773\n",
      "Gradient Descent(4/99): loss=0.39033770752770636, gradient=0.004482363159912994\n",
      "Gradient Descent(5/99): loss=0.3903361026625842, gradient=0.004101274258212946\n",
      "Gradient Descent(6/99): loss=0.3903347290930653, gradient=0.003786237716064888\n",
      "Gradient Descent(7/99): loss=0.3903335365873074, gradient=0.003519851093265629\n",
      "Gradient Descent(8/99): loss=0.39033249180842444, gradient=0.0032801760794629897\n",
      "Gradient Descent(9/99): loss=0.3903315612869148, gradient=0.0030905570568594684\n",
      "Gradient Descent(10/99): loss=0.390330724798634, gradient=0.0029259144781500416\n",
      "Gradient Descent(11/99): loss=0.3903299668678186, gradient=0.002781405408450595\n",
      "Gradient Descent(12/99): loss=0.3903292754570014, gradient=0.0026532999942486674\n",
      "Gradient Descent(13/99): loss=0.390328641060147, gradient=0.002538697354768482\n",
      "Gradient Descent(14/99): loss=0.3903280560643366, gradient=0.0024353189500301146\n",
      "Gradient Descent(15/99): loss=0.3903275142932041, gradient=0.002341357821794102\n",
      "Gradient Descent(16/99): loss=0.39032701067529674, gradient=0.002255367922546415\n",
      "Gradient Descent(17/99): loss=0.3903265409996555, gradient=0.0021761821941432783\n",
      "Gradient Descent(18/99): loss=0.3903261058291874, gradient=0.002082347672130042\n",
      "Gradient Descent(19/99): loss=0.3903256975458598, gradient=0.0020152836105474326\n",
      "Gradient Descent(20/99): loss=0.3903253136471992, gradient=0.0019525649145204931\n",
      "Gradient Descent(21/99): loss=0.3903249519906642, gradient=0.0018936611906226106\n",
      "Gradient Descent(22/99): loss=0.3903246107103127, gradient=0.0018381343731260604\n",
      "Gradient Descent(23/99): loss=0.3903242887921724, gradient=0.0017817876839151849\n",
      "Gradient Descent(24/99): loss=0.3903239843663424, gradient=0.0017304318821801366\n",
      "Gradient Descent(25/99): loss=0.3903236958038447, gradient=0.0016836210355134049\n",
      "Gradient Descent(26/99): loss=0.39032342198666814, gradient=0.001638959520075902\n",
      "Gradient Descent(27/99): loss=0.3903231619094633, gradient=0.0015962636395801377\n",
      "Gradient Descent(28/99): loss=0.39032291466264507, gradient=0.0015553755421613038\n",
      "Gradient Descent(29/99): loss=0.3903226794195894, gradient=0.0015161586842432895\n",
      "Gradient Descent(30/99): loss=0.3903224554258431, gradient=0.0014784941823983583\n",
      "Gradient Descent(31/99): loss=0.3903222419902104, gradient=0.0014422778604880723\n",
      "Gradient Descent(32/99): loss=0.39032203847728597, gradient=0.0014074178498154812\n",
      "Gradient Descent(33/99): loss=0.3903218443011629, gradient=0.0013738326310679695\n",
      "Gradient Descent(34/99): loss=0.39032165892009607, gradient=0.0013414494304684094\n",
      "Gradient Descent(35/99): loss=0.39032148183194193, gradient=0.0013102029009143105\n",
      "Gradient Descent(36/99): loss=0.39032131257023905, gradient=0.0012800340332139602\n",
      "Gradient Descent(37/99): loss=0.39032115070081697, gradient=0.0012508892537506724\n",
      "Gradient Descent(38/99): loss=0.3903209958188432, gradient=0.001222719673719884\n",
      "Gradient Descent(39/99): loss=0.39032084754624036, gradient=0.0011954804620322179\n",
      "Gradient Descent(40/99): loss=0.3903207066364517, gradient=0.0011594459305145253\n",
      "Gradient Descent(41/99): loss=0.3903205715297192, gradient=0.0011341780036362987\n",
      "Gradient Descent(42/99): loss=0.3903204419663189, gradient=0.0011097812238926564\n",
      "Gradient Descent(43/99): loss=0.39032031764648795, gradient=0.0010861683075537765\n",
      "Gradient Descent(44/99): loss=0.3903201983031812, gradient=0.0010633033266561397\n",
      "Gradient Descent(45/99): loss=0.39032008368246807, gradient=0.0010411556658731385\n",
      "Gradient Descent(46/99): loss=0.39031997354629966, gradient=0.0010196970932063265\n",
      "Gradient Descent(47/99): loss=0.3903198676704618, gradient=0.0009989012524940894\n",
      "Gradient Descent(48/99): loss=0.3903197658437827, gradient=0.0009787433807598826\n",
      "Gradient Descent(49/99): loss=0.3903196678671853, gradient=0.0009592001014956523\n",
      "Gradient Descent(50/99): loss=0.39031957355288066, gradient=0.0009402492652245098\n",
      "Gradient Descent(51/99): loss=0.39031948272362277, gradient=0.0009218698226151701\n",
      "Gradient Descent(52/99): loss=0.390319395212027, gradient=0.0009040417208399431\n",
      "Gradient Descent(53/99): loss=0.3903193108599427, gradient=0.0008867458170656036\n",
      "Gradient Descent(54/99): loss=0.39031922951787573, gradient=0.0008699638049650033\n",
      "Gradient Descent(55/99): loss=0.3903191510444539, gradient=0.0008536781514080057\n",
      "Gradient Descent(56/99): loss=0.39031907530593246, gradient=0.0008378720413089707\n",
      "Gradient Descent(57/99): loss=0.3903190021757334, gradient=0.0008225293291502019\n",
      "Gradient Descent(58/99): loss=0.39031893153401936, gradient=0.0008076344960632568\n",
      "Gradient Descent(59/99): loss=0.3903188632672952, gradient=0.0007931726116024731\n",
      "Gradient Descent(60/99): loss=0.3903187972680371, gradient=0.0007791292995226089\n",
      "Gradient Descent(61/99): loss=0.3903187334343461, gradient=0.0007654907070040925\n",
      "Gradient Descent(62/99): loss=0.39031867166962475, gradient=0.0007522434768656011\n",
      "Gradient Descent(63/99): loss=0.3903186118822738, gradient=0.0007393747223804949\n",
      "Gradient Descent(64/99): loss=0.39031855398540954, gradient=0.0007268720043718274\n",
      "Gradient Descent(65/99): loss=0.3903184978965977, gradient=0.0007147233103087496\n",
      "Gradient Descent(66/99): loss=0.3903184435376035, gradient=0.0007029170351659464\n",
      "Gradient Descent(67/99): loss=0.3903183908341594, gradient=0.0006914419638401611\n",
      "Gradient Descent(68/99): loss=0.39031833971574453, gradient=0.0006802872549446568\n",
      "Gradient Descent(69/99): loss=0.3903182901153781, gradient=0.0006694424258253992\n",
      "Gradient Descent(70/99): loss=0.3903182419694258, gradient=0.0006588973386629064\n",
      "Gradient Descent(71/99): loss=0.3903181952174176, gradient=0.0006486421875393105\n",
      "Gradient Descent(72/99): loss=0.39031814980187474, gradient=0.0006386674863658158\n",
      "Gradient Descent(73/99): loss=0.3903181056681494, gradient=0.0006289640575779786\n",
      "Gradient Descent(74/99): loss=0.39031806276427117, gradient=0.0006195230215164392\n",
      "Gradient Descent(75/99): loss=0.3903180210408041, gradient=0.0006103357864219057\n",
      "Gradient Descent(76/99): loss=0.39031798045071087, gradient=0.000601394038979961\n",
      "Gradient Descent(77/99): loss=0.3903179409492254, gradient=0.0005926897353597134\n",
      "Gradient Descent(78/99): loss=0.3903179024937322, gradient=0.0005842150926966711\n",
      "Gradient Descent(79/99): loss=0.390317865043653, gradient=0.0005759625809753694\n",
      "Gradient Descent(80/99): loss=0.390317828560339, gradient=0.0005679249152735961\n",
      "Gradient Descent(81/99): loss=0.39031779300697017, gradient=0.0005600950483331357\n",
      "Gradient Descent(82/99): loss=0.39031775834845894, gradient=0.0005524661634272244\n",
      "Gradient Descent(83/99): loss=0.3903177245513604, gradient=0.0005450316674978158\n",
      "Gradient Descent(84/99): loss=0.3903176915837867, gradient=0.0005377851845392021\n",
      "Gradient Descent(85/99): loss=0.3903176594153262, gradient=0.0005307205492068603\n",
      "Gradient Descent(86/99): loss=0.390317628016968, gradient=0.0005238318006339948\n",
      "Gradient Descent(87/99): loss=0.3903175973610288, gradient=0.0005171131764392647\n",
      "Gradient Descent(88/99): loss=0.39031756742108537, gradient=0.0005105591069120179\n",
      "Gradient Descent(89/99): loss=0.39031753817190995, gradient=0.0005041642093623943\n",
      "Gradient Descent(90/99): loss=0.3903175095894096, gradient=0.0004979232826264764\n",
      "Gradient Descent(91/99): loss=0.39031748165056807, gradient=0.0004918313017161547\n",
      "Gradient Descent(92/99): loss=0.3903174543333913, gradient=0.0004858834126067294\n",
      "Gradient Descent(93/99): loss=0.39031742761685656, gradient=0.00048007492715437756\n",
      "Gradient Descent(94/99): loss=0.3903174014808626, gradient=0.00047440131813819336\n",
      "Gradient Descent(95/99): loss=0.3903173759061834, gradient=0.0004688582144211118\n",
      "Gradient Descent(96/99): loss=0.390317350874426, gradient=0.0004634413962255173\n",
      "Gradient Descent(97/99): loss=0.3903173263679867, gradient=0.0004581467905197253\n",
      "Gradient Descent(98/99): loss=0.3903173023700138, gradient=0.00045297046651187435\n",
      "Gradient Descent(99/99): loss=0.3903172788643692, gradient=0.00044790863124833286\n",
      "Gradient Descent(0/99): loss=0.3899700271243358, gradient=0.009998503029257505\n",
      "Gradient Descent(1/99): loss=0.3899638310700779, gradient=0.0082122704288493\n",
      "Gradient Descent(2/99): loss=0.38995944773674485, gradient=0.006880756895742978\n",
      "Gradient Descent(3/99): loss=0.3899562359516393, gradient=0.005866824199290854\n",
      "Gradient Descent(4/99): loss=0.38995380685101844, gradient=0.005073203352158334\n",
      "Gradient Descent(5/99): loss=0.3899518994422323, gradient=0.004478850327196209\n",
      "Gradient Descent(6/99): loss=0.3899503549692097, gradient=0.004017400492819715\n",
      "Gradient Descent(7/99): loss=0.3899490705071292, gradient=0.0036540638704013364\n",
      "Gradient Descent(8/99): loss=0.3899479779992547, gradient=0.0033630362650225905\n",
      "Gradient Descent(9/99): loss=0.3899470314288006, gradient=0.0031254646287222684\n",
      "Gradient Descent(10/99): loss=0.38994619891491494, gradient=0.002927728720072521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(11/99): loss=0.3899454577960979, gradient=0.0027600491162722302\n",
      "Gradient Descent(12/99): loss=0.38994479153862643, gradient=0.0026154087792581463\n",
      "Gradient Descent(13/99): loss=0.3899441877673986, gradient=0.002488749579575471\n",
      "Gradient Descent(14/99): loss=0.38994364053046243, gradient=0.0023613565260189226\n",
      "Gradient Descent(15/99): loss=0.38994314158723165, gradient=0.002246690648933717\n",
      "Gradient Descent(16/99): loss=0.3899426814997042, gradient=0.002156586219461036\n",
      "Gradient Descent(17/99): loss=0.389942255542071, gradient=0.002074439725130337\n",
      "Gradient Descent(18/99): loss=0.3899418597821992, gradient=0.0019990489863313536\n",
      "Gradient Descent(19/99): loss=0.38994149093284414, gradient=0.001929474792686389\n",
      "Gradient Descent(20/99): loss=0.3899411462070614, gradient=0.0018649651632387508\n",
      "Gradient Descent(21/99): loss=0.3899408232197651, gradient=0.0018049084877558162\n",
      "Gradient Descent(22/99): loss=0.389940519911452, gradient=0.0017488004436322384\n",
      "Gradient Descent(23/99): loss=0.3899402344900809, gradient=0.0016962200601676296\n",
      "Gradient Descent(24/99): loss=0.3899399653856452, gradient=0.0016468121100253108\n",
      "Gradient Descent(25/99): loss=0.3899397123287718, gradient=0.0015930931442201504\n",
      "Gradient Descent(26/99): loss=0.38993947288341635, gradient=0.0015493842752413774\n",
      "Gradient Descent(27/99): loss=0.3899392459509651, gradient=0.0015081093200283815\n",
      "Gradient Descent(28/99): loss=0.3899390305536097, gradient=0.0014690530038536413\n",
      "Gradient Descent(29/99): loss=0.3899388258162653, gradient=0.0014320291606312523\n",
      "Gradient Descent(30/99): loss=0.38993863095236975, gradient=0.0013968755971057178\n",
      "Gradient Descent(31/99): loss=0.3899384452520489, gradient=0.0013634500086420137\n",
      "Gradient Descent(32/99): loss=0.3899382680723081, gradient=0.0013316267069615768\n",
      "Gradient Descent(33/99): loss=0.38993809882881125, gradient=0.001301293980687119\n",
      "Gradient Descent(34/99): loss=0.38993793698894424, gradient=0.0012723519525626931\n",
      "Gradient Descent(35/99): loss=0.38993778206592444, gradient=0.0012447108290842208\n",
      "Gradient Descent(36/99): loss=0.38993763361376094, gradient=0.0012182894621368518\n",
      "Gradient Descent(37/99): loss=0.3899374912229218, gradient=0.0011930141602257232\n",
      "Gradient Descent(38/99): loss=0.38993735451658307, gradient=0.0011688177005478952\n",
      "Gradient Descent(39/99): loss=0.3899372231473674, gradient=0.0011456385035900486\n",
      "Gradient Descent(40/99): loss=0.3899370967944917, gradient=0.0011234199399638932\n",
      "Gradient Descent(41/99): loss=0.38993697516126297, gradient=0.0011021097453977\n",
      "Gradient Descent(42/99): loss=0.38993685797286837, gradient=0.0010816595246326782\n",
      "Gradient Descent(43/99): loss=0.38993674497441605, gradient=0.0010620243287497533\n",
      "Gradient Descent(44/99): loss=0.3899366359291967, gradient=0.0010431622934241938\n",
      "Gradient Descent(45/99): loss=0.3899365306171277, gradient=0.0010250343279544912\n",
      "Gradient Descent(46/99): loss=0.3899364288333635, gradient=0.0010076038467805345\n",
      "Gradient Descent(47/99): loss=0.3899363303870446, gradient=0.000990836536697346\n",
      "Gradient Descent(48/99): loss=0.38993623510017256, gradient=0.0009747001541696025\n",
      "Gradient Descent(49/99): loss=0.38993614280659283, gradient=0.0009591643481200026\n",
      "Gradient Descent(50/99): loss=0.389936053351074, gradient=0.0009442005043473214\n",
      "Gradient Descent(51/99): loss=0.3899359665884716, gradient=0.0009297816083715658\n",
      "Gradient Descent(52/99): loss=0.3899358823829681, gradient=0.0009158821240246331\n",
      "Gradient Descent(53/99): loss=0.38993580060737876, gradient=0.000902477885537195\n",
      "Gradient Descent(54/99): loss=0.3899357211425199, gradient=0.0008895460012262519\n",
      "Gradient Descent(55/99): loss=0.38993564387662855, gradient=0.0008770647671838122\n",
      "Gradient Descent(56/99): loss=0.3899355687048321, gradient=0.0008650135896114927\n",
      "Gradient Descent(57/99): loss=0.38993549552866086, gradient=0.0008533729146512938\n",
      "Gradient Descent(58/99): loss=0.3899354242556001, gradient=0.0008421241647340817\n",
      "Gradient Descent(59/99): loss=0.3899353547986787, gradient=0.0008312496806120547\n",
      "Gradient Descent(60/99): loss=0.38993528707608754, gradient=0.0008207326683633137\n",
      "Gradient Descent(61/99): loss=0.38993522101083095, gradient=0.0008105571507588714\n",
      "Gradient Descent(62/99): loss=0.3899351565304015, gradient=0.0008007079224709324\n",
      "Gradient Descent(63/99): loss=0.3899350935664813, gradient=0.0007911705086735484\n",
      "Gradient Descent(64/99): loss=0.38993503205466495, gradient=0.0007819311266516197\n",
      "Gradient Descent(65/99): loss=0.389934971934203, gradient=0.0007729766500865216\n",
      "Gradient Descent(66/99): loss=0.3899349131477631, gradient=0.0007642945757327115\n",
      "Gradient Descent(67/99): loss=0.38993485564121066, gradient=0.0007558729922390639\n",
      "Gradient Descent(68/99): loss=0.3899347993634025, gradient=0.000747700550901806\n",
      "Gradient Descent(69/99): loss=0.38993474426599756, gradient=0.000739766438164199\n",
      "Gradient Descent(70/99): loss=0.38993469030327854, gradient=0.0007320603497034115\n",
      "Gradient Descent(71/99): loss=0.38993463743198803, gradient=0.0007245724659643785\n",
      "Gradient Descent(72/99): loss=0.3899345856111748, gradient=0.0007172934290201733\n",
      "Gradient Descent(73/99): loss=0.38993453480205037, gradient=0.0007102143206522549\n",
      "Gradient Descent(74/99): loss=0.3899344849678568, gradient=0.0007033266415576903\n",
      "Gradient Descent(75/99): loss=0.38993443607374156, gradient=0.0006966222916015089\n",
      "Gradient Descent(76/99): loss=0.3899343880866419, gradient=0.0006900935510422898\n",
      "Gradient Descent(77/99): loss=0.3899343409751766, gradient=0.0006837330626665206\n",
      "Gradient Descent(78/99): loss=0.3899342947095457, gradient=0.0006775338147748789\n",
      "Gradient Descent(79/99): loss=0.3899342492614346, gradient=0.000671489124969971\n",
      "Gradient Descent(80/99): loss=0.38993420460392775, gradient=0.0006655926246989061\n",
      "Gradient Descent(81/99): loss=0.38993416071142417, gradient=0.0006598382445101683\n",
      "Gradient Descent(82/99): loss=0.38993411755956203, gradient=0.0006542201999862733\n",
      "Gradient Descent(83/99): loss=0.3899340751251458, gradient=0.0006487329783190656\n",
      "Gradient Descent(84/99): loss=0.38993403338607824, gradient=0.0006433713254948299\n",
      "Gradient Descent(85/99): loss=0.3899339923212977, gradient=0.0006381302340612165\n",
      "Gradient Descent(86/99): loss=0.3899339519107192, gradient=0.0006330049314483371\n",
      "Gradient Descent(87/99): loss=0.38993391213517803, gradient=0.0006279908688190042\n",
      "Gradient Descent(88/99): loss=0.3899338729763782, gradient=0.0006230837104248641\n",
      "Gradient Descent(89/99): loss=0.3899338344168441, gradient=0.0006182793234454345\n",
      "Gradient Descent(90/99): loss=0.3899337964398735, gradient=0.0006135737682902255\n",
      "Gradient Descent(91/99): loss=0.3899337590294962, gradient=0.0006089632893430237\n",
      "Gradient Descent(92/99): loss=0.38993372217043254, gradient=0.000604444306130144\n",
      "Gradient Descent(93/99): loss=0.389933685848056, gradient=0.0006000134048942543\n",
      "Gradient Descent(94/99): loss=0.38993365004835806, gradient=0.0005956673305571555\n",
      "Gradient Descent(95/99): loss=0.38993361475791405, gradient=0.0005914029790541312\n",
      "Gradient Descent(96/99): loss=0.3899335799638527, gradient=0.0005872173900250877\n",
      "Gradient Descent(97/99): loss=0.38993354565382643, gradient=0.0005831077398469303\n",
      "Gradient Descent(98/99): loss=0.3899335118159832, gradient=0.0005790713349921973\n",
      "Gradient Descent(99/99): loss=0.3899334784389404, gradient=0.0005751056057008585\n",
      "Gradient Descent(0/99): loss=0.3896717768089868, gradient=0.011061946656138088\n",
      "Gradient Descent(1/99): loss=0.38966679136495785, gradient=0.007438937262422139\n",
      "Gradient Descent(2/99): loss=0.3896632573337193, gradient=0.006177872715876445\n",
      "Gradient Descent(3/99): loss=0.3896606601567332, gradient=0.005279158457497555\n",
      "Gradient Descent(4/99): loss=0.389658718375317, gradient=0.004548783540805688\n",
      "Gradient Descent(5/99): loss=0.3896572345703586, gradient=0.003966804955826213\n",
      "Gradient Descent(6/99): loss=0.3896560791664889, gradient=0.003492410311762409\n",
      "Gradient Descent(7/99): loss=0.3896551629567911, gradient=0.0031033038605268056\n",
      "Gradient Descent(8/99): loss=0.38965442384211646, gradient=0.0027818717428030903\n",
      "Gradient Descent(9/99): loss=0.3896538180201296, gradient=0.0025142370766350086\n",
      "Gradient Descent(10/99): loss=0.38965331416869314, gradient=0.0022895082438502998\n",
      "Gradient Descent(11/99): loss=0.3896528895627635, gradient=0.0020991634535999426\n",
      "Gradient Descent(12/99): loss=0.3896525274670211, gradient=0.0019365475899242757\n",
      "Gradient Descent(13/99): loss=0.3896522153699252, gradient=0.0017964642883916816\n",
      "Gradient Descent(14/99): loss=0.38965194377712664, gradient=0.0016748490576638548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(15/99): loss=0.3896517053788779, gradient=0.001568510690735611\n",
      "Gradient Descent(16/99): loss=0.3896514944693652, gradient=0.0014749292362208088\n",
      "Gradient Descent(17/99): loss=0.38965130653705576, gradient=0.0013920998983818595\n",
      "Gradient Descent(18/99): loss=0.38965113797214707, gradient=0.001318413519052167\n",
      "Gradient Descent(19/99): loss=0.38965098585496544, gradient=0.0012525656960479515\n",
      "Gradient Descent(20/99): loss=0.3896508478008964, gradient=0.001193487991290856\n",
      "Gradient Descent(21/99): loss=0.389650721845226, gradient=0.0011402959764078222\n",
      "Gradient Descent(22/99): loss=0.3896506063564961, gradient=0.0010922499936673932\n",
      "Gradient Descent(23/99): loss=0.38965049997047085, gradient=0.0010487254536207774\n",
      "Gradient Descent(24/99): loss=0.3896504015391934, gradient=0.0010091902523135694\n",
      "Gradient Descent(25/99): loss=0.38965031009122875, gradient=0.0009731874899021656\n",
      "Gradient Descent(26/99): loss=0.3896502248003018, gradient=0.0009403221344612245\n",
      "Gradient Descent(27/99): loss=0.3896501449603133, gradient=0.0009102506257050255\n",
      "Gradient Descent(28/99): loss=0.38965006996525864, gradient=0.0008826726768456662\n",
      "Gradient Descent(29/99): loss=0.38964999929295696, gradient=0.0008573247288543864\n",
      "Gradient Descent(30/99): loss=0.38964993249176905, gradient=0.0008339746561910416\n",
      "Gradient Descent(31/99): loss=0.3896498691696875, gradient=0.0008124174293948214\n",
      "Gradient Descent(32/99): loss=0.3896498089853217, gradient=0.0007924715176456678\n",
      "Gradient Descent(33/99): loss=0.3896497516404067, gradient=0.0007739758710056266\n",
      "Gradient Descent(34/99): loss=0.38964969687355394, gradient=0.000756787363164431\n",
      "Gradient Descent(35/99): loss=0.3896496444550105, gradient=0.0007407786053418439\n",
      "Gradient Descent(36/99): loss=0.3896495941822479, gradient=0.0007258360636408053\n",
      "Gradient Descent(37/99): loss=0.3896495458762388, gradient=0.000711858427876122\n",
      "Gradient Descent(38/99): loss=0.3896494993782976, gradient=0.0006987551913853172\n",
      "Gradient Descent(39/99): loss=0.3896494545473975, gradient=0.0006864454097642411\n",
      "Gradient Descent(40/99): loss=0.3896494112578795, gradient=0.0006748566127256383\n",
      "Gradient Descent(41/99): loss=0.38964936939749395, gradient=0.0006639238479773893\n",
      "Gradient Descent(42/99): loss=0.38964932886572146, gradient=0.0006535888395995183\n",
      "Gradient Descent(43/99): loss=0.3896492895723282, gradient=0.0006437992461771978\n",
      "Gradient Descent(44/99): loss=0.38964925143612045, gradient=0.0006345080061452886\n",
      "Gradient Descent(45/99): loss=0.38964921438386757, gradient=0.0006256727595697005\n",
      "Gradient Descent(46/99): loss=0.38964917834937013, gradient=0.0006172553370450473\n",
      "Gradient Descent(47/99): loss=0.3896491432726503, gradient=0.0006092213076006252\n",
      "Gradient Descent(48/99): loss=0.38964910909924577, gradient=0.0006015395785339028\n",
      "Gradient Descent(49/99): loss=0.3896490757795955, gradient=0.0005941820409698867\n",
      "Gradient Descent(50/99): loss=0.389649043268502, gradient=0.0005871232557051964\n",
      "Gradient Descent(51/99): loss=0.3896490115246598, gradient=0.0005803401745566935\n",
      "Gradient Descent(52/99): loss=0.38964898051024266, gradient=0.0005738118930127325\n",
      "Gradient Descent(53/99): loss=0.3896489501905388, gradient=0.0005675194304921994\n",
      "Gradient Descent(54/99): loss=0.38964892053363065, gradient=0.0005614455349616263\n",
      "Gradient Descent(55/99): loss=0.3896488915101114, gradient=0.0005555745090525874\n",
      "Gradient Descent(56/99): loss=0.38964886309283403, gradient=0.0005498920551666636\n",
      "Gradient Descent(57/99): loss=0.3896488352566898, gradient=0.0005443851373573671\n",
      "Gradient Descent(58/99): loss=0.38964880797841017, gradient=0.0005390418580465634\n",
      "Gradient Descent(59/99): loss=0.3896487812363917, gradient=0.0005338513478656125\n",
      "Gradient Descent(60/99): loss=0.38964875501053925, gradient=0.0005288036671193836\n",
      "Gradient Descent(61/99): loss=0.3896487292821266, gradient=0.000523889717549814\n",
      "Gradient Descent(62/99): loss=0.38964870403367147, gradient=0.0005191011632365548\n",
      "Gradient Descent(63/99): loss=0.3896486792488244, gradient=0.0005144303596097453\n",
      "Gradient Descent(64/99): loss=0.389648654912267, gradient=0.000509870289673095\n",
      "Gradient Descent(65/99): loss=0.3896486310096238, gradient=0.0005054145066419153\n",
      "Gradient Descent(66/99): loss=0.3896486075273805, gradient=0.0005010570822949719\n",
      "Gradient Descent(67/99): loss=0.38964858445281086, gradient=0.0004967925604211068\n",
      "Gradient Descent(68/99): loss=0.3896485617739116, gradient=0.0004926159148134052\n",
      "Gradient Descent(69/99): loss=0.38964853947934275, gradient=0.0004885225113283014\n",
      "Gradient Descent(70/99): loss=0.3896485175583745, gradient=0.00048450807358057797\n",
      "Gradient Descent(71/99): loss=0.389648496000838, gradient=0.0004805686518957771\n",
      "Gradient Descent(72/99): loss=0.3896484747970825, gradient=0.000476700595183556\n",
      "Gradient Descent(73/99): loss=0.3896484539379346, gradient=0.00047290052543328786\n",
      "Gradient Descent(74/99): loss=0.3896484334146622, gradient=0.000469165314566586\n",
      "Gradient Descent(75/99): loss=0.3896484132189422, gradient=0.00046549206341018023\n",
      "Gradient Descent(76/99): loss=0.38964839334283036, gradient=0.00046187808257934173\n",
      "Gradient Descent(77/99): loss=0.38964837377873346, gradient=0.00045832087508317955\n",
      "Gradient Descent(78/99): loss=0.38964835451938556, gradient=0.00045481812048500387\n",
      "Gradient Descent(79/99): loss=0.3896483355578241, gradient=0.00045136766046707837\n",
      "Gradient Descent(80/99): loss=0.38964831688737006, gradient=0.00044796748566608213\n",
      "Gradient Descent(81/99): loss=0.38964829850160887, gradient=0.0004446157236585131\n",
      "Gradient Descent(82/99): loss=0.38964828039437277, gradient=0.00044131062798843034\n",
      "Gradient Descent(83/99): loss=0.3896482625597251, gradient=0.0004380505681404546\n",
      "Gradient Descent(84/99): loss=0.38964824499194595, gradient=0.00043483402037071536\n",
      "Gradient Descent(85/99): loss=0.3896482276855183, gradient=0.00043165955931739213\n",
      "Gradient Descent(86/99): loss=0.3896482106351163, gradient=0.00042852585031999114\n",
      "Gradient Descent(87/99): loss=0.38964819383559307, gradient=0.0004254316423835949\n",
      "Gradient Descent(88/99): loss=0.3896481772819717, gradient=0.00042237576173034596\n",
      "Gradient Descent(89/99): loss=0.38964816096943355, gradient=0.0004193571058861947\n",
      "Gradient Descent(90/99): loss=0.38964814489331195, gradient=0.0004163746382556643\n",
      "Gradient Descent(91/99): loss=0.38964812904908197, gradient=0.0004134273831419431\n",
      "Gradient Descent(92/99): loss=0.3896481134323535, gradient=0.00041051442117361156\n",
      "Gradient Descent(93/99): loss=0.3896480980388652, gradient=0.00040763488510284023\n",
      "Gradient Descent(94/99): loss=0.3896480828644759, gradient=0.00040478795594345855\n",
      "Gradient Descent(95/99): loss=0.389648067905161, gradient=0.00040197285941932396\n",
      "Gradient Descent(96/99): loss=0.38964805315700496, gradient=0.000399188862697295\n",
      "Gradient Descent(97/99): loss=0.3896480386161976, gradient=0.0003964352713804462\n",
      "Gradient Descent(98/99): loss=0.389648024279028, gradient=0.0003937114267397372\n",
      "Gradient Descent(99/99): loss=0.38964801014188094, gradient=0.0003910167031644001\n",
      "Gradient Descent(0/99): loss=0.388956789700498, gradient=0.008206088586367333\n",
      "Gradient Descent(1/99): loss=0.3889540218439597, gradient=0.005557513298936885\n",
      "Gradient Descent(2/99): loss=0.3889521049919195, gradient=0.004570764175834766\n",
      "Gradient Descent(3/99): loss=0.38895072426506466, gradient=0.0038670660268923375\n",
      "Gradient Descent(4/99): loss=0.3889497061122368, gradient=0.0033141455732267405\n",
      "Gradient Descent(5/99): loss=0.3889489353216027, gradient=0.0028771392465490285\n",
      "Gradient Descent(6/99): loss=0.38894833549688396, gradient=0.0025323519687387008\n",
      "Gradient Descent(7/99): loss=0.3889478552716789, gradient=0.0022607452379266903\n",
      "Gradient Descent(8/99): loss=0.38894745993390006, gradient=0.002046799884301605\n",
      "Gradient Descent(9/99): loss=0.38894712582023877, gradient=0.001877910506739753\n",
      "Gradient Descent(10/99): loss=0.3889468366598219, gradient=0.0017439354875450935\n",
      "Gradient Descent(11/99): loss=0.3889465811528736, gradient=0.0016368223468242364\n",
      "Gradient Descent(12/99): loss=0.38894635136566086, gradient=0.0015502707939063585\n",
      "Gradient Descent(13/99): loss=0.38894614165984753, gradient=0.0014794209905312284\n",
      "Gradient Descent(14/99): loss=0.38894594797483006, gradient=0.0014205695087969657\n",
      "Gradient Descent(15/99): loss=0.3889457673438453, gradient=0.0013709199856773878\n",
      "Gradient Descent(16/99): loss=0.388945597565695, gradient=0.0013283732996587035\n",
      "Gradient Descent(17/99): loss=0.3889454369805408, gradient=0.001291357719609083\n",
      "Gradient Descent(18/99): loss=0.38894528431565645, gradient=0.0012586957574627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(19/99): loss=0.3889451385784683, gradient=0.0012295023715654069\n",
      "Gradient Descent(20/99): loss=0.38894499898174545, gradient=0.001203108568038176\n",
      "Gradient Descent(21/99): loss=0.38894486489079144, gradient=0.0011790048049196671\n",
      "Gradient Descent(22/99): loss=0.3889447357857901, gradient=0.0011567994194172022\n",
      "Gradient Descent(23/99): loss=0.388944611234675, gradient=0.0011361882349773698\n",
      "Gradient Descent(24/99): loss=0.3889444908733586, gradient=0.0011169323796220753\n",
      "Gradient Descent(25/99): loss=0.3889443743911546, gradient=0.0010988420844151264\n",
      "Gradient Descent(26/99): loss=0.38894426151990347, gradient=0.0010817648160691036\n",
      "Gradient Descent(27/99): loss=0.3889441520257546, gradient=0.001065576544440443\n",
      "Gradient Descent(28/99): loss=0.3889440457028837, gradient=0.0010501752781270995\n",
      "Gradient Descent(29/99): loss=0.38894394236862795, gradient=0.001035476244599014\n",
      "Gradient Descent(30/99): loss=0.388943841859675, gradient=0.001021408267195511\n",
      "Gradient Descent(31/99): loss=0.38894374402904214, gradient=0.0010079110176128706\n",
      "Gradient Descent(32/99): loss=0.3889436487436556, gradient=0.0009949329127784895\n",
      "Gradient Descent(33/99): loss=0.3889435558823886, gradient=0.0009824294894090933\n",
      "Gradient Descent(34/99): loss=0.38894346533445834, gradient=0.0009703621354806058\n",
      "Gradient Descent(35/99): loss=0.3889433769980997, gradient=0.0009586970906402727\n",
      "Gradient Descent(36/99): loss=0.38894329077946477, gradient=0.0009474046510783411\n",
      "Gradient Descent(37/99): loss=0.388943207362416, gradient=0.0009280570030555927\n",
      "Gradient Descent(38/99): loss=0.3889431258716588, gradient=0.0009176412370014278\n",
      "Gradient Descent(39/99): loss=0.3889430462129918, gradient=0.0009076355180862565\n",
      "Gradient Descent(40/99): loss=0.38894296830651215, gradient=0.0008979785308583988\n",
      "Gradient Descent(41/99): loss=0.38894289208151345, gradient=0.0008886235099131064\n",
      "Gradient Descent(42/99): loss=0.38894281747488507, gradient=0.0008795344221033625\n",
      "Gradient Descent(43/99): loss=0.38894274442944116, gradient=0.0008706831832383359\n",
      "Gradient Descent(44/99): loss=0.3889426728927929, gradient=0.0008620476199074456\n",
      "Gradient Descent(45/99): loss=0.3889426028164796, gradient=0.0008536099753393158\n",
      "Gradient Descent(46/99): loss=0.38894253415531876, gradient=0.0008453558125517663\n",
      "Gradient Descent(47/99): loss=0.3889424668669089, gradient=0.0008372732075609394\n",
      "Gradient Descent(48/99): loss=0.3889424009112463, gradient=0.0008293521543986346\n",
      "Gradient Descent(49/99): loss=0.3889423362504245, gradient=0.0008215841248955027\n",
      "Gradient Descent(50/99): loss=0.3889422728483971, gradient=0.0008139617416691532\n",
      "Gradient Descent(51/99): loss=0.3889422106707866, gradient=0.0008064785340402849\n",
      "Gradient Descent(52/99): loss=0.3889421496847274, gradient=0.0007991287548140735\n",
      "Gradient Descent(53/99): loss=0.38894208985873685, gradient=0.0007919072418412921\n",
      "Gradient Descent(54/99): loss=0.3889420311626051, gradient=0.0007848093126201175\n",
      "Gradient Descent(55/99): loss=0.3889419735673033, gradient=0.0007778306833627465\n",
      "Gradient Descent(56/99): loss=0.3889419170449016, gradient=0.0007709674062499464\n",
      "Gradient Descent(57/99): loss=0.3889418615684989, gradient=0.0007642158202739381\n",
      "Gradient Descent(58/99): loss=0.38894180711216086, gradient=0.0007575725122880105\n",
      "Gradient Descent(59/99): loss=0.38894175365086353, gradient=0.000751034285773067\n",
      "Gradient Descent(60/99): loss=0.388941701160443, gradient=0.000744598135480322\n",
      "Gradient Descent(61/99): loss=0.38894164961754996, gradient=0.0007382612265849485\n",
      "Gradient Descent(62/99): loss=0.388941598999608, gradient=0.0007320208773337927\n",
      "Gradient Descent(63/99): loss=0.3889415492847755, gradient=0.0007258745444260641\n",
      "Gradient Descent(64/99): loss=0.3889415004519098, gradient=0.0007198198105540988\n",
      "Gradient Descent(65/99): loss=0.3889414524805351, gradient=0.0007138543736705492\n",
      "Gradient Descent(66/99): loss=0.388941405350812, gradient=0.0007079760376508469\n",
      "Gradient Descent(67/99): loss=0.3889413590435088, gradient=0.0007021827040968385\n",
      "Gradient Descent(68/99): loss=0.3889413135399766, gradient=0.0006964723650845763\n",
      "Gradient Descent(69/99): loss=0.3889412688221222, gradient=0.0006908430967017637\n",
      "Gradient Descent(70/99): loss=0.3889412248723869, gradient=0.0006852930532533796\n",
      "Gradient Descent(71/99): loss=0.38894118167372416, gradient=0.0006798204620382343\n",
      "Gradient Descent(72/99): loss=0.38894113920957896, gradient=0.0006744236186180218\n",
      "Gradient Descent(73/99): loss=0.3889410974638685, gradient=0.0006691008825145988\n",
      "Gradient Descent(74/99): loss=0.38894105642096377, gradient=0.0006638506732837515\n",
      "Gradient Descent(75/99): loss=0.3889410160656729, gradient=0.0006586714669205607\n",
      "Gradient Descent(76/99): loss=0.3889409763832243, gradient=0.0006535617925599893\n",
      "Gradient Descent(77/99): loss=0.38894093735925156, gradient=0.0006485202294416156\n",
      "Gradient Descent(78/99): loss=0.3889408989797781, gradient=0.0006435454041106652\n",
      "Gradient Descent(79/99): loss=0.3889408612312042, gradient=0.0006386359878332343\n",
      "Gradient Descent(80/99): loss=0.38894082410029285, gradient=0.0006337906942045854\n",
      "Gradient Descent(81/99): loss=0.3889407875741576, gradient=0.0006290082769333905\n",
      "Gradient Descent(82/99): loss=0.38894075164025077, gradient=0.0006242875277856189\n",
      "Gradient Descent(83/99): loss=0.388940716286351, gradient=0.0006196272746744308\n",
      "Gradient Descent(84/99): loss=0.38894068150055394, gradient=0.0006150263798838745\n",
      "Gradient Descent(85/99): loss=0.38894064727125993, gradient=0.000610483738414663\n",
      "Gradient Descent(86/99): loss=0.3889406135871655, gradient=0.0006059982764424377\n",
      "Gradient Descent(87/99): loss=0.38894058043725327, gradient=0.0006015689498793961\n",
      "Gradient Descent(88/99): loss=0.3889405478107827, gradient=0.0005971947430307094\n",
      "Gradient Descent(89/99): loss=0.38894051569728144, gradient=0.0005928746673386928\n",
      "Gradient Descent(90/99): loss=0.38894048408653703, gradient=0.0005886077602078809\n",
      "Gradient Descent(91/99): loss=0.3889404529685889, gradient=0.0005843930839043787\n",
      "Gradient Descent(92/99): loss=0.38894042233372095, gradient=0.0005802297245244336\n",
      "Gradient Descent(93/99): loss=0.3889403921724527, gradient=0.0005761167910266049\n",
      "Gradient Descent(94/99): loss=0.3889403624755342, gradient=0.000572053414322957\n",
      "Gradient Descent(95/99): loss=0.3889403332339379, gradient=0.0005680387464248317\n",
      "Gradient Descent(96/99): loss=0.3889403044388523, gradient=0.0005640719596393086\n",
      "Gradient Descent(97/99): loss=0.3889402760816758, gradient=0.000560152245812558\n",
      "Gradient Descent(98/99): loss=0.38894024815401074, gradient=0.0005562788156167366\n",
      "Gradient Descent(99/99): loss=0.38894022064765693, gradient=0.0005524508978772389\n",
      "Gradient Descent(0/99): loss=0.39013812595670544, gradient=0.015400383318017427\n",
      "Gradient Descent(1/99): loss=0.3901342543625224, gradient=0.006938706221001907\n",
      "Gradient Descent(2/99): loss=0.3901316284898612, gradient=0.005368383092721628\n",
      "Gradient Descent(3/99): loss=0.3901296621793905, gradient=0.004621679423327366\n",
      "Gradient Descent(4/99): loss=0.39012814277147323, gradient=0.004053555221085522\n",
      "Gradient Descent(5/99): loss=0.3901269441675929, gradient=0.0035954067604701935\n",
      "Gradient Descent(6/99): loss=0.39012597896576506, gradient=0.0032220721093457944\n",
      "Gradient Descent(7/99): loss=0.39012518808257923, gradient=0.0029106126652575517\n",
      "Gradient Descent(8/99): loss=0.3901245273138534, gradient=0.00265813222695231\n",
      "Gradient Descent(9/99): loss=0.39012396621610923, gradient=0.00244756775111568\n",
      "Gradient Descent(10/99): loss=0.3901234828329263, gradient=0.002270479261678964\n",
      "Gradient Descent(11/99): loss=0.39012306094492605, gradient=0.0021203073292927967\n",
      "Gradient Descent(12/99): loss=0.39012268843650316, gradient=0.001991900797599136\n",
      "Gradient Descent(13/99): loss=0.39012235612469465, gradient=0.0018812009683875643\n",
      "Gradient Descent(14/99): loss=0.39012205695823937, gradient=0.0017849986234266142\n",
      "Gradient Descent(15/99): loss=0.3901217854527041, gradient=0.0017007454459063852\n",
      "Gradient Descent(16/99): loss=0.39012153729007165, gradient=0.001626407920551288\n",
      "Gradient Descent(17/99): loss=0.3901213090310655, gradient=0.001560354372541522\n",
      "Gradient Descent(18/99): loss=0.39012109790624216, gradient=0.001501267739941362\n",
      "Gradient Descent(19/99): loss=0.3901209016625398, gradient=0.001448078238389253\n",
      "Gradient Descent(20/99): loss=0.3901207184493105, gradient=0.0013999113439620435\n",
      "Gradient Descent(21/99): loss=0.39012054673274865, gradient=0.0013560475385070757\n",
      "Gradient Descent(22/99): loss=0.3901203852309585, gradient=0.0013158910715357465\n",
      "Gradient Descent(23/99): loss=0.3901202328641711, gradient=0.001278945629781609\n",
      "Gradient Descent(24/99): loss=0.39012008871619996, gradient=0.0012447953016543724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(25/99): loss=0.39011995200431177, gradient=0.0012130896069319928\n",
      "Gradient Descent(26/99): loss=0.3901198220554649, gradient=0.0011835316557955197\n",
      "Gradient Descent(27/99): loss=0.3901196982874076, gradient=0.0011558687253194208\n",
      "Gradient Descent(28/99): loss=0.3901195801935233, gradient=0.001129884711644056\n",
      "Gradient Descent(29/99): loss=0.39011946733058733, gradient=0.0011053940448755562\n",
      "Gradient Descent(30/99): loss=0.3901193593088079, gradient=0.001082236751153237\n",
      "Gradient Descent(31/99): loss=0.39011925578367035, gradient=0.0010602744199163306\n",
      "Gradient Descent(32/99): loss=0.39011915644922013, gradient=0.0010393868900457548\n",
      "Gradient Descent(33/99): loss=0.39011906103249727, gradient=0.0010194695106843937\n",
      "Gradient Descent(34/99): loss=0.39011896928890233, gradient=0.0010004308645230986\n",
      "Gradient Descent(35/99): loss=0.3901188809983174, gradient=0.0009821908656918066\n",
      "Gradient Descent(36/99): loss=0.3901187959618465, gradient=0.000964679163023302\n",
      "Gradient Descent(37/99): loss=0.3901187139990627, gradient=0.0009478337937688799\n",
      "Gradient Descent(38/99): loss=0.39011863494567506, gradient=0.0009316000439064658\n",
      "Gradient Descent(39/99): loss=0.3901185586515425, gradient=0.0009159294797785198\n",
      "Gradient Descent(40/99): loss=0.3901184849789786, gradient=0.0009007791225253095\n",
      "Gradient Descent(41/99): loss=0.39011841380129936, gradient=0.0008861107420763566\n",
      "Gradient Descent(42/99): loss=0.39011834539186874, gradient=0.0008678518308328341\n",
      "Gradient Descent(43/99): loss=0.3901182793625366, gradient=0.0008536055330196558\n",
      "Gradient Descent(44/99): loss=0.3901182155041765, gradient=0.0008401061100113731\n",
      "Gradient Descent(45/99): loss=0.3901181537316325, gradient=0.0008270194770932255\n",
      "Gradient Descent(46/99): loss=0.39011809394863467, gradient=0.000814295062734621\n",
      "Gradient Descent(47/99): loss=0.3901180360712288, gradient=0.0008019043587386425\n",
      "Gradient Descent(48/99): loss=0.3901179800205684, gradient=0.0007898247802394065\n",
      "Gradient Descent(49/99): loss=0.3901179257233884, gradient=0.0007780373863768236\n",
      "Gradient Descent(50/99): loss=0.39011787311102053, gradient=0.0007665258589545921\n",
      "Gradient Descent(51/99): loss=0.3901178221189367, gradient=0.0007552758652675461\n",
      "Gradient Descent(52/99): loss=0.3901177726863016, gradient=0.000744274635357449\n",
      "Gradient Descent(53/99): loss=0.3901177247556209, gradient=0.0007335106707685263\n",
      "Gradient Descent(54/99): loss=0.3901176782724381, gradient=0.0007229735359497016\n",
      "Gradient Descent(55/99): loss=0.3901176331850759, gradient=0.0007126537028143961\n",
      "Gradient Descent(56/99): loss=0.3901175894444104, gradient=0.000702542430469191\n",
      "Gradient Descent(57/99): loss=0.3901175470036728, gradient=0.0006926316690024823\n",
      "Gradient Descent(58/99): loss=0.39011750581827553, gradient=0.0006829139803558199\n",
      "Gradient Descent(59/99): loss=0.3901174658456558, gradient=0.000673382471800494\n",
      "Gradient Descent(60/99): loss=0.3901174270451362, gradient=0.0006640307390652805\n",
      "Gradient Descent(61/99): loss=0.39011738937779944, gradient=0.0006548528171023873\n",
      "Gradient Descent(62/99): loss=0.39011735280637594, gradient=0.0006458431370708614\n",
      "Gradient Descent(63/99): loss=0.3901173172951405, gradient=0.0006369964884936623\n",
      "Gradient Descent(64/99): loss=0.39011728280982017, gradient=0.0006283079857963202\n",
      "Gradient Descent(65/99): loss=0.3901172493175086, gradient=0.0006197730386039129\n",
      "Gradient Descent(66/99): loss=0.3901172167865892, gradient=0.000611387325292981\n",
      "Gradient Descent(67/99): loss=0.3901171851866638, gradient=0.0006031467693828341\n",
      "Gradient Descent(68/99): loss=0.39011715448848666, gradient=0.0005950475184160986\n",
      "Gradient Descent(69/99): loss=0.39011712466390497, gradient=0.00058708592503131\n",
      "Gradient Descent(70/99): loss=0.39011709534062144, gradient=0.0005859604735386247\n",
      "Gradient Descent(71/99): loss=0.3901170670714884, gradient=0.0005723271991306203\n",
      "Gradient Descent(72/99): loss=0.3901170397109602, gradient=0.0005640148447531384\n",
      "Gradient Descent(73/99): loss=0.39011701310948216, gradient=0.0005565256615420048\n",
      "Gradient Descent(74/99): loss=0.39011698726218075, gradient=0.0005491979147411902\n",
      "Gradient Descent(75/99): loss=0.3901169621407306, gradient=0.0005419930092677524\n",
      "Gradient Descent(76/99): loss=0.39011693772382805, gradient=0.0005349058175412119\n",
      "Gradient Descent(77/99): loss=0.39011691398970794, gradient=0.0005279333397608638\n",
      "Gradient Descent(78/99): loss=0.3901168909177313, gradient=0.0005210729359490856\n",
      "Gradient Descent(79/99): loss=0.39011686848794647, gradient=0.0005143221633046284\n",
      "Gradient Descent(80/99): loss=0.3901168466811265, gradient=0.0005076787246634023\n",
      "Gradient Descent(81/99): loss=0.39011682547870813, gradient=0.0005011404385577246\n",
      "Gradient Descent(82/99): loss=0.3901168048627594, gradient=0.0004947052198545289\n",
      "Gradient Descent(83/99): loss=0.39011678481594847, gradient=0.0004883710666613592\n",
      "Gradient Descent(84/99): loss=0.3901167650042664, gradient=0.000489588927395407\n",
      "Gradient Descent(85/99): loss=0.39011674593385925, gradient=0.00047692500769920944\n",
      "Gradient Descent(86/99): loss=0.3901167274974567, gradient=0.000469989198901775\n",
      "Gradient Descent(87/99): loss=0.3901167095545886, gradient=0.00046398575359520787\n",
      "Gradient Descent(88/99): loss=0.3901166921091054, gradient=0.00045812191325312457\n",
      "Gradient Descent(89/99): loss=0.3901166751410611, gradient=0.00045235291735069475\n",
      "Gradient Descent(90/99): loss=0.3901166586372037, gradient=0.00044667410849724525\n",
      "Gradient Descent(91/99): loss=0.3901166425835062, gradient=0.0004410833132582713\n",
      "Gradient Descent(92/99): loss=0.39011662696677274, gradient=0.0004355787215267097\n",
      "Gradient Descent(93/99): loss=0.39011661177421225, gradient=0.000430158693790106\n",
      "Gradient Descent(94/99): loss=0.3901165966942732, gradient=0.0004328364712633527\n",
      "Gradient Descent(95/99): loss=0.3901165822007069, gradient=0.00042062515268777686\n",
      "Gradient Descent(96/99): loss=0.39011656820916957, gradient=0.0004144349492016963\n",
      "Gradient Descent(97/99): loss=0.39011655458316463, gradient=0.00040927538534447786\n",
      "Gradient Descent(98/99): loss=0.39011654133040513, gradient=0.000404247693063944\n",
      "Gradient Descent(99/99): loss=0.3901165284346914, gradient=0.0003993015531900017\n",
      "Gradient Descent(0/99): loss=0.39034851510853796, gradient=0.007270179470994401\n",
      "Gradient Descent(1/99): loss=0.3903448825640999, gradient=0.006259419287429538\n",
      "Gradient Descent(2/99): loss=0.39034203238970433, gradient=0.005527266718529082\n",
      "Gradient Descent(3/99): loss=0.39033973638598696, gradient=0.004947545783629072\n",
      "Gradient Descent(4/99): loss=0.3903378442234238, gradient=0.00447985528803062\n",
      "Gradient Descent(5/99): loss=0.39033625422992135, gradient=0.004094536105116227\n",
      "Gradient Descent(6/99): loss=0.39033489297045, gradient=0.0037795788745981765\n",
      "Gradient Descent(7/99): loss=0.3903337101281077, gradient=0.0035155984037242146\n",
      "Gradient Descent(8/99): loss=0.3903326773635098, gradient=0.0032611460578515377\n",
      "Gradient Descent(9/99): loss=0.39033175601956704, gradient=0.0030748085515163183\n",
      "Gradient Descent(10/99): loss=0.390330926955334, gradient=0.0029117883945138467\n",
      "Gradient Descent(11/99): loss=0.390330174229072, gradient=0.002768720804757333\n",
      "Gradient Descent(12/99): loss=0.39032949182553023, gradient=0.002622438204177166\n",
      "Gradient Descent(13/99): loss=0.3903288646473487, gradient=0.002508862936976139\n",
      "Gradient Descent(14/99): loss=0.3903282855592756, gradient=0.0024069378452581313\n",
      "Gradient Descent(15/99): loss=0.3903277480854725, gradient=0.002315540134603228\n",
      "Gradient Descent(16/99): loss=0.39032724835430743, gradient=0.0022302907131312695\n",
      "Gradient Descent(17/99): loss=0.39032678105567065, gradient=0.002151971141736401\n",
      "Gradient Descent(18/99): loss=0.3903263515293786, gradient=0.0020399634835023637\n",
      "Gradient Descent(19/99): loss=0.3903259478231686, gradient=0.001974624360055918\n",
      "Gradient Descent(20/99): loss=0.39032556813240504, gradient=0.0019105750354585663\n",
      "Gradient Descent(21/99): loss=0.3903252099396235, gradient=0.0018530017251602118\n",
      "Gradient Descent(22/99): loss=0.3903248721917084, gradient=0.0017923792921987884\n",
      "Gradient Descent(23/99): loss=0.3903245524553084, gradient=0.0017421549240794949\n",
      "Gradient Descent(24/99): loss=0.3903242490986328, gradient=0.001694754521489889\n",
      "Gradient Descent(25/99): loss=0.39032396106843126, gradient=0.0016493096799061015\n",
      "Gradient Descent(26/99): loss=0.390323687229442, gradient=0.0016063070872517164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(27/99): loss=0.39032342673678444, gradient=0.0015646053406553227\n",
      "Gradient Descent(28/99): loss=0.3903231787732119, gradient=0.001524627644096666\n",
      "Gradient Descent(29/99): loss=0.39032294207403745, gradient=0.0014875483463233693\n",
      "Gradient Descent(30/99): loss=0.39032271665195134, gradient=0.0014501097504431115\n",
      "Gradient Descent(31/99): loss=0.3903225015678882, gradient=0.0014144838449639968\n",
      "Gradient Descent(32/99): loss=0.390322295773362, gradient=0.0013827673262241632\n",
      "Gradient Descent(33/99): loss=0.39032209969791903, gradient=0.0013489706939164322\n",
      "Gradient Descent(34/99): loss=0.3903219116773315, gradient=0.0013169913563490876\n",
      "Gradient Descent(35/99): loss=0.3903217319889824, gradient=0.0012869259978080347\n",
      "Gradient Descent(36/99): loss=0.39032156013191843, gradient=0.001256115594587145\n",
      "Gradient Descent(37/99): loss=0.3903213952492726, gradient=0.001228308783341175\n",
      "Gradient Descent(38/99): loss=0.3903212375855866, gradient=0.0012001712118256242\n",
      "Gradient Descent(39/99): loss=0.3903210859178448, gradient=0.0011765902471922978\n",
      "Gradient Descent(40/99): loss=0.39032094108536175, gradient=0.001149023130842452\n",
      "Gradient Descent(41/99): loss=0.39032080154446874, gradient=0.0011236169668833275\n",
      "Gradient Descent(42/99): loss=0.3903206679771818, gradient=0.0010986287859613066\n",
      "Gradient Descent(43/99): loss=0.39032053912638087, gradient=0.001078709933131728\n",
      "Gradient Descent(44/99): loss=0.39032041596586203, gradient=0.001053881636216985\n",
      "Gradient Descent(45/99): loss=0.3903202970588009, gradient=0.0010300842942728666\n",
      "Gradient Descent(46/99): loss=0.39032018272597624, gradient=0.0010096094756835035\n",
      "Gradient Descent(47/99): loss=0.39032007295947085, gradient=0.000988377268758023\n",
      "Gradient Descent(48/99): loss=0.3903199667273809, gradient=0.0009726696711335274\n",
      "Gradient Descent(49/99): loss=0.3903198651003128, gradient=0.0009505481928173259\n",
      "Gradient Descent(50/99): loss=0.39031976661909695, gradient=0.000931172528397622\n",
      "Gradient Descent(51/99): loss=0.39031967208383606, gradient=0.0009119564302102031\n",
      "Gradient Descent(52/99): loss=0.3903195805966049, gradient=0.0008935666967472548\n",
      "Gradient Descent(53/99): loss=0.39031949229989116, gradient=0.0008774867455807715\n",
      "Gradient Descent(54/99): loss=0.39031940734339315, gradient=0.0008600921823349665\n",
      "Gradient Descent(55/99): loss=0.3903193247170642, gradient=0.0008492230084834048\n",
      "Gradient Descent(56/99): loss=0.39031924561952885, gradient=0.000830013768865825\n",
      "Gradient Descent(57/99): loss=0.39031916862112925, gradient=0.0008141045010861184\n",
      "Gradient Descent(58/99): loss=0.3903190945717616, gradient=0.0007982503582537028\n",
      "Gradient Descent(59/99): loss=0.39031902262931, gradient=0.0007831152084447697\n",
      "Gradient Descent(60/99): loss=0.3903189529920513, gradient=0.0007703584374085979\n",
      "Gradient Descent(61/99): loss=0.3903188858459952, gradient=0.0007560560150476187\n",
      "Gradient Descent(62/99): loss=0.39031882021659037, gradient=0.0007491317267292375\n",
      "Gradient Descent(63/99): loss=0.3903187573741134, gradient=0.0007321533037695583\n",
      "Gradient Descent(64/99): loss=0.39031869596509094, gradient=0.0007171650877215633\n",
      "Gradient Descent(65/99): loss=0.39031863647602916, gradient=0.0007061337102671741\n",
      "Gradient Descent(66/99): loss=0.3903185790183819, gradient=0.0006937400213785028\n",
      "Gradient Descent(67/99): loss=0.39031852292365166, gradient=0.0006832971405102725\n",
      "Gradient Descent(68/99): loss=0.39031846880288595, gradient=0.0006712816326754791\n",
      "Gradient Descent(69/99): loss=0.3903184158916098, gradient=0.000661396853986499\n",
      "Gradient Descent(70/99): loss=0.39031836482571325, gradient=0.0006499716237244301\n",
      "Gradient Descent(71/99): loss=0.390318314894664, gradient=0.0006389677240375469\n",
      "Gradient Descent(72/99): loss=0.3903182663422499, gradient=0.0006304152184382451\n",
      "Gradient Descent(73/99): loss=0.39031821901231206, gradient=0.0006251783605102009\n",
      "Gradient Descent(74/99): loss=0.39031817324031, gradient=0.0006107802948171075\n",
      "Gradient Descent(75/99): loss=0.39031812876251426, gradient=0.0006013867899299901\n",
      "Gradient Descent(76/99): loss=0.3903180851716825, gradient=0.0005934607525250909\n",
      "Gradient Descent(77/99): loss=0.39031804302550854, gradient=0.0005840766171166157\n",
      "Gradient Descent(78/99): loss=0.3903180016278551, gradient=0.0005764255258703461\n",
      "Gradient Descent(79/99): loss=0.3903179616052808, gradient=0.000567419069659206\n",
      "Gradient Descent(80/99): loss=0.3903179222853668, gradient=0.0005585103265274368\n",
      "Gradient Descent(81/99): loss=0.39031788394641725, gradient=0.0005520387486999331\n",
      "Gradient Descent(82/99): loss=0.3903178467971919, gradient=0.0005438752747796758\n",
      "Gradient Descent(83/99): loss=0.3903178102269277, gradient=0.000537402879388776\n",
      "Gradient Descent(84/99): loss=0.39031777485111085, gradient=0.000529387895421835\n",
      "Gradient Descent(85/99): loss=0.39031773970209566, gradient=0.0005311555781609206\n",
      "Gradient Descent(86/99): loss=0.3903177061263159, gradient=0.0005183698018188293\n",
      "Gradient Descent(87/99): loss=0.39031767280966567, gradient=0.0005099289529724773\n",
      "Gradient Descent(88/99): loss=0.39031764061926166, gradient=0.0005025331650456048\n",
      "Gradient Descent(89/99): loss=0.39031760882034294, gradient=0.0004966029751518396\n",
      "Gradient Descent(90/99): loss=0.3903175780368074, gradient=0.0004898416649975558\n",
      "Gradient Descent(91/99): loss=0.39031754760980636, gradient=0.0004842993037557182\n",
      "Gradient Descent(92/99): loss=0.39031751814913523, gradient=0.00047785388107339277\n",
      "Gradient Descent(93/99): loss=0.3903174890185679, gradient=0.00047101126716654536\n",
      "Gradient Descent(94/99): loss=0.39031746053403504, gradient=0.00046650812332234195\n",
      "Gradient Descent(95/99): loss=0.39031743287930126, gradient=0.0004607666363787217\n",
      "Gradient Descent(96/99): loss=0.39031740549986743, gradient=0.0004560283604771507\n",
      "Gradient Descent(97/99): loss=0.39031737898411484, gradient=0.00045027946735674796\n",
      "Gradient Descent(98/99): loss=0.39031735242297677, gradient=0.0004549179536353955\n",
      "Gradient Descent(99/99): loss=0.39031732712869244, gradient=0.0004434571784170227\n",
      "Gradient Descent(0/99): loss=0.38996930187426593, gradient=0.009987096732208137\n",
      "Gradient Descent(1/99): loss=0.38996314615684324, gradient=0.008177890071058314\n",
      "Gradient Descent(2/99): loss=0.38995879485041557, gradient=0.006849965227719975\n",
      "Gradient Descent(3/99): loss=0.38995560945414776, gradient=0.005839208513325193\n",
      "Gradient Descent(4/99): loss=0.38995321098229946, gradient=0.00503048733192322\n",
      "Gradient Descent(5/99): loss=0.3899513291797289, gradient=0.004440016680375269\n",
      "Gradient Descent(6/99): loss=0.3899498065543491, gradient=0.003982243234446082\n",
      "Gradient Descent(7/99): loss=0.3899485410646229, gradient=0.0036223469394172456\n",
      "Gradient Descent(8/99): loss=0.3899474652862248, gradient=0.003334491342765332\n",
      "Gradient Descent(9/99): loss=0.38994653366414445, gradient=0.003099802278777159\n",
      "Gradient Descent(10/99): loss=0.38994571466125255, gradient=0.0029046534899600917\n",
      "Gradient Descent(11/99): loss=0.3899449858763032, gradient=0.0027392729435922764\n",
      "Gradient Descent(12/99): loss=0.3899443309756691, gradient=0.0025966604890771267\n",
      "Gradient Descent(13/99): loss=0.38994373774018576, gradient=0.0024717803835953155\n",
      "Gradient Descent(14/99): loss=0.38994320259231224, gradient=0.0023344933137606112\n",
      "Gradient Descent(15/99): loss=0.38994271119729157, gradient=0.0022375765570239636\n",
      "Gradient Descent(16/99): loss=0.38994225798546966, gradient=0.0021496839904706034\n",
      "Gradient Descent(17/99): loss=0.3899418383383485, gradient=0.0020693673047110133\n",
      "Gradient Descent(18/99): loss=0.38994144843969814, gradient=0.0019955119961115207\n",
      "Gradient Descent(19/99): loss=0.3899410850795292, gradient=0.0019272365053020148\n",
      "Gradient Descent(20/99): loss=0.3899407455316071, gradient=0.0018638318267240627\n",
      "Gradient Descent(21/99): loss=0.3899404274573344, gradient=0.0018047193166745637\n",
      "Gradient Descent(22/99): loss=0.3899401288338545, gradient=0.0017494204612760055\n",
      "Gradient Descent(23/99): loss=0.3899398478983136, gradient=0.001697534867076256\n",
      "Gradient Descent(24/99): loss=0.38993958511615545, gradient=0.001636240724332518\n",
      "Gradient Descent(25/99): loss=0.3899393369958328, gradient=0.0015903781357208397\n",
      "Gradient Descent(26/99): loss=0.38993910229296924, gradient=0.001547209304782688\n",
      "Gradient Descent(27/99): loss=0.38993887991257614, gradient=0.0015064667293451155\n",
      "Gradient Descent(28/99): loss=0.3899386740008787, gradient=0.0014321283356089891\n",
      "Gradient Descent(29/99): loss=0.38993847835101403, gradient=0.0013959754888193433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(30/99): loss=0.38993829210373876, gradient=0.001361872565035189\n",
      "Gradient Descent(31/99): loss=0.38993811458533495, gradient=0.00132949894655665\n",
      "Gradient Descent(32/99): loss=0.38993794517225283, gradient=0.0012987072412440196\n",
      "Gradient Descent(33/99): loss=0.38993778330460793, gradient=0.0012693770984207014\n",
      "Gradient Descent(34/99): loss=0.3899376284729601, gradient=0.0012414038776789708\n",
      "Gradient Descent(35/99): loss=0.3899374802131426, gradient=0.0012146953001531838\n",
      "Gradient Descent(36/99): loss=0.38993733810068637, gradient=0.0011891691980330562\n",
      "Gradient Descent(37/99): loss=0.3899372017464169, gradient=0.001164751806728688\n",
      "Gradient Descent(38/99): loss=0.3899370707926472, gradient=0.001141376436333575\n",
      "Gradient Descent(39/99): loss=0.3899369449099259, gradient=0.0011189824212534973\n",
      "Gradient Descent(40/99): loss=0.3899368237942206, gradient=0.0010975142778841451\n",
      "Gradient Descent(41/99): loss=0.38993670716446865, gradient=0.0010769210205062141\n",
      "Gradient Descent(42/99): loss=0.3899365947604271, gradient=0.0010571555993567137\n",
      "Gradient Descent(43/99): loss=0.38993648634078293, gradient=0.0010381744343736028\n",
      "Gradient Descent(44/99): loss=0.38993638168147576, gradient=0.0010199370248093523\n",
      "Gradient Descent(45/99): loss=0.38993628057420937, gradient=0.001002405619694392\n",
      "Gradient Descent(46/99): loss=0.3899361828251204, gradient=0.0009855449375973215\n",
      "Gradient Descent(47/99): loss=0.3899360882535872, gradient=0.0009693219266855486\n",
      "Gradient Descent(48/99): loss=0.3899359966911571, gradient=0.0009537055579940857\n",
      "Gradient Descent(49/99): loss=0.3899359079805798, gradient=0.000938666646257353\n",
      "Gradient Descent(50/99): loss=0.38993582197493354, gradient=0.0009241776937645151\n",
      "Gradient Descent(51/99): loss=0.38993573853683144, gradient=0.0009102127535593019\n",
      "Gradient Descent(52/99): loss=0.3899356575377018, gradient=0.0008967473089783205\n",
      "Gradient Descent(53/99): loss=0.3899355788571305, gradient=0.0008837581670563031\n",
      "Gradient Descent(54/99): loss=0.3899355023822641, gradient=0.0008712233637527593\n",
      "Gradient Descent(55/99): loss=0.38993542800725856, gradient=0.0008591220792985312\n",
      "Gradient Descent(56/99): loss=0.389935355632781, gradient=0.0008474345622395851\n",
      "Gradient Descent(57/99): loss=0.38993528516554643, gradient=0.0008361420609840324\n",
      "Gradient Descent(58/99): loss=0.3899352165178966, gradient=0.0008252267618448406\n",
      "Gradient Descent(59/99): loss=0.3899351496074096, gradient=0.0008146717327275207\n",
      "Gradient Descent(60/99): loss=0.3899350843565412, gradient=0.0008044608717397641\n",
      "Gradient Descent(61/99): loss=0.38993502069229397, gradient=0.0007945788601085113\n",
      "Gradient Descent(62/99): loss=0.3899349585459121, gradient=0.00078501111888013\n",
      "Gradient Descent(63/99): loss=0.3899348978525985, gradient=0.0007757437689553803\n",
      "Gradient Descent(64/99): loss=0.38993483855125316, gradient=0.0007667635940753163\n",
      "Gradient Descent(65/99): loss=0.3899347805842315, gradient=0.0007580580064281316\n",
      "Gradient Descent(66/99): loss=0.389934723897119, gradient=0.0007496150145938494\n",
      "Gradient Descent(67/99): loss=0.3899346684385236, gradient=0.000741423193581945\n",
      "Gradient Descent(68/99): loss=0.38993461415988234, gradient=0.0007334716567517232\n",
      "Gradient Descent(69/99): loss=0.38993456101528046, gradient=0.000725750029431901\n",
      "Gradient Descent(70/99): loss=0.3899345089612854, gradient=0.0007182484240824028\n",
      "Gradient Descent(71/99): loss=0.3899344579567906, gradient=0.0007109574168598804\n",
      "Gradient Descent(72/99): loss=0.38993440796287154, gradient=0.0007038680254680733\n",
      "Gradient Descent(73/99): loss=0.38993435894264916, gradient=0.0006969716881877163\n",
      "Gradient Descent(74/99): loss=0.38993431086116614, gradient=0.0006902602439952266\n",
      "Gradient Descent(75/99): loss=0.3899342636852682, gradient=0.0006837259136890757\n",
      "Gradient Descent(76/99): loss=0.38993421738349476, gradient=0.0006773612819532714\n",
      "Gradient Descent(77/99): loss=0.3899341719259779, gradient=0.0006711592802950248\n",
      "Gradient Descent(78/99): loss=0.3899341272843457, gradient=0.0006651131708005929\n",
      "Gradient Descent(79/99): loss=0.3899340834316336, gradient=0.000659216530660031\n",
      "Gradient Descent(80/99): loss=0.3899340403422013, gradient=0.0006534632374150686\n",
      "Gradient Descent(81/99): loss=0.38993399799165457, gradient=0.0006478474548908447\n",
      "Gradient Descent(82/99): loss=0.3899339563567726, gradient=0.000642363619774006\n",
      "Gradient Descent(83/99): loss=0.38993391541543926, gradient=0.0006370064288044156\n",
      "Gradient Descent(84/99): loss=0.3899338751465803, gradient=0.0006317708265495174\n",
      "Gradient Descent(85/99): loss=0.3899338355301022, gradient=0.0006266519937333648\n",
      "Gradient Descent(86/99): loss=0.3899337965468374, gradient=0.0006216453360940872\n",
      "Gradient Descent(87/99): loss=0.3899337581784913, gradient=0.0006167464737458068\n",
      "Gradient Descent(88/99): loss=0.38993372040759294, gradient=0.0006119512310219093\n",
      "Gradient Descent(89/99): loss=0.3899336832174487, gradient=0.0006072556267787341\n",
      "Gradient Descent(90/99): loss=0.38993364659209995, gradient=0.0006026558651393081\n",
      "Gradient Descent(91/99): loss=0.38993361051628117, gradient=0.0005981483266582419\n",
      "Gradient Descent(92/99): loss=0.3899335749753831, gradient=0.0005937295598900283\n",
      "Gradient Descent(93/99): loss=0.3899335399554159, gradient=0.0005893962733428442\n",
      "Gradient Descent(94/99): loss=0.3899335054429762, gradient=0.000585145327802437\n",
      "Gradient Descent(95/99): loss=0.38993347142521556, gradient=0.0005809737290095469\n",
      "Gradient Descent(96/99): loss=0.3899334378898102, gradient=0.0005768786206765232\n",
      "Gradient Descent(97/99): loss=0.38993340482493366, gradient=0.0005728572778283677\n",
      "Gradient Descent(98/99): loss=0.38993337221923074, gradient=0.0005689071004546831\n",
      "Gradient Descent(99/99): loss=0.3899333400617923, gradient=0.000565025607458904\n",
      "Gradient Descent(0/99): loss=0.3896713879428642, gradient=0.011061938070653666\n",
      "Gradient Descent(1/99): loss=0.3896664003535259, gradient=0.007439905036824801\n",
      "Gradient Descent(2/99): loss=0.38966286431295616, gradient=0.006178953258483637\n",
      "Gradient Descent(3/99): loss=0.3896602657719933, gradient=0.005280308532584833\n",
      "Gradient Descent(4/99): loss=0.3896583275979536, gradient=0.004539232607529479\n",
      "Gradient Descent(5/99): loss=0.389656846801136, gradient=0.0039577500961839015\n",
      "Gradient Descent(6/99): loss=0.389655694041192, gradient=0.0034837976754511766\n",
      "Gradient Descent(7/99): loss=0.3896547802723487, gradient=0.00309509584568575\n",
      "Gradient Descent(8/99): loss=0.38965404349905064, gradient=0.0027740447932533526\n",
      "Gradient Descent(9/99): loss=0.3896534399844021, gradient=0.002506778871935313\n",
      "Gradient Descent(10/99): loss=0.3896529384454373, gradient=0.002282414958417616\n",
      "Gradient Descent(11/99): loss=0.3896525161787239, gradient=0.002092437296678705\n",
      "Gradient Descent(12/99): loss=0.38965215645896095, gradient=0.0019301947228276208\n",
      "Gradient Descent(13/99): loss=0.3896518467771785, gradient=0.0017904931395303653\n",
      "Gradient Descent(14/99): loss=0.3896515776369836, gradient=0.0016692690079138355\n",
      "Gradient Descent(15/99): loss=0.38965134172389754, gradient=0.0015633310784941318\n",
      "Gradient Descent(16/99): loss=0.38965113332597534, gradient=0.0014701586259133765\n",
      "Gradient Descent(17/99): loss=0.3896509479249987, gradient=0.0013877455605380722\n",
      "Gradient Descent(18/99): loss=0.38965078190446784, gradient=0.0013144810784742672\n",
      "Gradient Descent(19/99): loss=0.38965063233833114, gradient=0.0012490589134370573\n",
      "Gradient Descent(20/99): loss=0.389650496836106, gradient=0.0011904086512431967\n",
      "Gradient Descent(21/99): loss=0.38965037342781633, gradient=0.0011376438604295664\n",
      "Gradient Descent(22/99): loss=0.38965026047738033, gradient=0.0010900229209214977\n",
      "Gradient Descent(23/99): loss=0.38965015661656993, gradient=0.0010469193749024109\n",
      "Gradient Descent(24/99): loss=0.38965006069403807, gradient=0.0010077993846894615\n",
      "Gradient Descent(25/99): loss=0.3896499717355149, gradient=0.0009722044809086359\n",
      "Gradient Descent(26/99): loss=0.38964988891239477, gradient=0.00093973824599563\n",
      "Gradient Descent(27/99): loss=0.3896498115166995, gradient=0.0009100559289266957\n",
      "Gradient Descent(28/99): loss=0.389649738940942, gradient=0.0008828562505965837\n",
      "Gradient Descent(29/99): loss=0.3896496706618068, gradient=0.0008578748553310549\n",
      "Gradient Descent(30/99): loss=0.3896496060003174, gradient=0.0008381536339006112\n",
      "Gradient Descent(31/99): loss=0.3896495449090961, gradient=0.0008141750215868211\n",
      "Gradient Descent(32/99): loss=0.3896494870411016, gradient=0.0007940389401754499\n",
      "Gradient Descent(33/99): loss=0.3896494319719811, gradient=0.0007758296241682351\n",
      "Gradient Descent(34/99): loss=0.3896493794590717, gradient=0.0007589417802628406\n",
      "Gradient Descent(35/99): loss=0.3896493290634525, gradient=0.0007465638699609152\n",
      "Gradient Descent(36/99): loss=0.38964928087985373, gradient=0.000729152021492003\n",
      "Gradient Descent(37/99): loss=0.38964923475035773, gradient=0.0007148555978348856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(38/99): loss=0.3896491903903208, gradient=0.0007019697243192737\n",
      "Gradient Descent(39/99): loss=0.38964914748912904, gradient=0.0006932746107305757\n",
      "Gradient Descent(40/99): loss=0.3896491061794139, gradient=0.0006791953908776264\n",
      "Gradient Descent(41/99): loss=0.3896490663895077, gradient=0.0006678465442172245\n",
      "Gradient Descent(42/99): loss=0.3896490278921764, gradient=0.0006576796359443551\n",
      "Gradient Descent(43/99): loss=0.38964899043905543, gradient=0.0006514898614925891\n",
      "Gradient Descent(44/99): loss=0.38964895418288487, gradient=0.000639688341494062\n",
      "Gradient Descent(45/99): loss=0.3896489191038599, gradient=0.0006303572655154971\n",
      "Gradient Descent(46/99): loss=0.3896488848406314, gradient=0.0006253733777485736\n",
      "Gradient Descent(47/99): loss=0.3896488518645131, gradient=0.0006093834075227841\n",
      "Gradient Descent(48/99): loss=0.3896488194896051, gradient=0.0006071774481164501\n",
      "Gradient Descent(49/99): loss=0.38964878835466993, gradient=0.0005951604631023224\n",
      "Gradient Descent(50/99): loss=0.38964875779339486, gradient=0.0005891642405179627\n",
      "Gradient Descent(51/99): loss=0.38964872790036864, gradient=0.0005811259543916619\n",
      "Gradient Descent(52/99): loss=0.38964869885630665, gradient=0.0005759150372228098\n",
      "Gradient Descent(53/99): loss=0.3896486703005972, gradient=0.0005684975040319382\n",
      "Gradient Descent(54/99): loss=0.3896486426063671, gradient=0.0005616248056780969\n",
      "Gradient Descent(55/99): loss=0.3896486154593834, gradient=0.0005570842356274579\n",
      "Gradient Descent(56/99): loss=0.3896485887525821, gradient=0.0005508494397082249\n",
      "Gradient Descent(57/99): loss=0.38964856279514987, gradient=0.0005461279201368488\n",
      "Gradient Descent(58/99): loss=0.38964853716901166, gradient=0.0005401544225828583\n",
      "Gradient Descent(59/99): loss=0.38964851227461683, gradient=0.0005355064070788181\n",
      "Gradient Descent(60/99): loss=0.38964848766206395, gradient=0.0005299579223100268\n",
      "Gradient Descent(61/99): loss=0.3896484637520654, gradient=0.0005254579081341989\n",
      "Gradient Descent(62/99): loss=0.38964844008343424, gradient=0.0005202776373540303\n",
      "Gradient Descent(63/99): loss=0.38964841709015857, gradient=0.0005159220090626269\n",
      "Gradient Descent(64/99): loss=0.3896483943030141, gradient=0.0005110581801704888\n",
      "Gradient Descent(65/99): loss=0.3896483721666939, gradient=0.0005068353835373671\n",
      "Gradient Descent(66/99): loss=0.38964835020541877, gradient=0.0005022490887144662\n",
      "Gradient Descent(67/99): loss=0.3896483288729762, gradient=0.0004981457882847074\n",
      "Gradient Descent(68/99): loss=0.38964830768791553, gradient=0.0004938071823238076\n",
      "Gradient Descent(69/99): loss=0.38964828711200006, gradient=0.0004898099580516172\n",
      "Gradient Descent(70/99): loss=0.3896482666586665, gradient=0.000485695479890229\n",
      "Gradient Descent(71/99): loss=0.38964824679683513, gradient=0.00048179164818680005\n",
      "Gradient Descent(72/99): loss=0.3896482270352033, gradient=0.00047788223776995363\n",
      "Gradient Descent(73/99): loss=0.38964820783893245, gradient=0.0004730244275225655\n",
      "Gradient Descent(74/99): loss=0.38964818894654063, gradient=0.0004698867064952319\n",
      "Gradient Descent(75/99): loss=0.38964817018260123, gradient=0.00046649281248932955\n",
      "Gradient Descent(76/99): loss=0.3896481519507021, gradient=0.0004629737111879307\n",
      "Gradient Descent(77/99): loss=0.38964813377532653, gradient=0.00045945583345633547\n",
      "Gradient Descent(78/99): loss=0.38964811613738465, gradient=0.0004558622646789866\n",
      "Gradient Descent(79/99): loss=0.38964809853659277, gradient=0.0004525140685344517\n",
      "Gradient Descent(80/99): loss=0.3896480814644475, gradient=0.00044894261087950093\n",
      "Gradient Descent(81/99): loss=0.3896480644139801, gradient=0.0004457630116490985\n",
      "Gradient Descent(82/99): loss=0.38964804788289026, gradient=0.000442221493298365\n",
      "Gradient Descent(83/99): loss=0.38964803135931486, gradient=0.00043919617546006465\n",
      "Gradient Descent(84/99): loss=0.3896480153462254, gradient=0.0004356848889371573\n",
      "Gradient Descent(85/99): loss=0.3896479993275052, gradient=0.00043280260098675616\n",
      "Gradient Descent(86/99): loss=0.38964798381085813, gradient=0.00042932003517641556\n",
      "Gradient Descent(87/99): loss=0.3896479682861987, gradient=0.0004249987102998601\n",
      "Gradient Descent(88/99): loss=0.38964795302520316, gradient=0.00042303786788468217\n",
      "Gradient Descent(89/99): loss=0.3896479381971947, gradient=0.0004198578753493768\n",
      "Gradient Descent(90/99): loss=0.3896479233454254, gradient=0.00041752125990163896\n",
      "Gradient Descent(91/99): loss=0.38964790897877005, gradient=0.0004140369058293197\n",
      "Gradient Descent(92/99): loss=0.38964789456660237, gradient=0.0004116718640332085\n",
      "Gradient Descent(93/99): loss=0.389647880633874, gradient=0.0004082183394694509\n",
      "Gradient Descent(94/99): loss=0.3896478666453926, gradient=0.00040593619926270304\n",
      "Gradient Descent(95/99): loss=0.389647853128808, gradient=0.0004025198269980157\n",
      "Gradient Descent(96/99): loss=0.38964783954723736, gradient=0.00040033265137596916\n",
      "Gradient Descent(97/99): loss=0.3896478264307715, gradient=0.00039694398181403645\n",
      "Gradient Descent(98/99): loss=0.3896478132405445, gradient=0.0003948549374654655\n",
      "Gradient Descent(99/99): loss=0.3896478005092724, gradient=0.0003914870450311465\n",
      "Gradient Descent(0/99): loss=0.38895639880413707, gradient=0.008208668116887663\n",
      "Gradient Descent(1/99): loss=0.3889536207181709, gradient=0.005592618867157687\n",
      "Gradient Descent(2/99): loss=0.38895171436103193, gradient=0.004570881819570179\n",
      "Gradient Descent(3/99): loss=0.3889503391553556, gradient=0.0038648148237456366\n",
      "Gradient Descent(4/99): loss=0.3889493264215756, gradient=0.0033146325146643355\n",
      "Gradient Descent(5/99): loss=0.3889485602215749, gradient=0.0028798291793907783\n",
      "Gradient Descent(6/99): loss=0.38894796480372795, gradient=0.002536601280803408\n",
      "Gradient Descent(7/99): loss=0.38894748888163155, gradient=0.0022660825202279944\n",
      "Gradient Descent(8/99): loss=0.3889470978097015, gradient=0.0020528956218610417\n",
      "Gradient Descent(9/99): loss=0.3889467679246203, gradient=0.0018845399520562515\n",
      "Gradient Descent(10/99): loss=0.3889464829411167, gradient=0.0017509498418828832\n",
      "Gradient Descent(11/99): loss=0.38894623153910757, gradient=0.0016441268165454904\n",
      "Gradient Descent(12/99): loss=0.38894600576643396, gradient=0.001557807975948517\n",
      "Gradient Descent(13/99): loss=0.3889457999704585, gradient=0.0014871585208024863\n",
      "Gradient Descent(14/99): loss=0.38894561008100226, gradient=0.0014284911401188415\n",
      "Gradient Descent(15/99): loss=0.38894543312605384, gradient=0.0013790193598202946\n",
      "Gradient Descent(16/99): loss=0.3889452669027568, gradient=0.0013366497542689842\n",
      "Gradient Descent(17/99): loss=0.38894510975239505, gradient=0.0012998135674773933\n",
      "Gradient Descent(18/99): loss=0.3889449604054047, gradient=0.0012673345885589396\n",
      "Gradient Descent(19/99): loss=0.38894481787378515, gradient=0.0012383280399137263\n",
      "Gradient Descent(20/99): loss=0.3889446813757882, gradient=0.0012121246221513393\n",
      "Gradient Descent(21/99): loss=0.38894455028271974, gradient=0.0011882141965537368\n",
      "Gradient Descent(22/99): loss=0.38894442408099866, gradient=0.0011662043812568728\n",
      "Gradient Descent(23/99): loss=0.38894430234481536, gradient=0.0011457902568719012\n",
      "Gradient Descent(24/99): loss=0.38894418471621456, gradient=0.0011267322392343078\n",
      "Gradient Descent(25/99): loss=0.3889440708904206, gradient=0.0011088399052961847\n",
      "Gradient Descent(26/99): loss=0.38894396060489955, gradient=0.0010919601371184098\n",
      "Gradient Descent(27/99): loss=0.388943853631106, gradient=0.0010759683915594286\n",
      "Gradient Descent(28/99): loss=0.38894374976818497, gradient=0.0010607622330746213\n",
      "Gradient Descent(29/99): loss=0.38894364883810467, gradient=0.0010462565085804362\n",
      "Gradient Descent(30/99): loss=0.3889435506818512, gradient=0.0010323797181926182\n",
      "Gradient Descent(31/99): loss=0.38894345515642076, gradient=0.0010190712612957377\n",
      "Gradient Descent(32/99): loss=0.38894336213241554, gradient=0.0010062793272854083\n",
      "Gradient Descent(33/99): loss=0.3889432714921007, gradient=0.0009939592644944198\n",
      "Gradient Descent(34/99): loss=0.38894318312781995, gradient=0.0009820723066121609\n",
      "Gradient Descent(35/99): loss=0.3889430979454021, gradient=0.0009606390447001971\n",
      "Gradient Descent(36/99): loss=0.3889430138420142, gradient=0.0009608855454231469\n",
      "Gradient Descent(37/99): loss=0.38894293275611, gradient=0.0009392357275814948\n",
      "Gradient Descent(38/99): loss=0.38894285370180565, gradient=0.0009282253451145874\n",
      "Gradient Descent(39/99): loss=0.388942776524865, gradient=0.000917917647973097\n",
      "Gradient Descent(40/99): loss=0.3889427011135524, gradient=0.0009081627925451366\n",
      "Gradient Descent(41/99): loss=0.388942627374067, gradient=0.0008988503811408092\n",
      "Gradient Descent(42/99): loss=0.3889425552288727, gradient=0.0008898987821532042\n",
      "Gradient Descent(43/99): loss=0.38894248461181835, gradient=0.0008812473776322132\n",
      "Gradient Descent(44/99): loss=0.3889424154654647, gradient=0.0008728509000499498\n",
      "Gradient Descent(45/99): loss=0.38894234773895525, gradient=0.0008646753015395653\n",
      "Gradient Descent(46/99): loss=0.38894228138651255, gradient=0.0008566947449307377\n",
      "Gradient Descent(47/99): loss=0.3889422163663355, gradient=0.000848889413480002\n",
      "Gradient Descent(48/99): loss=0.38894215263978954, gradient=0.0008412439167811233\n",
      "Gradient Descent(49/99): loss=0.3889420901708108, gradient=0.0008337461301883761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(50/99): loss=0.38894202892546353, gradient=0.0008263863491931528\n",
      "Gradient Descent(51/99): loss=0.38894196887160704, gradient=0.0008191566725256381\n",
      "Gradient Descent(52/99): loss=0.3889419099786451, gradient=0.0008120505513551005\n",
      "Gradient Descent(53/99): loss=0.38894185221733174, gradient=0.0008050624591481876\n",
      "Gradient Descent(54/99): loss=0.38894179555962216, gradient=0.0007981876492292194\n",
      "Gradient Descent(55/99): loss=0.3889417399785535, gradient=0.0007914219761472399\n",
      "Gradient Descent(56/99): loss=0.3889416854481501, gradient=0.0007847617635213949\n",
      "Gradient Descent(57/99): loss=0.38894163194334613, gradient=0.0007782037057967263\n",
      "Gradient Descent(58/99): loss=0.38894157943992075, gradient=0.0007717447947872516\n",
      "Gradient Descent(59/99): loss=0.3889415279144446, gradient=0.0007653822643815005\n",
      "Gradient Descent(60/99): loss=0.38894147734423135, gradient=0.0007591135485933678\n",
      "Gradient Descent(61/99): loss=0.38894142770729917, gradient=0.0007529362494523542\n",
      "Gradient Descent(62/99): loss=0.3889413789823342, gradient=0.0007468481121767898\n",
      "Gradient Descent(63/99): loss=0.38894133114865675, gradient=0.0007408470057633618\n",
      "Gradient Descent(64/99): loss=0.3889412841861946, gradient=0.0007349309076274975\n",
      "Gradient Descent(65/99): loss=0.3889412380754551, gradient=0.0007290978912903865\n",
      "Gradient Descent(66/99): loss=0.38894119279750167, gradient=0.0007233461163775232\n",
      "Gradient Descent(67/99): loss=0.38894114833392995, gradient=0.0007176738203816087\n",
      "Gradient Descent(68/99): loss=0.38894110466684867, gradient=0.0007120793117875407\n",
      "Gradient Descent(69/99): loss=0.388941061778858, gradient=0.0007065609642576806\n",
      "Gradient Descent(70/99): loss=0.3889410196530334, gradient=0.0007011172116529556\n",
      "Gradient Descent(71/99): loss=0.3889409782729061, gradient=0.0006957465437188312\n",
      "Gradient Descent(72/99): loss=0.3889409376224488, gradient=0.0006904475023088236\n",
      "Gradient Descent(73/99): loss=0.388940897686058, gradient=0.000685218678045477\n",
      "Gradient Descent(74/99): loss=0.3889408584485415, gradient=0.0006800587073440145\n",
      "Gradient Descent(75/99): loss=0.38894081989510276, gradient=0.0006749662697389621\n",
      "Gradient Descent(76/99): loss=0.3889407820113291, gradient=0.0006699400854674182\n",
      "Gradient Descent(77/99): loss=0.38894074478317725, gradient=0.0006649789132716088\n",
      "Gradient Descent(78/99): loss=0.38894070819696314, gradient=0.0006600815483917528\n",
      "Gradient Descent(79/99): loss=0.38894067223934886, gradient=0.0006552468207242707\n",
      "Gradient Descent(80/99): loss=0.38894063689733305, gradient=0.0006504735931259455\n",
      "Gradient Descent(81/99): loss=0.38894060215823867, gradient=0.0006457607598473191\n",
      "Gradient Descent(82/99): loss=0.3889405680097045, gradient=0.0006411072450817144\n",
      "Gradient Descent(83/99): loss=0.38894053443967447, gradient=0.0006365120016173474\n",
      "Gradient Descent(84/99): loss=0.38894050143638853, gradient=0.0006319740095834649\n",
      "Gradient Descent(85/99): loss=0.38894046898837403, gradient=0.000627492275280699\n",
      "Gradient Descent(86/99): loss=0.38894043708443676, gradient=0.0006230658300886578\n",
      "Gradient Descent(87/99): loss=0.38894040571365296, gradient=0.0006186937294439764\n",
      "Gradient Descent(88/99): loss=0.3889403748653613, gradient=0.000614375051882107\n",
      "Gradient Descent(89/99): loss=0.38894034452915494, gradient=0.0006101088981388117\n",
      "Gradient Descent(90/99): loss=0.38894031469487456, gradient=0.0006058943903055208\n",
      "Gradient Descent(91/99): loss=0.3889402853526013, gradient=0.0006017306710347196\n",
      "Gradient Descent(92/99): loss=0.38894025649264996, gradient=0.0005976169027915783\n",
      "Gradient Descent(93/99): loss=0.3889402281055615, gradient=0.0005935522671481022\n",
      "Gradient Descent(94/99): loss=0.38894020018209796, gradient=0.0005895359641169231\n",
      "Gradient Descent(95/99): loss=0.3889401727132357, gradient=0.0005855672115213697\n",
      "Gradient Descent(96/99): loss=0.3889401456901593, gradient=0.0005816452443997785\n",
      "Gradient Descent(97/99): loss=0.38894011910425635, gradient=0.0005777693144410612\n",
      "Gradient Descent(98/99): loss=0.3889400929471112, gradient=0.0005739386894497777\n",
      "Gradient Descent(99/99): loss=0.3889400672105005, gradient=0.0005701526528381609\n",
      "Gradient Descent(0/99): loss=0.3901373870655395, gradient=0.015404561417350407\n",
      "Gradient Descent(1/99): loss=0.3901335930702445, gradient=0.006924245739380847\n",
      "Gradient Descent(2/99): loss=0.3901310042935379, gradient=0.005365877554880759\n",
      "Gradient Descent(3/99): loss=0.3901290731650937, gradient=0.0046217197741368924\n",
      "Gradient Descent(4/99): loss=0.3901275821688554, gradient=0.004055098294556097\n",
      "Gradient Descent(5/99): loss=0.3901264081605416, gradient=0.0035981040099442494\n",
      "Gradient Descent(6/99): loss=0.3901254643371544, gradient=0.003225694992320858\n",
      "Gradient Descent(7/99): loss=0.390124691131481, gradient=0.0029200177734991393\n",
      "Gradient Descent(8/99): loss=0.3901240464195838, gradient=0.002667231745538997\n",
      "Gradient Descent(9/99): loss=0.3901235001322002, gradient=0.0024565284306783956\n",
      "Gradient Descent(10/99): loss=0.3901230304567782, gradient=0.0022794555426937386\n",
      "Gradient Descent(11/99): loss=0.3901226213485898, gradient=0.002129394181975767\n",
      "Gradient Descent(12/99): loss=0.3901222608342471, gradient=0.0020011507824415765\n",
      "Gradient Descent(13/99): loss=0.39012193985353133, gradient=0.0018906389270071486\n",
      "Gradient Descent(14/99): loss=0.39012165191875314, gradient=0.001792464287800778\n",
      "Gradient Descent(15/99): loss=0.3901213917504229, gradient=0.0017083780582259872\n",
      "Gradient Descent(16/99): loss=0.3901211540560359, gradient=0.001637871378820837\n",
      "Gradient Descent(17/99): loss=0.390120935575769, gradient=0.0015700241887743448\n",
      "Gradient Descent(18/99): loss=0.39012073469792763, gradient=0.0015104021142206769\n",
      "Gradient Descent(19/99): loss=0.39012054807262225, gradient=0.001460368732096581\n",
      "Gradient Descent(20/99): loss=0.39012037379118775, gradient=0.001410874836824032\n",
      "Gradient Descent(21/99): loss=0.39012021141678555, gradient=0.001369207999952201\n",
      "Gradient Descent(22/99): loss=0.3901200583448526, gradient=0.0013277061030468648\n",
      "Gradient Descent(23/99): loss=0.39011991497800297, gradient=0.0012924175557815776\n",
      "Gradient Descent(24/99): loss=0.3901197788865291, gradient=0.001257231508703004\n",
      "Gradient Descent(25/99): loss=0.39011965094604745, gradient=0.001224306843654226\n",
      "Gradient Descent(26/99): loss=0.39011952944212214, gradient=0.0011968092105337665\n",
      "Gradient Descent(27/99): loss=0.3901194133701492, gradient=0.0011688884518353485\n",
      "Gradient Descent(28/99): loss=0.3901193036229967, gradient=0.0011440268755010443\n",
      "Gradient Descent(29/99): loss=0.39011919812986645, gradient=0.0011190981710923202\n",
      "Gradient Descent(30/99): loss=0.3901190982403377, gradient=0.0010965868551165781\n",
      "Gradient Descent(31/99): loss=0.39011900183995163, gradient=0.001074447460733612\n",
      "Gradient Descent(32/99): loss=0.39011891034208857, gradient=0.0010525775112883266\n",
      "Gradient Descent(33/99): loss=0.3901188226318297, gradient=0.0010321316756863798\n",
      "Gradient Descent(34/99): loss=0.3901187385040288, gradient=0.0010138432964884993\n",
      "Gradient Descent(35/99): loss=0.3901186578144578, gradient=0.0009951809405647253\n",
      "Gradient Descent(36/99): loss=0.39011858024300505, gradient=0.000978447318531069\n",
      "Gradient Descent(37/99): loss=0.39011850572801604, gradient=0.0009611803926501739\n",
      "Gradient Descent(38/99): loss=0.3901184377638642, gradient=0.0009053654609024936\n",
      "Gradient Descent(39/99): loss=0.39011837271645533, gradient=0.0008932638450743442\n",
      "Gradient Descent(40/99): loss=0.390118308302562, gradient=0.000881794687171504\n",
      "Gradient Descent(41/99): loss=0.390118248191954, gradient=0.0008676399789656002\n",
      "Gradient Descent(42/99): loss=0.3901181881578155, gradient=0.0008542467616526217\n",
      "Gradient Descent(43/99): loss=0.3901181322850525, gradient=0.0008409395046007299\n",
      "Gradient Descent(44/99): loss=0.3901180762906132, gradient=0.0008279553272364519\n",
      "Gradient Descent(45/99): loss=0.39011802426640957, gradient=0.0008156059510834009\n",
      "Gradient Descent(46/99): loss=0.39011797195313846, gradient=0.000803249430404667\n",
      "Gradient Descent(47/99): loss=0.39011792345244456, gradient=0.0007916023933172149\n",
      "Gradient Descent(48/99): loss=0.39011787450931273, gradient=0.0007799191045826856\n",
      "Gradient Descent(49/99): loss=0.39011782924774674, gradient=0.000768790898884537\n",
      "Gradient Descent(50/99): loss=0.3901177834028846, gradient=0.0007577916779808546\n",
      "Gradient Descent(51/99): loss=0.3901177411281841, gradient=0.0007470573559283309\n",
      "Gradient Descent(52/99): loss=0.3901176981409782, gradient=0.0007367367591772923\n",
      "Gradient Descent(53/99): loss=0.39011765862752723, gradient=0.0007263077865867057\n",
      "Gradient Descent(54/99): loss=0.39011761828304115, gradient=0.0007166523408099253\n",
      "Gradient Descent(55/99): loss=0.3901175813278142, gradient=0.0007064629415781554\n",
      "Gradient Descent(56/99): loss=0.3901175434326795, gradient=0.0006974559830829753\n",
      "Gradient Descent(57/99): loss=0.3901175088520578, gradient=0.0006874549579262922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(58/99): loss=0.3901174732313151, gradient=0.0006790793778404324\n",
      "Gradient Descent(59/99): loss=0.39011744085851163, gradient=0.0006692251172084783\n",
      "Gradient Descent(60/99): loss=0.3901174073531075, gradient=0.0006614648088286711\n",
      "Gradient Descent(61/99): loss=0.3901173770360458, gradient=0.0006517221935336425\n",
      "Gradient Descent(62/99): loss=0.3901173455008043, gradient=0.0006445627293602552\n",
      "Gradient Descent(63/99): loss=0.3901173171003531, gradient=0.0006349011672032144\n",
      "Gradient Descent(64/99): loss=0.3901172874023079, gradient=0.0006283300382292869\n",
      "Gradient Descent(65/99): loss=0.3901172607907958, gradient=0.0006187221954878743\n",
      "Gradient Descent(66/99): loss=0.39011723268540205, gradient=0.0006047020175083298\n",
      "Gradient Descent(67/99): loss=0.39011720677193695, gradient=0.0005937402341015535\n",
      "Gradient Descent(68/99): loss=0.3901171814743398, gradient=0.0005845102239314954\n",
      "Gradient Descent(69/99): loss=0.3901171569269725, gradient=0.0005783587641098651\n",
      "Gradient Descent(70/99): loss=0.39011713319757213, gradient=0.0005696011450149342\n",
      "Gradient Descent(71/99): loss=0.3901171101044016, gradient=0.000563772528307435\n",
      "Gradient Descent(72/99): loss=0.3901170877937104, gradient=0.0005552417311181131\n",
      "Gradient Descent(73/99): loss=0.3901170660688566, gradient=0.0005497083881441285\n",
      "Gradient Descent(74/99): loss=0.39011704508214334, gradient=0.0005413923690086319\n",
      "Gradient Descent(75/99): loss=0.39011702463802705, gradient=0.0005361407003566176\n",
      "Gradient Descent(76/99): loss=0.39011700489062523, gradient=0.0005280291805492105\n",
      "Gradient Descent(77/99): loss=0.3901169859502085, gradient=0.0005204183538926727\n",
      "Gradient Descent(78/99): loss=0.3901169664119838, gradient=0.0005274567535282765\n",
      "Gradient Descent(79/99): loss=0.3901169494475424, gradient=0.000519498487523069\n",
      "Gradient Descent(80/99): loss=0.3901169307346221, gradient=0.000518804742478451\n",
      "Gradient Descent(81/99): loss=0.3901169148690018, gradient=0.0005084460842179747\n",
      "Gradient Descent(82/99): loss=0.39011689702920965, gradient=0.0004980381656986032\n",
      "Gradient Descent(83/99): loss=0.39011688121816124, gradient=0.00048510074391450663\n",
      "Gradient Descent(84/99): loss=0.3901168655363158, gradient=0.0004806508925262688\n",
      "Gradient Descent(85/99): loss=0.3901168504712235, gradient=0.00047334186471385104\n",
      "Gradient Descent(86/99): loss=0.39011683609590087, gradient=0.00046565555356346437\n",
      "Gradient Descent(87/99): loss=0.39011682091551975, gradient=0.00047678071227387996\n",
      "Gradient Descent(88/99): loss=0.3901168082465956, gradient=0.000467996367515557\n",
      "Gradient Descent(89/99): loss=0.3901167936342512, gradient=0.0004702197951936212\n",
      "Gradient Descent(90/99): loss=0.39011678180748993, gradient=0.00045887523716008495\n",
      "Gradient Descent(91/99): loss=0.39011676796721984, gradient=0.00046048345282864337\n",
      "Gradient Descent(92/99): loss=0.39011675690016456, gradient=0.0004491058709612615\n",
      "Gradient Descent(93/99): loss=0.390116743791423, gradient=0.0004509710644482306\n",
      "Gradient Descent(94/99): loss=0.3901167334359401, gradient=0.0004395735028138528\n",
      "Gradient Descent(95/99): loss=0.39011672120212953, gradient=0.0004165864208918079\n",
      "Gradient Descent(96/99): loss=0.3901167095598974, gradient=0.0004302457555902768\n",
      "Gradient Descent(97/99): loss=0.3901167001053974, gradient=0.0004226774476271204\n",
      "Gradient Descent(98/99): loss=0.3901166885946653, gradient=0.00042898191201397535\n",
      "Gradient Descent(99/99): loss=0.39011667982863124, gradient=0.00041630368752210026\n",
      "Gradient Descent(0/99): loss=0.39034989693842137, gradient=0.00724754868107983\n",
      "Gradient Descent(1/99): loss=0.3903463075161406, gradient=0.006248725596373353\n",
      "Gradient Descent(2/99): loss=0.3903434964213697, gradient=0.005513221617020678\n",
      "Gradient Descent(3/99): loss=0.3903412287499311, gradient=0.004933641314839234\n",
      "Gradient Descent(4/99): loss=0.3903393611864368, gradient=0.004463622502062448\n",
      "Gradient Descent(5/99): loss=0.39033778787120577, gradient=0.00408145523009053\n",
      "Gradient Descent(6/99): loss=0.390336439728231, gradient=0.0037659398361664736\n",
      "Gradient Descent(7/99): loss=0.39033526622427106, gradient=0.0035018104494151017\n",
      "Gradient Descent(8/99): loss=0.3903342472213065, gradient=0.003222903944452646\n",
      "Gradient Descent(9/99): loss=0.390333336607269, gradient=0.003037532146757691\n",
      "Gradient Descent(10/99): loss=0.3903325126717547, gradient=0.002878025036117983\n",
      "Gradient Descent(11/99): loss=0.39033176260678626, gradient=0.0027383542353611964\n",
      "Gradient Descent(12/99): loss=0.3903310735081232, gradient=0.0026225329609614254\n",
      "Gradient Descent(13/99): loss=0.390330441430585, gradient=0.002506187193153467\n",
      "Gradient Descent(14/99): loss=0.3903298539566368, gradient=0.00240322821055402\n",
      "Gradient Descent(15/99): loss=0.39032930883678724, gradient=0.0023087132161834383\n",
      "Gradient Descent(16/99): loss=0.390328798417206, gradient=0.0022326922180135383\n",
      "Gradient Descent(17/99): loss=0.39032832205146556, gradient=0.0021522241701991333\n",
      "Gradient Descent(18/99): loss=0.390327875876697, gradient=0.0020763353694632017\n",
      "Gradient Descent(19/99): loss=0.39032746899697224, gradient=0.0019360897444620803\n",
      "Gradient Descent(20/99): loss=0.39032708653093945, gradient=0.0018685801455510956\n",
      "Gradient Descent(21/99): loss=0.39032672431911114, gradient=0.0018187165503531133\n",
      "Gradient Descent(22/99): loss=0.39032637979621887, gradient=0.0017664104627827843\n",
      "Gradient Descent(23/99): loss=0.39032605438843476, gradient=0.001713378159700871\n",
      "Gradient Descent(24/99): loss=0.39032574291237015, gradient=0.0016726916465079008\n",
      "Gradient Descent(25/99): loss=0.39032544826344734, gradient=0.0016237003100220678\n",
      "Gradient Descent(26/99): loss=0.39032516554569235, gradient=0.0015870972848961021\n",
      "Gradient Descent(27/99): loss=0.39032489779532564, gradient=0.0015415061619329573\n",
      "Gradient Descent(28/99): loss=0.39032464015481705, gradient=0.0015092604934727693\n",
      "Gradient Descent(29/99): loss=0.3903243960372452, gradient=0.0014658485088454162\n",
      "Gradient Descent(30/99): loss=0.3903241607590649, gradient=0.0014360333958919698\n",
      "Gradient Descent(31/99): loss=0.3903239376200224, gradient=0.0013956408840802756\n",
      "Gradient Descent(32/99): loss=0.39032372294154066, gradient=0.0013609756560461375\n",
      "Gradient Descent(33/99): loss=0.39032351649011243, gradient=0.001337638979544313\n",
      "Gradient Descent(34/99): loss=0.3903233195907021, gradient=0.0013045440021399546\n",
      "Gradient Descent(35/99): loss=0.3903231316023543, gradient=0.0012688967290402128\n",
      "Gradient Descent(36/99): loss=0.3903229504098194, gradient=0.001239006438268009\n",
      "Gradient Descent(37/99): loss=0.39032277665110143, gradient=0.0012108461616607167\n",
      "Gradient Descent(38/99): loss=0.39032260970394916, gradient=0.001183661938962102\n",
      "Gradient Descent(39/99): loss=0.3903224484037004, gradient=0.0011662274392222978\n",
      "Gradient Descent(40/99): loss=0.39032229388097484, gradient=0.001142463104639846\n",
      "Gradient Descent(41/99): loss=0.39032214636164403, gradient=0.001108567140308553\n",
      "Gradient Descent(42/99): loss=0.39032200357938424, gradient=0.001083715831773889\n",
      "Gradient Descent(43/99): loss=0.39032186542895664, gradient=0.00106980064961294\n",
      "Gradient Descent(44/99): loss=0.3903217340600673, gradient=0.0010397924006468903\n",
      "Gradient Descent(45/99): loss=0.3903216064400311, gradient=0.001016428650761019\n",
      "Gradient Descent(46/99): loss=0.3903214836369228, gradient=0.0009952160221057427\n",
      "Gradient Descent(47/99): loss=0.3903213651148133, gradient=0.0009747414693859165\n",
      "Gradient Descent(48/99): loss=0.3903212489111697, gradient=0.0009804049888415289\n",
      "Gradient Descent(49/99): loss=0.3903211400708135, gradient=0.0009433914872649131\n",
      "Gradient Descent(50/99): loss=0.39032103314848926, gradient=0.0009177115695119212\n",
      "Gradient Descent(51/99): loss=0.390320930337781, gradient=0.0008991475538911004\n",
      "Gradient Descent(52/99): loss=0.39032082996089706, gradient=0.0008929425668918797\n",
      "Gradient Descent(53/99): loss=0.3903207346597718, gradient=0.0008665960051087254\n",
      "Gradient Descent(54/99): loss=0.39032064141422473, gradient=0.0008480612062368996\n",
      "Gradient Descent(55/99): loss=0.39032055140036015, gradient=0.00083184376164125\n",
      "Gradient Descent(56/99): loss=0.3903204641628489, gradient=0.0008162140780672358\n",
      "Gradient Descent(57/99): loss=0.39032037878686976, gradient=0.0008137725278999936\n",
      "Gradient Descent(58/99): loss=0.3903202978260459, gradient=0.0007886510835270779\n",
      "Gradient Descent(59/99): loss=0.39032021696538793, gradient=0.0007921599771956926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(60/99): loss=0.3903201409811443, gradient=0.0007610037862353644\n",
      "Gradient Descent(61/99): loss=0.390320065357702, gradient=0.0007589814148501803\n",
      "Gradient Descent(62/99): loss=0.39031999391132405, gradient=0.0007346374596350228\n",
      "Gradient Descent(63/99): loss=0.39031992343915156, gradient=0.0007193265153846009\n",
      "Gradient Descent(64/99): loss=0.3903198552275698, gradient=0.0007067616541537148\n",
      "Gradient Descent(65/99): loss=0.39031978801549483, gradient=0.0007089519107967005\n",
      "Gradient Descent(66/99): loss=0.39031972443028257, gradient=0.000685710064211701\n",
      "Gradient Descent(67/99): loss=0.3903196615463365, gradient=0.0006718904031404665\n",
      "Gradient Descent(68/99): loss=0.39031960060552984, gradient=0.0006606861815586488\n",
      "Gradient Descent(69/99): loss=0.3903195412181165, gradient=0.0006499328413440326\n",
      "Gradient Descent(70/99): loss=0.39031948255554183, gradient=0.0006549900565080219\n",
      "Gradient Descent(71/99): loss=0.39031942718098955, gradient=0.0006322119950922096\n",
      "Gradient Descent(72/99): loss=0.3903193721909831, gradient=0.0006197171598743089\n",
      "Gradient Descent(73/99): loss=0.3903193188383793, gradient=0.0006099554424997465\n",
      "Gradient Descent(74/99): loss=0.3903192667486286, gradient=0.0006006015633989443\n",
      "Gradient Descent(75/99): loss=0.39031921511520706, gradient=0.0006081081190061729\n",
      "Gradient Descent(76/99): loss=0.3903191664979848, gradient=0.0005857472557120561\n",
      "Gradient Descent(77/99): loss=0.3903191166136397, gradient=0.0006019549280386475\n",
      "Gradient Descent(78/99): loss=0.39031907043894287, gradient=0.0005693631357987983\n",
      "Gradient Descent(79/99): loss=0.39031902348613096, gradient=0.0005756732322586315\n",
      "Gradient Descent(80/99): loss=0.39031897954735967, gradient=0.000553500377770748\n",
      "Gradient Descent(81/99): loss=0.3903189355127051, gradient=0.0005423255142896621\n",
      "Gradient Descent(82/99): loss=0.390318892719843, gradient=0.0005345190009540171\n",
      "Gradient Descent(83/99): loss=0.390318850002717, gradient=0.0005449200986055344\n",
      "Gradient Descent(84/99): loss=0.39031880994337176, gradient=0.0005234533522054613\n",
      "Gradient Descent(85/99): loss=0.3903187697042634, gradient=0.0005132325043294668\n",
      "Gradient Descent(86/99): loss=0.3903187305685747, gradient=0.0005062021418667352\n",
      "Gradient Descent(87/99): loss=0.3903186913978979, gradient=0.0005182117916264462\n",
      "Gradient Descent(88/99): loss=0.3903186547668437, gradient=0.0004967279290808629\n",
      "Gradient Descent(89/99): loss=0.3903186178447149, gradient=0.0004870189480662582\n",
      "Gradient Descent(90/99): loss=0.3903185819192202, gradient=0.0004806332040601922\n",
      "Gradient Descent(91/99): loss=0.3903185466441778, gradient=0.00047458975288391236\n",
      "Gradient Descent(92/99): loss=0.3903185112933511, gradient=0.0004884189823444127\n",
      "Gradient Descent(93/99): loss=0.39031847834895556, gradient=0.00046683992658955154\n",
      "Gradient Descent(94/99): loss=0.39031844500327645, gradient=0.000457679966641611\n",
      "Gradient Descent(95/99): loss=0.3903184125446096, gradient=0.000451991985133515\n",
      "Gradient Descent(96/99): loss=0.39031837986680457, gradient=0.00046698261780802973\n",
      "Gradient Descent(97/99): loss=0.3903183495095304, gradient=0.00044548642659642565\n",
      "Gradient Descent(98/99): loss=0.390318318669199, gradient=0.0004366142324806421\n",
      "Gradient Descent(99/99): loss=0.3903182872168964, gradient=0.0004661961441604649\n",
      "Gradient Descent(0/99): loss=0.3899684771376373, gradient=0.009972960498749841\n",
      "Gradient Descent(1/99): loss=0.38996239562290475, gradient=0.00811762806677116\n",
      "Gradient Descent(2/99): loss=0.38995810597307445, gradient=0.006794645374839859\n",
      "Gradient Descent(3/99): loss=0.3899549727163441, gradient=0.005788702292856884\n",
      "Gradient Descent(4/99): loss=0.38995263501459054, gradient=0.004949976473276461\n",
      "Gradient Descent(5/99): loss=0.3899508046705411, gradient=0.004366401947666115\n",
      "Gradient Descent(6/99): loss=0.3899493264007765, gradient=0.003915375735874683\n",
      "Gradient Descent(7/99): loss=0.3899480997380841, gradient=0.0035619533383957173\n",
      "Gradient Descent(8/99): loss=0.3899470584127899, gradient=0.0032801830124565942\n",
      "Gradient Descent(9/99): loss=0.38994615772921026, gradient=0.003051109750983689\n",
      "Gradient Descent(10/99): loss=0.3899453668006399, gradient=0.002861066977566149\n",
      "Gradient Descent(11/99): loss=0.38994466372511966, gradient=0.0027002772754304272\n",
      "Gradient Descent(12/99): loss=0.389944032557273, gradient=0.0025617600962756504\n",
      "Gradient Descent(13/99): loss=0.3899434728043657, gradient=0.0023896981116160025\n",
      "Gradient Descent(14/99): loss=0.3899429620707618, gradient=0.0022844333226045314\n",
      "Gradient Descent(15/99): loss=0.3899424918775533, gradient=0.0021953105889610364\n",
      "Gradient Descent(16/99): loss=0.389942059610391, gradient=0.0021084277010233647\n",
      "Gradient Descent(17/99): loss=0.38994165853522583, gradient=0.00203085476363533\n",
      "Gradient Descent(18/99): loss=0.38994128549928975, gradient=0.0019640162065282163\n",
      "Gradient Descent(19/99): loss=0.3899409390778209, gradient=0.0018958106916775927\n",
      "Gradient Descent(20/99): loss=0.38994061456910556, gradient=0.0018343315281282204\n",
      "Gradient Descent(21/99): loss=0.3899403136953205, gradient=0.0017623577584342034\n",
      "Gradient Descent(22/99): loss=0.389940032654655, gradient=0.0017058463110728173\n",
      "Gradient Descent(23/99): loss=0.3899397675876758, gradient=0.0016550005321590135\n",
      "Gradient Descent(24/99): loss=0.389939517580512, gradient=0.0016117145441582824\n",
      "Gradient Descent(25/99): loss=0.3899392824831475, gradient=0.0015690822008561642\n",
      "Gradient Descent(26/99): loss=0.3899390582050805, gradient=0.0015252181890971057\n",
      "Gradient Descent(27/99): loss=0.3899388462325389, gradient=0.001489619406348953\n",
      "Gradient Descent(28/99): loss=0.3899386454272146, gradient=0.0014481452253084968\n",
      "Gradient Descent(29/99): loss=0.38993845450789993, gradient=0.001415113083139785\n",
      "Gradient Descent(30/99): loss=0.38993827160887606, gradient=0.0013871381761655273\n",
      "Gradient Descent(31/99): loss=0.38993809921430245, gradient=0.0013509871636683286\n",
      "Gradient Descent(32/99): loss=0.38993793317802433, gradient=0.0013238463714097175\n",
      "Gradient Descent(33/99): loss=0.3899377753715806, gradient=0.0012907091626596639\n",
      "Gradient Descent(34/99): loss=0.3899376242536775, gradient=0.0012657583217715294\n",
      "Gradient Descent(35/99): loss=0.3899374786448719, gradient=0.0012436942019596157\n",
      "Gradient Descent(36/99): loss=0.389937340812716, gradient=0.0012200561106740701\n",
      "Gradient Descent(37/99): loss=0.38993720666735365, gradient=0.001196215872264641\n",
      "Gradient Descent(38/99): loss=0.3899370798880149, gradient=0.0011683461368337052\n",
      "Gradient Descent(39/99): loss=0.3899369568598749, gradient=0.0011464742525733374\n",
      "Gradient Descent(40/99): loss=0.38993683806564194, gradient=0.0011283329476183077\n",
      "Gradient Descent(41/99): loss=0.38993672517853917, gradient=0.0011114013675224267\n",
      "Gradient Descent(42/99): loss=0.38993661465660495, gradient=0.0010905078177847727\n",
      "Gradient Descent(43/99): loss=0.3899365099881319, gradient=0.0010675261563230542\n",
      "Gradient Descent(44/99): loss=0.3899364076029559, gradient=0.0010509420411895954\n",
      "Gradient Descent(45/99): loss=0.38993630937139717, gradient=0.001029763674262452\n",
      "Gradient Descent(46/99): loss=0.3899362145564875, gradient=0.0010131787220640563\n",
      "Gradient Descent(47/99): loss=0.38993612321361837, gradient=0.0010060236644357757\n",
      "Gradient Descent(48/99): loss=0.3899360334097685, gradient=0.0009876394701955143\n",
      "Gradient Descent(49/99): loss=0.389935947954852, gradient=0.0009778919119774463\n",
      "Gradient Descent(50/99): loss=0.389935863553083, gradient=0.0009589296056451648\n",
      "Gradient Descent(51/99): loss=0.3899357834502719, gradient=0.0009419380704862489\n",
      "Gradient Descent(52/99): loss=0.38993570981350895, gradient=0.0008706853649414296\n",
      "Gradient Descent(53/99): loss=0.38993563974275147, gradient=0.0008488329462381046\n",
      "Gradient Descent(54/99): loss=0.3899355714701945, gradient=0.0008439309157422767\n",
      "Gradient Descent(55/99): loss=0.38993550423504536, gradient=0.0008347008109279566\n",
      "Gradient Descent(56/99): loss=0.38993543910953965, gradient=0.0008209229629387373\n",
      "Gradient Descent(57/99): loss=0.389935376003734, gradient=0.0008076335569083015\n",
      "Gradient Descent(58/99): loss=0.3899353142362939, gradient=0.000806989934523202\n",
      "Gradient Descent(59/99): loss=0.38993525367634174, gradient=0.0007925364808976834\n",
      "Gradient Descent(60/99): loss=0.38993519511000624, gradient=0.0007795780368553924\n",
      "Gradient Descent(61/99): loss=0.3899351375513051, gradient=0.000788138845923714\n",
      "Gradient Descent(62/99): loss=0.38993508080389894, gradient=0.0007707989137023363\n",
      "Gradient Descent(63/99): loss=0.38993502655474915, gradient=0.0007533930375836877\n",
      "Gradient Descent(64/99): loss=0.38993497302413455, gradient=0.0007523095336558333\n",
      "Gradient Descent(65/99): loss=0.389934920021351, gradient=0.0007463754943931614\n",
      "Gradient Descent(66/99): loss=0.3899348688953053, gradient=0.0007313864775255508\n",
      "Gradient Descent(67/99): loss=0.3899348187687285, gradient=0.0007315075886018479\n",
      "Gradient Descent(68/99): loss=0.38993476894528545, gradient=0.0007249815577446147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(69/99): loss=0.38993472040958194, gradient=0.0007154890330131757\n",
      "Gradient Descent(70/99): loss=0.3899346732702339, gradient=0.0007041601250298346\n",
      "Gradient Descent(71/99): loss=0.3899346268412572, gradient=0.0007076778973771459\n",
      "Gradient Descent(72/99): loss=0.3899345809300414, gradient=0.0006966888766601327\n",
      "Gradient Descent(73/99): loss=0.3899345365078696, gradient=0.0006850756785897815\n",
      "Gradient Descent(74/99): loss=0.38993449266575125, gradient=0.0006895921144616788\n",
      "Gradient Descent(75/99): loss=0.3899344492427328, gradient=0.0006792290032674923\n",
      "Gradient Descent(76/99): loss=0.38993440723568873, gradient=0.0006677105321476423\n",
      "Gradient Descent(77/99): loss=0.3899343655538363, gradient=0.000683141968549642\n",
      "Gradient Descent(78/99): loss=0.38993432415998075, gradient=0.0006664591687748743\n",
      "Gradient Descent(79/99): loss=0.38993428470580305, gradient=0.0006505179300826249\n",
      "Gradient Descent(80/99): loss=0.389934245376588, gradient=0.0006543237326800754\n",
      "Gradient Descent(81/99): loss=0.38993420616275637, gradient=0.0006499674424955683\n",
      "Gradient Descent(82/99): loss=0.38993416833240246, gradient=0.0006366862730854063\n",
      "Gradient Descent(83/99): loss=0.38993413099534036, gradient=0.0006412305621859475\n",
      "Gradient Descent(84/99): loss=0.3899340936151533, gradient=0.0006358375336791489\n",
      "Gradient Descent(85/99): loss=0.3899340575991159, gradient=0.000622703074481634\n",
      "Gradient Descent(86/99): loss=0.38993402201662497, gradient=0.0006278373490891694\n",
      "Gradient Descent(87/99): loss=0.38993398635517396, gradient=0.0006225536691708732\n",
      "Gradient Descent(88/99): loss=0.38993395200338454, gradient=0.000609643293931996\n",
      "Gradient Descent(89/99): loss=0.38993391803820526, gradient=0.0006152918558695908\n",
      "Gradient Descent(90/99): loss=0.3899338839620047, gradient=0.0006100732039368209\n",
      "Gradient Descent(91/99): loss=0.3899338511469656, gradient=0.0005973597553683257\n",
      "Gradient Descent(92/99): loss=0.38993381867729676, gradient=0.000603482166512546\n",
      "Gradient Descent(93/99): loss=0.38993378606881396, gradient=0.0005983064457210411\n",
      "Gradient Descent(94/99): loss=0.3899337546775372, gradient=0.000585773890438819\n",
      "Gradient Descent(95/99): loss=0.3899337235957445, gradient=0.0005923282795315888\n",
      "Gradient Descent(96/99): loss=0.38993369235094355, gradient=0.0005871775228156754\n",
      "Gradient Descent(97/99): loss=0.38993366228320664, gradient=0.0005748152787632357\n",
      "Gradient Descent(98/99): loss=0.38993363249385055, gradient=0.0005817615805828385\n",
      "Gradient Descent(99/99): loss=0.3899336031715631, gradient=0.0005690885463378255\n",
      "Gradient Descent(0/99): loss=0.38967131828444196, gradient=0.011121448776209011\n",
      "Gradient Descent(1/99): loss=0.3896662935773548, gradient=0.007477810576816906\n",
      "Gradient Descent(2/99): loss=0.3896627457804154, gradient=0.0061902529393594264\n",
      "Gradient Descent(3/99): loss=0.3896601589952974, gradient=0.005248207454033535\n",
      "Gradient Descent(4/99): loss=0.3896582262192938, gradient=0.00451890500751906\n",
      "Gradient Descent(5/99): loss=0.38965674735970446, gradient=0.003939972513532638\n",
      "Gradient Descent(6/99): loss=0.3896555962145643, gradient=0.0034686412308669377\n",
      "Gradient Descent(7/99): loss=0.3896546838719818, gradient=0.0030818294527066013\n",
      "Gradient Descent(8/99): loss=0.3896539487851377, gradient=0.0027621379885544102\n",
      "Gradient Descent(9/99): loss=0.3896533473303845, gradient=0.00249588748394665\n",
      "Gradient Descent(10/99): loss=0.3896528482681847, gradient=0.0022723257961827\n",
      "Gradient Descent(11/99): loss=0.3896524283544862, gradient=0.0020866298388927448\n",
      "Gradient Descent(12/99): loss=0.3896520727594411, gradient=0.0019220916626993634\n",
      "Gradient Descent(13/99): loss=0.38965176658356454, gradient=0.0017822612667372846\n",
      "Gradient Descent(14/99): loss=0.38965150096317447, gradient=0.001665095076284643\n",
      "Gradient Descent(15/99): loss=0.3896512699409808, gradient=0.0015574061683879387\n",
      "Gradient Descent(16/99): loss=0.38965106569565416, gradient=0.0014638696545516766\n",
      "Gradient Descent(17/99): loss=0.3896508843483121, gradient=0.0013859342653564908\n",
      "Gradient Descent(18/99): loss=0.38965072360319497, gradient=0.001310785946039553\n",
      "Gradient Descent(19/99): loss=0.3896505780995412, gradient=0.001249625206574582\n",
      "Gradient Descent(20/99): loss=0.3896504480491221, gradient=0.0011884438354679183\n",
      "Gradient Descent(21/99): loss=0.3896503291824229, gradient=0.0011347264302627549\n",
      "Gradient Descent(22/99): loss=0.38965022053813547, gradient=0.0010927686063395734\n",
      "Gradient Descent(23/99): loss=0.3896501221275491, gradient=0.0010468667880865817\n",
      "Gradient Descent(24/99): loss=0.3896500307569061, gradient=0.0010067341425764416\n",
      "Gradient Descent(25/99): loss=0.38964994603462577, gradient=0.0009777567275551765\n",
      "Gradient Descent(26/99): loss=0.38964986860798684, gradient=0.0009416335619747175\n",
      "Gradient Descent(27/99): loss=0.3896497956891012, gradient=0.000910702822138919\n",
      "Gradient Descent(28/99): loss=0.3896497272247475, gradient=0.0008910386937240979\n",
      "Gradient Descent(29/99): loss=0.38964966423791086, gradient=0.0008615219063248969\n",
      "Gradient Descent(30/99): loss=0.389649604163673, gradient=0.0008370495865527093\n",
      "Gradient Descent(31/99): loss=0.3896495471376187, gradient=0.000824243724038053\n",
      "Gradient Descent(32/99): loss=0.3896494944255146, gradient=0.0007992284416646014\n",
      "Gradient Descent(33/99): loss=0.3896494435871077, gradient=0.0007793552798197281\n",
      "Gradient Descent(34/99): loss=0.38964939486595124, gradient=0.0007717013304582359\n",
      "Gradient Descent(35/99): loss=0.3896493496926783, gradient=0.0007497603977075199\n",
      "Gradient Descent(36/99): loss=0.3896493056971723, gradient=0.0007332011926101851\n",
      "Gradient Descent(37/99): loss=0.38964926318500476, gradient=0.0007294914270409771\n",
      "Gradient Descent(38/99): loss=0.38964922370383753, gradient=0.0007096439259851641\n",
      "Gradient Descent(39/99): loss=0.38964918492234374, gradient=0.0006954989951973078\n",
      "Gradient Descent(40/99): loss=0.38964914778937615, gradient=0.0006841898793636712\n",
      "Gradient Descent(41/99): loss=0.38964911130846486, gradient=0.0006850337802023425\n",
      "Gradient Descent(42/99): loss=0.38964907748402006, gradient=0.0006665646701804288\n",
      "Gradient Descent(43/99): loss=0.38964904244797327, gradient=0.0006801509083917413\n",
      "Gradient Descent(44/99): loss=0.3896490102891437, gradient=0.0006554633567912151\n",
      "Gradient Descent(45/99): loss=0.38964897969723294, gradient=0.0006392327313947606\n",
      "Gradient Descent(46/99): loss=0.38964894932046734, gradient=0.0006294405976842801\n",
      "Gradient Descent(47/99): loss=0.3896489193214427, gradient=0.0006335469991597379\n",
      "Gradient Descent(48/99): loss=0.38964889149362236, gradient=0.0006171108280921043\n",
      "Gradient Descent(49/99): loss=0.3896488635532265, gradient=0.0006072794144201712\n",
      "Gradient Descent(50/99): loss=0.3896488365334059, gradient=0.0006002316243651678\n",
      "Gradient Descent(51/99): loss=0.38964880946661634, gradient=0.0006072085579408193\n",
      "Gradient Descent(52/99): loss=0.38964878450735063, gradient=0.0005905036007887772\n",
      "Gradient Descent(53/99): loss=0.38964875928107673, gradient=0.0005815611112185244\n",
      "Gradient Descent(54/99): loss=0.3896487341680067, gradient=0.0005893313480261726\n",
      "Gradient Descent(55/99): loss=0.3896487110225537, gradient=0.000572986003423614\n",
      "Gradient Descent(56/99): loss=0.389648687533761, gradient=0.0005646071296436796\n",
      "Gradient Descent(57/99): loss=0.38964866475126564, gradient=0.0005590500288651361\n",
      "Gradient Descent(58/99): loss=0.3896486416928035, gradient=0.0005689781729636707\n",
      "Gradient Descent(59/99): loss=0.3896486205964865, gradient=0.0005521707220278191\n",
      "Gradient Descent(60/99): loss=0.3896485990637911, gradient=0.0005442297360967373\n",
      "Gradient Descent(61/99): loss=0.38964857816391824, gradient=0.0005392406923274706\n",
      "Gradient Descent(62/99): loss=0.3896485568995859, gradient=0.0005505522208311894\n",
      "Gradient Descent(63/99): loss=0.3896485375483822, gradient=0.0005334996652851028\n",
      "Gradient Descent(64/99): loss=0.38964851769728537, gradient=0.0005259266437987541\n",
      "Gradient Descent(65/99): loss=0.3896484977028936, gradient=0.0005374252262664431\n",
      "Gradient Descent(66/99): loss=0.38964847951891635, gradient=0.0005206282589692822\n",
      "Gradient Descent(67/99): loss=0.38964846080324195, gradient=0.0005133268780674077\n",
      "Gradient Descent(68/99): loss=0.38964844261816, gradient=0.0005090287922521301\n",
      "Gradient Descent(69/99): loss=0.38964842395870697, gradient=0.0005222539844397727\n",
      "Gradient Descent(70/99): loss=0.3896484071438464, gradient=0.0005047762604032831\n",
      "Gradient Descent(71/99): loss=0.3896483897478829, gradient=0.0004976180320520768\n",
      "Gradient Descent(72/99): loss=0.38964837284880227, gradient=0.0004936189380657435\n",
      "Gradient Descent(73/99): loss=0.3896483554271961, gradient=0.0005079046913899734\n",
      "Gradient Descent(74/99): loss=0.3896483398298708, gradient=0.0004900304406846403\n",
      "Gradient Descent(75/99): loss=0.38964832361863627, gradient=0.00048303796286520046\n",
      "Gradient Descent(76/99): loss=0.38964830787303184, gradient=0.00047928458831225906\n",
      "Gradient Descent(77/99): loss=0.3896482915652017, gradient=0.0004945314108920855\n",
      "Gradient Descent(78/99): loss=0.3896482770635795, gradient=0.00047626810162907637\n",
      "Gradient Descent(79/99): loss=0.3896482619205108, gradient=0.00046939814137183535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(80/99): loss=0.38964824721671387, gradient=0.0004658505767312847\n",
      "Gradient Descent(81/99): loss=0.3896482319168546, gradient=0.00048199891386357827\n",
      "Gradient Descent(82/99): loss=0.38964821840798675, gradient=0.0004633381572080509\n",
      "Gradient Descent(83/99): loss=0.3896482042343155, gradient=0.0004565595681510945\n",
      "Gradient Descent(84/99): loss=0.3896481904774737, gradient=0.0004531887303245793\n",
      "Gradient Descent(85/99): loss=0.3896481760952741, gradient=0.00047019819550993616\n",
      "Gradient Descent(86/99): loss=0.3896481634913413, gradient=0.0004511292040423629\n",
      "Gradient Descent(87/99): loss=0.3896481502025364, gradient=0.0004444186420848794\n",
      "Gradient Descent(88/99): loss=0.38964813731120684, gradient=0.0004412026067780998\n",
      "Gradient Descent(89/99): loss=0.38964812376882174, gradient=0.0004590435083280113\n",
      "Gradient Descent(90/99): loss=0.3896481119939763, gradient=0.00043955678309025325\n",
      "Gradient Descent(91/99): loss=0.38964809951678914, gradient=0.000432896187584283\n",
      "Gradient Descent(92/99): loss=0.38964808742017515, gradient=0.0004298179964286942\n",
      "Gradient Descent(93/99): loss=0.38964807543111996, gradient=0.0004269897836112406\n",
      "Gradient Descent(94/99): loss=0.3896480628235413, gradient=0.00044597833816049367\n",
      "Gradient Descent(95/99): loss=0.389648051975058, gradient=0.00042579924130196586\n",
      "Gradient Descent(96/99): loss=0.38964804041090173, gradient=0.0004192276300609419\n",
      "Gradient Descent(97/99): loss=0.3896480292109204, gradient=0.00041631766385133396\n",
      "Gradient Descent(98/99): loss=0.3896480173118783, gradient=0.00043603341286616173\n",
      "Gradient Descent(99/99): loss=0.3896480071666587, gradient=0.00041550058404636446\n",
      "Gradient Descent(0/99): loss=0.388956423477078, gradient=0.008269386035453428\n",
      "Gradient Descent(1/99): loss=0.38895361664954625, gradient=0.005683728287598596\n",
      "Gradient Descent(2/99): loss=0.3889517274415744, gradient=0.00460150651887177\n",
      "Gradient Descent(3/99): loss=0.38895036228233604, gradient=0.0038896302245833227\n",
      "Gradient Descent(4/99): loss=0.38894936178998, gradient=0.003343287330093137\n",
      "Gradient Descent(5/99): loss=0.38894860725201613, gradient=0.0029123080719843934\n",
      "Gradient Descent(6/99): loss=0.3889480236181185, gradient=0.002572428528974114\n",
      "Gradient Descent(7/99): loss=0.3889475594125639, gradient=0.002304839112632512\n",
      "Gradient Descent(8/99): loss=0.3889471799534847, gradient=0.002094218367948469\n",
      "Gradient Descent(9/99): loss=0.38894686149758406, gradient=0.0019281070899032588\n",
      "Gradient Descent(10/99): loss=0.38894658770130536, gradient=0.0017964766635099982\n",
      "Gradient Descent(11/99): loss=0.38894634720009486, gradient=0.0016913660701026996\n",
      "Gradient Descent(12/99): loss=0.3889461320135775, gradient=0.0016065503858573784\n",
      "Gradient Descent(13/99): loss=0.3889459364743337, gradient=0.0015372313592468398\n",
      "Gradient Descent(14/99): loss=0.3889457565082357, gradient=0.0014797547296095418\n",
      "Gradient Descent(15/99): loss=0.3889455891474515, gradient=0.0014313622911075053\n",
      "Gradient Descent(16/99): loss=0.3889454321991112, gradient=0.0013899837306643366\n",
      "Gradient Descent(17/99): loss=0.388945284018394, gradient=0.001354068546782118\n",
      "Gradient Descent(18/99): loss=0.38894514335204905, gradient=0.0013224545836494702\n",
      "Gradient Descent(19/99): loss=0.38894500922968506, gradient=0.0012942676950813544\n",
      "Gradient Descent(20/99): loss=0.38894488088763285, gradient=0.0012688465338922205\n",
      "Gradient Descent(21/99): loss=0.38894475771516407, gradient=0.0012456868770025376\n",
      "Gradient Descent(22/99): loss=0.38894463921614775, gradient=0.0012244007418070173\n",
      "Gradient Descent(23/99): loss=0.3889445249814495, gradient=0.001204686495419829\n",
      "Gradient Descent(24/99): loss=0.3889444146688561, gradient=0.0011863070316462223\n",
      "Gradient Descent(25/99): loss=0.3889443079883197, gradient=0.0011690738215462985\n",
      "Gradient Descent(26/99): loss=0.3889442046909904, gradient=0.0011528352209663022\n",
      "Gradient Descent(27/99): loss=0.3889441049666878, gradient=0.0011393757561696037\n",
      "Gradient Descent(28/99): loss=0.3889440047124535, gradient=0.0011283282836733652\n",
      "Gradient Descent(29/99): loss=0.3889439118779262, gradient=0.0011110122824851169\n",
      "Gradient Descent(30/99): loss=0.38894381702218594, gradient=0.00110205727698282\n",
      "Gradient Descent(31/99): loss=0.3889437297078374, gradient=0.0010834387248141255\n",
      "Gradient Descent(32/99): loss=0.38894363979983204, gradient=0.0010812821240729382\n",
      "Gradient Descent(33/99): loss=0.38894355775481904, gradient=0.001061286252643801\n",
      "Gradient Descent(34/99): loss=0.38894347308014027, gradient=0.001056017225813217\n",
      "Gradient Descent(35/99): loss=0.3889433955504123, gradient=0.0010410372918571518\n",
      "Gradient Descent(36/99): loss=0.38894331639826674, gradient=0.001016753989036037\n",
      "Gradient Descent(37/99): loss=0.3889432386057605, gradient=0.0010300964683849455\n",
      "Gradient Descent(38/99): loss=0.3889431677279038, gradient=0.0010010685478047759\n",
      "Gradient Descent(39/99): loss=0.388943092453373, gradient=0.0010158932836860789\n",
      "Gradient Descent(40/99): loss=0.38894302536302305, gradient=0.0009827392859705624\n",
      "Gradient Descent(41/99): loss=0.38894295620464076, gradient=0.0009627578881053184\n",
      "Gradient Descent(42/99): loss=0.3889428866307666, gradient=0.000992753285158095\n",
      "Gradient Descent(43/99): loss=0.38894282467027047, gradient=0.0009521097726401735\n",
      "Gradient Descent(44/99): loss=0.38894276052678883, gradient=0.0009343499863552538\n",
      "Gradient Descent(45/99): loss=0.38894269510799123, gradient=0.0009729468976154547\n",
      "Gradient Descent(46/99): loss=0.38894263771450155, gradient=0.0009257352176983162\n",
      "Gradient Descent(47/99): loss=0.38894257790427567, gradient=0.0009088332222332357\n",
      "Gradient Descent(48/99): loss=0.3889425163019554, gradient=0.000953507992332797\n",
      "Gradient Descent(49/99): loss=0.3889424629647068, gradient=0.0009016486493029528\n",
      "Gradient Descent(50/99): loss=0.3889424070477363, gradient=0.0008852853018674373\n",
      "Gradient Descent(51/99): loss=0.38894234897694485, gradient=0.0009345910765889229\n",
      "Gradient Descent(52/99): loss=0.3889422993140894, gradient=0.0008792544481037587\n",
      "Gradient Descent(53/99): loss=0.3889422469471158, gradient=0.0008632441833767214\n",
      "Gradient Descent(54/99): loss=0.38894219215807146, gradient=0.0009163072166789129\n",
      "Gradient Descent(55/99): loss=0.3889421458615778, gradient=0.0008582095141327292\n",
      "Gradient Descent(56/99): loss=0.38894209676045804, gradient=0.0008424398500178528\n",
      "Gradient Descent(57/99): loss=0.3889420450272305, gradient=0.000898721943915148\n",
      "Gradient Descent(58/99): loss=0.3889420018368072, gradient=0.0008383004128782802\n",
      "Gradient Descent(59/99): loss=0.3889419557571215, gradient=0.0008226986906591692\n",
      "Gradient Descent(60/99): loss=0.3889419068730266, gradient=0.0008818608677406153\n",
      "Gradient Descent(61/99): loss=0.3889418665619878, gradient=0.0008193830107618007\n",
      "Gradient Descent(62/99): loss=0.3889418232882829, gradient=0.0008038995578495344\n",
      "Gradient Descent(63/99): loss=0.3889417821838199, gradient=0.0007953385447502752\n",
      "Gradient Descent(64/99): loss=0.3889417363157444, gradient=0.0008649468643259445\n",
      "Gradient Descent(65/99): loss=0.3889416997519328, gradient=0.0007938381364107445\n",
      "Gradient Descent(66/99): loss=0.3889416600223358, gradient=0.0007786817601235096\n",
      "Gradient Descent(67/99): loss=0.38894161965292734, gradient=0.0008087870275696819\n",
      "Gradient Descent(68/99): loss=0.3889415793636551, gradient=0.0008114742103239676\n",
      "Gradient Descent(69/99): loss=0.3889415461980793, gradient=0.0007697262844971415\n",
      "Gradient Descent(70/99): loss=0.38894150708441183, gradient=0.0007923618419807691\n",
      "Gradient Descent(71/99): loss=0.3889414721033106, gradient=0.0007524642155270211\n",
      "Gradient Descent(72/99): loss=0.3889414348896146, gradient=0.0007906505605006035\n",
      "Gradient Descent(73/99): loss=0.38894140476951533, gradient=0.0007468433535910535\n",
      "Gradient Descent(74/99): loss=0.3889413683685044, gradient=0.0007745025470038643\n",
      "Gradient Descent(75/99): loss=0.388941336297366, gradient=0.0007301921152337847\n",
      "Gradient Descent(76/99): loss=0.38894130178111197, gradient=0.0007717317619550532\n",
      "Gradient Descent(77/99): loss=0.3889412716989882, gradient=0.0007661704661423768\n",
      "Gradient Descent(78/99): loss=0.3889412403750284, gradient=0.0007166189947752395\n",
      "Gradient Descent(79/99): loss=0.38894121104077006, gradient=0.0007088114646807652\n",
      "Gradient Descent(80/99): loss=0.38894117900959374, gradient=0.0007536712105251895\n",
      "Gradient Descent(81/99): loss=0.38894115125429374, gradient=0.000750111859734171\n",
      "Gradient Descent(82/99): loss=0.38894112241867246, gradient=0.0006964741958647692\n",
      "Gradient Descent(83/99): loss=0.3889410954934363, gradient=0.0006890815941689118\n",
      "Gradient Descent(84/99): loss=0.38894106298356035, gradient=0.0007770917881587691\n",
      "Gradient Descent(85/99): loss=0.3889410399895049, gradient=0.000691952904108222\n",
      "Gradient Descent(86/99): loss=0.3889410135262716, gradient=0.0006768898156390411\n",
      "Gradient Descent(87/99): loss=0.3889409862158773, gradient=0.0007122292539311412\n",
      "Gradient Descent(88/99): loss=0.38894096144964685, gradient=0.0006697062373828405\n",
      "Gradient Descent(89/99): loss=0.3889409340643252, gradient=0.0007204794164649491\n",
      "Gradient Descent(90/99): loss=0.38894091396131303, gradient=0.0006668208661424787\n",
      "Gradient Descent(91/99): loss=0.3889408873717074, gradient=0.0006993609146730768\n",
      "Gradient Descent(92/99): loss=0.3889408648857194, gradient=0.0006517917139682231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(93/99): loss=0.38894084283715585, gradient=0.0006453532530713401\n",
      "Gradient Descent(94/99): loss=0.3889408151740163, gradient=0.0007423357837450817\n",
      "Gradient Descent(95/99): loss=0.3889407970913781, gradient=0.0006500118508711143\n",
      "Gradient Descent(96/99): loss=0.38894077544290934, gradient=0.0006349276456495598\n",
      "Gradient Descent(97/99): loss=0.3889407554549537, gradient=0.0006285014445902522\n",
      "Gradient Descent(98/99): loss=0.38894073285404884, gradient=0.0006727027989720323\n",
      "Gradient Descent(99/99): loss=0.38894070965664507, gradient=0.0006875037989107663\n",
      "Gradient Descent(0/99): loss=0.3901373033001326, gradient=0.015309908733909288\n",
      "Gradient Descent(1/99): loss=0.39013369009175664, gradient=0.00690961062871302\n",
      "Gradient Descent(2/99): loss=0.3901311763516161, gradient=0.005361195821747999\n",
      "Gradient Descent(3/99): loss=0.3901293238549432, gradient=0.004618143344613821\n",
      "Gradient Descent(4/99): loss=0.39012789546022486, gradient=0.004052026078458865\n",
      "Gradient Descent(5/99): loss=0.3901267756616201, gradient=0.0035954649880260013\n",
      "Gradient Descent(6/99): loss=0.3901258783578802, gradient=0.003223427539973524\n",
      "Gradient Descent(7/99): loss=0.39012514575206725, gradient=0.002918047985560538\n",
      "Gradient Descent(8/99): loss=0.3901245368422458, gradient=0.0026654805058768894\n",
      "Gradient Descent(9/99): loss=0.3901240225099149, gradient=0.002454918299133078\n",
      "Gradient Descent(10/99): loss=0.3901235809546025, gradient=0.0022820564981471864\n",
      "Gradient Descent(11/99): loss=0.390123201562439, gradient=0.002132584925630949\n",
      "Gradient Descent(12/99): loss=0.39012286319193934, gradient=0.00200974277388469\n",
      "Gradient Descent(13/99): loss=0.39012256850611965, gradient=0.0018950356325989441\n",
      "Gradient Descent(14/99): loss=0.39012229892719014, gradient=0.0018067543381443416\n",
      "Gradient Descent(15/99): loss=0.39012206165216656, gradient=0.0017153403596286924\n",
      "Gradient Descent(16/99): loss=0.39012184165147223, gradient=0.0016364035946789103\n",
      "Gradient Descent(17/99): loss=0.3901216396484092, gradient=0.0015844803194905496\n",
      "Gradient Descent(18/99): loss=0.3901214591547985, gradient=0.0015155827772484949\n",
      "Gradient Descent(19/99): loss=0.39012128818456926, gradient=0.001457965891121663\n",
      "Gradient Descent(20/99): loss=0.39012112840212587, gradient=0.0014276198765746425\n",
      "Gradient Descent(21/99): loss=0.3901209851104386, gradient=0.0013707445043293943\n",
      "Gradient Descent(22/99): loss=0.390120846913746, gradient=0.0013258207923039164\n",
      "Gradient Descent(23/99): loss=0.3901207160584977, gradient=0.001309783222162717\n",
      "Gradient Descent(24/99): loss=0.39012059877312033, gradient=0.0012599911389752545\n",
      "Gradient Descent(25/99): loss=0.3901204839372694, gradient=0.001223195228627429\n",
      "Gradient Descent(26/99): loss=0.3901203764929645, gradient=0.001193205017622933\n",
      "Gradient Descent(27/99): loss=0.39012027055192783, gradient=0.0012189855209397123\n",
      "Gradient Descent(28/99): loss=0.3901201805796466, gradient=0.0011640187352446494\n",
      "Gradient Descent(29/99): loss=0.39012008731960707, gradient=0.001117480976816076\n",
      "Gradient Descent(30/99): loss=0.39011999683920867, gradient=0.001146617545135416\n",
      "Gradient Descent(31/99): loss=0.39011992054555306, gradient=0.0010965751971247887\n",
      "Gradient Descent(32/99): loss=0.3901198400297586, gradient=0.0010526275241957363\n",
      "Gradient Descent(33/99): loss=0.3901197628131883, gradient=0.0010586507454879966\n",
      "Gradient Descent(34/99): loss=0.3901196917851527, gradient=0.00103965639161826\n",
      "Gradient Descent(35/99): loss=0.39011962634953035, gradient=0.0009995100196173482\n",
      "Gradient Descent(36/99): loss=0.3901195577065569, gradient=0.001009538827525654\n",
      "Gradient Descent(37/99): loss=0.39011949304962956, gradient=0.0010231887509208192\n",
      "Gradient Descent(38/99): loss=0.3901194359476402, gradient=0.000952242602509767\n",
      "Gradient Descent(39/99): loss=0.39011937525838336, gradient=0.0009654329696532468\n",
      "Gradient Descent(40/99): loss=0.39011932157293505, gradient=0.0009540355843968942\n",
      "Gradient Descent(41/99): loss=0.3901192646277387, gradient=0.0009443689849996297\n",
      "Gradient Descent(42/99): loss=0.39011921756770346, gradient=0.0008992461911229896\n",
      "Gradient Descent(43/99): loss=0.3901191630924416, gradient=0.0009404198364746043\n",
      "Gradient Descent(44/99): loss=0.3901191166511068, gradient=0.0009002514062121139\n",
      "Gradient Descent(45/99): loss=0.390119074506271, gradient=0.0008587379066038207\n",
      "Gradient Descent(46/99): loss=0.3901190260164746, gradient=0.0009038668723183476\n",
      "Gradient Descent(47/99): loss=0.39011898786463933, gradient=0.0008408842310134927\n",
      "Gradient Descent(48/99): loss=0.39011894387517754, gradient=0.0008628190440733988\n",
      "Gradient Descent(49/99): loss=0.3901189073858358, gradient=0.0008418652612053377\n",
      "Gradient Descent(50/99): loss=0.39011886633974724, gradient=0.000841918779905738\n",
      "Gradient Descent(51/99): loss=0.39011883446499807, gradient=0.000796150703386087\n",
      "Gradient Descent(52/99): loss=0.39011879441021613, gradient=0.0008476663205803471\n",
      "Gradient Descent(53/99): loss=0.3901187620607646, gradient=0.0008042283794648437\n",
      "Gradient Descent(54/99): loss=0.39011873178702194, gradient=0.0007850121054915499\n",
      "Gradient Descent(55/99): loss=0.39011869720658743, gradient=0.000799006982721228\n",
      "Gradient Descent(56/99): loss=0.3901186715675794, gradient=0.0007493592035231763\n",
      "Gradient Descent(57/99): loss=0.39011863800493113, gradient=0.0007992723690427253\n",
      "Gradient Descent(58/99): loss=0.39011861433316963, gradient=0.0007334519585494678\n",
      "Gradient Descent(59/99): loss=0.3901185841345571, gradient=0.000763654238650117\n",
      "Gradient Descent(60/99): loss=0.3901185582055065, gradient=0.0007675800373210414\n",
      "Gradient Descent(61/99): loss=0.39011853676950164, gradient=0.0007033566908246473\n",
      "Gradient Descent(62/99): loss=0.39011851141065, gradient=0.0007097758807367502\n",
      "Gradient Descent(63/99): loss=0.3901184855284973, gradient=0.0007330976725502649\n",
      "Gradient Descent(64/99): loss=0.3901184645019004, gradient=0.0007134433088454268\n",
      "Gradient Descent(65/99): loss=0.39011844533011486, gradient=0.0006952666102676086\n",
      "Gradient Descent(66/99): loss=0.39011842446289946, gradient=0.0006582238380574553\n",
      "Gradient Descent(67/99): loss=0.39011839912551366, gradient=0.0007692333643178465\n",
      "Gradient Descent(68/99): loss=0.39011838672720905, gradient=0.0006858309128933409\n",
      "Gradient Descent(69/99): loss=0.3901183672291076, gradient=0.0006368866209396704\n",
      "Gradient Descent(70/99): loss=0.390118349196698, gradient=0.0006524247811541735\n",
      "Gradient Descent(71/99): loss=0.39011832738893093, gradient=0.0007311376285234125\n",
      "Gradient Descent(72/99): loss=0.3901183168409707, gradient=0.0006786837281685091\n",
      "Gradient Descent(73/99): loss=0.39011829958603444, gradient=0.0006137135835259695\n",
      "Gradient Descent(74/99): loss=0.3901182855062979, gradient=0.0005999721853497313\n",
      "Gradient Descent(75/99): loss=0.39011826490328017, gradient=0.0007305527331880923\n",
      "Gradient Descent(76/99): loss=0.390118257797154, gradient=0.0006345507411302288\n",
      "Gradient Descent(77/99): loss=0.39011824198418676, gradient=0.0006081165951689438\n",
      "Gradient Descent(78/99): loss=0.39011822976841976, gradient=0.00057927688107522\n",
      "Gradient Descent(79/99): loss=0.39011821265364804, gradient=0.0006886592568943183\n",
      "Gradient Descent(80/99): loss=0.3901182064843043, gradient=0.000636398084719957\n",
      "Gradient Descent(81/99): loss=0.3901181937125909, gradient=0.0005633813335321731\n",
      "Gradient Descent(82/99): loss=0.39011817949477406, gradient=0.000634478798139661\n",
      "Gradient Descent(83/99): loss=0.3901181694935607, gradient=0.0006082543184675446\n",
      "Gradient Descent(84/99): loss=0.3901181632285319, gradient=0.0005503167237962582\n",
      "Gradient Descent(85/99): loss=0.3901181522363535, gradient=0.0005656308971651872\n",
      "Gradient Descent(86/99): loss=0.39011814041337256, gradient=0.0005966058307115233\n",
      "Gradient Descent(87/99): loss=0.39011813460632205, gradient=0.0005640349168538905\n",
      "Gradient Descent(88/99): loss=0.39011812218920194, gradient=0.000598381293400738\n",
      "Gradient Descent(89/99): loss=0.3901181186019831, gradient=0.0005300983650793966\n",
      "Gradient Descent(90/99): loss=0.3901181060561861, gradient=0.0006094022838770201\n",
      "Gradient Descent(91/99): loss=0.3901181029458839, gradient=0.0005231949176230673\n",
      "Gradient Descent(92/99): loss=0.39011809099693695, gradient=0.0006019932711088805\n",
      "Gradient Descent(93/99): loss=0.3901180884217211, gradient=0.0005164946533719989\n",
      "Gradient Descent(94/99): loss=0.390118078553085, gradient=0.0005666693368165073\n",
      "Gradient Descent(95/99): loss=0.3901180755561336, gradient=0.0005343183020611682\n",
      "Gradient Descent(96/99): loss=0.3901180658423162, gradient=0.0005654867514759234\n",
      "Gradient Descent(97/99): loss=0.3901180635655366, gradient=0.000523169167775447\n",
      "Gradient Descent(98/99): loss=0.3901180544386934, gradient=0.0005569589598231831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(99/99): loss=0.39011805391599863, gradient=0.0004869321553561852\n",
      "Gradient Descent(0/99): loss=0.3903536074078572, gradient=0.007246293671638002\n",
      "Gradient Descent(1/99): loss=0.39035009662069686, gradient=0.006248228879074112\n",
      "Gradient Descent(2/99): loss=0.3903473563087593, gradient=0.0055084255254108936\n",
      "Gradient Descent(3/99): loss=0.39034514425507727, gradient=0.004925765685879407\n",
      "Gradient Descent(4/99): loss=0.39034332034313174, gradient=0.004457414698844311\n",
      "Gradient Descent(5/99): loss=0.390341782705386, gradient=0.0040753286477133105\n",
      "Gradient Descent(6/99): loss=0.390340462440808, gradient=0.003759584340598174\n",
      "Gradient Descent(7/99): loss=0.3903393448412304, gradient=0.0033831512900088945\n",
      "Gradient Descent(8/99): loss=0.390338345425065, gradient=0.003191595584346541\n",
      "Gradient Descent(9/99): loss=0.39033745335020453, gradient=0.0030014970329952984\n",
      "Gradient Descent(10/99): loss=0.3903366336840232, gradient=0.002866950696746501\n",
      "Gradient Descent(11/99): loss=0.3903358929349784, gradient=0.0027145166133442514\n",
      "Gradient Descent(12/99): loss=0.3903352058987654, gradient=0.002591888170703926\n",
      "Gradient Descent(13/99): loss=0.3903345705688834, gradient=0.0024851592115156486\n",
      "Gradient Descent(14/99): loss=0.3903339750507998, gradient=0.002406341966405917\n",
      "Gradient Descent(15/99): loss=0.39033342627131157, gradient=0.002303077314489541\n",
      "Gradient Descent(16/99): loss=0.3903329078224312, gradient=0.0022165226823415785\n",
      "Gradient Descent(17/99): loss=0.39033242508278865, gradient=0.0021234353629457714\n",
      "Gradient Descent(18/99): loss=0.39033196939162984, gradient=0.002053487862279544\n",
      "Gradient Descent(19/99): loss=0.3903315588827218, gradient=0.0018869639894360166\n",
      "Gradient Descent(20/99): loss=0.3903311764420672, gradient=0.0018180766765309078\n",
      "Gradient Descent(21/99): loss=0.39033080056721126, gradient=0.0018127426297043039\n",
      "Gradient Descent(22/99): loss=0.39033045435201924, gradient=0.0017216922596630254\n",
      "Gradient Descent(23/99): loss=0.3903301207753285, gradient=0.0016648500378773716\n",
      "Gradient Descent(24/99): loss=0.3903298036307694, gradient=0.0016192378723512658\n",
      "Gradient Descent(25/99): loss=0.39032949863014677, gradient=0.0015885414784078426\n",
      "Gradient Descent(26/99): loss=0.3903292080299043, gradient=0.0015385754794869227\n",
      "Gradient Descent(27/99): loss=0.39032892410555486, gradient=0.0015429801032615343\n",
      "Gradient Descent(28/99): loss=0.3903286608555366, gradient=0.001469134028044258\n",
      "Gradient Descent(29/99): loss=0.39032839550570514, gradient=0.001501235618883637\n",
      "Gradient Descent(30/99): loss=0.3903281529039761, gradient=0.0014140678841288517\n",
      "Gradient Descent(31/99): loss=0.39032791714762766, gradient=0.0013638140297901984\n",
      "Gradient Descent(32/99): loss=0.3903276903749359, gradient=0.001343174088042228\n",
      "Gradient Descent(33/99): loss=0.3903274731155236, gradient=0.0013018489048514752\n",
      "Gradient Descent(34/99): loss=0.3903272601300616, gradient=0.0013092311419589963\n",
      "Gradient Descent(35/99): loss=0.39032706135253525, gradient=0.0012624399104077067\n",
      "Gradient Descent(36/99): loss=0.3903268673206761, gradient=0.0012160053714658568\n",
      "Gradient Descent(37/99): loss=0.39032668167326423, gradient=0.0011871741692739775\n",
      "Gradient Descent(38/99): loss=0.3903264956712022, gradient=0.0012247149050297476\n",
      "Gradient Descent(39/99): loss=0.3903263178894728, gradient=0.0011873045458031712\n",
      "Gradient Descent(40/99): loss=0.3903261533444688, gradient=0.0011362225012011282\n",
      "Gradient Descent(41/99): loss=0.39032599174995325, gradient=0.0010931782203365348\n",
      "Gradient Descent(42/99): loss=0.39032583258720155, gradient=0.0011131332000371237\n",
      "Gradient Descent(43/99): loss=0.3903256847273031, gradient=0.001070755157540145\n",
      "Gradient Descent(44/99): loss=0.39032553924538216, gradient=0.0010265929833857285\n",
      "Gradient Descent(45/99): loss=0.3903253998032767, gradient=0.001003441489384248\n",
      "Gradient Descent(46/99): loss=0.39032526276605445, gradient=0.0010044072028526935\n",
      "Gradient Descent(47/99): loss=0.39032513164532434, gradient=0.0009656208088744424\n",
      "Gradient Descent(48/99): loss=0.3903249998886231, gradient=0.0010025683410468781\n",
      "Gradient Descent(49/99): loss=0.39032487555235734, gradient=0.0009514883202808585\n",
      "Gradient Descent(50/99): loss=0.39032475663087246, gradient=0.0009120545742612897\n",
      "Gradient Descent(51/99): loss=0.39032463715705057, gradient=0.0009482548615740998\n",
      "Gradient Descent(52/99): loss=0.3903245272884825, gradient=0.0009091831595024675\n",
      "Gradient Descent(53/99): loss=0.3903244183762649, gradient=0.0008624908115962231\n",
      "Gradient Descent(54/99): loss=0.39032431209188007, gradient=0.0008676635512674896\n",
      "Gradient Descent(55/99): loss=0.3903242098804843, gradient=0.0008328629079205413\n",
      "Gradient Descent(56/99): loss=0.3903241055855163, gradient=0.0008856652664990584\n",
      "Gradient Descent(57/99): loss=0.3903240112295265, gradient=0.0008347451833192844\n",
      "Gradient Descent(58/99): loss=0.3903239118995204, gradient=0.0008589362070502969\n",
      "Gradient Descent(59/99): loss=0.39032382184634884, gradient=0.0007756896427489771\n",
      "Gradient Descent(60/99): loss=0.39032372761569545, gradient=0.00085121579681568\n",
      "Gradient Descent(61/99): loss=0.3903236454160772, gradient=0.0007606333153937666\n",
      "Gradient Descent(62/99): loss=0.3903235616752653, gradient=0.0007374832280312013\n",
      "Gradient Descent(63/99): loss=0.3903234790625719, gradient=0.000754916414227578\n",
      "Gradient Descent(64/99): loss=0.3903233999039852, gradient=0.0007155402064867961\n",
      "Gradient Descent(65/99): loss=0.390323323049574, gradient=0.0007016687732758446\n",
      "Gradient Descent(66/99): loss=0.3903232460495856, gradient=0.0007229065527803589\n",
      "Gradient Descent(67/99): loss=0.3903231726057558, gradient=0.0006826654957457097\n",
      "Gradient Descent(68/99): loss=0.39032309497801126, gradient=0.000768040955926273\n",
      "Gradient Descent(69/99): loss=0.39032302335390184, gradient=0.0007543824250320662\n",
      "Gradient Descent(70/99): loss=0.3903229551812243, gradient=0.0006544410165870429\n",
      "Gradient Descent(71/99): loss=0.39032288772506546, gradient=0.0006749195681330632\n",
      "Gradient Descent(72/99): loss=0.39032282302083754, gradient=0.0006372721333205282\n",
      "Gradient Descent(73/99): loss=0.39032276012429934, gradient=0.0006250972174826379\n",
      "Gradient Descent(74/99): loss=0.39032269670691455, gradient=0.000650195113340645\n",
      "Gradient Descent(75/99): loss=0.39032263625125624, gradient=0.0006105914517980999\n",
      "Gradient Descent(76/99): loss=0.39032257741991494, gradient=0.0005993128797102793\n",
      "Gradient Descent(77/99): loss=0.39032251347979807, gradient=0.0007033641045347086\n",
      "Gradient Descent(78/99): loss=0.3903224605351242, gradient=0.0005976697507263592\n",
      "Gradient Descent(79/99): loss=0.3903224050074509, gradient=0.0005767690478753688\n",
      "Gradient Descent(80/99): loss=0.3903223495649678, gradient=0.0006074459187082648\n",
      "Gradient Descent(81/99): loss=0.39032229146266617, gradient=0.0006592580240859869\n",
      "Gradient Descent(82/99): loss=0.39032224034215174, gradient=0.0005567564644052085\n",
      "Gradient Descent(83/99): loss=0.3903221881342159, gradient=0.0005902189413653383\n",
      "Gradient Descent(84/99): loss=0.39032213867403515, gradient=0.0005461078477143659\n",
      "Gradient Descent(85/99): loss=0.3903220886335692, gradient=0.0005730862119512321\n",
      "Gradient Descent(86/99): loss=0.3903220365353881, gradient=0.0006222885805772345\n",
      "Gradient Descent(87/99): loss=0.3903219936976674, gradient=0.0005388670482999626\n",
      "Gradient Descent(88/99): loss=0.39032194045068186, gradient=0.0006619390200384213\n",
      "Gradient Descent(89/99): loss=0.39032189950034546, gradient=0.0005318553073234185\n",
      "Gradient Descent(90/99): loss=0.3903218554181227, gradient=0.0005094599815290903\n",
      "Gradient Descent(91/99): loss=0.3903218111918308, gradient=0.0005441395373714751\n",
      "Gradient Descent(92/99): loss=0.3903217690281231, gradient=0.0005009285618608524\n",
      "Gradient Descent(93/99): loss=0.3903217184742294, gradient=0.0006743004070358396\n",
      "Gradient Descent(94/99): loss=0.3903216800892443, gradient=0.0005460511292397644\n",
      "Gradient Descent(95/99): loss=0.3903216399667985, gradient=0.0004886078774577415\n",
      "Gradient Descent(96/99): loss=0.3903216017669076, gradient=0.0004787867007170402\n",
      "Gradient Descent(97/99): loss=0.39032156189945605, gradient=0.0005195309333172408\n",
      "Gradient Descent(98/99): loss=0.39032152439577655, gradient=0.0004725668301457674\n",
      "Gradient Descent(99/99): loss=0.39032148776442577, gradient=0.00046416510725369724\n",
      "Gradient Descent(0/99): loss=0.3899684718956089, gradient=0.009902912201317499\n",
      "Gradient Descent(1/99): loss=0.38996247370378173, gradient=0.00804513409603356\n",
      "Gradient Descent(2/99): loss=0.38995825617349916, gradient=0.006730206848715292\n",
      "Gradient Descent(3/99): loss=0.38995518419922587, gradient=0.005733947294365151\n",
      "Gradient Descent(4/99): loss=0.38995292939897536, gradient=0.004839730354502904\n",
      "Gradient Descent(5/99): loss=0.38995116963981963, gradient=0.004269043907056861\n",
      "Gradient Descent(6/99): loss=0.3899497545560061, gradient=0.003829161627650758\n",
      "Gradient Descent(7/99): loss=0.38994858095295526, gradient=0.003489263702560114\n",
      "Gradient Descent(8/99): loss=0.38994759182885713, gradient=0.003212701406845229\n",
      "Gradient Descent(9/99): loss=0.3899467343970767, gradient=0.00299643664958378\n",
      "Gradient Descent(10/99): loss=0.38994601192474676, gradient=0.002704598415395431\n",
      "Gradient Descent(11/99): loss=0.3899453693386706, gradient=0.002554313881380027\n",
      "Gradient Descent(12/99): loss=0.38994478559849616, gradient=0.002440913182807692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(13/99): loss=0.38994426293554396, gradient=0.0023228494369374533\n",
      "Gradient Descent(14/99): loss=0.3899437770210621, gradient=0.0022472585375257264\n",
      "Gradient Descent(15/99): loss=0.38994334021152227, gradient=0.002139947541417882\n",
      "Gradient Descent(16/99): loss=0.3899429304116509, gradient=0.0020751417229208343\n",
      "Gradient Descent(17/99): loss=0.3899425585669038, gradient=0.0019899994852585513\n",
      "Gradient Descent(18/99): loss=0.3899422087679094, gradient=0.001920236059302087\n",
      "Gradient Descent(19/99): loss=0.38994188521767936, gradient=0.00185730914387912\n",
      "Gradient Descent(20/99): loss=0.38994159081946883, gradient=0.0017813809068476536\n",
      "Gradient Descent(21/99): loss=0.38994130753646705, gradient=0.0017516089943263125\n",
      "Gradient Descent(22/99): loss=0.38994105117678934, gradient=0.001674563670102003\n",
      "Gradient Descent(23/99): loss=0.389940803197489, gradient=0.00164459639653204\n",
      "Gradient Descent(24/99): loss=0.38994057706389773, gradient=0.0015836966744662445\n",
      "Gradient Descent(25/99): loss=0.3899403590089947, gradient=0.0015381395074185187\n",
      "Gradient Descent(26/99): loss=0.3899401537724664, gradient=0.0014993908838544106\n",
      "Gradient Descent(27/99): loss=0.3899399562182932, gradient=0.0014857891456776158\n",
      "Gradient Descent(28/99): loss=0.38993977116289796, gradient=0.0014571797800644517\n",
      "Gradient Descent(29/99): loss=0.38993959886955243, gradient=0.001399869949022853\n",
      "Gradient Descent(30/99): loss=0.38993942882727256, gradient=0.0013900619998197564\n",
      "Gradient Descent(31/99): loss=0.3899392736807313, gradient=0.0013426498527996204\n",
      "Gradient Descent(32/99): loss=0.38993912099286654, gradient=0.0013096114186717227\n",
      "Gradient Descent(33/99): loss=0.3899389724938225, gradient=0.0013146247951229106\n",
      "Gradient Descent(34/99): loss=0.3899388348692211, gradient=0.0012775516039593014\n",
      "Gradient Descent(35/99): loss=0.3899387053298087, gradient=0.0012370113443913098\n",
      "Gradient Descent(36/99): loss=0.3899385775438112, gradient=0.001210430988138649\n",
      "Gradient Descent(37/99): loss=0.38993845318481013, gradient=0.0012126282161052917\n",
      "Gradient Descent(38/99): loss=0.38993833627437147, gradient=0.0011956678589825534\n",
      "Gradient Descent(39/99): loss=0.38993822690069624, gradient=0.0011503272630404573\n",
      "Gradient Descent(40/99): loss=0.38993811580496307, gradient=0.0011537580032592058\n",
      "Gradient Descent(41/99): loss=0.3899380155561391, gradient=0.0011149380780787921\n",
      "Gradient Descent(42/99): loss=0.3899379112664552, gradient=0.0011256021639442972\n",
      "Gradient Descent(43/99): loss=0.389937815999765, gradient=0.001094470249263462\n",
      "Gradient Descent(44/99): loss=0.3899377258163146, gradient=0.001060429187334236\n",
      "Gradient Descent(45/99): loss=0.3899376351141877, gradient=0.001040275813640074\n",
      "Gradient Descent(46/99): loss=0.38993754408877584, gradient=0.0010842083100153035\n",
      "Gradient Descent(47/99): loss=0.3899374663531793, gradient=0.0010360412013742145\n",
      "Gradient Descent(48/99): loss=0.38993738318556176, gradient=0.0009969076579314292\n",
      "Gradient Descent(49/99): loss=0.3899373008307006, gradient=0.0010403276296383668\n",
      "Gradient Descent(50/99): loss=0.38993723084998716, gradient=0.000995457540131923\n",
      "Gradient Descent(51/99): loss=0.3899371550831653, gradient=0.0009569870849054467\n",
      "Gradient Descent(52/99): loss=0.389937081374132, gradient=0.0009701009593917869\n",
      "Gradient Descent(53/99): loss=0.3899370123431223, gradient=0.000960291013179313\n",
      "Gradient Descent(54/99): loss=0.3899369476680509, gradient=0.0009242545338091219\n",
      "Gradient Descent(55/99): loss=0.38993687903361446, gradient=0.0009390715511218345\n",
      "Gradient Descent(56/99): loss=0.389936816032121, gradient=0.0009268904709216958\n",
      "Gradient Descent(57/99): loss=0.38993675678243783, gradient=0.0008919513746144328\n",
      "Gradient Descent(58/99): loss=0.3899366934066629, gradient=0.0009083633488179238\n",
      "Gradient Descent(59/99): loss=0.38993663815284635, gradient=0.0008755019560946844\n",
      "Gradient Descent(60/99): loss=0.38993657709002166, gradient=0.000895587508934065\n",
      "Gradient Descent(61/99): loss=0.3899365224504206, gradient=0.000873620763470002\n",
      "Gradient Descent(62/99): loss=0.3899364707191342, gradient=0.0008446831004407347\n",
      "Gradient Descent(63/99): loss=0.3899364141702361, gradient=0.0008682057783445134\n",
      "Gradient Descent(64/99): loss=0.38993636971998336, gradient=0.0007734419832922876\n",
      "Gradient Descent(65/99): loss=0.38993632953088436, gradient=0.0007534511400034046\n",
      "Gradient Descent(66/99): loss=0.3899362851655054, gradient=0.0007706897275367582\n",
      "Gradient Descent(67/99): loss=0.38993624515404735, gradient=0.0007517059586878935\n",
      "Gradient Descent(68/99): loss=0.3899362073143805, gradient=0.0007296776390636447\n",
      "Gradient Descent(69/99): loss=0.38993616495855565, gradient=0.0007855175970352322\n",
      "Gradient Descent(70/99): loss=0.38993613088872786, gradient=0.0007518356491278946\n",
      "Gradient Descent(71/99): loss=0.3899360897420525, gradient=0.0007504754549922879\n",
      "Gradient Descent(72/99): loss=0.38993605358126454, gradient=0.0007278979919570739\n",
      "Gradient Descent(73/99): loss=0.3899360193687644, gradient=0.0007046515010798907\n",
      "Gradient Descent(74/99): loss=0.38993598089031484, gradient=0.0007323695983177494\n",
      "Gradient Descent(75/99): loss=0.3899359463710974, gradient=0.0007153801415886237\n",
      "Gradient Descent(76/99): loss=0.38993591398421984, gradient=0.0006915382774089057\n",
      "Gradient Descent(77/99): loss=0.3899358766623199, gradient=0.0007551678057185861\n",
      "Gradient Descent(78/99): loss=0.3899358476755763, gradient=0.0007181340357816881\n",
      "Gradient Descent(79/99): loss=0.389935813351832, gradient=0.0006753549669184204\n",
      "Gradient Descent(80/99): loss=0.38993577818817377, gradient=0.0007379757986387588\n",
      "Gradient Descent(81/99): loss=0.389935750694794, gradient=0.000704645743632075\n",
      "Gradient Descent(82/99): loss=0.38993571793697945, gradient=0.0006638580993660125\n",
      "Gradient Descent(83/99): loss=0.3899356841936656, gradient=0.0007289934332155639\n",
      "Gradient Descent(84/99): loss=0.3899356581717708, gradient=0.0006944053555526131\n",
      "Gradient Descent(85/99): loss=0.3899356251013712, gradient=0.0006951359217170665\n",
      "Gradient Descent(86/99): loss=0.3899355962092966, gradient=0.0006813502457920835\n",
      "Gradient Descent(87/99): loss=0.389935569458067, gradient=0.0006517532746727294\n",
      "Gradient Descent(88/99): loss=0.3899355384638636, gradient=0.0006804854816390444\n",
      "Gradient Descent(89/99): loss=0.3899355107037191, gradient=0.0006720547066482516\n",
      "Gradient Descent(90/99): loss=0.38993548524056176, gradient=0.0006422569641234535\n",
      "Gradient Descent(91/99): loss=0.3899354554372136, gradient=0.0006721201061925951\n",
      "Gradient Descent(92/99): loss=0.38993542399150594, gradient=0.0007471917660401586\n",
      "Gradient Descent(93/99): loss=0.3899353972923486, gradient=0.0006287852453352307\n",
      "Gradient Descent(94/99): loss=0.3899353694972383, gradient=0.000659264517866146\n",
      "Gradient Descent(95/99): loss=0.3899353442156755, gradient=0.0006540231443544692\n",
      "Gradient Descent(96/99): loss=0.38993532123945474, gradient=0.0006248809494355456\n",
      "Gradient Descent(97/99): loss=0.3899352929931632, gradient=0.0006974943793876386\n",
      "Gradient Descent(98/99): loss=0.38993527298023345, gradient=0.0006571832076997951\n",
      "Gradient Descent(99/99): loss=0.38993524564839066, gradient=0.0006567489393973184\n",
      "Gradient Descent(0/99): loss=0.3896721716687104, gradient=0.011098899213238565\n",
      "Gradient Descent(1/99): loss=0.3896671123367245, gradient=0.007499413213101411\n",
      "Gradient Descent(2/99): loss=0.38966358609204865, gradient=0.006143767282553856\n",
      "Gradient Descent(3/99): loss=0.3896610122113461, gradient=0.0052076820170196825\n",
      "Gradient Descent(4/99): loss=0.3896590785316974, gradient=0.004498100367720242\n",
      "Gradient Descent(5/99): loss=0.3896576015280312, gradient=0.003922868681804797\n",
      "Gradient Descent(6/99): loss=0.389656452435084, gradient=0.0034538129727902417\n",
      "Gradient Descent(7/99): loss=0.3896555431929154, gradient=0.003069106797118149\n",
      "Gradient Descent(8/99): loss=0.38965481208201763, gradient=0.0027514825000568193\n",
      "Gradient Descent(9/99): loss=0.38965421546060885, gradient=0.0024873055378398586\n",
      "Gradient Descent(10/99): loss=0.3896537219993127, gradient=0.00226584835679038\n",
      "Gradient Descent(11/99): loss=0.3896533088894188, gradient=0.002078695784889823\n",
      "Gradient Descent(12/99): loss=0.3896529592745067, gradient=0.0019192581143649228\n",
      "Gradient Descent(13/99): loss=0.38965266050974534, gradient=0.0017823755092836783\n",
      "Gradient Descent(14/99): loss=0.3896524029685056, gradient=0.0016640002434671617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(15/99): loss=0.38965217921679285, gradient=0.0015609446832402854\n",
      "Gradient Descent(16/99): loss=0.38965198343580437, gradient=0.0014706838995652035\n",
      "Gradient Descent(17/99): loss=0.38965181101338175, gradient=0.0013912028008981692\n",
      "Gradient Descent(18/99): loss=0.38965165462321893, gradient=0.0013526229481631191\n",
      "Gradient Descent(19/99): loss=0.38965152162513084, gradient=0.0012638888385572676\n",
      "Gradient Descent(20/99): loss=0.38965139945107924, gradient=0.001203259176021601\n",
      "Gradient Descent(21/99): loss=0.38965129022998435, gradient=0.0011530865658488732\n",
      "Gradient Descent(22/99): loss=0.38965119124779807, gradient=0.001108268759499915\n",
      "Gradient Descent(23/99): loss=0.3896510976643453, gradient=0.0011072608724947814\n",
      "Gradient Descent(24/99): loss=0.3896510187119723, gradient=0.001038366374573548\n",
      "Gradient Descent(25/99): loss=0.3896509432783402, gradient=0.000999345842492506\n",
      "Gradient Descent(26/99): loss=0.3896508746618875, gradient=0.0009690288439734181\n",
      "Gradient Descent(27/99): loss=0.3896508111051086, gradient=0.0009417448016669398\n",
      "Gradient Descent(28/99): loss=0.38965075224281137, gradient=0.0009168925612205365\n",
      "Gradient Descent(29/99): loss=0.38965069367905614, gradient=0.0009423306173727556\n",
      "Gradient Descent(30/99): loss=0.3896506457100407, gradient=0.0008814064061184313\n",
      "Gradient Descent(31/99): loss=0.3896505976638966, gradient=0.0008551175806346422\n",
      "Gradient Descent(32/99): loss=0.3896505533428273, gradient=0.0008370295105783718\n",
      "Gradient Descent(33/99): loss=0.38965051141058427, gradient=0.0008206832125573602\n",
      "Gradient Descent(34/99): loss=0.38965047185912294, gradient=0.0008055879773382902\n",
      "Gradient Descent(35/99): loss=0.38965043048981024, gradient=0.000846906584442373\n",
      "Gradient Descent(36/99): loss=0.38965039807729357, gradient=0.0007875170454008016\n",
      "Gradient Descent(37/99): loss=0.3896503640549005, gradient=0.0007673414081419459\n",
      "Gradient Descent(38/99): loss=0.38965033241407104, gradient=0.0007554586283552555\n",
      "Gradient Descent(39/99): loss=0.3896503019813246, gradient=0.0007447263199243649\n",
      "Gradient Descent(40/99): loss=0.38965027288898785, gradient=0.0007346854486068898\n",
      "Gradient Descent(41/99): loss=0.3896502409778062, gradient=0.0007863336409218389\n",
      "Gradient Descent(42/99): loss=0.3896502172977802, gradient=0.0007260045876499522\n",
      "Gradient Descent(43/99): loss=0.3896501912847515, gradient=0.0007088580581234437\n",
      "Gradient Descent(44/99): loss=0.38965016700985705, gradient=0.0007003000097646108\n",
      "Gradient Descent(45/99): loss=0.38965014336918696, gradient=0.0006926163477707696\n",
      "Gradient Descent(46/99): loss=0.38965012055593123, gradient=0.0006853492167876211\n",
      "Gradient Descent(47/99): loss=0.38965009847664916, gradient=0.0006784340985978362\n",
      "Gradient Descent(48/99): loss=0.3896500730048614, gradient=0.0007386292975114833\n",
      "Gradient Descent(49/99): loss=0.3896500554105245, gradient=0.0006758305953066376\n",
      "Gradient Descent(50/99): loss=0.3896500366420115, gradient=0.0006409522208847762\n",
      "Gradient Descent(51/99): loss=0.38965001619334233, gradient=0.0006288946192154083\n",
      "Gradient Descent(52/99): loss=0.3896499952440578, gradient=0.0006561941968516266\n",
      "Gradient Descent(53/99): loss=0.3896499797189765, gradient=0.0006256966285402946\n",
      "Gradient Descent(54/99): loss=0.3896499606351073, gradient=0.0006096128026499534\n",
      "Gradient Descent(55/99): loss=0.3896499411461672, gradient=0.0006425583721451102\n",
      "Gradient Descent(56/99): loss=0.3896499271373697, gradient=0.0006083451891853437\n",
      "Gradient Descent(57/99): loss=0.38964990497464874, gradient=0.00067895682359458\n",
      "Gradient Descent(58/99): loss=0.38964989034776576, gradient=0.0006278827501842648\n",
      "Gradient Descent(59/99): loss=0.38964987733228057, gradient=0.0005887043770386539\n",
      "Gradient Descent(60/99): loss=0.38964986141438984, gradient=0.0005780173181529224\n",
      "Gradient Descent(61/99): loss=0.389649844299377, gradient=0.000620132768392565\n",
      "Gradient Descent(62/99): loss=0.3896498328710419, gradient=0.000579714192174311\n",
      "Gradient Descent(63/99): loss=0.38964981759128253, gradient=0.0005652380056053757\n",
      "Gradient Descent(64/99): loss=0.3896498013381111, gradient=0.0006088996896765257\n",
      "Gradient Descent(65/99): loss=0.38964978595212457, gradient=0.0006384211810750524\n",
      "Gradient Descent(66/99): loss=0.38964977579086285, gradient=0.0005596224791737998\n",
      "Gradient Descent(67/99): loss=0.38964975975170885, gradient=0.0006046193998876776\n",
      "Gradient Descent(68/99): loss=0.38964975046074324, gradient=0.0005570324873616779\n",
      "Gradient Descent(69/99): loss=0.3896497369997345, gradient=0.0005418144107934459\n",
      "Gradient Descent(70/99): loss=0.3896497249077222, gradient=0.000537126842695016\n",
      "Gradient Descent(71/99): loss=0.38964971022742995, gradient=0.0005890268301430633\n",
      "Gradient Descent(72/99): loss=0.3896497016154047, gradient=0.0005419929976476531\n",
      "Gradient Descent(73/99): loss=0.3896496845423802, gradient=0.0006216888762720248\n",
      "Gradient Descent(74/99): loss=0.3896496743440751, gradient=0.0005761698282946841\n",
      "Gradient Descent(75/99): loss=0.3896496659755782, gradient=0.0005271477525982589\n",
      "Gradient Descent(76/99): loss=0.38964965450196465, gradient=0.0005174731098968262\n",
      "Gradient Descent(77/99): loss=0.389649643994915, gradient=0.0005135621266110549\n",
      "Gradient Descent(78/99): loss=0.38964963079219334, gradient=0.0005702780350104592\n",
      "Gradient Descent(79/99): loss=0.3896496237099582, gradient=0.000519719051567284\n",
      "Gradient Descent(80/99): loss=0.3896496125729861, gradient=0.0005053438967525428\n",
      "Gradient Descent(81/99): loss=0.38964959641712277, gradient=0.000644982913298966\n",
      "Gradient Descent(82/99): loss=0.3896495928836936, gradient=0.0005458158413911481\n",
      "Gradient Descent(83/99): loss=0.3896495816412811, gradient=0.000498480404291163\n",
      "Gradient Descent(84/99): loss=0.389649572762032, gradient=0.000492841249508731\n",
      "Gradient Descent(85/99): loss=0.389649560745421, gradient=0.0005538910475538793\n",
      "Gradient Descent(86/99): loss=0.38964955495981896, gradient=0.0005000242043013617\n",
      "Gradient Descent(87/99): loss=0.38964954506116517, gradient=0.0004854634754672426\n",
      "Gradient Descent(88/99): loss=0.3896495364056106, gradient=0.00048175553155935393\n",
      "Gradient Descent(89/99): loss=0.3896495212705054, gradient=0.0006314331256047736\n",
      "Gradient Descent(90/99): loss=0.38964951779334944, gradient=0.0005235952104911606\n",
      "Gradient Descent(91/99): loss=0.3896495103895815, gradient=0.00047599794853404203\n",
      "Gradient Descent(92/99): loss=0.38964949890901573, gradient=0.0005608236479181919\n",
      "Gradient Descent(93/99): loss=0.389649495027302, gradient=0.000514235136266976\n",
      "Gradient Descent(94/99): loss=0.3896494841686164, gradient=0.0004920844044318848\n",
      "Gradient Descent(95/99): loss=0.38964947779710357, gradient=0.00047856643257874267\n",
      "Gradient Descent(96/99): loss=0.3896494658612283, gradient=0.0005488229105447979\n",
      "Gradient Descent(97/99): loss=0.38964945637140025, gradient=0.0005778271436850743\n",
      "Gradient Descent(98/99): loss=0.3896494499955194, gradient=0.00046160555902267496\n",
      "Gradient Descent(99/99): loss=0.38964944024125253, gradient=0.000522706043952671\n",
      "Gradient Descent(0/99): loss=0.3889578581371255, gradient=0.008259735881427687\n",
      "Gradient Descent(1/99): loss=0.3889550331972487, gradient=0.005826156475402695\n",
      "Gradient Descent(2/99): loss=0.3889532177278926, gradient=0.004595327012836999\n",
      "Gradient Descent(3/99): loss=0.38895188332867586, gradient=0.003850737897146508\n",
      "Gradient Descent(4/99): loss=0.3889509139408552, gradient=0.0033274207243601554\n",
      "Gradient Descent(5/99): loss=0.3889501820706332, gradient=0.0029187976322800685\n",
      "Gradient Descent(6/99): loss=0.3889496187499096, gradient=0.0025975723172839736\n",
      "Gradient Descent(7/99): loss=0.3889491736582917, gradient=0.0023451625255038887\n",
      "Gradient Descent(8/99): loss=0.38894881328907477, gradient=0.002146640867147611\n",
      "Gradient Descent(9/99): loss=0.38894849967658124, gradient=0.0020787497666530886\n",
      "Gradient Descent(10/99): loss=0.38894825737131133, gradient=0.0018737536518831975\n",
      "Gradient Descent(11/99): loss=0.38894803633881075, gradient=0.001763596254088031\n",
      "Gradient Descent(12/99): loss=0.38894784432480856, gradient=0.0016827738316599068\n",
      "Gradient Descent(13/99): loss=0.3889476715896707, gradient=0.0016167236753148382\n",
      "Gradient Descent(14/99): loss=0.38894751490803026, gradient=0.0015615043565605001\n",
      "Gradient Descent(15/99): loss=0.38894735987850554, gradient=0.0016030953897075307\n",
      "Gradient Descent(16/99): loss=0.38894723668963266, gradient=0.0014886945602907216\n",
      "Gradient Descent(17/99): loss=0.3889471099163438, gradient=0.001439214291347606\n",
      "Gradient Descent(18/99): loss=0.38894699395063287, gradient=0.0014071471055057167\n",
      "Gradient Descent(19/99): loss=0.38894687361490826, gradient=0.001467056133062468\n",
      "Gradient Descent(20/99): loss=0.38894677890698287, gradient=0.0013710192842719881\n",
      "Gradient Descent(21/99): loss=0.38894667687986934, gradient=0.0013317404077970213\n",
      "Gradient Descent(22/99): loss=0.3889465730562042, gradient=0.0013951588594623987\n",
      "Gradient Descent(23/99): loss=0.388946491746999, gradient=0.0013076870742742823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(24/99): loss=0.38894640205052616, gradient=0.001272288841487139\n",
      "Gradient Descent(25/99): loss=0.38894631913372657, gradient=0.0012531819968233516\n",
      "Gradient Descent(26/99): loss=0.3889462290232047, gradient=0.001328759424242875\n",
      "Gradient Descent(27/99): loss=0.38894615335662436, gradient=0.00131410353902216\n",
      "Gradient Descent(28/99): loss=0.38894607781239515, gradient=0.0012155230552294068\n",
      "Gradient Descent(29/99): loss=0.3889459982197107, gradient=0.0012910466086177525\n",
      "Gradient Descent(30/99): loss=0.38894593268480865, gradient=0.0012674301053768866\n",
      "Gradient Descent(31/99): loss=0.38894586506657575, gradient=0.0011824725679530424\n",
      "Gradient Descent(32/99): loss=0.38894580263087825, gradient=0.00116523619326009\n",
      "Gradient Descent(33/99): loss=0.38894572546949646, gradient=0.0013066996537779004\n",
      "Gradient Descent(34/99): loss=0.3889456764551725, gradient=0.0011714612432975615\n",
      "Gradient Descent(35/99): loss=0.3889456123592094, gradient=0.0011819437501152847\n",
      "Gradient Descent(36/99): loss=0.3889455583084437, gradient=0.0011351224724213895\n",
      "Gradient Descent(37/99): loss=0.38894549138825657, gradient=0.0012571461147836568\n",
      "Gradient Descent(38/99): loss=0.3889454493797628, gradient=0.001142824306304589\n",
      "Gradient Descent(39/99): loss=0.38894539661109034, gradient=0.0011052797859258709\n",
      "Gradient Descent(40/99): loss=0.3889453340442898, gradient=0.0012364871742484033\n",
      "Gradient Descent(41/99): loss=0.38894529641815806, gradient=0.0011135849668383352\n",
      "Gradient Descent(42/99): loss=0.38894524466514285, gradient=0.001113114908228335\n",
      "Gradient Descent(43/99): loss=0.3889452011260523, gradient=0.0010781724494442304\n",
      "Gradient Descent(44/99): loss=0.3889451447355308, gradient=0.0012025406899834298\n",
      "Gradient Descent(45/99): loss=0.3889451124783468, gradient=0.0010883214105396435\n",
      "Gradient Descent(46/99): loss=0.38894506699420534, gradient=0.0010743267477910062\n",
      "Gradient Descent(47/99): loss=0.3889450282810812, gradient=0.0010516782191736865\n",
      "Gradient Descent(48/99): loss=0.38894497909330294, gradient=0.0011536707313450488\n",
      "Gradient Descent(49/99): loss=0.38894494780307365, gradient=0.0010825751998259534\n",
      "Gradient Descent(50/99): loss=0.3889449092645529, gradient=0.0010243595800287053\n",
      "Gradient Descent(51/99): loss=0.3889448720302863, gradient=0.0010409514916557277\n",
      "Gradient Descent(52/99): loss=0.3889448269401467, gradient=0.0011360200184645619\n",
      "Gradient Descent(53/99): loss=0.38894480105696494, gradient=0.0010475041686083464\n",
      "Gradient Descent(54/99): loss=0.38894476648587223, gradient=0.001001678384670979\n",
      "Gradient Descent(55/99): loss=0.3889447340495845, gradient=0.0010083214822187848\n",
      "Gradient Descent(56/99): loss=0.3889446924567364, gradient=0.001117676166389354\n",
      "Gradient Descent(57/99): loss=0.3889446709134444, gradient=0.0010178368304368847\n",
      "Gradient Descent(58/99): loss=0.38894463988340194, gradient=0.0009784886174639634\n",
      "Gradient Descent(59/99): loss=0.38894461141133724, gradient=0.0009798025104557878\n",
      "Gradient Descent(60/99): loss=0.3889445729853088, gradient=0.0010991006576223257\n",
      "Gradient Descent(61/99): loss=0.38894455511579834, gradient=0.0009914024108158871\n",
      "Gradient Descent(62/99): loss=0.38894452730079365, gradient=0.0009554928593233561\n",
      "Gradient Descent(63/99): loss=0.38894450387628665, gradient=0.0009343572268909266\n",
      "Gradient Descent(64/99): loss=0.3889444652131335, gradient=0.001097638397404221\n",
      "Gradient Descent(65/99): loss=0.3889444418383708, gradient=0.001115239647126288\n",
      "Gradient Descent(66/99): loss=0.3889444069943257, gradient=0.0009793110906197303\n",
      "Gradient Descent(67/99): loss=0.3889443926672661, gradient=0.0009207815417044737\n",
      "Gradient Descent(68/99): loss=0.3889443722106969, gradient=0.0009328686237277198\n",
      "Gradient Descent(69/99): loss=0.3889443293721165, gradient=0.001168403605563103\n",
      "Gradient Descent(70/99): loss=0.38894431559312265, gradient=0.0009262062421604727\n",
      "Gradient Descent(71/99): loss=0.3889443019821301, gradient=0.0009029638285392778\n",
      "Gradient Descent(72/99): loss=0.3889442863863649, gradient=0.0009093160116553281\n",
      "Gradient Descent(73/99): loss=0.38894426230975804, gradient=0.0010189493579914827\n",
      "Gradient Descent(74/99): loss=0.388944228029817, gradient=0.0010957801303203233\n",
      "Gradient Descent(75/99): loss=0.38894422855918215, gradient=0.0009227696363359736\n",
      "Gradient Descent(76/99): loss=0.38894421253464256, gradient=0.0008904431614723987\n",
      "Gradient Descent(77/99): loss=0.3889441919229581, gradient=0.0010037640875007199\n",
      "Gradient Descent(78/99): loss=0.3889441601189537, gradient=0.0010796090981722718\n",
      "Gradient Descent(79/99): loss=0.38894416294277967, gradient=0.0009064992350887146\n",
      "Gradient Descent(80/99): loss=0.3889441493324502, gradient=0.0008708260243802547\n",
      "Gradient Descent(81/99): loss=0.38894413097288466, gradient=0.0009875717530165966\n",
      "Gradient Descent(82/99): loss=0.388944114367484, gradient=0.0008620842571217159\n",
      "Gradient Descent(83/99): loss=0.388944094913219, gradient=0.0010165146125315659\n",
      "Gradient Descent(84/99): loss=0.3889440887887674, gradient=0.0010134570590535667\n",
      "Gradient Descent(85/99): loss=0.3889440683107971, gradient=0.0008937359684601942\n",
      "Gradient Descent(86/99): loss=0.3889440672873305, gradient=0.0008302448450168541\n",
      "Gradient Descent(87/99): loss=0.3889440594680261, gradient=0.0008438549962446281\n",
      "Gradient Descent(88/99): loss=0.38894402886758594, gradient=0.0010955494880224253\n",
      "Gradient Descent(89/99): loss=0.3889440262584323, gradient=0.0008403198415213353\n",
      "Gradient Descent(90/99): loss=0.3889440234054444, gradient=0.0008154681970253305\n",
      "Gradient Descent(91/99): loss=0.38894400871595075, gradient=0.0009517317918009407\n",
      "Gradient Descent(92/99): loss=0.38894399752097175, gradient=0.000832261082114979\n",
      "Gradient Descent(93/99): loss=0.3889439843375299, gradient=0.000974407626694538\n",
      "Gradient Descent(94/99): loss=0.3889439915079501, gradient=0.0008270054633193319\n",
      "Gradient Descent(95/99): loss=0.3889439747836619, gradient=0.0009327374533335248\n",
      "Gradient Descent(96/99): loss=0.3889439652406675, gradient=0.0008157598838024157\n",
      "Gradient Descent(97/99): loss=0.38894396597650704, gradient=0.0007992410046918523\n",
      "Gradient Descent(98/99): loss=0.3889439350660506, gradient=0.0011699280078338284\n",
      "Gradient Descent(99/99): loss=0.3889439431003744, gradient=0.001080848962775716\n",
      "Gradient Descent(0/99): loss=0.3901385743641483, gradient=0.015741255598235925\n",
      "Gradient Descent(1/99): loss=0.39013539361411625, gradient=0.00689476555283799\n",
      "Gradient Descent(2/99): loss=0.39013313030671676, gradient=0.0052670002093425546\n",
      "Gradient Descent(3/99): loss=0.3901315012241696, gradient=0.004549501963762611\n",
      "Gradient Descent(4/99): loss=0.3901302431346608, gradient=0.004010417286611285\n",
      "Gradient Descent(5/99): loss=0.3901292625358123, gradient=0.003576315602594453\n",
      "Gradient Descent(6/99): loss=0.3901284800108613, gradient=0.0032226453312225504\n",
      "Gradient Descent(7/99): loss=0.39012784452304783, gradient=0.0029322591630190284\n",
      "Gradient Descent(8/99): loss=0.39012731943766576, gradient=0.0026919258945062373\n",
      "Gradient Descent(9/99): loss=0.3901268788973523, gradient=0.0024913515454975032\n",
      "Gradient Descent(10/99): loss=0.39012649262830995, gradient=0.0023787142706162864\n",
      "Gradient Descent(11/99): loss=0.39012618419003076, gradient=0.0021937872576335148\n",
      "Gradient Descent(12/99): loss=0.3901259004857055, gradient=0.002059276307610084\n",
      "Gradient Descent(13/99): loss=0.39012565526784976, gradient=0.0019520188432682218\n",
      "Gradient Descent(14/99): loss=0.3901254240799593, gradient=0.0019392277488136079\n",
      "Gradient Descent(15/99): loss=0.3901252444106843, gradient=0.0017944833195577305\n",
      "Gradient Descent(16/99): loss=0.39012506726015955, gradient=0.0017088077613041707\n",
      "Gradient Descent(17/99): loss=0.39012490387661947, gradient=0.0017022557608419031\n",
      "Gradient Descent(18/99): loss=0.3901247614623124, gradient=0.0015897033674817885\n",
      "Gradient Descent(19/99): loss=0.3901246178331066, gradient=0.0016408457132331348\n",
      "Gradient Descent(20/99): loss=0.39012450921999103, gradient=0.0015538793574209379\n",
      "Gradient Descent(21/99): loss=0.3901243969505005, gradient=0.001458834011098939\n",
      "Gradient Descent(22/99): loss=0.39012428013477773, gradient=0.001557994310264707\n",
      "Gradient Descent(23/99): loss=0.39012420120376967, gradient=0.0014108733954664326\n",
      "Gradient Descent(24/99): loss=0.3901240985406883, gradient=0.0014795667182685214\n",
      "Gradient Descent(25/99): loss=0.390124032572627, gradient=0.0013747185090762163\n",
      "Gradient Descent(26/99): loss=0.390123954846222, gradient=0.0013093801748714868\n",
      "Gradient Descent(27/99): loss=0.39012387090051887, gradient=0.001419547491465545\n",
      "Gradient Descent(28/99): loss=0.39012382011817764, gradient=0.0012851288939319087\n",
      "Gradient Descent(29/99): loss=0.390123742640124, gradient=0.0013786986402014847\n",
      "Gradient Descent(30/99): loss=0.3901237005425675, gradient=0.0012509755250334916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(31/99): loss=0.3901236450888727, gradient=0.0012050546154382881\n",
      "Gradient Descent(32/99): loss=0.3901235952725825, gradient=0.0011925433665427414\n",
      "Gradient Descent(33/99): loss=0.3901235336701942, gradient=0.0013092628582093858\n",
      "Gradient Descent(34/99): loss=0.3901235033213713, gradient=0.0011837741406442908\n",
      "Gradient Descent(35/99): loss=0.39012344572182234, gradient=0.001281890132947675\n",
      "Gradient Descent(36/99): loss=0.39012342149060997, gradient=0.0011586283359460575\n",
      "Gradient Descent(37/99): loss=0.3901233836061712, gradient=0.0011065426638860803\n",
      "Gradient Descent(38/99): loss=0.39012334963732065, gradient=0.0011060205859025989\n",
      "Gradient Descent(39/99): loss=0.39012330316987964, gradient=0.0012332786003783974\n",
      "Gradient Descent(40/99): loss=0.39012328981627986, gradient=0.0010782057916864697\n",
      "Gradient Descent(41/99): loss=0.39012324126220693, gradient=0.0012380717376776682\n",
      "Gradient Descent(42/99): loss=0.390123232666737, gradient=0.0010618494141141767\n",
      "Gradient Descent(43/99): loss=0.39012320305589204, gradient=0.0010584576835645828\n",
      "Gradient Descent(44/99): loss=0.3901231850395344, gradient=0.0010210253915161817\n",
      "Gradient Descent(45/99): loss=0.3901231702315533, gradient=0.0010478675676867922\n",
      "Gradient Descent(46/99): loss=0.3901231280633921, gradient=0.0013245235126305476\n",
      "Gradient Descent(47/99): loss=0.39012313789091047, gradient=0.0011687058357339603\n",
      "Gradient Descent(48/99): loss=0.3901231086123027, gradient=0.0010351076430112722\n",
      "Gradient Descent(49/99): loss=0.39012310268564265, gradient=0.0009866498641094314\n",
      "Gradient Descent(50/99): loss=0.39012308244079574, gradient=0.0009741482051290803\n",
      "Gradient Descent(51/99): loss=0.3901230754325111, gradient=0.0009703568813502167\n",
      "Gradient Descent(52/99): loss=0.3901230342687228, gradient=0.0013052219179517498\n",
      "Gradient Descent(53/99): loss=0.39012305183482326, gradient=0.0010117442520176574\n",
      "Gradient Descent(54/99): loss=0.3901230380187713, gradient=0.0009196473001777817\n",
      "Gradient Descent(55/99): loss=0.3901230245320239, gradient=0.0009176988136094442\n",
      "Gradient Descent(56/99): loss=0.3901230270732746, gradient=0.0008305487126431386\n",
      "Gradient Descent(57/99): loss=0.3901230139177067, gradient=0.0009628495387263484\n",
      "Gradient Descent(58/99): loss=0.3901229806551971, gradient=0.0012365762873250332\n",
      "Gradient Descent(59/99): loss=0.39012299887826, gradient=0.0009852582544853602\n",
      "Gradient Descent(60/99): loss=0.3901229880713919, gradient=0.000893733053543595\n",
      "Gradient Descent(61/99): loss=0.390122984417897, gradient=0.0008013763764112739\n",
      "Gradient Descent(62/99): loss=0.390122985881296, gradient=0.0008331230117476333\n",
      "Gradient Descent(63/99): loss=0.3901229806764704, gradient=0.0008836370838465459\n",
      "Gradient Descent(64/99): loss=0.3901229715673376, gradient=0.0008340822702277133\n",
      "Gradient Descent(65/99): loss=0.39012296352150255, gradient=0.0009511193287867739\n",
      "Gradient Descent(66/99): loss=0.3901229486563087, gradient=0.0011029508181032998\n",
      "Gradient Descent(67/99): loss=0.390122958920865, gradient=0.0007564239375743594\n",
      "Gradient Descent(68/99): loss=0.3901229684376273, gradient=0.0007246929667764575\n",
      "Gradient Descent(69/99): loss=0.39012297112523386, gradient=0.0007795633059363429\n",
      "Gradient Descent(70/99): loss=0.39012296865932833, gradient=0.0008513298201912495\n",
      "Gradient Descent(71/99): loss=0.3901229480200778, gradient=0.0010184787618007208\n",
      "Gradient Descent(72/99): loss=0.3901229703800538, gradient=0.0007552748148125886\n",
      "Gradient Descent(73/99): loss=0.39012296023086956, gradient=0.0009249546241856793\n",
      "Gradient Descent(74/99): loss=0.3901229670694324, gradient=0.000956488561289302\n",
      "Gradient Descent(75/99): loss=0.3901229648877181, gradient=0.0007242606872847095\n",
      "Gradient Descent(76/99): loss=0.3901229744459551, gradient=0.0007354632586904882\n",
      "Gradient Descent(77/99): loss=0.3901229741586314, gradient=0.0008190359392801543\n",
      "Gradient Descent(78/99): loss=0.390122971915513, gradient=0.0007434477084182298\n",
      "Gradient Descent(79/99): loss=0.3901229695585283, gradient=0.000885934884468299\n",
      "Gradient Descent(80/99): loss=0.39012299188914806, gradient=0.0006755135136929488\n",
      "Gradient Descent(81/99): loss=0.39012296567752824, gradient=0.0010583380849515172\n",
      "Gradient Descent(82/99): loss=0.3901229843262923, gradient=0.0006505812126546169\n",
      "Gradient Descent(83/99): loss=0.39012299347853246, gradient=0.0006981844848970881\n",
      "Gradient Descent(84/99): loss=0.39012299657543476, gradient=0.0007941232432390544\n",
      "Gradient Descent(85/99): loss=0.39012299662023686, gradient=0.0007111060725953148\n",
      "Gradient Descent(86/99): loss=0.39012299643587145, gradient=0.0008608410788037296\n",
      "Gradient Descent(87/99): loss=0.390123020763176, gradient=0.0006428566456959529\n",
      "Gradient Descent(88/99): loss=0.39012301551040124, gradient=0.0008277220064799581\n",
      "Gradient Descent(89/99): loss=0.3901230051802941, gradient=0.0009120497994563222\n",
      "Gradient Descent(90/99): loss=0.3901230286495633, gradient=0.0007187622556434184\n",
      "Gradient Descent(91/99): loss=0.39012303995220887, gradient=0.0006021946101343785\n",
      "Gradient Descent(92/99): loss=0.39012304356451805, gradient=0.0007615213448695918\n",
      "Gradient Descent(93/99): loss=0.390123027352331, gradient=0.0009544244718216761\n",
      "Gradient Descent(94/99): loss=0.3901230557620382, gradient=0.0006423784365636537\n",
      "Gradient Descent(95/99): loss=0.3901230626244874, gradient=0.0006487504657587274\n",
      "Gradient Descent(96/99): loss=0.3901230664589836, gradient=0.0007634702143313938\n",
      "Gradient Descent(97/99): loss=0.39012306736848806, gradient=0.0006642100879964909\n",
      "Gradient Descent(98/99): loss=0.3901230675079716, gradient=0.0008336060918287028\n",
      "Gradient Descent(99/99): loss=0.3901230838861975, gradient=0.0008076525520540719\n",
      "Gradient Descent(0/99): loss=0.3903634926900599, gradient=0.007213855788375745\n",
      "Gradient Descent(1/99): loss=0.3903601129042788, gradient=0.0062422702001430725\n",
      "Gradient Descent(2/99): loss=0.3903575005546417, gradient=0.005477375453445329\n",
      "Gradient Descent(3/99): loss=0.39035538148719584, gradient=0.00489121160637803\n",
      "Gradient Descent(4/99): loss=0.3903536297192985, gradient=0.004426681793512796\n",
      "Gradient Descent(5/99): loss=0.3903521475908629, gradient=0.004045309389607667\n",
      "Gradient Descent(6/99): loss=0.3903509347515754, gradient=0.003523340308565518\n",
      "Gradient Descent(7/99): loss=0.39034985372800957, gradient=0.0033031323491533268\n",
      "Gradient Descent(8/99): loss=0.390348884470162, gradient=0.0030991182986376655\n",
      "Gradient Descent(9/99): loss=0.3903479992112121, gradient=0.0029424185089084592\n",
      "Gradient Descent(10/99): loss=0.39034719017437536, gradient=0.0027888607641263427\n",
      "Gradient Descent(11/99): loss=0.3903464392973799, gradient=0.002670758059827114\n",
      "Gradient Descent(12/99): loss=0.3903457252398274, gradient=0.0026312625691021834\n",
      "Gradient Descent(13/99): loss=0.3903450918401961, gradient=0.002437999031020838\n",
      "Gradient Descent(14/99): loss=0.3903444669064491, gradient=0.002428606858758127\n",
      "Gradient Descent(15/99): loss=0.39034389955145293, gradient=0.0023162568050594796\n",
      "Gradient Descent(16/99): loss=0.3903433601662637, gradient=0.0021847197073560968\n",
      "Gradient Descent(17/99): loss=0.3903428521977733, gradient=0.0021078270473235353\n",
      "Gradient Descent(18/99): loss=0.3903423553444535, gradient=0.0021105992254216222\n",
      "Gradient Descent(19/99): loss=0.39034189514359874, gradient=0.001989547079233451\n",
      "Gradient Descent(20/99): loss=0.3903414528050001, gradient=0.001946737766287675\n",
      "Gradient Descent(21/99): loss=0.3903410551401135, gradient=0.0017671119029532521\n",
      "Gradient Descent(22/99): loss=0.3903406853680003, gradient=0.0016790788338784748\n",
      "Gradient Descent(23/99): loss=0.39034033463413503, gradient=0.0016150485750171203\n",
      "Gradient Descent(24/99): loss=0.3903399874098556, gradient=0.001645132772009976\n",
      "Gradient Descent(25/99): loss=0.390339646087103, gradient=0.0016710216322443515\n",
      "Gradient Descent(26/99): loss=0.390339348493647, gradient=0.0015051384720589913\n",
      "Gradient Descent(27/99): loss=0.39033903810185605, gradient=0.0015451413521833824\n",
      "Gradient Descent(28/99): loss=0.39033875465926415, gradient=0.0014247931888341785\n",
      "Gradient Descent(29/99): loss=0.39033847719261977, gradient=0.0014107808462382133\n",
      "Gradient Descent(30/99): loss=0.3903381848809296, gradient=0.0015727409766510734\n",
      "Gradient Descent(31/99): loss=0.39033791729680867, gradient=0.0015401135611600725\n",
      "Gradient Descent(32/99): loss=0.3903376714010667, gradient=0.0013171814778412444\n",
      "Gradient Descent(33/99): loss=0.3903374266533625, gradient=0.0013570187606723335\n",
      "Gradient Descent(34/99): loss=0.390337197989015, gradient=0.0012649189207239913\n",
      "Gradient Descent(35/99): loss=0.390336975743152, gradient=0.0012350668076432357\n",
      "Gradient Descent(36/99): loss=0.39033675262316386, gradient=0.001285558315985841\n",
      "Gradient Descent(37/99): loss=0.39033654315598, gradient=0.00119463392694036\n",
      "Gradient Descent(38/99): loss=0.39033634170954873, gradient=0.0011626112249672942\n",
      "Gradient Descent(39/99): loss=0.39033613581006826, gradient=0.0012273448544630045\n",
      "Gradient Descent(40/99): loss=0.3903359458417694, gradient=0.001125665768639226\n",
      "Gradient Descent(41/99): loss=0.3903357599205095, gradient=0.001107580475255839\n",
      "Gradient Descent(42/99): loss=0.3903355722530064, gradient=0.0011654502834937297\n",
      "Gradient Descent(43/99): loss=0.39033539635641945, gradient=0.0010765520925884275\n",
      "Gradient Descent(44/99): loss=0.39033522740015186, gradient=0.0010448291365676907\n",
      "Gradient Descent(45/99): loss=0.3903350525367032, gradient=0.0011242383274418636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(46/99): loss=0.39033489242165104, gradient=0.0010176166409641594\n",
      "Gradient Descent(47/99): loss=0.39033473709226224, gradient=0.000982559702408727\n",
      "Gradient Descent(48/99): loss=0.39033457339729505, gradient=0.0010925554869682908\n",
      "Gradient Descent(49/99): loss=0.390334426065385, gradient=0.0009621171398182063\n",
      "Gradient Descent(50/99): loss=0.3903342589897456, gradient=0.0011957384862644324\n",
      "Gradient Descent(51/99): loss=0.3903341002042836, gradient=0.0011559448857173125\n",
      "Gradient Descent(52/99): loss=0.3903339436640092, gradient=0.0012270197198945502\n",
      "Gradient Descent(53/99): loss=0.390333823220961, gradient=0.00094005005075379\n",
      "Gradient Descent(54/99): loss=0.3903336950971514, gradient=0.0009070598879866021\n",
      "Gradient Descent(55/99): loss=0.3903335631945022, gradient=0.0009997836057453614\n",
      "Gradient Descent(56/99): loss=0.3903334409759397, gradient=0.0008902921591707859\n",
      "Gradient Descent(57/99): loss=0.3903333231010205, gradient=0.0008641389742186197\n",
      "Gradient Descent(58/99): loss=0.39033319632464564, gradient=0.0009741578058779447\n",
      "Gradient Descent(59/99): loss=0.39033308291373625, gradient=0.0008494494660438301\n",
      "Gradient Descent(60/99): loss=0.3903329701695812, gradient=0.0008438621184068438\n",
      "Gradient Descent(61/99): loss=0.3903328524160366, gradient=0.0009357430836656009\n",
      "Gradient Descent(62/99): loss=0.3903327439425171, gradient=0.0008329594312279494\n",
      "Gradient Descent(63/99): loss=0.3903326398622892, gradient=0.000804345359951706\n",
      "Gradient Descent(64/99): loss=0.3903325269501511, gradient=0.0009218398204525885\n",
      "Gradient Descent(65/99): loss=0.3903324268549823, gradient=0.0007953944441399065\n",
      "Gradient Descent(66/99): loss=0.39033232716185207, gradient=0.0007924679222483583\n",
      "Gradient Descent(67/99): loss=0.39033222242351423, gradient=0.0008884821376734581\n",
      "Gradient Descent(68/99): loss=0.390332106053073, gradient=0.0010212141720552371\n",
      "Gradient Descent(69/99): loss=0.39033201391229877, gradient=0.0007692932503879695\n",
      "Gradient Descent(70/99): loss=0.3903319051994304, gradient=0.0010027679847351648\n",
      "Gradient Descent(71/99): loss=0.3903318175332397, gradient=0.0009143431730376932\n",
      "Gradient Descent(72/99): loss=0.3903317196464382, gradient=0.0009079816812903162\n",
      "Gradient Descent(73/99): loss=0.390331630589806, gradient=0.0007674565612225495\n",
      "Gradient Descent(74/99): loss=0.39033154537273496, gradient=0.0008574385539858484\n",
      "Gradient Descent(75/99): loss=0.3903314658160154, gradient=0.00074070736383648\n",
      "Gradient Descent(76/99): loss=0.39033137856329875, gradient=0.0008804106043901866\n",
      "Gradient Descent(77/99): loss=0.3903312857239973, gradient=0.0008776222595790026\n",
      "Gradient Descent(78/99): loss=0.3903312164511264, gradient=0.0007112963299859275\n",
      "Gradient Descent(79/99): loss=0.39033114309684075, gradient=0.0007135655476652138\n",
      "Gradient Descent(80/99): loss=0.39033105287068653, gradient=0.0009682742821024744\n",
      "Gradient Descent(81/99): loss=0.39033097484832935, gradient=0.000742020339925311\n",
      "Gradient Descent(82/99): loss=0.390330911215758, gradient=0.0006849376281832996\n",
      "Gradient Descent(83/99): loss=0.3903308341587445, gradient=0.0008243347181941802\n",
      "Gradient Descent(84/99): loss=0.3903307582616131, gradient=0.000849908638358154\n",
      "Gradient Descent(85/99): loss=0.3903306854816311, gradient=0.0007179710155832892\n",
      "Gradient Descent(86/99): loss=0.39033060564476546, gradient=0.0009378549771226099\n",
      "Gradient Descent(87/99): loss=0.3903305353497985, gradient=0.0008074946913680789\n",
      "Gradient Descent(88/99): loss=0.3903304647981317, gradient=0.0008485871668426708\n",
      "Gradient Descent(89/99): loss=0.39033039893830923, gradient=0.0006871489312387714\n",
      "Gradient Descent(90/99): loss=0.39033033527021355, gradient=0.0008089559953688704\n",
      "Gradient Descent(91/99): loss=0.3903302462146552, gradient=0.0010512398323372601\n",
      "Gradient Descent(92/99): loss=0.3903301985577941, gradient=0.0006642791740336087\n",
      "Gradient Descent(93/99): loss=0.3903301370961976, gradient=0.0007974784273758342\n",
      "Gradient Descent(94/99): loss=0.39033008591077456, gradient=0.0006541572107332914\n",
      "Gradient Descent(95/99): loss=0.39033002314196413, gradient=0.000807526273819412\n",
      "Gradient Descent(96/99): loss=0.3903299561893944, gradient=0.0008035025802899626\n",
      "Gradient Descent(97/99): loss=0.39032990786801686, gradient=0.0006558863754530118\n",
      "Gradient Descent(98/99): loss=0.3903298489790968, gradient=0.0007906166784844138\n",
      "Gradient Descent(99/99): loss=0.39032979230481224, gradient=0.0006659385006860354\n",
      "Gradient Descent(0/99): loss=0.38997229965551705, gradient=0.009672191777759704\n",
      "Gradient Descent(1/99): loss=0.3899663685893474, gradient=0.007935910564248397\n",
      "Gradient Descent(2/99): loss=0.3899622062942784, gradient=0.006639967640076467\n",
      "Gradient Descent(3/99): loss=0.38995931595928485, gradient=0.0054258700381157555\n",
      "Gradient Descent(4/99): loss=0.38995712413177985, gradient=0.004715533768625846\n",
      "Gradient Descent(5/99): loss=0.38995552866010114, gradient=0.003934981505643668\n",
      "Gradient Descent(6/99): loss=0.3899542123564006, gradient=0.0035326581452968134\n",
      "Gradient Descent(7/99): loss=0.38995315205850806, gradient=0.0031833193461244876\n",
      "Gradient Descent(8/99): loss=0.3899522255390761, gradient=0.0029934344423643835\n",
      "Gradient Descent(9/99): loss=0.3899514573578214, gradient=0.0027370472566737923\n",
      "Gradient Descent(10/99): loss=0.3899507733157799, gradient=0.0025631104137902525\n",
      "Gradient Descent(11/99): loss=0.38995015470041505, gradient=0.0024969097729275517\n",
      "Gradient Descent(12/99): loss=0.38994962666503574, gradient=0.0023216523894833193\n",
      "Gradient Descent(13/99): loss=0.3899491353774587, gradient=0.002234953001653344\n",
      "Gradient Descent(14/99): loss=0.38994869789597153, gradient=0.002119941399005756\n",
      "Gradient Descent(15/99): loss=0.389948296505407, gradient=0.0020406238468971155\n",
      "Gradient Descent(16/99): loss=0.3899479278288189, gradient=0.0019760519663874505\n",
      "Gradient Descent(17/99): loss=0.3899475888688792, gradient=0.0019078321079075637\n",
      "Gradient Descent(18/99): loss=0.3899472770790976, gradient=0.0018385863300277737\n",
      "Gradient Descent(19/99): loss=0.38994696864017264, gradient=0.0019020045517585385\n",
      "Gradient Descent(20/99): loss=0.38994671329066155, gradient=0.0017397824687347297\n",
      "Gradient Descent(21/99): loss=0.3899464464351935, gradient=0.001794969325950353\n",
      "Gradient Descent(22/99): loss=0.38994622208506124, gradient=0.0016869611397358334\n",
      "Gradient Descent(23/99): loss=0.3899460050638074, gradient=0.0015801330455370367\n",
      "Gradient Descent(24/99): loss=0.38994580574868604, gradient=0.0015314650864180271\n",
      "Gradient Descent(25/99): loss=0.38994560687996055, gradient=0.0015645565355095225\n",
      "Gradient Descent(26/99): loss=0.3899454115245041, gradient=0.0015842439038337868\n",
      "Gradient Descent(27/99): loss=0.38994525583511225, gradient=0.0014408090577772788\n",
      "Gradient Descent(28/99): loss=0.38994508433979985, gradient=0.001476412764705729\n",
      "Gradient Descent(29/99): loss=0.38994493416820325, gradient=0.0013656256084346725\n",
      "Gradient Descent(30/99): loss=0.3899447914994151, gradient=0.0013314456635752112\n",
      "Gradient Descent(31/99): loss=0.3899446562369192, gradient=0.0013002179321742729\n",
      "Gradient Descent(32/99): loss=0.38994451412753456, gradient=0.0013813152227917362\n",
      "Gradient Descent(33/99): loss=0.3899443720934009, gradient=0.0014080576003599574\n",
      "Gradient Descent(34/99): loss=0.38994425034957897, gradient=0.0013531504291470818\n",
      "Gradient Descent(35/99): loss=0.38994416764032874, gradient=0.001049380562515918\n",
      "Gradient Descent(36/99): loss=0.3899440746749727, gradient=0.0011592151884608332\n",
      "Gradient Descent(37/99): loss=0.3899439974680392, gradient=0.0010061580194767245\n",
      "Gradient Descent(38/99): loss=0.38994392343421796, gradient=0.0009828837518302924\n",
      "Gradient Descent(39/99): loss=0.3899438378399398, gradient=0.001120285789818059\n",
      "Gradient Descent(40/99): loss=0.389943761568162, gradient=0.0011039144662041284\n",
      "Gradient Descent(41/99): loss=0.3899436898980071, gradient=0.0011123441867433206\n",
      "Gradient Descent(42/99): loss=0.3899436266397702, gradient=0.0009332241572074174\n",
      "Gradient Descent(43/99): loss=0.38994354909258794, gradient=0.0011155920290404468\n",
      "Gradient Descent(44/99): loss=0.38994346857577294, gradient=0.0011212675127630805\n",
      "Gradient Descent(45/99): loss=0.3899434217745594, gradient=0.0009318879157840423\n",
      "Gradient Descent(46/99): loss=0.38994333597239583, gradient=0.0012173302287858947\n",
      "Gradient Descent(47/99): loss=0.3899432920764637, gradient=0.0009112857125297165\n",
      "Gradient Descent(48/99): loss=0.3899432403826446, gradient=0.0008673945514364554\n",
      "Gradient Descent(49/99): loss=0.3899431918233505, gradient=0.0008533311289636906\n",
      "Gradient Descent(50/99): loss=0.38994312904382167, gradient=0.0010213884392603122\n",
      "Gradient Descent(51/99): loss=0.38994307538490436, gradient=0.0010430269735047144\n",
      "Gradient Descent(52/99): loss=0.38994301300781176, gradient=0.0010365101825642992\n",
      "Gradient Descent(53/99): loss=0.3899429638678049, gradient=0.0010796757993424878\n",
      "Gradient Descent(54/99): loss=0.3899429107842015, gradient=0.0008594478831514895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(55/99): loss=0.38994285792936967, gradient=0.0009776083838886709\n",
      "Gradient Descent(56/99): loss=0.38994282622643106, gradient=0.0008288272944199658\n",
      "Gradient Descent(57/99): loss=0.38994277067806116, gradient=0.0009909305971842134\n",
      "Gradient Descent(58/99): loss=0.38994271719936846, gradient=0.000988845616773652\n",
      "Gradient Descent(59/99): loss=0.3899426758210327, gradient=0.0010270550637624637\n",
      "Gradient Descent(60/99): loss=0.3899426162984384, gradient=0.0010419387601558793\n",
      "Gradient Descent(61/99): loss=0.38994259130309944, gradient=0.0008249092152340784\n",
      "Gradient Descent(62/99): loss=0.389942540659884, gradient=0.0009662498247092384\n",
      "Gradient Descent(63/99): loss=0.3899425075335177, gradient=0.0007686171927739232\n",
      "Gradient Descent(64/99): loss=0.389942474019684, gradient=0.0007588102584161264\n",
      "Gradient Descent(65/99): loss=0.38994240993225476, gradient=0.0010999654816793351\n",
      "Gradient Descent(66/99): loss=0.38994236717135466, gradient=0.0009326673962721865\n",
      "Gradient Descent(67/99): loss=0.389942346070047, gradient=0.0007776996737415589\n",
      "Gradient Descent(68/99): loss=0.3899423003067706, gradient=0.0009474802454229326\n",
      "Gradient Descent(69/99): loss=0.38994226085026235, gradient=0.0009019839170546363\n",
      "Gradient Descent(70/99): loss=0.3899422229107515, gradient=0.0007636014814703691\n",
      "Gradient Descent(71/99): loss=0.3899421809239901, gradient=0.0009234582735653018\n",
      "Gradient Descent(72/99): loss=0.38994214653054193, gradient=0.0008974598587060844\n",
      "Gradient Descent(73/99): loss=0.3899421281638303, gradient=0.0007405072594914763\n",
      "Gradient Descent(74/99): loss=0.389942076324352, gradient=0.0010436935028689007\n",
      "Gradient Descent(75/99): loss=0.38994204228349416, gradient=0.0007472504915928812\n",
      "Gradient Descent(76/99): loss=0.38994201870156225, gradient=0.0007105674096635915\n",
      "Gradient Descent(77/99): loss=0.3899419526331085, gradient=0.0011839531186495806\n",
      "Gradient Descent(78/99): loss=0.3899419391320026, gradient=0.0007506254114737307\n",
      "Gradient Descent(79/99): loss=0.38994190063732803, gradient=0.0009155506817618702\n",
      "Gradient Descent(80/99): loss=0.3899418787878602, gradient=0.0007033421812467099\n",
      "Gradient Descent(81/99): loss=0.38994184776335894, gradient=0.0008322249737201563\n",
      "Gradient Descent(82/99): loss=0.38994181607685907, gradient=0.0007209580441734953\n",
      "Gradient Descent(83/99): loss=0.38994177046581746, gradient=0.0010500526710184796\n",
      "Gradient Descent(84/99): loss=0.38994175613687326, gradient=0.0009969272044585973\n",
      "Gradient Descent(85/99): loss=0.3899417069403158, gradient=0.0009570133321273376\n",
      "Gradient Descent(86/99): loss=0.38994168965841874, gradient=0.0006895604546872008\n",
      "Gradient Descent(87/99): loss=0.38994166915890155, gradient=0.0006764894489739123\n",
      "Gradient Descent(88/99): loss=0.3899416239719442, gradient=0.0009845231639463306\n",
      "Gradient Descent(89/99): loss=0.38994159277810814, gradient=0.0008769164760373594\n",
      "Gradient Descent(90/99): loss=0.38994158323282907, gradient=0.0007060871327392335\n",
      "Gradient Descent(91/99): loss=0.3899415421026758, gradient=0.0009736164328369471\n",
      "Gradient Descent(92/99): loss=0.38994151548830874, gradient=0.0007020016116385971\n",
      "Gradient Descent(93/99): loss=0.38994149907169845, gradient=0.0006635593799366812\n",
      "Gradient Descent(94/99): loss=0.3899414564264295, gradient=0.0009630289599983574\n",
      "Gradient Descent(95/99): loss=0.3899414272335401, gradient=0.0008652559422374392\n",
      "Gradient Descent(96/99): loss=0.3899414040283152, gradient=0.0009093463273621705\n",
      "Gradient Descent(97/99): loss=0.3899413810821269, gradient=0.0007556981040779285\n",
      "Gradient Descent(98/99): loss=0.3899413565883064, gradient=0.0006868536119614783\n",
      "Gradient Descent(99/99): loss=0.3899413420859317, gradient=0.0006488817457338319\n",
      "Gradient Descent(0/99): loss=0.38967528390299716, gradient=0.011180666381559061\n",
      "Gradient Descent(1/99): loss=0.3896703094867346, gradient=0.0074918718626938935\n",
      "Gradient Descent(2/99): loss=0.3896669392229151, gradient=0.006014405304823569\n",
      "Gradient Descent(3/99): loss=0.3896644277477806, gradient=0.005093308263083959\n",
      "Gradient Descent(4/99): loss=0.3896625522673624, gradient=0.0044043906505962585\n",
      "Gradient Descent(5/99): loss=0.38966111457666924, gradient=0.0038505655205135682\n",
      "Gradient Descent(6/99): loss=0.3896599958719716, gradient=0.0034007575248296736\n",
      "Gradient Descent(7/99): loss=0.38965911061928576, gradient=0.0030331516455523815\n",
      "Gradient Descent(8/99): loss=0.3896583997888479, gradient=0.00273070169519786\n",
      "Gradient Descent(9/99): loss=0.3896578213108175, gradient=0.0024800207939471137\n",
      "Gradient Descent(10/99): loss=0.38965734493210175, gradient=0.002270627238938718\n",
      "Gradient Descent(11/99): loss=0.3896569485184901, gradient=0.002094335817226683\n",
      "Gradient Descent(12/99): loss=0.3896566156300263, gradient=0.0019447621175141116\n",
      "Gradient Descent(13/99): loss=0.3896563338624385, gradient=0.0018169224678439321\n",
      "Gradient Descent(14/99): loss=0.3896560937142861, gradient=0.0017069151557000358\n",
      "Gradient Descent(15/99): loss=0.38965587046931677, gradient=0.0017300998189388622\n",
      "Gradient Descent(16/99): loss=0.38965570501232955, gradient=0.0015454760828656315\n",
      "Gradient Descent(17/99): loss=0.3896555497119833, gradient=0.0014906354260088525\n",
      "Gradient Descent(18/99): loss=0.3896554240240301, gradient=0.0014033607407689644\n",
      "Gradient Descent(19/99): loss=0.3896553062136556, gradient=0.0013463595397283118\n",
      "Gradient Descent(20/99): loss=0.38965520925782254, gradient=0.0012993644619354499\n",
      "Gradient Descent(21/99): loss=0.38965511820939597, gradient=0.0012363001602032477\n",
      "Gradient Descent(22/99): loss=0.38965504235943477, gradient=0.0012196313293398762\n",
      "Gradient Descent(23/99): loss=0.38965495553687807, gradient=0.001326367030152663\n",
      "Gradient Descent(24/99): loss=0.3896549047168342, gradient=0.0012134591654556163\n",
      "Gradient Descent(25/99): loss=0.38965484724125404, gradient=0.0010969339800834038\n",
      "Gradient Descent(26/99): loss=0.3896548074994028, gradient=0.0010317665697279301\n",
      "Gradient Descent(27/99): loss=0.3896547673922353, gradient=0.0010756001958726461\n",
      "Gradient Descent(28/99): loss=0.3896547161584169, gradient=0.0010578057864899656\n",
      "Gradient Descent(29/99): loss=0.3896546903585932, gradient=0.0010641474597295841\n",
      "Gradient Descent(30/99): loss=0.38965464571125463, gradient=0.0010306966787541052\n",
      "Gradient Descent(31/99): loss=0.38965463230016417, gradient=0.000917007233857607\n",
      "Gradient Descent(32/99): loss=0.3896545921351714, gradient=0.0010021382905878575\n",
      "Gradient Descent(33/99): loss=0.38965457575618484, gradient=0.0008422633741875183\n",
      "Gradient Descent(34/99): loss=0.38965455855411835, gradient=0.0008131117829081762\n",
      "Gradient Descent(35/99): loss=0.389654526967774, gradient=0.000975262129580805\n",
      "Gradient Descent(36/99): loss=0.38965449725356555, gradient=0.0010211111685949528\n",
      "Gradient Descent(37/99): loss=0.3896544984738908, gradient=0.0007981619393063723\n",
      "Gradient Descent(38/99): loss=0.38965446962422884, gradient=0.0009582512795181533\n",
      "Gradient Descent(39/99): loss=0.3896544656066875, gradient=0.000750852284620124\n",
      "Gradient Descent(40/99): loss=0.3896544590844876, gradient=0.0007302083751314243\n",
      "Gradient Descent(41/99): loss=0.3896544345348265, gradient=0.0009410302042178006\n",
      "Gradient Descent(42/99): loss=0.3896544323745479, gradient=0.0007182083311002244\n",
      "Gradient Descent(43/99): loss=0.3896544282485684, gradient=0.0007004902592428648\n",
      "Gradient Descent(44/99): loss=0.3896544168546991, gradient=0.0008081018675544399\n",
      "Gradient Descent(45/99): loss=0.3896544129006861, gradient=0.0008897664560379694\n",
      "Gradient Descent(46/99): loss=0.3896543969296771, gradient=0.0007179138427414667\n",
      "Gradient Descent(47/99): loss=0.3896543902892601, gradient=0.0007829619659685869\n",
      "Gradient Descent(48/99): loss=0.3896543879793645, gradient=0.0008693612444302234\n",
      "Gradient Descent(49/99): loss=0.3896543741243802, gradient=0.0006985281525063796\n",
      "Gradient Descent(50/99): loss=0.38965436902074224, gradient=0.0007692047820002059\n",
      "Gradient Descent(51/99): loss=0.38965435641489377, gradient=0.000904852727860119\n",
      "Gradient Descent(52/99): loss=0.38965435706274526, gradient=0.0008379420373689786\n",
      "Gradient Descent(53/99): loss=0.3896543394431954, gradient=0.0008566429547341866\n",
      "Gradient Descent(54/99): loss=0.3896543529376767, gradient=0.0006796883251570253\n",
      "Gradient Descent(55/99): loss=0.38965435060460807, gradient=0.0006183186427186214\n",
      "Gradient Descent(56/99): loss=0.3896543304972279, gradient=0.0008973794398896295\n",
      "Gradient Descent(57/99): loss=0.3896543348310171, gradient=0.0006150042869030806\n",
      "Gradient Descent(58/99): loss=0.389654336808408, gradient=0.0006028181859104349\n",
      "Gradient Descent(59/99): loss=0.3896543172002675, gradient=0.0008874748057455818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(60/99): loss=0.38965432259639105, gradient=0.0006032372115858829\n",
      "Gradient Descent(61/99): loss=0.3896543253846264, gradient=0.0005907189701679014\n",
      "Gradient Descent(62/99): loss=0.38965432755930574, gradient=0.0005808401177583948\n",
      "Gradient Descent(63/99): loss=0.389654307054244, gradient=0.0008847316755095886\n",
      "Gradient Descent(64/99): loss=0.3896543127343378, gradient=0.0005828489135172159\n",
      "Gradient Descent(65/99): loss=0.38965429663036827, gradient=0.000871586274155506\n",
      "Gradient Descent(66/99): loss=0.38965430201178686, gradient=0.0007153961426653633\n",
      "Gradient Descent(67/99): loss=0.38965430346348523, gradient=0.000792088761413363\n",
      "Gradient Descent(68/99): loss=0.3896542958532153, gradient=0.000608642990309282\n",
      "Gradient Descent(69/99): loss=0.38965429473975244, gradient=0.000711082601279799\n",
      "Gradient Descent(70/99): loss=0.3896543072478185, gradient=0.0005881318104215994\n",
      "Gradient Descent(71/99): loss=0.3896542965678907, gradient=0.0007304793382786088\n",
      "Gradient Descent(72/99): loss=0.3896542918205155, gradient=0.000576231155328181\n",
      "Gradient Descent(73/99): loss=0.3896542897822212, gradient=0.0007029380035587094\n",
      "Gradient Descent(74/99): loss=0.38965430226264, gradient=0.0005742686835403876\n",
      "Gradient Descent(75/99): loss=0.38965429124342116, gradient=0.0007235705010005508\n",
      "Gradient Descent(76/99): loss=0.38965427873494524, gradient=0.0007678911931878005\n",
      "Gradient Descent(77/99): loss=0.38965429422050135, gradient=0.0005880053402394233\n",
      "Gradient Descent(78/99): loss=0.38965429450720573, gradient=0.0005256184762857221\n",
      "Gradient Descent(79/99): loss=0.3896542870435413, gradient=0.000726929160757368\n",
      "Gradient Descent(80/99): loss=0.38965427373148986, gradient=0.0007679411546584701\n",
      "Gradient Descent(81/99): loss=0.3896542896291831, gradient=0.0005768948187737312\n",
      "Gradient Descent(82/99): loss=0.38965427096757144, gradient=0.0008487639181634312\n",
      "Gradient Descent(83/99): loss=0.3896542641835789, gradient=0.0008808990126180586\n",
      "Gradient Descent(84/99): loss=0.38965427062077523, gradient=0.0005170909068107782\n",
      "Gradient Descent(85/99): loss=0.389654277558656, gradient=0.0005052435904922911\n",
      "Gradient Descent(86/99): loss=0.3896542738162891, gradient=0.0006838744177557742\n",
      "Gradient Descent(87/99): loss=0.38965427710638406, gradient=0.0007829496901351616\n",
      "Gradient Descent(88/99): loss=0.38965426954391835, gradient=0.000554030128622582\n",
      "Gradient Descent(89/99): loss=0.3896542782647054, gradient=0.0004943898058004422\n",
      "Gradient Descent(90/99): loss=0.3896542733856735, gradient=0.0006793986126220977\n",
      "Gradient Descent(91/99): loss=0.38965427618422305, gradient=0.0007802783497706926\n",
      "Gradient Descent(92/99): loss=0.3896542684150704, gradient=0.0005432207771118375\n",
      "Gradient Descent(93/99): loss=0.3896542770327077, gradient=0.00048270407991056794\n",
      "Gradient Descent(94/99): loss=0.3896542719816785, gradient=0.0006730302331103024\n",
      "Gradient Descent(95/99): loss=0.38965427460842145, gradient=0.0007757360792784288\n",
      "Gradient Descent(96/99): loss=0.38965424880655636, gradient=0.0008930027527501145\n",
      "Gradient Descent(97/99): loss=0.38965425856789726, gradient=0.0006614524528842241\n",
      "Gradient Descent(98/99): loss=0.3896542720971786, gradient=0.0004929361986486045\n",
      "Gradient Descent(99/99): loss=0.3896542754629231, gradient=0.00046329676537096363\n",
      "Gradient Descent(0/99): loss=0.3889618774524286, gradient=0.00886922426388552\n",
      "Gradient Descent(1/99): loss=0.3889591062201568, gradient=0.0063151476857985726\n",
      "Gradient Descent(2/99): loss=0.38895758620251747, gradient=0.0048352744320802905\n",
      "Gradient Descent(3/99): loss=0.38895643209502334, gradient=0.0039742086877727475\n",
      "Gradient Descent(4/99): loss=0.388955657075016, gradient=0.0034751737380817158\n",
      "Gradient Descent(5/99): loss=0.38895509696728753, gradient=0.003095166605982958\n",
      "Gradient Descent(6/99): loss=0.3889546960061591, gradient=0.002798449602983523\n",
      "Gradient Descent(7/99): loss=0.3889544044930934, gradient=0.002566160797471856\n",
      "Gradient Descent(8/99): loss=0.3889541912427948, gradient=0.0023836799081194955\n",
      "Gradient Descent(9/99): loss=0.3889539876594563, gradient=0.002481301383431852\n",
      "Gradient Descent(10/99): loss=0.3889539109934407, gradient=0.002160933382831286\n",
      "Gradient Descent(11/99): loss=0.38895381664769146, gradient=0.0020312680722599824\n",
      "Gradient Descent(12/99): loss=0.38895375724359227, gradient=0.0019538940434953096\n",
      "Gradient Descent(13/99): loss=0.3889536710133315, gradient=0.0021416544169250555\n",
      "Gradient Descent(14/99): loss=0.3889536769708293, gradient=0.0018859560204490299\n",
      "Gradient Descent(15/99): loss=0.3889536457065441, gradient=0.0017963717032377115\n",
      "Gradient Descent(16/99): loss=0.38895363535609656, gradient=0.001753914707572423\n",
      "Gradient Descent(17/99): loss=0.38895359008821195, gradient=0.0019763541166749454\n",
      "Gradient Descent(18/99): loss=0.3889536261817135, gradient=0.0017415940312667476\n",
      "Gradient Descent(19/99): loss=0.388953619623231, gradient=0.0016640842594052817\n",
      "Gradient Descent(20/99): loss=0.38895358716523964, gradient=0.001912787445581542\n",
      "Gradient Descent(21/99): loss=0.38895356754035687, gradient=0.001896042707994441\n",
      "Gradient Descent(22/99): loss=0.38895362457087834, gradient=0.0016627134847409873\n",
      "Gradient Descent(23/99): loss=0.3889535994682524, gradient=0.0018350518670293549\n",
      "Gradient Descent(24/99): loss=0.38895359392485573, gradient=0.0018502245661263083\n",
      "Gradient Descent(25/99): loss=0.38895365997262377, gradient=0.0016216660354673774\n",
      "Gradient Descent(26/99): loss=0.3889536806506394, gradient=0.0015454350068328487\n",
      "Gradient Descent(27/99): loss=0.3889536772429429, gradient=0.0017793116920276138\n",
      "Gradient Descent(28/99): loss=0.3889536766363002, gradient=0.0018084012826576573\n",
      "Gradient Descent(29/99): loss=0.38895375088711565, gradient=0.001566084555415544\n",
      "Gradient Descent(30/99): loss=0.38895374372216646, gradient=0.0017334128669425097\n",
      "Gradient Descent(31/99): loss=0.38895378921768614, gradient=0.001497711794867095\n",
      "Gradient Descent(32/99): loss=0.388953792415933, gradient=0.0017651594220038007\n",
      "Gradient Descent(33/99): loss=0.3889538382084849, gradient=0.0017428167528579105\n",
      "Gradient Descent(34/99): loss=0.38895387439822304, gradient=0.0014714245540647835\n",
      "Gradient Descent(35/99): loss=0.38895388391243046, gradient=0.0017327157558040176\n",
      "Gradient Descent(36/99): loss=0.38895393326065764, gradient=0.0017106309241819563\n",
      "Gradient Descent(37/99): loss=0.3889539725074802, gradient=0.0014427455757858578\n",
      "Gradient Descent(38/99): loss=0.38895402419847747, gradient=0.0014100196094295006\n",
      "Gradient Descent(39/99): loss=0.3889540008941692, gradient=0.0019001267179733852\n",
      "Gradient Descent(40/99): loss=0.38895408940245957, gradient=0.0014742751339297262\n",
      "Gradient Descent(41/99): loss=0.38895413058044176, gradient=0.0013857082595863039\n",
      "Gradient Descent(42/99): loss=0.38895415265342403, gradient=0.0015993098388763973\n",
      "Gradient Descent(43/99): loss=0.3889541658023454, gradient=0.0016989985328976355\n",
      "Gradient Descent(44/99): loss=0.38895425612061796, gradient=0.0014189383471512216\n",
      "Gradient Descent(45/99): loss=0.3889542671728161, gradient=0.0015802581682695452\n",
      "Gradient Descent(46/99): loss=0.38895432614908604, gradient=0.0013515559541550368\n",
      "Gradient Descent(47/99): loss=0.3889543399025349, gradient=0.00165969907505758\n",
      "Gradient Descent(48/99): loss=0.38895439955247607, gradient=0.0016114813019422435\n",
      "Gradient Descent(49/99): loss=0.3889544472491532, gradient=0.0013305870568551801\n",
      "Gradient Descent(50/99): loss=0.38895445218710584, gradient=0.0017266006294709636\n",
      "Gradient Descent(51/99): loss=0.38895441211770637, gradient=0.0019384327189428618\n",
      "Gradient Descent(52/99): loss=0.38895452890316756, gradient=0.0014038494756028674\n",
      "Gradient Descent(53/99): loss=0.3889545085412784, gradient=0.001871373373196557\n",
      "Gradient Descent(54/99): loss=0.38895459467915217, gradient=0.0015835168460606225\n",
      "Gradient Descent(55/99): loss=0.38895461621865335, gradient=0.001662036186799572\n",
      "Gradient Descent(56/99): loss=0.3889546780826565, gradient=0.0017956234025436621\n",
      "Gradient Descent(57/99): loss=0.38895467382689575, gradient=0.0015781743713583801\n",
      "Gradient Descent(58/99): loss=0.3889547684425987, gradient=0.0012661237921761089\n",
      "Gradient Descent(59/99): loss=0.3889548012206288, gradient=0.0016004195905561238\n",
      "Gradient Descent(60/99): loss=0.38895487938575934, gradient=0.0015408089558216012\n",
      "Gradient Descent(61/99): loss=0.3889548903506699, gradient=0.0016367517735470431\n",
      "Gradient Descent(62/99): loss=0.3889549100795922, gradient=0.001488706460775992\n",
      "Gradient Descent(63/99): loss=0.3889549592799189, gradient=0.0015884679855991716\n",
      "Gradient Descent(64/99): loss=0.3889550739270401, gradient=0.001278392806997959\n",
      "Gradient Descent(65/99): loss=0.3889551104554083, gradient=0.001436710352786486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(66/99): loss=0.3889551363020194, gradient=0.0016373184489441076\n",
      "Gradient Descent(67/99): loss=0.38895514098879586, gradient=0.0016854593884570645\n",
      "Gradient Descent(68/99): loss=0.3889552365111322, gradient=0.0015163550799236446\n",
      "Gradient Descent(69/99): loss=0.38895530536122724, gradient=0.0012002480293977998\n",
      "Gradient Descent(70/99): loss=0.3889553856723952, gradient=0.001160828762496105\n",
      "Gradient Descent(71/99): loss=0.38895538458317386, gradient=0.0017391840815392725\n",
      "Gradient Descent(72/99): loss=0.3889554457339907, gradient=0.0017363876193618676\n",
      "Gradient Descent(73/99): loss=0.38895547737277125, gradient=0.0012421239920658576\n",
      "Gradient Descent(74/99): loss=0.3889555401248789, gradient=0.0014064235116189848\n",
      "Gradient Descent(75/99): loss=0.38895561971004383, gradient=0.001148777839783805\n",
      "Gradient Descent(76/99): loss=0.38895565359516965, gradient=0.0015180668622984852\n",
      "Gradient Descent(77/99): loss=0.38895568289999166, gradient=0.0018836280204506649\n",
      "Gradient Descent(78/99): loss=0.38895571791031525, gradient=0.0012313373643551583\n",
      "Gradient Descent(79/99): loss=0.38895581490632847, gradient=0.0011112313517791903\n",
      "Gradient Descent(80/99): loss=0.3889558171199792, gradient=0.0017197137227615775\n",
      "Gradient Descent(81/99): loss=0.3889558506019185, gradient=0.001708691891324585\n",
      "Gradient Descent(82/99): loss=0.3889559686249228, gradient=0.0011373622919854862\n",
      "Gradient Descent(83/99): loss=0.3889559570672398, gradient=0.001728279252898138\n",
      "Gradient Descent(84/99): loss=0.3889559700120833, gradient=0.001630702866830677\n",
      "Gradient Descent(85/99): loss=0.38895610233569744, gradient=0.0011864132944382215\n",
      "Gradient Descent(86/99): loss=0.38895614129620254, gradient=0.0013586373786005587\n",
      "Gradient Descent(87/99): loss=0.38895622693983867, gradient=0.001079327521815271\n",
      "Gradient Descent(88/99): loss=0.38895622515413414, gradient=0.00170520173743096\n",
      "Gradient Descent(89/99): loss=0.3889562335270274, gradient=0.0016312531715073424\n",
      "Gradient Descent(90/99): loss=0.38895636443603177, gradient=0.0011767894096346258\n",
      "Gradient Descent(91/99): loss=0.3889564056784364, gradient=0.0013177549944416746\n",
      "Gradient Descent(92/99): loss=0.3889564900015191, gradient=0.0010664246494439828\n",
      "Gradient Descent(93/99): loss=0.38895645741276647, gradient=0.0018311814162536978\n",
      "Gradient Descent(94/99): loss=0.3889565228213691, gradient=0.0013214092850415895\n",
      "Gradient Descent(95/99): loss=0.38895661018403405, gradient=0.0010574043765109898\n",
      "Gradient Descent(96/99): loss=0.3889566938840379, gradient=0.001019905329288729\n",
      "Gradient Descent(97/99): loss=0.38895674180268247, gradient=0.0012965407021146129\n",
      "Gradient Descent(98/99): loss=0.3889567744202729, gradient=0.0014629246549155458\n",
      "Gradient Descent(99/99): loss=0.3889568356056435, gradient=0.0016300891884513725\n",
      "Gradient Descent(0/99): loss=0.39014784295834815, gradient=0.015888721939834427\n",
      "Gradient Descent(1/99): loss=0.39014543514932976, gradient=0.006920917474993396\n",
      "Gradient Descent(2/99): loss=0.39014357008961387, gradient=0.0052528110652943156\n",
      "Gradient Descent(3/99): loss=0.3901422788203937, gradient=0.004706927359371176\n",
      "Gradient Descent(4/99): loss=0.3901413340843856, gradient=0.004072492993014124\n",
      "Gradient Descent(5/99): loss=0.3901406214273798, gradient=0.0036767606802506608\n",
      "Gradient Descent(6/99): loss=0.3901400689875491, gradient=0.0033555735090845477\n",
      "Gradient Descent(7/99): loss=0.39013961333828223, gradient=0.003194764268044261\n",
      "Gradient Descent(8/99): loss=0.39013927043222746, gradient=0.00288049748374425\n",
      "Gradient Descent(9/99): loss=0.3901389360011427, gradient=0.0029390865638428414\n",
      "Gradient Descent(10/99): loss=0.3901386934231253, gradient=0.00285441374222944\n",
      "Gradient Descent(11/99): loss=0.39013855881747733, gradient=0.0024717782621784235\n",
      "Gradient Descent(12/99): loss=0.39013840246664977, gradient=0.002367346119468108\n",
      "Gradient Descent(13/99): loss=0.3901382825375071, gradient=0.0022579015469719817\n",
      "Gradient Descent(14/99): loss=0.39013818659026456, gradient=0.0021711084100902965\n",
      "Gradient Descent(15/99): loss=0.3901380361380617, gradient=0.002436951870468985\n",
      "Gradient Descent(16/99): loss=0.39013803179011874, gradient=0.002080459799040103\n",
      "Gradient Descent(17/99): loss=0.39013789535540067, gradient=0.0023765606909901147\n",
      "Gradient Descent(18/99): loss=0.3901379267603496, gradient=0.001928375717507145\n",
      "Gradient Descent(19/99): loss=0.3901378642625687, gradient=0.0022447509696158375\n",
      "Gradient Descent(20/99): loss=0.3901378501549331, gradient=0.0018508497825313061\n",
      "Gradient Descent(21/99): loss=0.3901378804958503, gradient=0.0017535338377173539\n",
      "Gradient Descent(22/99): loss=0.39013780957914207, gradient=0.002185407958651173\n",
      "Gradient Descent(23/99): loss=0.3901378127206984, gradient=0.0019327826710033213\n",
      "Gradient Descent(24/99): loss=0.39013784798184964, gradient=0.001647061670366858\n",
      "Gradient Descent(25/99): loss=0.3901377844645291, gradient=0.0021529394952352317\n",
      "Gradient Descent(26/99): loss=0.3901378415022209, gradient=0.0016105122125070423\n",
      "Gradient Descent(27/99): loss=0.390137901273526, gradient=0.0014818763545640283\n",
      "Gradient Descent(28/99): loss=0.3901378564233569, gradient=0.0021020839268147003\n",
      "Gradient Descent(29/99): loss=0.3901378188527053, gradient=0.002005175652509749\n",
      "Gradient Descent(30/99): loss=0.3901379245859464, gradient=0.0015744413435247466\n",
      "Gradient Descent(31/99): loss=0.39013793832986593, gradient=0.0016818550999194235\n",
      "Gradient Descent(32/99): loss=0.3901379218092421, gradient=0.0017628505722894934\n",
      "Gradient Descent(33/99): loss=0.39013799192844434, gradient=0.0014413606155270274\n",
      "Gradient Descent(34/99): loss=0.39013806026920567, gradient=0.0013419180679569488\n",
      "Gradient Descent(35/99): loss=0.39013798789119186, gradient=0.002134118809094066\n",
      "Gradient Descent(36/99): loss=0.39013808406377637, gradient=0.001315461559732327\n",
      "Gradient Descent(37/99): loss=0.3901380995045794, gradient=0.0017721492059020558\n",
      "Gradient Descent(38/99): loss=0.390138176141693, gradient=0.0013208335790869201\n",
      "Gradient Descent(39/99): loss=0.39013811447908986, gradient=0.0020879690987098227\n",
      "Gradient Descent(40/99): loss=0.39013821577032726, gradient=0.001287283376360638\n",
      "Gradient Descent(41/99): loss=0.39013824810707404, gradient=0.0016152529214064888\n",
      "Gradient Descent(42/99): loss=0.39013832038018853, gradient=0.0013144940572698547\n",
      "Gradient Descent(43/99): loss=0.39013840017915125, gradient=0.0012175291369017538\n",
      "Gradient Descent(44/99): loss=0.39013841198236704, gradient=0.0016745079837065587\n",
      "Gradient Descent(45/99): loss=0.39013845966819205, gradient=0.0012391188897275974\n",
      "Gradient Descent(46/99): loss=0.3901384885363652, gradient=0.0016911331591538089\n",
      "Gradient Descent(47/99): loss=0.39013849549517904, gradient=0.00175576446527005\n",
      "Gradient Descent(48/99): loss=0.39013861144920525, gradient=0.0013125212606181075\n",
      "Gradient Descent(49/99): loss=0.39013855479608706, gradient=0.0020148067062048455\n",
      "Gradient Descent(50/99): loss=0.3901386131505619, gradient=0.0015602283063750762\n",
      "Gradient Descent(51/99): loss=0.39013869629812864, gradient=0.0012411238404635271\n",
      "Gradient Descent(52/99): loss=0.3901387884578087, gradient=0.0011380675181450971\n",
      "Gradient Descent(53/99): loss=0.390138860509017, gradient=0.0012123031289857232\n",
      "Gradient Descent(54/99): loss=0.3901388394510264, gradient=0.001857514456186\n",
      "Gradient Descent(55/99): loss=0.3901388836927504, gradient=0.001245781854673068\n",
      "Gradient Descent(56/99): loss=0.390138982255821, gradient=0.001137083332059858\n",
      "Gradient Descent(57/99): loss=0.39013905536376414, gradient=0.001168971897116388\n",
      "Gradient Descent(58/99): loss=0.39013913922186216, gradient=0.001078332607726337\n",
      "Gradient Descent(59/99): loss=0.39013902677037465, gradient=0.0022535529552583696\n",
      "Gradient Descent(60/99): loss=0.3901389971286225, gradient=0.0021105510112692494\n",
      "Gradient Descent(61/99): loss=0.3901391658282, gradient=0.001365313233527271\n",
      "Gradient Descent(62/99): loss=0.39013922904156784, gradient=0.001180153864275858\n",
      "Gradient Descent(63/99): loss=0.39013927528431697, gradient=0.0014618805986639114\n",
      "Gradient Descent(64/99): loss=0.3901393544603768, gradient=0.0011222444391161778\n",
      "Gradient Descent(65/99): loss=0.39013938499694006, gradient=0.001540294870177067\n",
      "Gradient Descent(66/99): loss=0.3901394281948681, gradient=0.0011860139228781243\n",
      "Gradient Descent(67/99): loss=0.39013953052447875, gradient=0.0010304707599347032\n",
      "Gradient Descent(68/99): loss=0.39013956113518555, gradient=0.00145170306315142\n",
      "Gradient Descent(69/99): loss=0.3901396344957055, gradient=0.0011141707599468997\n",
      "Gradient Descent(70/99): loss=0.39013966217405704, gradient=0.0015060754033695966\n",
      "Gradient Descent(71/99): loss=0.39013963389303, gradient=0.0017883346328547982\n",
      "Gradient Descent(72/99): loss=0.39013972576375733, gradient=0.0015265097361262716\n",
      "Gradient Descent(73/99): loss=0.39013967090148677, gradient=0.002128604377952702\n",
      "Gradient Descent(74/99): loss=0.39013983175833, gradient=0.001355405446868917\n",
      "Gradient Descent(75/99): loss=0.39013989028516605, gradient=0.0010939899464130968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(76/99): loss=0.3901398785168136, gradient=0.0018010830182488033\n",
      "Gradient Descent(77/99): loss=0.3901399354744907, gradient=0.0010738992677597865\n",
      "Gradient Descent(78/99): loss=0.3901400249385437, gradient=0.0010936153177113026\n",
      "Gradient Descent(79/99): loss=0.3901401141159771, gradient=0.0009526624626762611\n",
      "Gradient Descent(80/99): loss=0.39014018404776757, gradient=0.001073508539959094\n",
      "Gradient Descent(81/99): loss=0.3901401567060443, gradient=0.0017977712847121552\n",
      "Gradient Descent(82/99): loss=0.39014020000505456, gradient=0.0011154581528627053\n",
      "Gradient Descent(83/99): loss=0.3901402984575141, gradient=0.0009866137227174097\n",
      "Gradient Descent(84/99): loss=0.3901402457373174, gradient=0.002053293459867118\n",
      "Gradient Descent(85/99): loss=0.39014034816364523, gradient=0.0016309521798096847\n",
      "Gradient Descent(86/99): loss=0.39014034248572427, gradient=0.0016531420840412987\n",
      "Gradient Descent(87/99): loss=0.3901404626515252, gradient=0.0011618458321264243\n",
      "Gradient Descent(88/99): loss=0.3901404844168292, gradient=0.0014233335407308368\n",
      "Gradient Descent(89/99): loss=0.3901405271839513, gradient=0.0011178862212335241\n",
      "Gradient Descent(90/99): loss=0.39014057435824695, gradient=0.00139168314854258\n",
      "Gradient Descent(91/99): loss=0.39014065070389764, gradient=0.0010170151847418612\n",
      "Gradient Descent(92/99): loss=0.39014073327390664, gradient=0.0009463755211042861\n",
      "Gradient Descent(93/99): loss=0.39014066738564485, gradient=0.0019450897052244293\n",
      "Gradient Descent(94/99): loss=0.39014071423610885, gradient=0.0013811890914130663\n",
      "Gradient Descent(95/99): loss=0.39014073067592886, gradient=0.001580310167770628\n",
      "Gradient Descent(96/99): loss=0.3901408483315573, gradient=0.0011251728670890057\n",
      "Gradient Descent(97/99): loss=0.3901409264333669, gradient=0.000894696059257057\n",
      "Gradient Descent(98/99): loss=0.3901409480646886, gradient=0.0014949761809300393\n",
      "Gradient Descent(99/99): loss=0.39014102495951225, gradient=0.0009597597441574736\n",
      "Gradient Descent(0/99): loss=0.39039003178942594, gradient=0.0075796952335101956\n",
      "Gradient Descent(1/99): loss=0.3903868562296219, gradient=0.006409864847873341\n",
      "Gradient Descent(2/99): loss=0.3903845536011096, gradient=0.005508064623950044\n",
      "Gradient Descent(3/99): loss=0.3903826560578646, gradient=0.004886360687415076\n",
      "Gradient Descent(4/99): loss=0.39038104024083176, gradient=0.004566675846653575\n",
      "Gradient Descent(5/99): loss=0.3903798196575802, gradient=0.003745718843238509\n",
      "Gradient Descent(6/99): loss=0.39037873194599293, gradient=0.0034885987618317386\n",
      "Gradient Descent(7/99): loss=0.3903777600194986, gradient=0.003258609153626125\n",
      "Gradient Descent(8/99): loss=0.39037682528549317, gradient=0.0032430506784123033\n",
      "Gradient Descent(9/99): loss=0.3903759934226903, gradient=0.002983503802319324\n",
      "Gradient Descent(10/99): loss=0.39037514576850724, gradient=0.003107377415008601\n",
      "Gradient Descent(11/99): loss=0.3903744795688891, gradient=0.00272445207623597\n",
      "Gradient Descent(12/99): loss=0.3903737307140225, gradient=0.0028845580642920493\n",
      "Gradient Descent(13/99): loss=0.3903730967876786, gradient=0.0025437603798607682\n",
      "Gradient Descent(14/99): loss=0.39037247226414684, gradient=0.002519325443682017\n",
      "Gradient Descent(15/99): loss=0.39037189723605004, gradient=0.0023795340901789416\n",
      "Gradient Descent(16/99): loss=0.3903712834966725, gradient=0.0026026376728143395\n",
      "Gradient Descent(17/99): loss=0.3903707543137042, gradient=0.002288935017049491\n",
      "Gradient Descent(18/99): loss=0.39037024727510994, gradient=0.0021992654777320745\n",
      "Gradient Descent(19/99): loss=0.39036970473363775, gradient=0.0024140108117504393\n",
      "Gradient Descent(20/99): loss=0.3903692394777448, gradient=0.002103883690856853\n",
      "Gradient Descent(21/99): loss=0.3903687794212477, gradient=0.0021418632247598955\n",
      "Gradient Descent(22/99): loss=0.39036834542753174, gradient=0.0020223567285405507\n",
      "Gradient Descent(23/99): loss=0.39036787583892596, gradient=0.0022433802587628225\n",
      "Gradient Descent(24/99): loss=0.39036747482655937, gradient=0.0019454320757519602\n",
      "Gradient Descent(25/99): loss=0.3903670930435094, gradient=0.0018977587623378847\n",
      "Gradient Descent(26/99): loss=0.3903667027098683, gradient=0.001953028138363942\n",
      "Gradient Descent(27/99): loss=0.3903663007403814, gradient=0.002054965120834912\n",
      "Gradient Descent(28/99): loss=0.39036592898719946, gradient=0.0019658278286094426\n",
      "Gradient Descent(29/99): loss=0.39036561657134394, gradient=0.0018108509357158125\n",
      "Gradient Descent(30/99): loss=0.3903652333774741, gradient=0.0021325737564459046\n",
      "Gradient Descent(31/99): loss=0.39036494074144995, gradient=0.0018156896216989025\n",
      "Gradient Descent(32/99): loss=0.3903646390733459, gradient=0.0017339747725959836\n",
      "Gradient Descent(33/99): loss=0.3903643560408104, gradient=0.001826923785004039\n",
      "Gradient Descent(34/99): loss=0.39036403260072294, gradient=0.001971378173208898\n",
      "Gradient Descent(35/99): loss=0.3903637651818561, gradient=0.0018244135517275107\n",
      "Gradient Descent(36/99): loss=0.3903634272397077, gradient=0.002135296916007352\n",
      "Gradient Descent(37/99): loss=0.3903631790384037, gradient=0.0020699900337826107\n",
      "Gradient Descent(38/99): loss=0.3903628581678699, gradient=0.0021289134473707915\n",
      "Gradient Descent(39/99): loss=0.39036268649246597, gradient=0.0017810512942862333\n",
      "Gradient Descent(40/99): loss=0.39036244028198847, gradient=0.0016924984898316095\n",
      "Gradient Descent(41/99): loss=0.3903622162099907, gradient=0.0017748974991495447\n",
      "Gradient Descent(42/99): loss=0.3903619970429149, gradient=0.0017300957987233562\n",
      "Gradient Descent(43/99): loss=0.39036180018527716, gradient=0.001517019453977876\n",
      "Gradient Descent(44/99): loss=0.3903615542857729, gradient=0.0019689561465384936\n",
      "Gradient Descent(45/99): loss=0.39036135983718573, gradient=0.001539627163595277\n",
      "Gradient Descent(46/99): loss=0.3903612002975742, gradient=0.0014148568981545901\n",
      "Gradient Descent(47/99): loss=0.39036099966897303, gradient=0.001719888612601289\n",
      "Gradient Descent(48/99): loss=0.3903607820417943, gradient=0.0017483575961048836\n",
      "Gradient Descent(49/99): loss=0.3903606218453132, gradient=0.0015286055300594763\n",
      "Gradient Descent(50/99): loss=0.39036044170914164, gradient=0.0016171115980089929\n",
      "Gradient Descent(51/99): loss=0.3903602718936584, gradient=0.0014741463590242753\n",
      "Gradient Descent(52/99): loss=0.39036009147263767, gradient=0.0017094455641015766\n",
      "Gradient Descent(53/99): loss=0.3903599238971176, gradient=0.0016274767111789134\n",
      "Gradient Descent(54/99): loss=0.3903597576250959, gradient=0.0014765507276218515\n",
      "Gradient Descent(55/99): loss=0.39035959492959815, gradient=0.0016076168232227745\n",
      "Gradient Descent(56/99): loss=0.39035942096295584, gradient=0.0016868731403426023\n",
      "Gradient Descent(57/99): loss=0.39035928270786646, gradient=0.0013619180715978515\n",
      "Gradient Descent(58/99): loss=0.39035915566907925, gradient=0.0013990857900454063\n",
      "Gradient Descent(59/99): loss=0.3903589589952194, gradient=0.0018533894373122775\n",
      "Gradient Descent(60/99): loss=0.39035876750164006, gradient=0.001738460046705894\n",
      "Gradient Descent(61/99): loss=0.39035863072130833, gradient=0.0014568589224566292\n",
      "Gradient Descent(62/99): loss=0.39035850887701296, gradient=0.0016023838065660477\n",
      "Gradient Descent(63/99): loss=0.3903583581710959, gradient=0.0017387699048395577\n",
      "Gradient Descent(64/99): loss=0.3903582485421882, gradient=0.0012882329841326075\n",
      "Gradient Descent(65/99): loss=0.39035815360637227, gradient=0.001354317411579341\n",
      "Gradient Descent(66/99): loss=0.39035802038989426, gradient=0.0016131901466338854\n",
      "Gradient Descent(67/99): loss=0.39035783353260095, gradient=0.002006085703616233\n",
      "Gradient Descent(68/99): loss=0.39035768630831374, gradient=0.0014851460466780574\n",
      "Gradient Descent(69/99): loss=0.39035758726526654, gradient=0.0015373353280770919\n",
      "Gradient Descent(70/99): loss=0.3903575011780961, gradient=0.0013353307044288146\n",
      "Gradient Descent(71/99): loss=0.39035738046686186, gradient=0.001637952382640772\n",
      "Gradient Descent(72/99): loss=0.3903572748222913, gradient=0.001347753788791242\n",
      "Gradient Descent(73/99): loss=0.3903571052995212, gradient=0.0019332410088413613\n",
      "Gradient Descent(74/99): loss=0.39035694056139514, gradient=0.0018797101578475832\n",
      "Gradient Descent(75/99): loss=0.3903568672578871, gradient=0.001847555375158345\n",
      "Gradient Descent(76/99): loss=0.39035674403683746, gradient=0.0015978353601536163\n",
      "Gradient Descent(77/99): loss=0.39035668007412155, gradient=0.001383070311854516\n",
      "Gradient Descent(78/99): loss=0.3903566213201577, gradient=0.0012046931324988452\n",
      "Gradient Descent(79/99): loss=0.3903565006347205, gradient=0.0017198318395532336\n",
      "Gradient Descent(80/99): loss=0.3903563184391674, gradient=0.001940916820184683\n",
      "Gradient Descent(81/99): loss=0.39035626508534327, gradient=0.001238488656908185\n",
      "Gradient Descent(82/99): loss=0.3903562020869649, gradient=0.0013243582754358153\n",
      "Gradient Descent(83/99): loss=0.39035615644386384, gradient=0.0011689720874533223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(84/99): loss=0.3903559919102081, gradient=0.001999001236240072\n",
      "Gradient Descent(85/99): loss=0.3903558595200015, gradient=0.0016806806942743857\n",
      "Gradient Descent(86/99): loss=0.39035579774129514, gradient=0.0013005017578515232\n",
      "Gradient Descent(87/99): loss=0.39035571580071177, gradient=0.0015071792867294542\n",
      "Gradient Descent(88/99): loss=0.3903556746228762, gradient=0.0011517161990600546\n",
      "Gradient Descent(89/99): loss=0.39035561639120187, gradient=0.0013206410237607356\n",
      "Gradient Descent(90/99): loss=0.39035546034800145, gradient=0.001973615229806618\n",
      "Gradient Descent(91/99): loss=0.3903553139623405, gradient=0.0016879210452551284\n",
      "Gradient Descent(92/99): loss=0.3903552916525499, gradient=0.0011843614655871997\n",
      "Gradient Descent(93/99): loss=0.3903552438557745, gradient=0.0012863225533848947\n",
      "Gradient Descent(94/99): loss=0.3903551656354083, gradient=0.0015326098467328831\n",
      "Gradient Descent(95/99): loss=0.39035501892267055, gradient=0.0020010756296153665\n",
      "Gradient Descent(96/99): loss=0.3903549181709192, gradient=0.001424099181820168\n",
      "Gradient Descent(97/99): loss=0.3903549058096579, gradient=0.0011700124628673948\n",
      "Gradient Descent(98/99): loss=0.3903548164621208, gradient=0.0016343508398642484\n",
      "Gradient Descent(99/99): loss=0.3903547851436701, gradient=0.001208670857677655\n",
      "Gradient Descent(0/99): loss=0.3899891768666359, gradient=0.010420563991625754\n",
      "Gradient Descent(1/99): loss=0.3899838571992661, gradient=0.007136600448792025\n",
      "Gradient Descent(2/99): loss=0.38998025864198593, gradient=0.005858063812224085\n",
      "Gradient Descent(3/99): loss=0.3899775268612974, gradient=0.004998968331997525\n",
      "Gradient Descent(4/99): loss=0.3899755243431408, gradient=0.004306785125337022\n",
      "Gradient Descent(5/99): loss=0.389974025141548, gradient=0.003682676110645282\n",
      "Gradient Descent(6/99): loss=0.3899728001211036, gradient=0.0032734774224697717\n",
      "Gradient Descent(7/99): loss=0.3899717877030905, gradient=0.0031246281860327343\n",
      "Gradient Descent(8/99): loss=0.3899710144010645, gradient=0.002759161349912641\n",
      "Gradient Descent(9/99): loss=0.3899703467713085, gradient=0.0024716700725241378\n",
      "Gradient Descent(10/99): loss=0.389969781240919, gradient=0.0024581052678747683\n",
      "Gradient Descent(11/99): loss=0.38996914047719744, gradient=0.003018087319281056\n",
      "Gradient Descent(12/99): loss=0.38996879748297314, gradient=0.002267124457783157\n",
      "Gradient Descent(13/99): loss=0.3899684473343528, gradient=0.001950431546966507\n",
      "Gradient Descent(14/99): loss=0.38996813017128507, gradient=0.0020001168108584157\n",
      "Gradient Descent(15/99): loss=0.38996781835478045, gradient=0.0020854734630928293\n",
      "Gradient Descent(16/99): loss=0.38996752904416654, gradient=0.001893278981746158\n",
      "Gradient Descent(17/99): loss=0.38996732096961384, gradient=0.001715716373162574\n",
      "Gradient Descent(18/99): loss=0.38996712232133773, gradient=0.0016534770133290013\n",
      "Gradient Descent(19/99): loss=0.38996687846447003, gradient=0.0020162948481122363\n",
      "Gradient Descent(20/99): loss=0.3899666149553559, gradient=0.0021289640722174604\n",
      "Gradient Descent(21/99): loss=0.3899664920246031, gradient=0.0017136079334386484\n",
      "Gradient Descent(22/99): loss=0.38996635201137037, gradient=0.0015551505648639429\n",
      "Gradient Descent(23/99): loss=0.38996622427111266, gradient=0.0015562796533277682\n",
      "Gradient Descent(24/99): loss=0.38996606380515675, gradient=0.0018272350448938145\n",
      "Gradient Descent(25/99): loss=0.38996584929241435, gradient=0.002079334693406166\n",
      "Gradient Descent(26/99): loss=0.3899657866958878, gradient=0.0015738855866424695\n",
      "Gradient Descent(27/99): loss=0.38996568920417984, gradient=0.0014620822331405585\n",
      "Gradient Descent(28/99): loss=0.38996556864558785, gradient=0.0017543771011549235\n",
      "Gradient Descent(29/99): loss=0.38996545373133384, gradient=0.0014807798313539572\n",
      "Gradient Descent(30/99): loss=0.38996538582419327, gradient=0.0014209539471094336\n",
      "Gradient Descent(31/99): loss=0.3899653126998761, gradient=0.0013836889396751024\n",
      "Gradient Descent(32/99): loss=0.38996512698781793, gradient=0.002094843943911676\n",
      "Gradient Descent(33/99): loss=0.3899650767166595, gradient=0.0013668423795623339\n",
      "Gradient Descent(34/99): loss=0.38996502435253383, gradient=0.0013739869952129984\n",
      "Gradient Descent(35/99): loss=0.38996497331495755, gradient=0.0013352308714355008\n",
      "Gradient Descent(36/99): loss=0.38996489012804936, gradient=0.0016283545921855567\n",
      "Gradient Descent(37/99): loss=0.38996480732668026, gradient=0.0013809085831150416\n",
      "Gradient Descent(38/99): loss=0.3899647675055546, gradient=0.0013351100389451004\n",
      "Gradient Descent(39/99): loss=0.3899646913383588, gradient=0.001560239946409781\n",
      "Gradient Descent(40/99): loss=0.38996461428217005, gradient=0.0013779959092655885\n",
      "Gradient Descent(41/99): loss=0.38996458024419367, gradient=0.001283852952579287\n",
      "Gradient Descent(42/99): loss=0.38996442877412163, gradient=0.0019990701153217555\n",
      "Gradient Descent(43/99): loss=0.38996440511916236, gradient=0.0012800226526720943\n",
      "Gradient Descent(44/99): loss=0.38996431212734856, gradient=0.001772664005481136\n",
      "Gradient Descent(45/99): loss=0.38996429971607, gradient=0.0016402095809878632\n",
      "Gradient Descent(46/99): loss=0.3899642341520632, gradient=0.0013841877474486501\n",
      "Gradient Descent(47/99): loss=0.3899642208071987, gradient=0.0012566870730510349\n",
      "Gradient Descent(48/99): loss=0.3899641753658409, gradient=0.0014739702111213962\n",
      "Gradient Descent(49/99): loss=0.3899641176638866, gradient=0.001309039625080864\n",
      "Gradient Descent(50/99): loss=0.38996410155708633, gradient=0.0012574255006604863\n",
      "Gradient Descent(51/99): loss=0.3899640564575415, gradient=0.0014337550115608242\n",
      "Gradient Descent(52/99): loss=0.38996400605825726, gradient=0.0012554069250834458\n",
      "Gradient Descent(53/99): loss=0.389963971307991, gradient=0.0014892482154073218\n",
      "Gradient Descent(54/99): loss=0.3899638524215679, gradient=0.0018562719741660886\n",
      "Gradient Descent(55/99): loss=0.3899637988735146, gradient=0.0017217054903540488\n",
      "Gradient Descent(56/99): loss=0.38996380732139246, gradient=0.0014758761388971107\n",
      "Gradient Descent(57/99): loss=0.38996375682476575, gradient=0.001353506231827341\n",
      "Gradient Descent(58/99): loss=0.38996375797548266, gradient=0.0011999186018530267\n",
      "Gradient Descent(59/99): loss=0.38996373098469234, gradient=0.0013959164187212498\n",
      "Gradient Descent(60/99): loss=0.3899636854477643, gradient=0.0012628437537872158\n",
      "Gradient Descent(61/99): loss=0.38996366727019116, gradient=0.0014060056279335226\n",
      "Gradient Descent(62/99): loss=0.3899636170119748, gradient=0.0012705440296836323\n",
      "Gradient Descent(63/99): loss=0.38996360015447484, gradient=0.0013897026385193102\n",
      "Gradient Descent(64/99): loss=0.3899635488012598, gradient=0.0012698035763771154\n",
      "Gradient Descent(65/99): loss=0.3899635416964608, gradient=0.0012169477314566547\n",
      "Gradient Descent(66/99): loss=0.3899634360490948, gradient=0.0018082881322356373\n",
      "Gradient Descent(67/99): loss=0.38996336972722445, gradient=0.0017104157906061195\n",
      "Gradient Descent(68/99): loss=0.389963395121117, gradient=0.001469915431258253\n",
      "Gradient Descent(69/99): loss=0.3899633562396697, gradient=0.0012673661055261909\n",
      "Gradient Descent(70/99): loss=0.38996335502893914, gradient=0.0013793237291387095\n",
      "Gradient Descent(71/99): loss=0.3899633165104458, gradient=0.0012428771325751911\n",
      "Gradient Descent(72/99): loss=0.3899633116751401, gradient=0.0013545676526820992\n",
      "Gradient Descent(73/99): loss=0.3899632692750819, gradient=0.001243704326743139\n",
      "Gradient Descent(74/99): loss=0.38996326669392056, gradient=0.0012214228765890245\n",
      "Gradient Descent(75/99): loss=0.38996325269922383, gradient=0.0012550122604559335\n",
      "Gradient Descent(76/99): loss=0.389963144931881, gradient=0.0018429636802198373\n",
      "Gradient Descent(77/99): loss=0.38996317595935437, gradient=0.001478457014652921\n",
      "Gradient Descent(78/99): loss=0.38996306417347637, gradient=0.0018923924201252923\n",
      "Gradient Descent(79/99): loss=0.3899631030570805, gradient=0.0014856201699837376\n",
      "Gradient Descent(80/99): loss=0.389963055780566, gradient=0.0013284894188705075\n",
      "Gradient Descent(81/99): loss=0.38996306490362787, gradient=0.0012692103676379032\n",
      "Gradient Descent(82/99): loss=0.38996302386562076, gradient=0.0012818189815477035\n",
      "Gradient Descent(83/99): loss=0.3899630277381367, gradient=0.0012565917502852838\n",
      "Gradient Descent(84/99): loss=0.38996299163050413, gradient=0.0012119277773184375\n",
      "Gradient Descent(85/99): loss=0.38996299141522006, gradient=0.001311653914183377\n",
      "Gradient Descent(86/99): loss=0.38996295141881315, gradient=0.0012209186230609799\n",
      "Gradient Descent(87/99): loss=0.38996295144866144, gradient=0.0012944229262540979\n",
      "Gradient Descent(88/99): loss=0.38996284726093455, gradient=0.0018132455687814965\n",
      "Gradient Descent(89/99): loss=0.38996288363803716, gradient=0.001482815285276092\n",
      "Gradient Descent(90/99): loss=0.3899627765734283, gradient=0.0018591708217316123\n",
      "Gradient Descent(91/99): loss=0.38996282095820084, gradient=0.0014824127957860584\n",
      "Gradient Descent(92/99): loss=0.38996277679566926, gradient=0.0012906746694761266\n",
      "Gradient Descent(93/99): loss=0.3899627919071236, gradient=0.0012586510356337793\n",
      "Gradient Descent(94/99): loss=0.38996275326704916, gradient=0.0012475940755639074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(95/99): loss=0.38996269719937066, gradient=0.001834605915786576\n",
      "Gradient Descent(96/99): loss=0.3899626304083095, gradient=0.0014215765247315281\n",
      "Gradient Descent(97/99): loss=0.3899626587091767, gradient=0.0012743413802759264\n",
      "Gradient Descent(98/99): loss=0.3899625652123012, gradient=0.0018221380296468378\n",
      "Gradient Descent(99/99): loss=0.3899626170865109, gradient=0.0014478954375562489\n",
      "Gradient Descent(0/99): loss=0.38969371008991155, gradient=0.01078714966423373\n",
      "Gradient Descent(1/99): loss=0.3896885817044755, gradient=0.0077329725642981875\n",
      "Gradient Descent(2/99): loss=0.3896852769389361, gradient=0.0060843351053170855\n",
      "Gradient Descent(3/99): loss=0.3896827352517731, gradient=0.00510131682462774\n",
      "Gradient Descent(4/99): loss=0.38968085180500517, gradient=0.0044465035916875315\n",
      "Gradient Descent(5/99): loss=0.38967939628497394, gradient=0.003930237056980901\n",
      "Gradient Descent(6/99): loss=0.38967824785769606, gradient=0.0036013570616610937\n",
      "Gradient Descent(7/99): loss=0.38967737925213897, gradient=0.003183877143348629\n",
      "Gradient Descent(8/99): loss=0.38967665169739996, gradient=0.0029387622666689275\n",
      "Gradient Descent(9/99): loss=0.38967609211124615, gradient=0.002692192933828609\n",
      "Gradient Descent(10/99): loss=0.38967561386633315, gradient=0.0024804149632712065\n",
      "Gradient Descent(11/99): loss=0.3896752372684985, gradient=0.002366002554216257\n",
      "Gradient Descent(12/99): loss=0.3896749063293483, gradient=0.0021755573989788554\n",
      "Gradient Descent(13/99): loss=0.38967461664427316, gradient=0.002100181367953975\n",
      "Gradient Descent(14/99): loss=0.38967441055043006, gradient=0.0018857673460489957\n",
      "Gradient Descent(15/99): loss=0.38967423483936375, gradient=0.0017920266672110622\n",
      "Gradient Descent(16/99): loss=0.3896741005375468, gradient=0.0019445697529000122\n",
      "Gradient Descent(17/99): loss=0.38967393144082324, gradient=0.0018023460661207049\n",
      "Gradient Descent(18/99): loss=0.3896738854416218, gradient=0.0015624341564519214\n",
      "Gradient Descent(19/99): loss=0.38967373709969694, gradient=0.0017818946615254475\n",
      "Gradient Descent(20/99): loss=0.38967369475763686, gradient=0.0013580183357736666\n",
      "Gradient Descent(21/99): loss=0.38967357774756534, gradient=0.0018352874180383213\n",
      "Gradient Descent(22/99): loss=0.3896735723711591, gradient=0.0013900975704848539\n",
      "Gradient Descent(23/99): loss=0.389673544676881, gradient=0.0016141519883900656\n",
      "Gradient Descent(24/99): loss=0.3896734740410095, gradient=0.001543122546461494\n",
      "Gradient Descent(25/99): loss=0.38967351880420564, gradient=0.001224331095126206\n",
      "Gradient Descent(26/99): loss=0.3896735099069201, gradient=0.0010665420317402699\n",
      "Gradient Descent(27/99): loss=0.3896734350843783, gradient=0.0016594716830307373\n",
      "Gradient Descent(28/99): loss=0.38967345969632267, gradient=0.0010303393722920844\n",
      "Gradient Descent(29/99): loss=0.3896734793818544, gradient=0.0009856637858212535\n",
      "Gradient Descent(30/99): loss=0.38967347288774823, gradient=0.0012607821225142002\n",
      "Gradient Descent(31/99): loss=0.3896735274452647, gradient=0.0010272519958570464\n",
      "Gradient Descent(32/99): loss=0.3896734848354389, gradient=0.0013926687673700721\n",
      "Gradient Descent(33/99): loss=0.38967345363721145, gradient=0.0013760137266880842\n",
      "Gradient Descent(34/99): loss=0.389673526788346, gradient=0.0010252321534764326\n",
      "Gradient Descent(35/99): loss=0.38967354390424785, gradient=0.0008777888421606239\n",
      "Gradient Descent(36/99): loss=0.3896735450100624, gradient=0.0012021466133997937\n",
      "Gradient Descent(37/99): loss=0.389673557350803, gradient=0.0015256223029215612\n",
      "Gradient Descent(38/99): loss=0.38967354254870773, gradient=0.0009652290501908623\n",
      "Gradient Descent(39/99): loss=0.38967356087821453, gradient=0.0011724938193234029\n",
      "Gradient Descent(40/99): loss=0.389673629688512, gradient=0.0009114909570089624\n",
      "Gradient Descent(41/99): loss=0.38967365225004735, gradient=0.0008068830544586183\n",
      "Gradient Descent(42/99): loss=0.3896735889687675, gradient=0.0016172304649295915\n",
      "Gradient Descent(43/99): loss=0.38967363775846486, gradient=0.0008136936152443569\n",
      "Gradient Descent(44/99): loss=0.389673599600182, gradient=0.0015428921046967638\n",
      "Gradient Descent(45/99): loss=0.3896736863068166, gradient=0.0008889010336121969\n",
      "Gradient Descent(46/99): loss=0.3896736853286733, gradient=0.001221140066194573\n",
      "Gradient Descent(47/99): loss=0.38967376184176145, gradient=0.0008732751074020968\n",
      "Gradient Descent(48/99): loss=0.38967373185206294, gradient=0.0013171720652979644\n",
      "Gradient Descent(49/99): loss=0.38967371062818607, gradient=0.0013185850333850549\n",
      "Gradient Descent(50/99): loss=0.3896737965764535, gradient=0.0008997151897729415\n",
      "Gradient Descent(51/99): loss=0.38967382422252406, gradient=0.0007431648928683518\n",
      "Gradient Descent(52/99): loss=0.38967383072432743, gradient=0.0011482128429723265\n",
      "Gradient Descent(53/99): loss=0.38967389999333746, gradient=0.0008341913980115108\n",
      "Gradient Descent(54/99): loss=0.3896738654103763, gradient=0.0013159860726259913\n",
      "Gradient Descent(55/99): loss=0.3896738717523388, gradient=0.0008012298255753634\n",
      "Gradient Descent(56/99): loss=0.3896738884997734, gradient=0.0011346553555547536\n",
      "Gradient Descent(57/99): loss=0.38967396177095703, gradient=0.0008137113249903045\n",
      "Gradient Descent(58/99): loss=0.3896739871956868, gradient=0.0007014164674426099\n",
      "Gradient Descent(59/99): loss=0.3896739885641907, gradient=0.0011343992736535427\n",
      "Gradient Descent(60/99): loss=0.3896740016713561, gradient=0.0014759146857612868\n",
      "Gradient Descent(61/99): loss=0.3896739911691844, gradient=0.000839585430314973\n",
      "Gradient Descent(62/99): loss=0.38967404381217285, gradient=0.0006921747713853367\n",
      "Gradient Descent(63/99): loss=0.38967404533526223, gradient=0.0011475580394300217\n",
      "Gradient Descent(64/99): loss=0.3896741164580724, gradient=0.0007981325645905586\n",
      "Gradient Descent(65/99): loss=0.3896741380326735, gradient=0.000672242763595303\n",
      "Gradient Descent(66/99): loss=0.3896740676893141, gradient=0.001604761974632698\n",
      "Gradient Descent(67/99): loss=0.3896741166480124, gradient=0.0006899521081349847\n",
      "Gradient Descent(68/99): loss=0.3896741564927642, gradient=0.0006693212466926407\n",
      "Gradient Descent(69/99): loss=0.3896741602058888, gradient=0.0011233196184785034\n",
      "Gradient Descent(70/99): loss=0.3896742298235143, gradient=0.0007827909823389922\n",
      "Gradient Descent(71/99): loss=0.38967417148869327, gradient=0.001513081626159177\n",
      "Gradient Descent(72/99): loss=0.389674251596333, gradient=0.0007855001680048919\n",
      "Gradient Descent(73/99): loss=0.3896741719064002, gradient=0.0016102209306314214\n",
      "Gradient Descent(74/99): loss=0.38967422792313866, gradient=0.0006678855099507156\n",
      "Gradient Descent(75/99): loss=0.3896742699493691, gradient=0.0006455154962352434\n",
      "Gradient Descent(76/99): loss=0.38967427443853797, gradient=0.001122760307570142\n",
      "Gradient Descent(77/99): loss=0.3896743455035344, gradient=0.0007615560430334673\n",
      "Gradient Descent(78/99): loss=0.3896743671276812, gradient=0.0006332160927783807\n",
      "Gradient Descent(79/99): loss=0.3896743992001394, gradient=0.0006154515378855752\n",
      "Gradient Descent(80/99): loss=0.3896743209277655, gradient=0.001606364566509012\n",
      "Gradient Descent(81/99): loss=0.389674368618602, gradient=0.0006436129365374358\n",
      "Gradient Descent(82/99): loss=0.3896744062756294, gradient=0.0006241563086553741\n",
      "Gradient Descent(83/99): loss=0.38967440673410225, gradient=0.0011096806967305236\n",
      "Gradient Descent(84/99): loss=0.38967447432625335, gradient=0.0007468400601046317\n",
      "Gradient Descent(85/99): loss=0.38967449305618546, gradient=0.0006153786473140608\n",
      "Gradient Descent(86/99): loss=0.3896744648386526, gradient=0.0013028881186385557\n",
      "Gradient Descent(87/99): loss=0.3896744311043342, gradient=0.001279766044901202\n",
      "Gradient Descent(88/99): loss=0.3896745114690968, gradient=0.0007945047734568977\n",
      "Gradient Descent(89/99): loss=0.38967453317488265, gradient=0.0006137185630453758\n",
      "Gradient Descent(90/99): loss=0.38967453183812084, gradient=0.0010938267061345655\n",
      "Gradient Descent(91/99): loss=0.3896745957791952, gradient=0.0007290273378605365\n",
      "Gradient Descent(92/99): loss=0.389674611884324, gradient=0.0005978172499895089\n",
      "Gradient Descent(93/99): loss=0.3896745804772309, gradient=0.001301888030522341\n",
      "Gradient Descent(94/99): loss=0.38967454460517165, gradient=0.0012731437156160604\n",
      "Gradient Descent(95/99): loss=0.3896746232395095, gradient=0.0007816048996994951\n",
      "Gradient Descent(96/99): loss=0.38967456510471205, gradient=0.0014863933729073744\n",
      "Gradient Descent(97/99): loss=0.3896746048949477, gradient=0.001072460368014825\n",
      "Gradient Descent(98/99): loss=0.3896746680819295, gradient=0.0006615196378016447\n",
      "Gradient Descent(99/99): loss=0.3896746917066818, gradient=0.0005769833312406646\n",
      "Gradient Descent(0/99): loss=0.38898171834040896, gradient=0.01011030527362959\n",
      "Gradient Descent(1/99): loss=0.3889790422853365, gradient=0.008204361280917873\n",
      "Gradient Descent(2/99): loss=0.38897860087197783, gradient=0.005925577555791168\n",
      "Gradient Descent(3/99): loss=0.3889779525534101, gradient=0.005152019395844133\n",
      "Gradient Descent(4/99): loss=0.38897808212471235, gradient=0.004207935159652331\n",
      "Gradient Descent(5/99): loss=0.38897817427554643, gradient=0.003763902608845742\n",
      "Gradient Descent(6/99): loss=0.38897843962491757, gradient=0.0034952803873259854\n",
      "Gradient Descent(7/99): loss=0.3889787673334107, gradient=0.003289483601159402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(8/99): loss=0.38897898588480395, gradient=0.0037043574261735935\n",
      "Gradient Descent(9/99): loss=0.38897954091002485, gradient=0.0031071216969020513\n",
      "Gradient Descent(10/99): loss=0.3889799341799411, gradient=0.002899627646064403\n",
      "Gradient Descent(11/99): loss=0.38898038776506433, gradient=0.002806201839231312\n",
      "Gradient Descent(12/99): loss=0.3889805130733958, gradient=0.003883372603000154\n",
      "Gradient Descent(13/99): loss=0.388981132621826, gradient=0.002860180163842877\n",
      "Gradient Descent(14/99): loss=0.3889815690027326, gradient=0.002676393384541462\n",
      "Gradient Descent(15/99): loss=0.38898205236441025, gradient=0.0026098423171080935\n",
      "Gradient Descent(16/99): loss=0.38898220134745193, gradient=0.00374583988577689\n",
      "Gradient Descent(17/99): loss=0.38898282982167554, gradient=0.0027193262824491255\n",
      "Gradient Descent(18/99): loss=0.3889832704455013, gradient=0.0025399463508333714\n",
      "Gradient Descent(19/99): loss=0.38898345134204065, gradient=0.0036290191628720017\n",
      "Gradient Descent(20/99): loss=0.3889840756201127, gradient=0.002658625653628001\n",
      "Gradient Descent(21/99): loss=0.3889845144340562, gradient=0.0024782345223899847\n",
      "Gradient Descent(22/99): loss=0.38898499565670913, gradient=0.0024195232232553125\n",
      "Gradient Descent(23/99): loss=0.38898514695907155, gradient=0.0035957390210601264\n",
      "Gradient Descent(24/99): loss=0.38898576381752087, gradient=0.00256642214235608\n",
      "Gradient Descent(25/99): loss=0.38898619206989127, gradient=0.002381984303805973\n",
      "Gradient Descent(26/99): loss=0.38898636185243624, gradient=0.003514734713304362\n",
      "Gradient Descent(27/99): loss=0.38898697019656925, gradient=0.0025247602336233984\n",
      "Gradient Descent(28/99): loss=0.3889873932924414, gradient=0.002338237318119423\n",
      "Gradient Descent(29/99): loss=0.38898785889431425, gradient=0.0022804118294258878\n",
      "Gradient Descent(30/99): loss=0.3889877160989067, gradient=0.004196533085245167\n",
      "Gradient Descent(31/99): loss=0.3889882646408932, gradient=0.0023205676518573905\n",
      "Gradient Descent(32/99): loss=0.38898876895782347, gradient=0.0022584669091667503\n",
      "Gradient Descent(33/99): loss=0.3889891082224405, gradient=0.002879227210659004\n",
      "Gradient Descent(34/99): loss=0.38898919482259264, gradient=0.003936861088185148\n",
      "Gradient Descent(35/99): loss=0.3889899457837661, gradient=0.002774052318006761\n",
      "Gradient Descent(36/99): loss=0.38899035403891846, gradient=0.002201147054693583\n",
      "Gradient Descent(37/99): loss=0.3889904654844976, gradient=0.0036437795770373365\n",
      "Gradient Descent(38/99): loss=0.3889906912993416, gradient=0.003187441666840322\n",
      "Gradient Descent(39/99): loss=0.388991370829538, gradient=0.0023727270017133532\n",
      "Gradient Descent(40/99): loss=0.38899166170803057, gradient=0.002814298357058405\n",
      "Gradient Descent(41/99): loss=0.38899216347741944, gradient=0.0021602385911325757\n",
      "Gradient Descent(42/99): loss=0.388992475441145, gradient=0.00287204744402741\n",
      "Gradient Descent(43/99): loss=0.38899307198680627, gradient=0.002229458802590876\n",
      "Gradient Descent(44/99): loss=0.3889930862675043, gradient=0.0035616731993942784\n",
      "Gradient Descent(45/99): loss=0.38899344943127573, gradient=0.002211870813256558\n",
      "Gradient Descent(46/99): loss=0.38899381437870295, gradient=0.002797731881800466\n",
      "Gradient Descent(47/99): loss=0.388994266820696, gradient=0.0028485524036094374\n",
      "Gradient Descent(48/99): loss=0.3889947002921157, gradient=0.0020816524095838293\n",
      "Gradient Descent(49/99): loss=0.38899516416258756, gradient=0.0020146348519845185\n",
      "Gradient Descent(50/99): loss=0.3889951752637106, gradient=0.0035805816049485793\n",
      "Gradient Descent(51/99): loss=0.38899552280207694, gradient=0.0027529150647127316\n",
      "Gradient Descent(52/99): loss=0.388996009401593, gradient=0.0020218992409655233\n",
      "Gradient Descent(53/99): loss=0.388996479339459, gradient=0.0019616427449694607\n",
      "Gradient Descent(54/99): loss=0.3889966149522978, gradient=0.003310928452189522\n",
      "Gradient Descent(55/99): loss=0.3889972052802211, gradient=0.0021608804686973153\n",
      "Gradient Descent(56/99): loss=0.38899728741440603, gradient=0.003335128608554627\n",
      "Gradient Descent(57/99): loss=0.38899770367472397, gradient=0.003218807044824368\n",
      "Gradient Descent(58/99): loss=0.388997446449792, gradient=0.004582218546471852\n",
      "Gradient Descent(59/99): loss=0.38899823015857965, gradient=0.002725067348996578\n",
      "Gradient Descent(60/99): loss=0.38899863256267625, gradient=0.0019464899981226384\n",
      "Gradient Descent(61/99): loss=0.3889989702262142, gradient=0.0026181680354312974\n",
      "Gradient Descent(62/99): loss=0.38899926332143225, gradient=0.0027753628747702918\n",
      "Gradient Descent(63/99): loss=0.3889998490760747, gradient=0.002037366900939333\n",
      "Gradient Descent(64/99): loss=0.3890000121781901, gradient=0.0028608439133473783\n",
      "Gradient Descent(65/99): loss=0.38900016984819713, gradient=0.0027216370797929898\n",
      "Gradient Descent(66/99): loss=0.38900050700911637, gradient=0.00269211699391203\n",
      "Gradient Descent(67/99): loss=0.3890010903620381, gradient=0.001983474195368387\n",
      "Gradient Descent(68/99): loss=0.38900132692083256, gradient=0.0025752885544144833\n",
      "Gradient Descent(69/99): loss=0.38900176663667857, gradient=0.0018424520260026725\n",
      "Gradient Descent(70/99): loss=0.3890020140040985, gradient=0.0026765529436115565\n",
      "Gradient Descent(71/99): loss=0.3890023407793341, gradient=0.003143044538803152\n",
      "Gradient Descent(72/99): loss=0.3890024144489539, gradient=0.002748440231153346\n",
      "Gradient Descent(73/99): loss=0.38900291075066695, gradient=0.0018143511776937177\n",
      "Gradient Descent(74/99): loss=0.389003333401208, gradient=0.0017414176206357725\n",
      "Gradient Descent(75/99): loss=0.3890034266344916, gradient=0.003213942684704977\n",
      "Gradient Descent(76/99): loss=0.3890039812213, gradient=0.0019685695028814907\n",
      "Gradient Descent(77/99): loss=0.38900434167245673, gradient=0.001737404099463085\n",
      "Gradient Descent(78/99): loss=0.3890045109848662, gradient=0.0028456791273969895\n",
      "Gradient Descent(79/99): loss=0.38900446493316587, gradient=0.0034568368173433376\n",
      "Gradient Descent(80/99): loss=0.38900506864134626, gradient=0.002014010722698698\n",
      "Gradient Descent(81/99): loss=0.3890054388193562, gradient=0.0017063131189452782\n",
      "Gradient Descent(82/99): loss=0.3890053927121111, gradient=0.0035859303699299003\n",
      "Gradient Descent(83/99): loss=0.38900579402476493, gradient=0.0025984888393556665\n",
      "Gradient Descent(84/99): loss=0.38900632416427655, gradient=0.001787552886502559\n",
      "Gradient Descent(85/99): loss=0.3890064677864945, gradient=0.0027583607852133163\n",
      "Gradient Descent(86/99): loss=0.3890065923212333, gradient=0.0026070415681157984\n",
      "Gradient Descent(87/99): loss=0.3890068953876816, gradient=0.0025882988991517515\n",
      "Gradient Descent(88/99): loss=0.38900744753751065, gradient=0.001819729232196333\n",
      "Gradient Descent(89/99): loss=0.389007651735813, gradient=0.0024561694761836236\n",
      "Gradient Descent(90/99): loss=0.3890080589419751, gradient=0.00167264519335832\n",
      "Gradient Descent(91/99): loss=0.3890082707781325, gradient=0.0025757794736862094\n",
      "Gradient Descent(92/99): loss=0.38900856134195394, gradient=0.0030604445747930377\n",
      "Gradient Descent(93/99): loss=0.3890086013960808, gradient=0.002644987676016522\n",
      "Gradient Descent(94/99): loss=0.3890090642730513, gradient=0.0016543719436182957\n",
      "Gradient Descent(95/99): loss=0.3890094533484084, gradient=0.0015774109424353446\n",
      "Gradient Descent(96/99): loss=0.3890095108408006, gradient=0.0031368146802542335\n",
      "Gradient Descent(97/99): loss=0.389010031942028, gradient=0.0018298883285601468\n",
      "Gradient Descent(98/99): loss=0.38901004265923017, gradient=0.0031433993002554787\n",
      "Gradient Descent(99/99): loss=0.389010381685604, gradient=0.0030433282880493275\n",
      "Gradient Descent(0/99): loss=0.39019415570579863, gradient=0.017099856182684087\n",
      "Gradient Descent(1/99): loss=0.39019315262261745, gradient=0.007856981190617167\n",
      "Gradient Descent(2/99): loss=0.3901919562286558, gradient=0.00563068613569631\n",
      "Gradient Descent(3/99): loss=0.3901914906837889, gradient=0.00504250206270284\n",
      "Gradient Descent(4/99): loss=0.3901911518713282, gradient=0.004621614576358165\n",
      "Gradient Descent(5/99): loss=0.39019101990878835, gradient=0.004257700083336054\n",
      "Gradient Descent(6/99): loss=0.3901909436042698, gradient=0.004210799079383814\n",
      "Gradient Descent(7/99): loss=0.3901910169364115, gradient=0.004106507012784195\n",
      "Gradient Descent(8/99): loss=0.3901910721207395, gradient=0.0036948424746082106\n",
      "Gradient Descent(9/99): loss=0.3901910388466054, gradient=0.004248801950120923\n",
      "Gradient Descent(10/99): loss=0.39019103489588575, gradient=0.00436138721121291\n",
      "Gradient Descent(11/99): loss=0.3901914363342396, gradient=0.003672990147743171\n",
      "Gradient Descent(12/99): loss=0.390191497232588, gradient=0.0035307990771759412\n",
      "Gradient Descent(13/99): loss=0.39019190750367155, gradient=0.0029615371005255685\n",
      "Gradient Descent(14/99): loss=0.3901921020664141, gradient=0.0033012923558985636\n",
      "Gradient Descent(15/99): loss=0.3901922450452116, gradient=0.003127482155776192\n",
      "Gradient Descent(16/99): loss=0.3901926525889852, gradient=0.002672398998485353\n",
      "Gradient Descent(17/99): loss=0.39019274628980805, gradient=0.0035865635150525207\n",
      "Gradient Descent(18/99): loss=0.39019301484201224, gradient=0.0038277674119448404\n",
      "Gradient Descent(19/99): loss=0.39019278490035747, gradient=0.004633846138164643\n",
      "Gradient Descent(20/99): loss=0.39019338500554895, gradient=0.0033598190107716483\n",
      "Gradient Descent(21/99): loss=0.39019368225942525, gradient=0.0025377161930608388\n",
      "Gradient Descent(22/99): loss=0.39019392767904304, gradient=0.0031364544652660825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(23/99): loss=0.39019406248632627, gradient=0.0030453967207617516\n",
      "Gradient Descent(24/99): loss=0.39019451322094373, gradient=0.0023958135004688995\n",
      "Gradient Descent(25/99): loss=0.39019487267018044, gradient=0.002367135490521568\n",
      "Gradient Descent(26/99): loss=0.39019515681130434, gradient=0.002642951440816278\n",
      "Gradient Descent(27/99): loss=0.39019500359582576, gradient=0.0039600228039739435\n",
      "Gradient Descent(28/99): loss=0.3901954528774818, gradient=0.0022486936804647015\n",
      "Gradient Descent(29/99): loss=0.39019572425950716, gradient=0.002819380316951201\n",
      "Gradient Descent(30/99): loss=0.3901961101715185, gradient=0.0022770951347583024\n",
      "Gradient Descent(31/99): loss=0.3901963803576695, gradient=0.0026931143369425146\n",
      "Gradient Descent(32/99): loss=0.3901967298189809, gradient=0.0023040997577050432\n",
      "Gradient Descent(33/99): loss=0.3901962517283987, gradient=0.004792077488486116\n",
      "Gradient Descent(34/99): loss=0.39019679360659004, gradient=0.0027878388041635356\n",
      "Gradient Descent(35/99): loss=0.3901969296310144, gradient=0.003310764166102039\n",
      "Gradient Descent(36/99): loss=0.3901975040493283, gradient=0.0023321007442868237\n",
      "Gradient Descent(37/99): loss=0.390197745501408, gradient=0.0027081897935793196\n",
      "Gradient Descent(38/99): loss=0.39019813144195065, gradient=0.0022104725499081353\n",
      "Gradient Descent(39/99): loss=0.39019840480755863, gradient=0.0024678693260512057\n",
      "Gradient Descent(40/99): loss=0.3901985087973346, gradient=0.003156805324634459\n",
      "Gradient Descent(41/99): loss=0.390198768456695, gradient=0.0021840442272645874\n",
      "Gradient Descent(42/99): loss=0.3901990664013683, gradient=0.0027151401864933407\n",
      "Gradient Descent(43/99): loss=0.3901992050374948, gradient=0.003146010619669529\n",
      "Gradient Descent(44/99): loss=0.39019968355077905, gradient=0.00228725729372715\n",
      "Gradient Descent(45/99): loss=0.3901998997073312, gradient=0.0024659558866006607\n",
      "Gradient Descent(46/99): loss=0.3902002328052745, gradient=0.0021109395525895514\n",
      "Gradient Descent(47/99): loss=0.3902003451499258, gradient=0.0030594600458877562\n",
      "Gradient Descent(48/99): loss=0.39020017956819214, gradient=0.0038750611642232257\n",
      "Gradient Descent(49/99): loss=0.3902007623883791, gradient=0.0023599425447240327\n",
      "Gradient Descent(50/99): loss=0.390200773589276, gradient=0.0035099346130358284\n",
      "Gradient Descent(51/99): loss=0.39020128366983936, gradient=0.0023354070332568203\n",
      "Gradient Descent(52/99): loss=0.39020128550967165, gradient=0.003350952301753909\n",
      "Gradient Descent(53/99): loss=0.3902017280658183, gradient=0.0025868565212463874\n",
      "Gradient Descent(54/99): loss=0.3902018035265182, gradient=0.003029126893149778\n",
      "Gradient Descent(55/99): loss=0.390202067793526, gradient=0.002145600344137632\n",
      "Gradient Descent(56/99): loss=0.39020237323434437, gradient=0.002606047422646434\n",
      "Gradient Descent(57/99): loss=0.3902025060869892, gradient=0.0030986433912746927\n",
      "Gradient Descent(58/99): loss=0.3902029103895541, gradient=0.002507161976823909\n",
      "Gradient Descent(59/99): loss=0.39020318687932365, gradient=0.0020865283358427987\n",
      "Gradient Descent(60/99): loss=0.390203541181425, gradient=0.001923655543671368\n",
      "Gradient Descent(61/99): loss=0.39020349444937874, gradient=0.0034940819405944365\n",
      "Gradient Descent(62/99): loss=0.390203713344693, gradient=0.0021519827864423468\n",
      "Gradient Descent(63/99): loss=0.3902038618835998, gradient=0.003003130055904601\n",
      "Gradient Descent(64/99): loss=0.39020424594741054, gradient=0.002509015292033996\n",
      "Gradient Descent(65/99): loss=0.3902042303665543, gradient=0.0033127960302116693\n",
      "Gradient Descent(66/99): loss=0.39020475626813483, gradient=0.002124591748370295\n",
      "Gradient Descent(67/99): loss=0.39020492983469607, gradient=0.002600670739165903\n",
      "Gradient Descent(68/99): loss=0.3902052676620775, gradient=0.0020088033001680636\n",
      "Gradient Descent(69/99): loss=0.3902049765336861, gradient=0.004081212693159789\n",
      "Gradient Descent(70/99): loss=0.39020535926033095, gradient=0.0021035562066087274\n",
      "Gradient Descent(71/99): loss=0.3902056898221963, gradient=0.00199444265088995\n",
      "Gradient Descent(72/99): loss=0.3902059325270784, gradient=0.002360926157553677\n",
      "Gradient Descent(73/99): loss=0.3902059403563546, gradient=0.003255121087564171\n",
      "Gradient Descent(74/99): loss=0.3902064430258835, gradient=0.0021257295402863697\n",
      "Gradient Descent(75/99): loss=0.3902064155307397, gradient=0.003298433656199623\n",
      "Gradient Descent(76/99): loss=0.390206852566885, gradient=0.002195600479773498\n",
      "Gradient Descent(77/99): loss=0.39020689725475066, gradient=0.0029147109002671937\n",
      "Gradient Descent(78/99): loss=0.39020696142068967, gradient=0.0027376658699218397\n",
      "Gradient Descent(79/99): loss=0.39020734321959866, gradient=0.0019462826311915041\n",
      "Gradient Descent(80/99): loss=0.39020754739507113, gradient=0.0025149424244751725\n",
      "Gradient Descent(81/99): loss=0.3902076434013351, gradient=0.0029807242245187228\n",
      "Gradient Descent(82/99): loss=0.3902080583604175, gradient=0.0021288905339724956\n",
      "Gradient Descent(83/99): loss=0.39020791788097964, gradient=0.0035202541874578\n",
      "Gradient Descent(84/99): loss=0.3902083735042723, gradient=0.002159425203174317\n",
      "Gradient Descent(85/99): loss=0.3902086420740512, gradient=0.0018959713697963396\n",
      "Gradient Descent(86/99): loss=0.3902083263369599, gradient=0.00408801176647281\n",
      "Gradient Descent(87/99): loss=0.3902086896159806, gradient=0.0020080467189012214\n",
      "Gradient Descent(88/99): loss=0.3902089160875198, gradient=0.0024532636534946883\n",
      "Gradient Descent(89/99): loss=0.39020921468956893, gradient=0.0020507996901964736\n",
      "Gradient Descent(90/99): loss=0.39020949204669286, gradient=0.0019309419373594408\n",
      "Gradient Descent(91/99): loss=0.3902096812650654, gradient=0.00231361020576143\n",
      "Gradient Descent(92/99): loss=0.39020976328907114, gradient=0.002865612692214938\n",
      "Gradient Descent(93/99): loss=0.39020990477007766, gradient=0.003322745961404471\n",
      "Gradient Descent(94/99): loss=0.3902099176871567, gradient=0.00250309807012957\n",
      "Gradient Descent(95/99): loss=0.3902102447147635, gradient=0.0019883221809208745\n",
      "Gradient Descent(96/99): loss=0.39021054578461, gradient=0.0018090505762445657\n",
      "Gradient Descent(97/99): loss=0.39021069463549407, gradient=0.0025689929411649498\n",
      "Gradient Descent(98/99): loss=0.3902107659956402, gradient=0.002889075745654699\n",
      "Gradient Descent(99/99): loss=0.39021105937551326, gradient=0.0025913385379955655\n",
      "Gradient Descent(0/99): loss=0.3904768092227711, gradient=0.007750115640770127\n",
      "Gradient Descent(1/99): loss=0.3904745036456405, gradient=0.006483148466165354\n",
      "Gradient Descent(2/99): loss=0.39047261435346525, gradient=0.005772030418787163\n",
      "Gradient Descent(3/99): loss=0.3904715127342754, gradient=0.004832329644338267\n",
      "Gradient Descent(4/99): loss=0.3904702983061417, gradient=0.004643333406135149\n",
      "Gradient Descent(5/99): loss=0.39046911780581317, gradient=0.0047581620143635345\n",
      "Gradient Descent(6/99): loss=0.39046825716394507, gradient=0.003911383889434036\n",
      "Gradient Descent(7/99): loss=0.3904673651276214, gradient=0.0040231806153764324\n",
      "Gradient Descent(8/99): loss=0.39046662279008665, gradient=0.003616498767719426\n",
      "Gradient Descent(9/99): loss=0.39046583333818197, gradient=0.0037623225275757993\n",
      "Gradient Descent(10/99): loss=0.3904649594924759, gradient=0.004041267007042188\n",
      "Gradient Descent(11/99): loss=0.39046433331187164, gradient=0.003398636440814308\n",
      "Gradient Descent(12/99): loss=0.39046363597966943, gradient=0.003628529239991342\n",
      "Gradient Descent(13/99): loss=0.39046305478047155, gradient=0.0032912452676926813\n",
      "Gradient Descent(14/99): loss=0.39046249362036284, gradient=0.0031467186293278448\n",
      "Gradient Descent(15/99): loss=0.3904616648920196, gradient=0.003993122354851135\n",
      "Gradient Descent(16/99): loss=0.39046114855502156, gradient=0.003083451045336111\n",
      "Gradient Descent(17/99): loss=0.3904605700383911, gradient=0.00330624809821076\n",
      "Gradient Descent(18/99): loss=0.39046009029478523, gradient=0.002989833059448726\n",
      "Gradient Descent(19/99): loss=0.3904594479642512, gradient=0.0036111551461051592\n",
      "Gradient Descent(20/99): loss=0.3904589070090158, gradient=0.003328164119434695\n",
      "Gradient Descent(21/99): loss=0.3904584750563671, gradient=0.0029854004030220567\n",
      "Gradient Descent(22/99): loss=0.39045805412443363, gradient=0.0028592361265578226\n",
      "Gradient Descent(23/99): loss=0.3904573761900359, gradient=0.0037336983027546214\n",
      "Gradient Descent(24/99): loss=0.390456981362679, gradient=0.00284608062407768\n",
      "Gradient Descent(25/99): loss=0.3904565199128877, gradient=0.003099742146964742\n",
      "Gradient Descent(26/99): loss=0.3904561518630182, gradient=0.0027782792828978348\n",
      "Gradient Descent(27/99): loss=0.39045562488319424, gradient=0.003420639289911502\n",
      "Gradient Descent(28/99): loss=0.39045518329823226, gradient=0.0031703411095146735\n",
      "Gradient Descent(29/99): loss=0.39045486576119603, gradient=0.0027434447612600683\n",
      "Gradient Descent(30/99): loss=0.39045444683403735, gradient=0.003122103023880536\n",
      "Gradient Descent(31/99): loss=0.3904539592508628, gradient=0.0033946428793666153\n",
      "Gradient Descent(32/99): loss=0.39045362920652743, gradient=0.0027720593776747993\n",
      "Gradient Descent(33/99): loss=0.3904532600188136, gradient=0.00293720439909135\n",
      "Gradient Descent(34/99): loss=0.3904529578009112, gradient=0.0026990563751180603\n",
      "Gradient Descent(35/99): loss=0.3904525222664629, gradient=0.003275533975663023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(36/99): loss=0.3904518736119242, gradient=0.003977940826323307\n",
      "Gradient Descent(37/99): loss=0.3904517654572148, gradient=0.002790860342590298\n",
      "Gradient Descent(38/99): loss=0.39045138706970256, gradient=0.0030152236878318566\n",
      "Gradient Descent(39/99): loss=0.39045097295447323, gradient=0.0033370095760282418\n",
      "Gradient Descent(40/99): loss=0.3904504517648798, gradient=0.0036071012760938216\n",
      "Gradient Descent(41/99): loss=0.39045004828061525, gradient=0.002977672604563631\n",
      "Gradient Descent(42/99): loss=0.3904499102152256, gradient=0.0025942217920008554\n",
      "Gradient Descent(43/99): loss=0.3904494776116507, gradient=0.003473517777463281\n",
      "Gradient Descent(44/99): loss=0.39044929333137324, gradient=0.002618063806893482\n",
      "Gradient Descent(45/99): loss=0.3904491286656023, gradient=0.0025766838712683385\n",
      "Gradient Descent(46/99): loss=0.39044861712868006, gradient=0.0037951875310967033\n",
      "Gradient Descent(47/99): loss=0.3904484063917712, gradient=0.0033900086659463774\n",
      "Gradient Descent(48/99): loss=0.39044787285766047, gradient=0.003739110611951889\n",
      "Gradient Descent(49/99): loss=0.3904478112026396, gradient=0.0029984582032994652\n",
      "Gradient Descent(50/99): loss=0.3904476300802482, gradient=0.0025264703692999086\n",
      "Gradient Descent(51/99): loss=0.3904469877054757, gradient=0.004209522442431975\n",
      "Gradient Descent(52/99): loss=0.3904467365635465, gradient=0.002679092371449049\n",
      "Gradient Descent(53/99): loss=0.39044669565367385, gradient=0.002545442453045181\n",
      "Gradient Descent(54/99): loss=0.3904460264131871, gradient=0.004481831460068427\n",
      "Gradient Descent(55/99): loss=0.39044596210102267, gradient=0.0034864131622132843\n",
      "Gradient Descent(56/99): loss=0.39044545293428257, gradient=0.003953515901524704\n",
      "Gradient Descent(57/99): loss=0.3904455369677385, gradient=0.0028739130347522593\n",
      "Gradient Descent(58/99): loss=0.3904454464471571, gradient=0.0024288440324788853\n",
      "Gradient Descent(59/99): loss=0.39044510747621747, gradient=0.003487403324199849\n",
      "Gradient Descent(60/99): loss=0.3904445161856122, gradient=0.004111779184338916\n",
      "Gradient Descent(61/99): loss=0.39044428013095117, gradient=0.0034624262978291396\n",
      "Gradient Descent(62/99): loss=0.3904442539758365, gradient=0.0029492933845038922\n",
      "Gradient Descent(63/99): loss=0.3904440353447814, gradient=0.003284228669541363\n",
      "Gradient Descent(64/99): loss=0.3904439356541729, gradient=0.0028815860216743457\n",
      "Gradient Descent(65/99): loss=0.3904438821095385, gradient=0.002645085828085225\n",
      "Gradient Descent(66/99): loss=0.3904436252867971, gradient=0.0033049678669733996\n",
      "Gradient Descent(67/99): loss=0.39044344017847965, gradient=0.0035571572603214857\n",
      "Gradient Descent(68/99): loss=0.39044310715753633, gradient=0.0034119601405293598\n",
      "Gradient Descent(69/99): loss=0.3904428259228275, gradient=0.0037700250826520213\n",
      "Gradient Descent(70/99): loss=0.39044267232275814, gradient=0.0026177264474308653\n",
      "Gradient Descent(71/99): loss=0.39044254810561957, gradient=0.0032340508513637404\n",
      "Gradient Descent(72/99): loss=0.39044224475369804, gradient=0.0036667138303700694\n",
      "Gradient Descent(73/99): loss=0.3904423722723521, gradient=0.0027296766138871324\n",
      "Gradient Descent(74/99): loss=0.3904420846486065, gradient=0.003355752269584775\n",
      "Gradient Descent(75/99): loss=0.390441885679825, gradient=0.0034969997214957436\n",
      "Gradient Descent(76/99): loss=0.3904418600915451, gradient=0.002507033413247484\n",
      "Gradient Descent(77/99): loss=0.39044176930826613, gradient=0.002812342902304247\n",
      "Gradient Descent(78/99): loss=0.39044125349364667, gradient=0.004055083323752419\n",
      "Gradient Descent(79/99): loss=0.3904409283479105, gradient=0.0038836550387277478\n",
      "Gradient Descent(80/99): loss=0.3904409492996064, gradient=0.002991534929816469\n",
      "Gradient Descent(81/99): loss=0.3904409916656004, gradient=0.0025184031435934316\n",
      "Gradient Descent(82/99): loss=0.3904409287919821, gradient=0.002928994159499923\n",
      "Gradient Descent(83/99): loss=0.3904405695201139, gradient=0.003882813516254855\n",
      "Gradient Descent(84/99): loss=0.39044069511074575, gradient=0.0026949236325370263\n",
      "Gradient Descent(85/99): loss=0.390440364715248, gradient=0.0035590630132414282\n",
      "Gradient Descent(86/99): loss=0.3904404458850615, gradient=0.0025754176256296813\n",
      "Gradient Descent(87/99): loss=0.3904400422708202, gradient=0.0039282401525912326\n",
      "Gradient Descent(88/99): loss=0.39043965981343004, gradient=0.0038920180767942148\n",
      "Gradient Descent(89/99): loss=0.39043989269585344, gradient=0.002724255376350851\n",
      "Gradient Descent(90/99): loss=0.3904395897488, gradient=0.0036546320651051675\n",
      "Gradient Descent(91/99): loss=0.390439538161323, gradient=0.0033063787652696935\n",
      "Gradient Descent(92/99): loss=0.390439550417832, gradient=0.002554501543026522\n",
      "Gradient Descent(93/99): loss=0.39043931631419854, gradient=0.003507988676676245\n",
      "Gradient Descent(94/99): loss=0.3904394779133464, gradient=0.0026199541006301607\n",
      "Gradient Descent(95/99): loss=0.3904389905128656, gradient=0.0040645405533086135\n",
      "Gradient Descent(96/99): loss=0.3904388550200785, gradient=0.003556026767853621\n",
      "Gradient Descent(97/99): loss=0.3904387793425889, gradient=0.002656309792947187\n",
      "Gradient Descent(98/99): loss=0.39043861452603323, gradient=0.003579635175613343\n",
      "Gradient Descent(99/99): loss=0.39043863422810815, gradient=0.003351136899471837\n",
      "Gradient Descent(0/99): loss=0.3900664548112916, gradient=0.01099991023112405\n",
      "Gradient Descent(1/99): loss=0.39006061191475744, gradient=0.007444667827188168\n",
      "Gradient Descent(2/99): loss=0.3900566818496655, gradient=0.006086042215053274\n",
      "Gradient Descent(3/99): loss=0.3900540696329151, gradient=0.004850985925773921\n",
      "Gradient Descent(4/99): loss=0.3900519333470591, gradient=0.0044826275364219905\n",
      "Gradient Descent(5/99): loss=0.3900504046367267, gradient=0.0041479607656645275\n",
      "Gradient Descent(6/99): loss=0.39004918434883457, gradient=0.003311144804026313\n",
      "Gradient Descent(7/99): loss=0.39004822239951786, gradient=0.0032227591982900406\n",
      "Gradient Descent(8/99): loss=0.39004744854424545, gradient=0.0034291327946742592\n",
      "Gradient Descent(9/99): loss=0.3900467840806712, gradient=0.0029023526367699345\n",
      "Gradient Descent(10/99): loss=0.3900463825780018, gradient=0.002778368861561628\n",
      "Gradient Descent(11/99): loss=0.3900455062113549, gradient=0.004130784377625045\n",
      "Gradient Descent(12/99): loss=0.39004538413912976, gradient=0.0030672678612618\n",
      "Gradient Descent(13/99): loss=0.3900447556513753, gradient=0.0037653784710558343\n",
      "Gradient Descent(14/99): loss=0.3900446306534297, gradient=0.00337151020459258\n",
      "Gradient Descent(15/99): loss=0.39004435308153257, gradient=0.0025717857697146354\n",
      "Gradient Descent(16/99): loss=0.3900443152014635, gradient=0.002249153548023193\n",
      "Gradient Descent(17/99): loss=0.390044112512636, gradient=0.0028648216741157524\n",
      "Gradient Descent(18/99): loss=0.390043945525908, gradient=0.0023547276865577625\n",
      "Gradient Descent(19/99): loss=0.3900439249881195, gradient=0.002425648036403295\n",
      "Gradient Descent(20/99): loss=0.39004365548267217, gradient=0.002757451650817635\n",
      "Gradient Descent(21/99): loss=0.390043668561285, gradient=0.002422195945091957\n",
      "Gradient Descent(22/99): loss=0.39004353373709194, gradient=0.0022570033449132183\n",
      "Gradient Descent(23/99): loss=0.3900434514959869, gradient=0.0027924958813093435\n",
      "Gradient Descent(24/99): loss=0.3900433389605026, gradient=0.0022690226638319884\n",
      "Gradient Descent(25/99): loss=0.3900433004731043, gradient=0.002698751887243053\n",
      "Gradient Descent(26/99): loss=0.39004320191268566, gradient=0.002282811125563004\n",
      "Gradient Descent(27/99): loss=0.39004327251851706, gradient=0.0022774135592670973\n",
      "Gradient Descent(28/99): loss=0.3900430806542605, gradient=0.0026375781641824955\n",
      "Gradient Descent(29/99): loss=0.3900431679216949, gradient=0.002284119761883468\n",
      "Gradient Descent(30/99): loss=0.3900430857070353, gradient=0.0021883094604101903\n",
      "Gradient Descent(31/99): loss=0.3900430739216977, gradient=0.0026511732665162316\n",
      "Gradient Descent(32/99): loss=0.39004300247580936, gradient=0.0022140630777713493\n",
      "Gradient Descent(33/99): loss=0.39004302151248355, gradient=0.0025738166239224324\n",
      "Gradient Descent(34/99): loss=0.3900426938986471, gradient=0.0035075628213708104\n",
      "Gradient Descent(35/99): loss=0.3900429482297738, gradient=0.0026059591345156305\n",
      "Gradient Descent(36/99): loss=0.3900425170752588, gradient=0.0038290563275181905\n",
      "Gradient Descent(37/99): loss=0.390042771110607, gradient=0.0023342496748782653\n",
      "Gradient Descent(38/99): loss=0.39004279565324684, gradient=0.0023976405948743684\n",
      "Gradient Descent(39/99): loss=0.390042794325135, gradient=0.002205319090997962\n",
      "Gradient Descent(40/99): loss=0.3900429527124266, gradient=0.002105449715924375\n",
      "Gradient Descent(41/99): loss=0.3900428221900726, gradient=0.002575246494780302\n",
      "Gradient Descent(42/99): loss=0.3900429782640083, gradient=0.002135649742750606\n",
      "Gradient Descent(43/99): loss=0.390042934928231, gradient=0.0021482202950201413\n",
      "Gradient Descent(44/99): loss=0.3900429806208077, gradient=0.002520976487540391\n",
      "Gradient Descent(45/99): loss=0.3900429544760723, gradient=0.0018904993675414937\n",
      "Gradient Descent(46/99): loss=0.39004254698395213, gradient=0.0035164619980603065\n",
      "Gradient Descent(47/99): loss=0.3900426446930267, gradient=0.0019300021366055766\n",
      "Gradient Descent(48/99): loss=0.3900426852759277, gradient=0.0021065375405207314\n",
      "Gradient Descent(49/99): loss=0.39004278001205084, gradient=0.0024539427128715714\n",
      "Gradient Descent(50/99): loss=0.3900427677894342, gradient=0.0021889163574260375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(51/99): loss=0.3900428687589619, gradient=0.0024057506235902696\n",
      "Gradient Descent(52/99): loss=0.3900428399297464, gradient=0.0022171880051412023\n",
      "Gradient Descent(53/99): loss=0.39004301444645423, gradient=0.00200307850575013\n",
      "Gradient Descent(54/99): loss=0.39004262069201817, gradient=0.0037158011155195673\n",
      "Gradient Descent(55/99): loss=0.3900429289490959, gradient=0.002506683823329212\n",
      "Gradient Descent(56/99): loss=0.39004287128681253, gradient=0.0022390377285042224\n",
      "Gradient Descent(57/99): loss=0.3900429654617398, gradient=0.002448867329182075\n",
      "Gradient Descent(58/99): loss=0.39004297248226116, gradient=0.0017802415802201574\n",
      "Gradient Descent(59/99): loss=0.3900428933804994, gradient=0.002492284716438528\n",
      "Gradient Descent(60/99): loss=0.39004274650101994, gradient=0.0031312765218362037\n",
      "Gradient Descent(61/99): loss=0.39004288538527715, gradient=0.0020263813123732404\n",
      "Gradient Descent(62/99): loss=0.3900429677130576, gradient=0.0023164750976037564\n",
      "Gradient Descent(63/99): loss=0.390042949710719, gradient=0.0021940466449057905\n",
      "Gradient Descent(64/99): loss=0.3900428023679207, gradient=0.003519994233183649\n",
      "Gradient Descent(65/99): loss=0.3900427012378098, gradient=0.002009012456244239\n",
      "Gradient Descent(66/99): loss=0.3900427854560841, gradient=0.002074892009480838\n",
      "Gradient Descent(67/99): loss=0.39004290339339376, gradient=0.002328349996213741\n",
      "Gradient Descent(68/99): loss=0.3900428922175769, gradient=0.0022030984306727784\n",
      "Gradient Descent(69/99): loss=0.39004301644694933, gradient=0.002304694747081377\n",
      "Gradient Descent(70/99): loss=0.3900430350822376, gradient=0.0017629730529468079\n",
      "Gradient Descent(71/99): loss=0.39004303844325144, gradient=0.0021115753532991266\n",
      "Gradient Descent(72/99): loss=0.39004312759090787, gradient=0.0023088616101120934\n",
      "Gradient Descent(73/99): loss=0.3900430813335826, gradient=0.0022093480440887464\n",
      "Gradient Descent(74/99): loss=0.3900432651679249, gradient=0.0018526671470360891\n",
      "Gradient Descent(75/99): loss=0.3900428539522938, gradient=0.0034649875161350146\n",
      "Gradient Descent(76/99): loss=0.3900430109162761, gradient=0.0020924323600786093\n",
      "Gradient Descent(77/99): loss=0.3900431048348177, gradient=0.002195784315640763\n",
      "Gradient Descent(78/99): loss=0.39004307776047187, gradient=0.0022284916713849654\n",
      "Gradient Descent(79/99): loss=0.39004327663202665, gradient=0.0018316524957884867\n",
      "Gradient Descent(80/99): loss=0.3900428714988929, gradient=0.003727057075529604\n",
      "Gradient Descent(81/99): loss=0.390042960871605, gradient=0.0036812035966381287\n",
      "Gradient Descent(82/99): loss=0.3900427484429104, gradient=0.002381775125748099\n",
      "Gradient Descent(83/99): loss=0.390042846974194, gradient=0.002160285804238609\n",
      "Gradient Descent(84/99): loss=0.3900430818029369, gradient=0.0017909104694032736\n",
      "Gradient Descent(85/99): loss=0.39004304054369743, gradient=0.0021254972188087715\n",
      "Gradient Descent(86/99): loss=0.39004305643439147, gradient=0.0021713055757073114\n",
      "Gradient Descent(87/99): loss=0.3900431834757782, gradient=0.0022148732032822964\n",
      "Gradient Descent(88/99): loss=0.3900431408352818, gradient=0.0022539263519123606\n",
      "Gradient Descent(89/99): loss=0.3900433408839869, gradient=0.001818893864581753\n",
      "Gradient Descent(90/99): loss=0.39004325902240117, gradient=0.002107232797065388\n",
      "Gradient Descent(91/99): loss=0.390043245030352, gradient=0.002165399527868336\n",
      "Gradient Descent(92/99): loss=0.3900434296224232, gradient=0.0017541644997470397\n",
      "Gradient Descent(93/99): loss=0.3900432611207778, gradient=0.0026147198640790003\n",
      "Gradient Descent(94/99): loss=0.39004345022428927, gradient=0.0018325030779797703\n",
      "Gradient Descent(95/99): loss=0.39004305971087977, gradient=0.00339556445444658\n",
      "Gradient Descent(96/99): loss=0.39004290638410066, gradient=0.003403006282571795\n",
      "Gradient Descent(97/99): loss=0.390043023089176, gradient=0.00159345687204591\n",
      "Gradient Descent(98/99): loss=0.39004305205683887, gradient=0.0021030585810701166\n",
      "Gradient Descent(99/99): loss=0.39004307623464374, gradient=0.002198172595751655\n",
      "Gradient Descent(0/99): loss=0.38976922932868535, gradient=0.012096041795021235\n",
      "Gradient Descent(1/99): loss=0.38976376201001983, gradient=0.00820470852551764\n",
      "Gradient Descent(2/99): loss=0.3897606004041866, gradient=0.0064000987885047406\n",
      "Gradient Descent(3/99): loss=0.3897580184119635, gradient=0.005430464529507797\n",
      "Gradient Descent(4/99): loss=0.3897562660037939, gradient=0.004683186834941112\n",
      "Gradient Descent(5/99): loss=0.38975481073974155, gradient=0.004608193140225452\n",
      "Gradient Descent(6/99): loss=0.38975354239580967, gradient=0.0040643052084966885\n",
      "Gradient Descent(7/99): loss=0.3897527632602389, gradient=0.004062577156008594\n",
      "Gradient Descent(8/99): loss=0.38975191496837963, gradient=0.0035829186288363324\n",
      "Gradient Descent(9/99): loss=0.38975159891701, gradient=0.003067510929872293\n",
      "Gradient Descent(10/99): loss=0.389751140173962, gradient=0.0029056784657795916\n",
      "Gradient Descent(11/99): loss=0.38975084060955567, gradient=0.003361973363551586\n",
      "Gradient Descent(12/99): loss=0.3897503970645515, gradient=0.002996192204307342\n",
      "Gradient Descent(13/99): loss=0.3897504265057197, gradient=0.0023871993102834574\n",
      "Gradient Descent(14/99): loss=0.38975020921506764, gradient=0.002505812209845483\n",
      "Gradient Descent(15/99): loss=0.38975010262571863, gradient=0.0031094573756113683\n",
      "Gradient Descent(16/99): loss=0.38974989433711504, gradient=0.002048975301427508\n",
      "Gradient Descent(17/99): loss=0.3897498668601585, gradient=0.0022534596935036674\n",
      "Gradient Descent(18/99): loss=0.38975002866822, gradient=0.0018220556971211826\n",
      "Gradient Descent(19/99): loss=0.3897500336687047, gradient=0.001560409281375344\n",
      "Gradient Descent(20/99): loss=0.3897497270007331, gradient=0.0031121430936650748\n",
      "Gradient Descent(21/99): loss=0.38974985355575115, gradient=0.0015157562756115313\n",
      "Gradient Descent(22/99): loss=0.38974995979412347, gradient=0.0014240099346126721\n",
      "Gradient Descent(23/99): loss=0.38974995422447734, gradient=0.0021628468640969003\n",
      "Gradient Descent(24/99): loss=0.38975019226003466, gradient=0.0015728537463701878\n",
      "Gradient Descent(25/99): loss=0.38975025222830817, gradient=0.0012995183242450114\n",
      "Gradient Descent(26/99): loss=0.38974996724919286, gradient=0.0031117569500954275\n",
      "Gradient Descent(27/99): loss=0.38975014041179334, gradient=0.001306425380679373\n",
      "Gradient Descent(28/99): loss=0.38975028509502735, gradient=0.0012407466072281324\n",
      "Gradient Descent(29/99): loss=0.38975042621562817, gradient=0.0011902066911368787\n",
      "Gradient Descent(30/99): loss=0.3897504308350249, gradient=0.0021199147885501797\n",
      "Gradient Descent(31/99): loss=0.3897506925849366, gradient=0.0014213196287434589\n",
      "Gradient Descent(32/99): loss=0.38975077162098226, gradient=0.0011434218856053533\n",
      "Gradient Descent(33/99): loss=0.3897507656730415, gradient=0.002085809730989044\n",
      "Gradient Descent(34/99): loss=0.3897507857733467, gradient=0.0028715808338059205\n",
      "Gradient Descent(35/99): loss=0.3897507440197806, gradient=0.001450429770510578\n",
      "Gradient Descent(36/99): loss=0.38975081447690296, gradient=0.0020401719012700472\n",
      "Gradient Descent(37/99): loss=0.38975109265419444, gradient=0.0013526026555797687\n",
      "Gradient Descent(38/99): loss=0.38975119258086677, gradient=0.0010923408464125573\n",
      "Gradient Descent(39/99): loss=0.38975133152497116, gradient=0.0010485761056815365\n",
      "Gradient Descent(40/99): loss=0.3897513196130088, gradient=0.00209485251832509\n",
      "Gradient Descent(41/99): loss=0.3897515761821178, gradient=0.0013306427956250423\n",
      "Gradient Descent(42/99): loss=0.38975105398524273, gradient=0.0037005561535588946\n",
      "Gradient Descent(43/99): loss=0.38975116240514396, gradient=0.002097815475952289\n",
      "Gradient Descent(44/99): loss=0.3897514863261504, gradient=0.001318872234022837\n",
      "Gradient Descent(45/99): loss=0.38975161948336245, gradient=0.0010486247506647944\n",
      "Gradient Descent(46/99): loss=0.3897517834338809, gradient=0.0010056502157684093\n",
      "Gradient Descent(47/99): loss=0.38975178487132356, gradient=0.0021026798523941176\n",
      "Gradient Descent(48/99): loss=0.38975205576387517, gradient=0.0012956757459121188\n",
      "Gradient Descent(49/99): loss=0.38975213960624566, gradient=0.0010019435231729094\n",
      "Gradient Descent(50/99): loss=0.38975212701450035, gradient=0.00206590391335267\n",
      "Gradient Descent(51/99): loss=0.3897521418063354, gradient=0.0028493765104837146\n",
      "Gradient Descent(52/99): loss=0.38975209945891043, gradient=0.0013630205152463024\n",
      "Gradient Descent(53/99): loss=0.38975229974685666, gradient=0.0010090064730635061\n",
      "Gradient Descent(54/99): loss=0.38975229700377134, gradient=0.0021004597678964537\n",
      "Gradient Descent(55/99): loss=0.38975256979923806, gradient=0.0012873350311350376\n",
      "Gradient Descent(56/99): loss=0.38975265308475043, gradient=0.00098268833651258\n",
      "Gradient Descent(57/99): loss=0.3897526397509751, gradient=0.0020549099062912774\n",
      "Gradient Descent(58/99): loss=0.3897528859902128, gradient=0.0012654708443843351\n",
      "Gradient Descent(59/99): loss=0.38975295155065537, gradient=0.0009703482226609823\n",
      "Gradient Descent(60/99): loss=0.38975281501518844, gradient=0.002557751468135037\n",
      "Gradient Descent(61/99): loss=0.3897526836682572, gradient=0.002418973685421625\n",
      "Gradient Descent(62/99): loss=0.389752995469276, gradient=0.0013867978306145456\n",
      "Gradient Descent(63/99): loss=0.38975308387026397, gradient=0.000980136793992711\n",
      "Gradient Descent(64/99): loss=0.3897530811859405, gradient=0.0020473350150824665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(65/99): loss=0.3897533334852393, gradient=0.0012537166827647078\n",
      "Gradient Descent(66/99): loss=0.3897534031399068, gradient=0.00095564445841373\n",
      "Gradient Descent(67/99): loss=0.3897535159116875, gradient=0.0009169493158804942\n",
      "Gradient Descent(68/99): loss=0.3897534722364502, gradient=0.002072129607706716\n",
      "Gradient Descent(69/99): loss=0.38975346860740556, gradient=0.0028503873174862498\n",
      "Gradient Descent(70/99): loss=0.38975341087853865, gradient=0.0013199749655201582\n",
      "Gradient Descent(71/99): loss=0.38975359698923584, gradient=0.0009520689269800392\n",
      "Gradient Descent(72/99): loss=0.3897535774794339, gradient=0.0020895536233118657\n",
      "Gradient Descent(73/99): loss=0.3897538369274688, gradient=0.001246542090985922\n",
      "Gradient Descent(74/99): loss=0.38975358903107177, gradient=0.002877956309672016\n",
      "Gradient Descent(75/99): loss=0.3897537260262705, gradient=0.002007296144579309\n",
      "Gradient Descent(76/99): loss=0.38975396262073314, gradient=0.0011007302109870777\n",
      "Gradient Descent(77/99): loss=0.38975404974344363, gradient=0.0009072645965717171\n",
      "Gradient Descent(78/99): loss=0.38975416423882836, gradient=0.0008795796521137724\n",
      "Gradient Descent(79/99): loss=0.38975352164869864, gradient=0.00411069695103292\n",
      "Gradient Descent(80/99): loss=0.3897538788862831, gradient=0.001356020687073719\n",
      "Gradient Descent(81/99): loss=0.3897540048305387, gradient=0.0009549592822955135\n",
      "Gradient Descent(82/99): loss=0.38975402016266036, gradient=0.002074612902354189\n",
      "Gradient Descent(83/99): loss=0.38975429138179474, gradient=0.0012188590060567244\n",
      "Gradient Descent(84/99): loss=0.3897543725620886, gradient=0.0009124301817427943\n",
      "Gradient Descent(85/99): loss=0.3897544913839283, gradient=0.0008741189432044707\n",
      "Gradient Descent(86/99): loss=0.38975444590891956, gradient=0.0020747481989995256\n",
      "Gradient Descent(87/99): loss=0.38975467910021877, gradient=0.0012077336364350207\n",
      "Gradient Descent(88/99): loss=0.38975447568043214, gradient=0.0024976385270956596\n",
      "Gradient Descent(89/99): loss=0.3897543405738301, gradient=0.0023731644800817284\n",
      "Gradient Descent(90/99): loss=0.38975463526358545, gradient=0.0013410953083789403\n",
      "Gradient Descent(91/99): loss=0.3897547084903297, gradient=0.0009255660207129567\n",
      "Gradient Descent(92/99): loss=0.3897548291485178, gradient=0.0008748768273232044\n",
      "Gradient Descent(93/99): loss=0.38975478480851067, gradient=0.0020635825869937674\n",
      "Gradient Descent(94/99): loss=0.38975501682662456, gradient=0.001207166710947664\n",
      "Gradient Descent(95/99): loss=0.3897550641729283, gradient=0.000887460897591555\n",
      "Gradient Descent(96/99): loss=0.3897551561584907, gradient=0.0008495010399892216\n",
      "Gradient Descent(97/99): loss=0.3897547898576281, gradient=0.0031595202586672698\n",
      "Gradient Descent(98/99): loss=0.3897549502122753, gradient=0.0009292601373487582\n",
      "Gradient Descent(99/99): loss=0.3897550736375547, gradient=0.000888667615045395\n",
      "Gradient Descent(0/99): loss=0.38906362003252154, gradient=0.011184623929711108\n",
      "Gradient Descent(1/99): loss=0.38906212821519914, gradient=0.013089412174081237\n",
      "Gradient Descent(2/99): loss=0.38906568071607356, gradient=0.009935193040671073\n",
      "Gradient Descent(3/99): loss=0.3890671224046437, gradient=0.008026489758180284\n",
      "Gradient Descent(4/99): loss=0.3890696106950979, gradient=0.007854475968368422\n",
      "Gradient Descent(5/99): loss=0.3890710291641337, gradient=0.007920914372221147\n",
      "Gradient Descent(6/99): loss=0.3890740146699188, gradient=0.005825676700704784\n",
      "Gradient Descent(7/99): loss=0.38907632938975417, gradient=0.006708252287378333\n",
      "Gradient Descent(8/99): loss=0.3890792384143617, gradient=0.00641793583420998\n",
      "Gradient Descent(9/99): loss=0.3890819084761693, gradient=0.005297703261242294\n",
      "Gradient Descent(10/99): loss=0.38908276199887476, gradient=0.008286326161903522\n",
      "Gradient Descent(11/99): loss=0.3890832702440272, gradient=0.008653172832910793\n",
      "Gradient Descent(12/99): loss=0.38908688305062455, gradient=0.005583538571620867\n",
      "Gradient Descent(13/99): loss=0.38908959890578876, gradient=0.0050227227966573385\n",
      "Gradient Descent(14/99): loss=0.38909194880504844, gradient=0.00583316323503389\n",
      "Gradient Descent(15/99): loss=0.38909315243016596, gradient=0.00796284113133477\n",
      "Gradient Descent(16/99): loss=0.3890968803907501, gradient=0.005736733086135827\n",
      "Gradient Descent(17/99): loss=0.3890948622698554, gradient=0.011537126848199214\n",
      "Gradient Descent(18/99): loss=0.38909809569724024, gradient=0.006618656121032783\n",
      "Gradient Descent(19/99): loss=0.3891004846425084, gradient=0.006613752141579618\n",
      "Gradient Descent(20/99): loss=0.38910228790160173, gradient=0.005881600129191761\n",
      "Gradient Descent(21/99): loss=0.38910533192801783, gradient=0.004697873549357998\n",
      "Gradient Descent(22/99): loss=0.3891074769978782, gradient=0.005901249081617506\n",
      "Gradient Descent(23/99): loss=0.38911012681136686, gradient=0.005782822847870727\n",
      "Gradient Descent(24/99): loss=0.3891106094316764, gradient=0.007874031015318686\n",
      "Gradient Descent(25/99): loss=0.38911327488794073, gradient=0.004584122448702102\n",
      "Gradient Descent(26/99): loss=0.38911369890394243, gradient=0.007978822753582736\n",
      "Gradient Descent(27/99): loss=0.38911651217163495, gradient=0.004557796299890352\n",
      "Gradient Descent(28/99): loss=0.38911913565006223, gradient=0.00434210596319043\n",
      "Gradient Descent(29/99): loss=0.38912114752023175, gradient=0.005383644996033858\n",
      "Gradient Descent(30/99): loss=0.3891220787908473, gradient=0.007328768484180989\n",
      "Gradient Descent(31/99): loss=0.3891240588427434, gradient=0.00650893277003866\n",
      "Gradient Descent(32/99): loss=0.3891247828794196, gradient=0.007396573837691245\n",
      "Gradient Descent(33/99): loss=0.38912711569770314, gradient=0.006761008546833084\n",
      "Gradient Descent(34/99): loss=0.38912840404764426, gradient=0.006390623635924344\n",
      "Gradient Descent(35/99): loss=0.38913111958935626, gradient=0.005636118021144132\n",
      "Gradient Descent(36/99): loss=0.3891314026252481, gradient=0.008468662768452744\n",
      "Gradient Descent(37/99): loss=0.3891332839243653, gradient=0.0075308445217078055\n",
      "Gradient Descent(38/99): loss=0.38913471124640864, gradient=0.007185542350439693\n",
      "Gradient Descent(39/99): loss=0.3891371876438849, gradient=0.004330224861162512\n",
      "Gradient Descent(40/99): loss=0.38913965989019533, gradient=0.004088380974133511\n",
      "Gradient Descent(41/99): loss=0.38913984879622426, gradient=0.007804836165692557\n",
      "Gradient Descent(42/99): loss=0.38914247034516986, gradient=0.004119040746933617\n",
      "Gradient Descent(43/99): loss=0.389144894689486, gradient=0.003908030888793509\n",
      "Gradient Descent(44/99): loss=0.3891457152753052, gradient=0.006896422765972895\n",
      "Gradient Descent(45/99): loss=0.38914745443167503, gradient=0.005542208005157664\n",
      "Gradient Descent(46/99): loss=0.38914921540331393, gradient=0.006279050278365752\n",
      "Gradient Descent(47/99): loss=0.3891497364506393, gradient=0.007183521234423166\n",
      "Gradient Descent(48/99): loss=0.389151863446314, gradient=0.006540175571136134\n",
      "Gradient Descent(49/99): loss=0.3891529336018094, gradient=0.00619075377101363\n",
      "Gradient Descent(50/99): loss=0.38915545265314316, gradient=0.005384105542224869\n",
      "Gradient Descent(51/99): loss=0.38915658777665, gradient=0.0060406654548234035\n",
      "Gradient Descent(52/99): loss=0.389158931023653, gradient=0.003745058058893107\n",
      "Gradient Descent(53/99): loss=0.38915901916448076, gradient=0.007950163904793814\n",
      "Gradient Descent(54/99): loss=0.3891617470366051, gradient=0.004326911038659096\n",
      "Gradient Descent(55/99): loss=0.3891625602126947, gradient=0.006463419616963239\n",
      "Gradient Descent(56/99): loss=0.3891640644048429, gradient=0.007095285238514026\n",
      "Gradient Descent(57/99): loss=0.38916502253238505, gradient=0.006150522328212501\n",
      "Gradient Descent(58/99): loss=0.3891669890097114, gradient=0.006222122420546423\n",
      "Gradient Descent(59/99): loss=0.38916735931615426, gradient=0.007221001898505259\n",
      "Gradient Descent(60/99): loss=0.38916932304619045, gradient=0.006119188349032797\n",
      "Gradient Descent(61/99): loss=0.3891707823737695, gradient=0.005565091131537662\n",
      "Gradient Descent(62/99): loss=0.38917305837386, gradient=0.0050483199612684606\n",
      "Gradient Descent(63/99): loss=0.38917500851997056, gradient=0.0037094742757687134\n",
      "Gradient Descent(64/99): loss=0.389176192226298, gradient=0.005726418478023612\n",
      "Gradient Descent(65/99): loss=0.3891751932048521, gradient=0.008693179120466315\n",
      "Gradient Descent(66/99): loss=0.38917748492648463, gradient=0.005339107555240107\n",
      "Gradient Descent(67/99): loss=0.38917910613115947, gradient=0.005925905611684945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(68/99): loss=0.3891807595038848, gradient=0.0046748406321233905\n",
      "Gradient Descent(69/99): loss=0.38918288546223107, gradient=0.003666924010587259\n",
      "Gradient Descent(70/99): loss=0.38918425956708275, gradient=0.00524691091030074\n",
      "Gradient Descent(71/99): loss=0.38918538026652816, gradient=0.006979216019559448\n",
      "Gradient Descent(72/99): loss=0.38918574693571334, gradient=0.006093783246543296\n",
      "Gradient Descent(73/99): loss=0.3891880718971187, gradient=0.0034419558827121764\n",
      "Gradient Descent(74/99): loss=0.3891889003511589, gradient=0.00618524427662047\n",
      "Gradient Descent(75/99): loss=0.38919044878775827, gradient=0.005947824167516752\n",
      "Gradient Descent(76/99): loss=0.38919174624179786, gradient=0.0051347713610450105\n",
      "Gradient Descent(77/99): loss=0.389193555419592, gradient=0.004851388824641476\n",
      "Gradient Descent(78/99): loss=0.389193552806967, gradient=0.007472274993768392\n",
      "Gradient Descent(79/99): loss=0.3891945917140967, gradient=0.008456591954048517\n",
      "Gradient Descent(80/99): loss=0.38919469309983573, gradient=0.006742257610500925\n",
      "Gradient Descent(81/99): loss=0.3891972628344685, gradient=0.003980750737774967\n",
      "Gradient Descent(82/99): loss=0.38919731454737794, gradient=0.006981398327657832\n",
      "Gradient Descent(83/99): loss=0.38919879240374494, gradient=0.00558933505256156\n",
      "Gradient Descent(84/99): loss=0.38920079519257267, gradient=0.003773342656582854\n",
      "Gradient Descent(85/99): loss=0.3892023515651266, gradient=0.0033640191950448897\n",
      "Gradient Descent(86/99): loss=0.3892016611960543, gradient=0.008282386413807918\n",
      "Gradient Descent(87/99): loss=0.38920403251894925, gradient=0.004690863135447844\n",
      "Gradient Descent(88/99): loss=0.38920311968388616, gradient=0.008450908046869522\n",
      "Gradient Descent(89/99): loss=0.38920333864925766, gradient=0.008251017787140621\n",
      "Gradient Descent(90/99): loss=0.3892057135545705, gradient=0.004292439188640144\n",
      "Gradient Descent(91/99): loss=0.3892056826840038, gradient=0.006526237558685307\n",
      "Gradient Descent(92/99): loss=0.38920637123808155, gradient=0.005946378862210507\n",
      "Gradient Descent(93/99): loss=0.38920759946880906, gradient=0.006050346401651394\n",
      "Gradient Descent(94/99): loss=0.3892087866069626, gradient=0.004738383708850239\n",
      "Gradient Descent(95/99): loss=0.38921026491453675, gradient=0.004983412220201827\n",
      "Gradient Descent(96/99): loss=0.38921106693069135, gradient=0.005167313210566041\n",
      "Gradient Descent(97/99): loss=0.3892116508246839, gradient=0.006426123684521411\n",
      "Gradient Descent(98/99): loss=0.38921294539064066, gradient=0.004711665615164356\n",
      "Gradient Descent(99/99): loss=0.38921425758813255, gradient=0.003508738427643062\n",
      "Gradient Descent(0/99): loss=0.3903815572338893, gradient=0.01817111535876613\n",
      "Gradient Descent(1/99): loss=0.3903832681315708, gradient=0.010133206984533422\n",
      "Gradient Descent(2/99): loss=0.3903832836479496, gradient=0.007926146217760147\n",
      "Gradient Descent(3/99): loss=0.39038474204296, gradient=0.0072138290674633974\n",
      "Gradient Descent(4/99): loss=0.39038512915999984, gradient=0.007857318489901866\n",
      "Gradient Descent(5/99): loss=0.3903872695150293, gradient=0.006280248043081489\n",
      "Gradient Descent(6/99): loss=0.39038621275219004, gradient=0.009123704112446084\n",
      "Gradient Descent(7/99): loss=0.39038758193733536, gradient=0.0065735291135447915\n",
      "Gradient Descent(8/99): loss=0.390388278321749, gradient=0.007510751950348769\n",
      "Gradient Descent(9/99): loss=0.39038988600745883, gradient=0.006780826622094975\n",
      "Gradient Descent(10/99): loss=0.3903884843153692, gradient=0.010714626535099487\n",
      "Gradient Descent(11/99): loss=0.39039102543100146, gradient=0.007678233558605284\n",
      "Gradient Descent(12/99): loss=0.39039128061890144, gradient=0.006964534273050093\n",
      "Gradient Descent(13/99): loss=0.39039292048221064, gradient=0.005488532450185221\n",
      "Gradient Descent(14/99): loss=0.39039133834167045, gradient=0.010001021025400904\n",
      "Gradient Descent(15/99): loss=0.39039306364523013, gradient=0.0076047057155318246\n",
      "Gradient Descent(16/99): loss=0.39039448679001537, gradient=0.005143453441646761\n",
      "Gradient Descent(17/99): loss=0.39039551335050343, gradient=0.0064078781843380644\n",
      "Gradient Descent(18/99): loss=0.39039743333457516, gradient=0.005169653649470363\n",
      "Gradient Descent(19/99): loss=0.3903970105907043, gradient=0.007864389746981552\n",
      "Gradient Descent(20/99): loss=0.39039730315921717, gradient=0.008315885883101904\n",
      "Gradient Descent(21/99): loss=0.39039672724557445, gradient=0.008646823545844746\n",
      "Gradient Descent(22/99): loss=0.3903992238489363, gradient=0.006110101418128888\n",
      "Gradient Descent(23/99): loss=0.39039885977130223, gradient=0.008321045498630839\n",
      "Gradient Descent(24/99): loss=0.3904013890482864, gradient=0.005951483154021096\n",
      "Gradient Descent(25/99): loss=0.39040123731375537, gradient=0.007524818650340689\n",
      "Gradient Descent(26/99): loss=0.39040213571164945, gradient=0.006944714487133433\n",
      "Gradient Descent(27/99): loss=0.39040354776263975, gradient=0.00462237811708989\n",
      "Gradient Descent(28/99): loss=0.3904033160258203, gradient=0.007687078284525796\n",
      "Gradient Descent(29/99): loss=0.390404266326692, gradient=0.006444627786127186\n",
      "Gradient Descent(30/99): loss=0.39040546125403747, gradient=0.006474388873021146\n",
      "Gradient Descent(31/99): loss=0.3904066898701664, gradient=0.005106830506109053\n",
      "Gradient Descent(32/99): loss=0.39040823932724905, gradient=0.004645293657894657\n",
      "Gradient Descent(33/99): loss=0.3904079221189294, gradient=0.007851383686197973\n",
      "Gradient Descent(34/99): loss=0.3904089723133847, gradient=0.006187962929681872\n",
      "Gradient Descent(35/99): loss=0.3904085687109603, gradient=0.008273852959351582\n",
      "Gradient Descent(36/99): loss=0.39040990028932593, gradient=0.006090322396545281\n",
      "Gradient Descent(37/99): loss=0.3904107969215632, gradient=0.006640920987403368\n",
      "Gradient Descent(38/99): loss=0.39041079253513017, gradient=0.006914201770858795\n",
      "Gradient Descent(39/99): loss=0.39041300786236827, gradient=0.004824398010785535\n",
      "Gradient Descent(40/99): loss=0.3904117218969377, gradient=0.008876241193817332\n",
      "Gradient Descent(41/99): loss=0.3904136864419329, gradient=0.004801062184540802\n",
      "Gradient Descent(42/99): loss=0.39041277799340257, gradient=0.008833666147870083\n",
      "Gradient Descent(43/99): loss=0.39041451661791254, gradient=0.006428675932903052\n",
      "Gradient Descent(44/99): loss=0.3904155958864884, gradient=0.006077882291013582\n",
      "Gradient Descent(45/99): loss=0.3904166121983956, gradient=0.006249842192243855\n",
      "Gradient Descent(46/99): loss=0.3904179838878411, gradient=0.004431829807316238\n",
      "Gradient Descent(47/99): loss=0.39041733376382626, gradient=0.007985234777461356\n",
      "Gradient Descent(48/99): loss=0.3904173820241902, gradient=0.007591864678308095\n",
      "Gradient Descent(49/99): loss=0.3904191825141505, gradient=0.005180196072229287\n",
      "Gradient Descent(50/99): loss=0.390419692786496, gradient=0.006263502169272186\n",
      "Gradient Descent(51/99): loss=0.39042104916145404, gradient=0.004903594521600532\n",
      "Gradient Descent(52/99): loss=0.3904217026759184, gradient=0.0062158613873854384\n",
      "Gradient Descent(53/99): loss=0.390422805156938, gradient=0.0061047329409843675\n",
      "Gradient Descent(54/99): loss=0.3904243573448365, gradient=0.004726615211557701\n",
      "Gradient Descent(55/99): loss=0.3904236571628286, gradient=0.00764018721740825\n",
      "Gradient Descent(56/99): loss=0.3904239232854685, gradient=0.006674499387738965\n",
      "Gradient Descent(57/99): loss=0.39042374788829226, gradient=0.007863424485026058\n",
      "Gradient Descent(58/99): loss=0.3904258663685995, gradient=0.005793566845942989\n",
      "Gradient Descent(59/99): loss=0.3904257076657219, gradient=0.006971434253649488\n",
      "Gradient Descent(60/99): loss=0.3904268508958498, gradient=0.006547051082150557\n",
      "Gradient Descent(61/99): loss=0.390426456666313, gradient=0.007954332787689228\n",
      "Gradient Descent(62/99): loss=0.390426804882473, gradient=0.00792220247943317\n",
      "Gradient Descent(63/99): loss=0.39042737154498713, gradient=0.006648318147523713\n",
      "Gradient Descent(64/99): loss=0.39042952172376705, gradient=0.0045535280470421855\n",
      "Gradient Descent(65/99): loss=0.39042911540676434, gradient=0.007784474168236022\n",
      "Gradient Descent(66/99): loss=0.39043018548340924, gradient=0.005974227541957404\n",
      "Gradient Descent(67/99): loss=0.39043083407416507, gradient=0.00703168688131124\n",
      "Gradient Descent(68/99): loss=0.3904315878667567, gradient=0.004775357867249461\n",
      "Gradient Descent(69/99): loss=0.39043235100566626, gradient=0.006177713444232788\n",
      "Gradient Descent(70/99): loss=0.3904322012831527, gradient=0.007914117248032466\n",
      "Gradient Descent(71/99): loss=0.39043258462551744, gradient=0.006442124889220316\n",
      "Gradient Descent(72/99): loss=0.3904344297497073, gradient=0.004919774248032237\n",
      "Gradient Descent(73/99): loss=0.39043489905309103, gradient=0.005896628618024242\n",
      "Gradient Descent(74/99): loss=0.3904363529196458, gradient=0.004895426728585749\n",
      "Gradient Descent(75/99): loss=0.3904357971022557, gradient=0.0073366353249479845\n",
      "Gradient Descent(76/99): loss=0.39043734507735206, gradient=0.004963443749630517\n",
      "Gradient Descent(77/99): loss=0.3904367002237743, gradient=0.007524387007594901\n",
      "Gradient Descent(78/99): loss=0.39043721283301863, gradient=0.007055836942208378\n",
      "Gradient Descent(79/99): loss=0.39043778203531876, gradient=0.004640592636771513\n",
      "Gradient Descent(80/99): loss=0.3904375645888772, gradient=0.007439141629951473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(81/99): loss=0.3904393892160883, gradient=0.004606198435173897\n",
      "Gradient Descent(82/99): loss=0.3904388862072968, gradient=0.007915574531291557\n",
      "Gradient Descent(83/99): loss=0.3904408374762228, gradient=0.005814691416343667\n",
      "Gradient Descent(84/99): loss=0.39044108100309605, gradient=0.006153217313375776\n",
      "Gradient Descent(85/99): loss=0.39044084129718715, gradient=0.008149050120121215\n",
      "Gradient Descent(86/99): loss=0.3904402315863939, gradient=0.008109152719542684\n",
      "Gradient Descent(87/99): loss=0.3904422697222444, gradient=0.005741763907221323\n",
      "Gradient Descent(88/99): loss=0.3904414985177498, gradient=0.00823533783543127\n",
      "Gradient Descent(89/99): loss=0.39044284311211613, gradient=0.007854344814698544\n",
      "Gradient Descent(90/99): loss=0.3904417960863221, gradient=0.008075707190699851\n",
      "Gradient Descent(91/99): loss=0.39044376003724063, gradient=0.004686933438227069\n",
      "Gradient Descent(92/99): loss=0.3904437100192334, gradient=0.006610586381577188\n",
      "Gradient Descent(93/99): loss=0.39044435671179084, gradient=0.005905100814375288\n",
      "Gradient Descent(94/99): loss=0.3904452798295466, gradient=0.00598406237013241\n",
      "Gradient Descent(95/99): loss=0.39044589215190395, gradient=0.0061474013659395515\n",
      "Gradient Descent(96/99): loss=0.39044675148527397, gradient=0.004647739792507645\n",
      "Gradient Descent(97/99): loss=0.39044716354964626, gradient=0.005871917011788261\n",
      "Gradient Descent(98/99): loss=0.3904479315681715, gradient=0.005907298535424609\n",
      "Gradient Descent(99/99): loss=0.39044920594374305, gradient=0.00439365807449767\n",
      "Gradient Descent(0/99): loss=0.3907490284655296, gradient=0.00788164297879109\n",
      "Gradient Descent(1/99): loss=0.3907475875979759, gradient=0.007822082080887357\n",
      "Gradient Descent(2/99): loss=0.39074579081141025, gradient=0.008488800272704758\n",
      "Gradient Descent(3/99): loss=0.39074463688774796, gradient=0.008252404523133871\n",
      "Gradient Descent(4/99): loss=0.3907431057978984, gradient=0.008304071043736143\n",
      "Gradient Descent(5/99): loss=0.3907434006251298, gradient=0.006249477104389635\n",
      "Gradient Descent(6/99): loss=0.3907426820280369, gradient=0.006680749418089918\n",
      "Gradient Descent(7/99): loss=0.39074250468697985, gradient=0.00592179017270627\n",
      "Gradient Descent(8/99): loss=0.3907422005599402, gradient=0.0061806580422695484\n",
      "Gradient Descent(9/99): loss=0.3907408779228313, gradient=0.007789932972571425\n",
      "Gradient Descent(10/99): loss=0.39074114314736663, gradient=0.005895656199644835\n",
      "Gradient Descent(11/99): loss=0.3907393451432981, gradient=0.008154441868845059\n",
      "Gradient Descent(12/99): loss=0.39073969036025713, gradient=0.0060066373427586734\n",
      "Gradient Descent(13/99): loss=0.3907389519839017, gradient=0.006440302278984779\n",
      "Gradient Descent(14/99): loss=0.3907387797776093, gradient=0.00577704579814157\n",
      "Gradient Descent(15/99): loss=0.39073781197401614, gradient=0.006999855952275068\n",
      "Gradient Descent(16/99): loss=0.39073759163313926, gradient=0.006485957824342979\n",
      "Gradient Descent(17/99): loss=0.3907372118776873, gradient=0.0055312699574068995\n",
      "Gradient Descent(18/99): loss=0.3907366684632944, gradient=0.0061934610063168775\n",
      "Gradient Descent(19/99): loss=0.39073554271807515, gradient=0.007178756442169342\n",
      "Gradient Descent(20/99): loss=0.3907347901131995, gradient=0.007340674092913982\n",
      "Gradient Descent(21/99): loss=0.39073483212584675, gradient=0.005682893509718658\n",
      "Gradient Descent(22/99): loss=0.39073460929571574, gradient=0.005434472579474881\n",
      "Gradient Descent(23/99): loss=0.3907329402180558, gradient=0.008072338890588716\n",
      "Gradient Descent(24/99): loss=0.3907324743835632, gradient=0.0056911349033607495\n",
      "Gradient Descent(25/99): loss=0.39073152914009324, gradient=0.0075547104759244406\n",
      "Gradient Descent(26/99): loss=0.39073103227465505, gradient=0.007019829925306142\n",
      "Gradient Descent(27/99): loss=0.3907298903685881, gradient=0.00812953729269531\n",
      "Gradient Descent(28/99): loss=0.3907285808674931, gradient=0.0077755674003396925\n",
      "Gradient Descent(29/99): loss=0.3907292977785871, gradient=0.006003802487864369\n",
      "Gradient Descent(30/99): loss=0.39072827478932515, gradient=0.007515062981590545\n",
      "Gradient Descent(31/99): loss=0.3907279500780234, gradient=0.006812003777771661\n",
      "Gradient Descent(32/99): loss=0.39072804830764846, gradient=0.006274306398493558\n",
      "Gradient Descent(33/99): loss=0.3907279651955363, gradient=0.0054313123044658785\n",
      "Gradient Descent(34/99): loss=0.3907279312019357, gradient=0.005618462698744485\n",
      "Gradient Descent(35/99): loss=0.39072697718527183, gradient=0.007219226329922771\n",
      "Gradient Descent(36/99): loss=0.3907272470685444, gradient=0.005842039161309877\n",
      "Gradient Descent(37/99): loss=0.3907260731491902, gradient=0.007292398492492823\n",
      "Gradient Descent(38/99): loss=0.3907257115499732, gradient=0.00768199273234321\n",
      "Gradient Descent(39/99): loss=0.3907230275023561, gradient=0.00982971269608326\n",
      "Gradient Descent(40/99): loss=0.39072424021531726, gradient=0.006568242628527234\n",
      "Gradient Descent(41/99): loss=0.39072281666928754, gradient=0.007970843170105612\n",
      "Gradient Descent(42/99): loss=0.3907227002808561, gradient=0.007305348890466623\n",
      "Gradient Descent(43/99): loss=0.39072327764997283, gradient=0.005906025617384945\n",
      "Gradient Descent(44/99): loss=0.39072245263828703, gradient=0.007340429314202779\n",
      "Gradient Descent(45/99): loss=0.3907217308578193, gradient=0.007886720629361571\n",
      "Gradient Descent(46/99): loss=0.3907215869333769, gradient=0.005998626714762876\n",
      "Gradient Descent(47/99): loss=0.39072101868558434, gradient=0.00685767552131846\n",
      "Gradient Descent(48/99): loss=0.3907216195204671, gradient=0.005850643736609847\n",
      "Gradient Descent(49/99): loss=0.39072068145118577, gradient=0.007245234437628174\n",
      "Gradient Descent(50/99): loss=0.3907210928421874, gradient=0.005864812770347756\n",
      "Gradient Descent(51/99): loss=0.3907208535747464, gradient=0.005761550126252211\n",
      "Gradient Descent(52/99): loss=0.39071888938568694, gradient=0.008519693052663068\n",
      "Gradient Descent(53/99): loss=0.39071939438898173, gradient=0.005312452507249149\n",
      "Gradient Descent(54/99): loss=0.3907185957665799, gradient=0.0074129668831815\n",
      "Gradient Descent(55/99): loss=0.39071927437095283, gradient=0.005550479844294108\n",
      "Gradient Descent(56/99): loss=0.3907192098198159, gradient=0.00579478331225135\n",
      "Gradient Descent(57/99): loss=0.39071847067527427, gradient=0.007224347613117141\n",
      "Gradient Descent(58/99): loss=0.39071906592766165, gradient=0.006031225559759079\n",
      "Gradient Descent(59/99): loss=0.39071815065308846, gradient=0.007299421649786344\n",
      "Gradient Descent(60/99): loss=0.39071875186438076, gradient=0.005685994394003149\n",
      "Gradient Descent(61/99): loss=0.39071763882480615, gradient=0.007492112765047272\n",
      "Gradient Descent(62/99): loss=0.39071830737999763, gradient=0.005806439855564239\n",
      "Gradient Descent(63/99): loss=0.39071679174061846, gradient=0.007937036019905298\n",
      "Gradient Descent(64/99): loss=0.3907162839333402, gradient=0.006980148704335974\n",
      "Gradient Descent(65/99): loss=0.39071572505699204, gradient=0.007804238127681491\n",
      "Gradient Descent(66/99): loss=0.39071451990071315, gradient=0.007519369323155992\n",
      "Gradient Descent(67/99): loss=0.39071560881312645, gradient=0.0056526361927198825\n",
      "Gradient Descent(68/99): loss=0.3907154356212562, gradient=0.006105067839120379\n",
      "Gradient Descent(69/99): loss=0.3907150530169363, gradient=0.006814283605018073\n",
      "Gradient Descent(70/99): loss=0.3907154013365348, gradient=0.00618897220386313\n",
      "Gradient Descent(71/99): loss=0.39071477607414207, gradient=0.006821889532419539\n",
      "Gradient Descent(72/99): loss=0.39071547244024296, gradient=0.005957559620456342\n",
      "Gradient Descent(73/99): loss=0.3907153103165987, gradient=0.005971512587416859\n",
      "Gradient Descent(74/99): loss=0.39071486164931885, gradient=0.006845399742203085\n",
      "Gradient Descent(75/99): loss=0.3907143392820764, gradient=0.007299590998989785\n",
      "Gradient Descent(76/99): loss=0.39071493728596157, gradient=0.005670802548611263\n",
      "Gradient Descent(77/99): loss=0.3907146703779986, gradient=0.006131158884793743\n",
      "Gradient Descent(78/99): loss=0.39071389635928594, gradient=0.007318135843677506\n",
      "Gradient Descent(79/99): loss=0.3907126739473481, gradient=0.007801986346844936\n",
      "Gradient Descent(80/99): loss=0.3907127669020744, gradient=0.006807879992602453\n",
      "Gradient Descent(81/99): loss=0.3907135148742185, gradient=0.005431837046781454\n",
      "Gradient Descent(82/99): loss=0.39071214487883105, gradient=0.007921139588801354\n",
      "Gradient Descent(83/99): loss=0.3907126097408145, gradient=0.005294993410252328\n",
      "Gradient Descent(84/99): loss=0.39071193857054914, gradient=0.007287541674989515\n",
      "Gradient Descent(85/99): loss=0.3907120162703733, gradient=0.006765794769515402\n",
      "Gradient Descent(86/99): loss=0.390712292235853, gradient=0.006113187873362721\n",
      "Gradient Descent(87/99): loss=0.3907124305216871, gradient=0.0054090806048390574\n",
      "Gradient Descent(88/99): loss=0.39071214812492877, gradient=0.006619242440702414\n",
      "Gradient Descent(89/99): loss=0.39071253746910817, gradient=0.00649528178764604\n",
      "Gradient Descent(90/99): loss=0.39071076729374304, gradient=0.008383029477221726\n",
      "Gradient Descent(91/99): loss=0.3907108675248844, gradient=0.006221731500726664\n",
      "Gradient Descent(92/99): loss=0.39071129255585424, gradient=0.005515996383496432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(93/99): loss=0.3907109127605768, gradient=0.006979094716451386\n",
      "Gradient Descent(94/99): loss=0.39071160762345636, gradient=0.0062022689691519335\n",
      "Gradient Descent(95/99): loss=0.39071091791621554, gradient=0.007221650284831788\n",
      "Gradient Descent(96/99): loss=0.390711431255556, gradient=0.0061619114767793816\n",
      "Gradient Descent(97/99): loss=0.3907108751552122, gradient=0.0067270241061035945\n",
      "Gradient Descent(98/99): loss=0.3907113529308573, gradient=0.006253132726922086\n",
      "Gradient Descent(99/99): loss=0.39071022820071466, gradient=0.007610437359684\n",
      "Gradient Descent(0/99): loss=0.390323619744489, gradient=0.014377194689792327\n",
      "Gradient Descent(1/99): loss=0.390318969150381, gradient=0.006988819002118405\n",
      "Gradient Descent(2/99): loss=0.39031400807186606, gradient=0.008774510206011162\n",
      "Gradient Descent(3/99): loss=0.3903124468505471, gradient=0.006783291872581129\n",
      "Gradient Descent(4/99): loss=0.3903097275565766, gradient=0.006996901634834916\n",
      "Gradient Descent(5/99): loss=0.39030965766963316, gradient=0.004553890879190468\n",
      "Gradient Descent(6/99): loss=0.39030815095283417, gradient=0.006269101979060346\n",
      "Gradient Descent(7/99): loss=0.3903086112651788, gradient=0.004125727087124319\n",
      "Gradient Descent(8/99): loss=0.39030725985581954, gradient=0.006212433968394543\n",
      "Gradient Descent(9/99): loss=0.3903067430029189, gradient=0.0055466297501556424\n",
      "Gradient Descent(10/99): loss=0.3903075561448127, gradient=0.004002913223389536\n",
      "Gradient Descent(11/99): loss=0.39030682694565955, gradient=0.006500446821396625\n",
      "Gradient Descent(12/99): loss=0.39030629922402826, gradient=0.007516215357820381\n",
      "Gradient Descent(13/99): loss=0.39030640860762805, gradient=0.0036376498963903593\n",
      "Gradient Descent(14/99): loss=0.3903062168379271, gradient=0.005282938202742562\n",
      "Gradient Descent(15/99): loss=0.3903072157367146, gradient=0.0038924575375314306\n",
      "Gradient Descent(16/99): loss=0.3903058455218825, gradient=0.00821971617113852\n",
      "Gradient Descent(17/99): loss=0.3903069261481661, gradient=0.006578021965883842\n",
      "Gradient Descent(18/99): loss=0.3903070087490757, gradient=0.003563357250915602\n",
      "Gradient Descent(19/99): loss=0.3903073558627426, gradient=0.004227468936529719\n",
      "Gradient Descent(20/99): loss=0.39030791220979716, gradient=0.004788949661672359\n",
      "Gradient Descent(21/99): loss=0.390307493945776, gradient=0.006203832672430361\n",
      "Gradient Descent(22/99): loss=0.39030750766480204, gradient=0.006758647023771363\n",
      "Gradient Descent(23/99): loss=0.3903073336759418, gradient=0.004407238980883456\n",
      "Gradient Descent(24/99): loss=0.3903076158303699, gradient=0.0044118557641040996\n",
      "Gradient Descent(25/99): loss=0.39030558671582094, gradient=0.008863984791250556\n",
      "Gradient Descent(26/99): loss=0.39030547455006, gradient=0.0053716880601267555\n",
      "Gradient Descent(27/99): loss=0.390306744313961, gradient=0.0038659383314411165\n",
      "Gradient Descent(28/99): loss=0.3903073020989683, gradient=0.0032378483017399787\n",
      "Gradient Descent(29/99): loss=0.3903067044508345, gradient=0.006709132212733422\n",
      "Gradient Descent(30/99): loss=0.39030804753138754, gradient=0.0045089149662677595\n",
      "Gradient Descent(31/99): loss=0.3903061854475036, gradient=0.008952432417149733\n",
      "Gradient Descent(32/99): loss=0.3903075841363886, gradient=0.006768970582950824\n",
      "Gradient Descent(33/99): loss=0.39030716286225525, gradient=0.005613285341588199\n",
      "Gradient Descent(34/99): loss=0.3903079707599736, gradient=0.0040338219205934564\n",
      "Gradient Descent(35/99): loss=0.3903084500036913, gradient=0.004663107895722799\n",
      "Gradient Descent(36/99): loss=0.3903086237784022, gradient=0.004372196723458607\n",
      "Gradient Descent(37/99): loss=0.39030974743912883, gradient=0.0034172683189298675\n",
      "Gradient Descent(38/99): loss=0.39030666228464456, gradient=0.009478384522878918\n",
      "Gradient Descent(39/99): loss=0.3903073843788253, gradient=0.004340273137749579\n",
      "Gradient Descent(40/99): loss=0.3903084087797116, gradient=0.0034771372735411116\n",
      "Gradient Descent(41/99): loss=0.3903084652487734, gradient=0.004338346713138802\n",
      "Gradient Descent(42/99): loss=0.3903082398895591, gradient=0.006030227470917605\n",
      "Gradient Descent(43/99): loss=0.3903095313731008, gradient=0.004328787543588127\n",
      "Gradient Descent(44/99): loss=0.3903078785616648, gradient=0.00776878047789864\n",
      "Gradient Descent(45/99): loss=0.39030859130139567, gradient=0.007182710491927566\n",
      "Gradient Descent(46/99): loss=0.39030734565756176, gradient=0.006730208496722148\n",
      "Gradient Descent(47/99): loss=0.390308285317256, gradient=0.004273670761187043\n",
      "Gradient Descent(48/99): loss=0.3903093671698199, gradient=0.0032891423471823034\n",
      "Gradient Descent(49/99): loss=0.3903094498750632, gradient=0.004317570206409562\n",
      "Gradient Descent(50/99): loss=0.3903092367209475, gradient=0.005986390864377669\n",
      "Gradient Descent(51/99): loss=0.3903093942013932, gradient=0.0065861621124162235\n",
      "Gradient Descent(52/99): loss=0.3903088314955913, gradient=0.0055702770932845565\n",
      "Gradient Descent(53/99): loss=0.39030999084061035, gradient=0.003611150918285302\n",
      "Gradient Descent(54/99): loss=0.39030852005275457, gradient=0.007181125876478407\n",
      "Gradient Descent(55/99): loss=0.39030875159591744, gradient=0.005051625643191281\n",
      "Gradient Descent(56/99): loss=0.3903097012764952, gradient=0.0035606004395985244\n",
      "Gradient Descent(57/99): loss=0.39031015246618045, gradient=0.0030495447599066337\n",
      "Gradient Descent(58/99): loss=0.3903094062325319, gradient=0.0066621147316276615\n",
      "Gradient Descent(59/99): loss=0.3903106530513032, gradient=0.0043948019594244785\n",
      "Gradient Descent(60/99): loss=0.39031083166925934, gradient=0.003026685679279876\n",
      "Gradient Descent(61/99): loss=0.39031048031613585, gradient=0.005330821910310591\n",
      "Gradient Descent(62/99): loss=0.39031079421771847, gradient=0.004873667068724952\n",
      "Gradient Descent(63/99): loss=0.39031153212985253, gradient=0.003117728270871228\n",
      "Gradient Descent(64/99): loss=0.3903087325566947, gradient=0.009155863703882045\n",
      "Gradient Descent(65/99): loss=0.3903094490684234, gradient=0.007089503780915802\n",
      "Gradient Descent(66/99): loss=0.3903077431254609, gradient=0.009141783623146668\n",
      "Gradient Descent(67/99): loss=0.39030912117354166, gradient=0.00678771198419207\n",
      "Gradient Descent(68/99): loss=0.39030944860213557, gradient=0.0032042320562211558\n",
      "Gradient Descent(69/99): loss=0.3903098836169599, gradient=0.00427929049755735\n",
      "Gradient Descent(70/99): loss=0.3903087427197662, gradient=0.0078051255562986\n",
      "Gradient Descent(71/99): loss=0.3903097335789393, gradient=0.0034475295307093423\n",
      "Gradient Descent(72/99): loss=0.3903088062433238, gradient=0.006959400998427812\n",
      "Gradient Descent(73/99): loss=0.39031004228799576, gradient=0.005335512174058995\n",
      "Gradient Descent(74/99): loss=0.39030976924201405, gradient=0.005285322037376436\n",
      "Gradient Descent(75/99): loss=0.39031038126302237, gradient=0.0041947761961056975\n",
      "Gradient Descent(76/99): loss=0.3903108580233608, gradient=0.0044716284994878045\n",
      "Gradient Descent(77/99): loss=0.39031019226323216, gradient=0.005814806187875929\n",
      "Gradient Descent(78/99): loss=0.39030992413825416, gradient=0.005978144605788175\n",
      "Gradient Descent(79/99): loss=0.39031070938856904, gradient=0.0053233392275837425\n",
      "Gradient Descent(80/99): loss=0.39031095881951156, gradient=0.0030465727279938874\n",
      "Gradient Descent(81/99): loss=0.3903111034040348, gradient=0.004323584631705335\n",
      "Gradient Descent(82/99): loss=0.3903109155581773, gradient=0.005764262726802839\n",
      "Gradient Descent(83/99): loss=0.3903117008034152, gradient=0.003167331590588396\n",
      "Gradient Descent(84/99): loss=0.3903116022904505, gradient=0.0044177730614945844\n",
      "Gradient Descent(85/99): loss=0.3903110696143877, gradient=0.006983303074895451\n",
      "Gradient Descent(86/99): loss=0.39031036898575194, gradient=0.005612090382944859\n",
      "Gradient Descent(87/99): loss=0.3903098262732406, gradient=0.00657877636018384\n",
      "Gradient Descent(88/99): loss=0.39031046087728355, gradient=0.004347709553615028\n",
      "Gradient Descent(89/99): loss=0.3903109843140116, gradient=0.0031021904686226744\n",
      "Gradient Descent(90/99): loss=0.3903095615054392, gradient=0.007793561064665911\n",
      "Gradient Descent(91/99): loss=0.39031045247164087, gradient=0.00526242202519026\n",
      "Gradient Descent(92/99): loss=0.3903107490928584, gradient=0.0031290243659525467\n",
      "Gradient Descent(93/99): loss=0.39031051704356473, gradient=0.00531115358899564\n",
      "Gradient Descent(94/99): loss=0.3903109276174315, gradient=0.004869768942436568\n",
      "Gradient Descent(95/99): loss=0.3903117366752766, gradient=0.0031467773502399255\n",
      "Gradient Descent(96/99): loss=0.39031119298122013, gradient=0.00545795201286654\n",
      "Gradient Descent(97/99): loss=0.39031224375295026, gradient=0.0034242494184209796\n",
      "Gradient Descent(98/99): loss=0.39031193971251443, gradient=0.004932613520954816\n",
      "Gradient Descent(99/99): loss=0.3903118183549689, gradient=0.005274228844550361\n",
      "Gradient Descent(0/99): loss=0.3900214718294268, gradient=0.012799279323123006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1/99): loss=0.3900138572820092, gradient=0.01238898488761433\n",
      "Gradient Descent(2/99): loss=0.3900115199402269, gradient=0.010618306741660878\n",
      "Gradient Descent(3/99): loss=0.3900067415617758, gradient=0.009950496375269716\n",
      "Gradient Descent(4/99): loss=0.3900062173536529, gradient=0.006692171924667751\n",
      "Gradient Descent(5/99): loss=0.3900043976279033, gradient=0.006871640486454958\n",
      "Gradient Descent(6/99): loss=0.39000383332129157, gradient=0.007349252983097671\n",
      "Gradient Descent(7/99): loss=0.3900022730640507, gradient=0.0059472130549477345\n",
      "Gradient Descent(8/99): loss=0.39000209897158583, gradient=0.0054095426749752795\n",
      "Gradient Descent(9/99): loss=0.39000190985079924, gradient=0.004501750271763657\n",
      "Gradient Descent(10/99): loss=0.39000235164982494, gradient=0.0037134073429663543\n",
      "Gradient Descent(11/99): loss=0.39000129132300904, gradient=0.0057966381185755225\n",
      "Gradient Descent(12/99): loss=0.38999953980701235, gradient=0.007454558606110221\n",
      "Gradient Descent(13/99): loss=0.389999481947945, gradient=0.005939926947588624\n",
      "Gradient Descent(14/99): loss=0.3899999743267979, gradient=0.002920383332478846\n",
      "Gradient Descent(15/99): loss=0.3900001857169597, gradient=0.004309951295201641\n",
      "Gradient Descent(16/99): loss=0.39000136263551693, gradient=0.0030274162426658373\n",
      "Gradient Descent(17/99): loss=0.3900010492203417, gradient=0.005046761754911615\n",
      "Gradient Descent(18/99): loss=0.3900015176125737, gradient=0.004016638585261269\n",
      "Gradient Descent(19/99): loss=0.39000158004908014, gradient=0.0055519261942140125\n",
      "Gradient Descent(20/99): loss=0.3900015955638653, gradient=0.002864601980199691\n",
      "Gradient Descent(21/99): loss=0.3899998964516013, gradient=0.00795554483772204\n",
      "Gradient Descent(22/99): loss=0.3900005112114134, gradient=0.004880771148385889\n",
      "Gradient Descent(23/99): loss=0.39000166770269734, gradient=0.002644629986463283\n",
      "Gradient Descent(24/99): loss=0.39000182200408984, gradient=0.004309057178887846\n",
      "Gradient Descent(25/99): loss=0.39000312586589714, gradient=0.002772520429593054\n",
      "Gradient Descent(26/99): loss=0.3900037282633647, gradient=0.002113639676448054\n",
      "Gradient Descent(27/99): loss=0.39000451983129797, gradient=0.0019623894126957705\n",
      "Gradient Descent(28/99): loss=0.3900039889486074, gradient=0.00617856400048724\n",
      "Gradient Descent(29/99): loss=0.39000348831658443, gradient=0.008040041723688694\n",
      "Gradient Descent(30/99): loss=0.39000342403481447, gradient=0.0030343991985730947\n",
      "Gradient Descent(31/99): loss=0.3900045186995416, gradient=0.0021128858727628907\n",
      "Gradient Descent(32/99): loss=0.39000367022587507, gradient=0.0065251312774361825\n",
      "Gradient Descent(33/99): loss=0.3900051914867942, gradient=0.002673773266794277\n",
      "Gradient Descent(34/99): loss=0.3900050732290347, gradient=0.005014876641673892\n",
      "Gradient Descent(35/99): loss=0.3900057109534805, gradient=0.003987058624096446\n",
      "Gradient Descent(36/99): loss=0.3900058628644532, gradient=0.005267107663780495\n",
      "Gradient Descent(37/99): loss=0.3900065502660562, gradient=0.002124433374633754\n",
      "Gradient Descent(38/99): loss=0.39000737529532375, gradient=0.001977651246343269\n",
      "Gradient Descent(39/99): loss=0.3900075498231653, gradient=0.0042467586474563765\n",
      "Gradient Descent(40/99): loss=0.39000679845202363, gradient=0.006957221565930303\n",
      "Gradient Descent(41/99): loss=0.3900074959101606, gradient=0.002084801643273768\n",
      "Gradient Descent(42/99): loss=0.3900083557470159, gradient=0.0019202257327932965\n",
      "Gradient Descent(43/99): loss=0.39000647790369775, gradient=0.00798322016448612\n",
      "Gradient Descent(44/99): loss=0.3900080128053467, gradient=0.0027297809972870535\n",
      "Gradient Descent(45/99): loss=0.3900088039306824, gradient=0.002078859674330799\n",
      "Gradient Descent(46/99): loss=0.3900088320573797, gradient=0.004906103381019207\n",
      "Gradient Descent(47/99): loss=0.3900094053192778, gradient=0.0040164459405744295\n",
      "Gradient Descent(48/99): loss=0.39000929705998205, gradient=0.005732195383801938\n",
      "Gradient Descent(49/99): loss=0.39000880428327855, gradient=0.0057206767766001644\n",
      "Gradient Descent(50/99): loss=0.39000873485884513, gradient=0.0062169509555218905\n",
      "Gradient Descent(51/99): loss=0.3900101688656979, gradient=0.0023787739181057176\n",
      "Gradient Descent(52/99): loss=0.3900090908736047, gradient=0.006894897057163443\n",
      "Gradient Descent(53/99): loss=0.390009534783609, gradient=0.005734095321317544\n",
      "Gradient Descent(54/99): loss=0.39000929360400544, gradient=0.0051192880416427045\n",
      "Gradient Descent(55/99): loss=0.39001096756633424, gradient=0.0028640024699388895\n",
      "Gradient Descent(56/99): loss=0.3900117448389772, gradient=0.0019813114274975763\n",
      "Gradient Descent(57/99): loss=0.3900117214349322, gradient=0.00500378105477267\n",
      "Gradient Descent(58/99): loss=0.39001204154113367, gradient=0.004319190912055338\n",
      "Gradient Descent(59/99): loss=0.3900125228020575, gradient=0.004785320118946519\n",
      "Gradient Descent(60/99): loss=0.3900125310663391, gradient=0.005329475500273544\n",
      "Gradient Descent(61/99): loss=0.39001268737411704, gradient=0.004337344278746589\n",
      "Gradient Descent(62/99): loss=0.39001402129825136, gradient=0.0026174680124977402\n",
      "Gradient Descent(63/99): loss=0.3900133946930114, gradient=0.0057359919460121984\n",
      "Gradient Descent(64/99): loss=0.39001392943423163, gradient=0.0052819552990046965\n",
      "Gradient Descent(65/99): loss=0.39001337112545104, gradient=0.00651810446038005\n",
      "Gradient Descent(66/99): loss=0.3900140588820427, gradient=0.0065600157536736675\n",
      "Gradient Descent(67/99): loss=0.39001401646963163, gradient=0.0029078874698196954\n",
      "Gradient Descent(68/99): loss=0.390015118706188, gradient=0.0019399492033240645\n",
      "Gradient Descent(69/99): loss=0.39001531910056514, gradient=0.0042610493343177205\n",
      "Gradient Descent(70/99): loss=0.39001556948261484, gradient=0.005470548190290368\n",
      "Gradient Descent(71/99): loss=0.3900144533956366, gradient=0.006830285223157946\n",
      "Gradient Descent(72/99): loss=0.39001574899489194, gradient=0.0025992971485423308\n",
      "Gradient Descent(73/99): loss=0.3900157864705865, gradient=0.00438551886764652\n",
      "Gradient Descent(74/99): loss=0.39001707530741725, gradient=0.0025948812267646603\n",
      "Gradient Descent(75/99): loss=0.3900176356514581, gradient=0.0019062003813133817\n",
      "Gradient Descent(76/99): loss=0.3900177605973322, gradient=0.004178008573016355\n",
      "Gradient Descent(77/99): loss=0.39001895829624555, gradient=0.002400939372330626\n",
      "Gradient Descent(78/99): loss=0.3900175899161003, gradient=0.006786780618771205\n",
      "Gradient Descent(79/99): loss=0.39001770593140184, gradient=0.005716015576263689\n",
      "Gradient Descent(80/99): loss=0.390016128453989, gradient=0.007053576726198359\n",
      "Gradient Descent(81/99): loss=0.3900177238702068, gradient=0.0029180475054039648\n",
      "Gradient Descent(82/99): loss=0.39001842173384293, gradient=0.002017304165807615\n",
      "Gradient Descent(83/99): loss=0.39001930616977126, gradient=0.0018517750996069812\n",
      "Gradient Descent(84/99): loss=0.39001878134597234, gradient=0.006227145370242328\n",
      "Gradient Descent(85/99): loss=0.3900192315617786, gradient=0.005879368712392579\n",
      "Gradient Descent(86/99): loss=0.3900196123748465, gradient=0.0021520233288292133\n",
      "Gradient Descent(87/99): loss=0.3900203460621697, gradient=0.0019152241372282796\n",
      "Gradient Descent(88/99): loss=0.39002038108181397, gradient=0.004241412594353034\n",
      "Gradient Descent(89/99): loss=0.39002155178835635, gradient=0.002436452527172607\n",
      "Gradient Descent(90/99): loss=0.3900211288832849, gradient=0.004933775387183928\n",
      "Gradient Descent(91/99): loss=0.3900192316577556, gradient=0.008132159505057517\n",
      "Gradient Descent(92/99): loss=0.39001907412777587, gradient=0.0051093479469124965\n",
      "Gradient Descent(93/99): loss=0.39002000842359374, gradient=0.002121693005191298\n",
      "Gradient Descent(94/99): loss=0.3900208756355129, gradient=0.0019756968416807246\n",
      "Gradient Descent(95/99): loss=0.39002108686757286, gradient=0.00424477471972336\n",
      "Gradient Descent(96/99): loss=0.39002114023988993, gradient=0.005769948244799542\n",
      "Gradient Descent(97/99): loss=0.3900199061872099, gradient=0.007308476888242045\n",
      "Gradient Descent(98/99): loss=0.3900216506909737, gradient=0.003676805752662343\n",
      "Gradient Descent(99/99): loss=0.39002154657704596, gradient=0.004502530200805865\n",
      "Gradient Descent(0/99): loss=0.3893491672669014, gradient=0.020149716671345623\n",
      "Gradient Descent(1/99): loss=0.38935327858785496, gradient=0.023351105257703477\n",
      "Gradient Descent(2/99): loss=0.3893694955126894, gradient=0.021633164651819362\n",
      "Gradient Descent(3/99): loss=0.38937056674318166, gradient=0.021447659202683392\n",
      "Gradient Descent(4/99): loss=0.389385182997516, gradient=0.015615393306333361\n",
      "Gradient Descent(5/99): loss=0.38940009718295693, gradient=0.012186360044656785\n",
      "Gradient Descent(6/99): loss=0.3894095225839694, gradient=0.015149410037700034\n",
      "Gradient Descent(7/99): loss=0.3894149217254102, gradient=0.017998286152644438\n",
      "Gradient Descent(8/99): loss=0.3894273036203415, gradient=0.012824931618916222\n",
      "Gradient Descent(9/99): loss=0.38943823752400714, gradient=0.01320927301048562\n",
      "Gradient Descent(10/99): loss=0.38944532243004487, gradient=0.015704306862206967\n",
      "Gradient Descent(11/99): loss=0.3894547347108821, gradient=0.017044550778053384\n",
      "Gradient Descent(12/99): loss=0.3894539458256376, gradient=0.021414850364375897\n",
      "Gradient Descent(13/99): loss=0.3894689353076148, gradient=0.015652002598357816\n",
      "Gradient Descent(14/99): loss=0.38947155428368124, gradient=0.019452630220638888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(15/99): loss=0.38948466335566906, gradient=0.015898811488367685\n",
      "Gradient Descent(16/99): loss=0.38948737311624104, gradient=0.017668473167652982\n",
      "Gradient Descent(17/99): loss=0.38949853095811005, gradient=0.014627490157609568\n",
      "Gradient Descent(18/99): loss=0.3895048483569365, gradient=0.014045619508565439\n",
      "Gradient Descent(19/99): loss=0.3895161147894015, gradient=0.011669264014201758\n",
      "Gradient Descent(20/99): loss=0.38951834381162354, gradient=0.016249720289005107\n",
      "Gradient Descent(21/99): loss=0.3895275797618541, gradient=0.013531318000732661\n",
      "Gradient Descent(22/99): loss=0.3895348233436688, gradient=0.012668541289730032\n",
      "Gradient Descent(23/99): loss=0.3895423703255228, gradient=0.013265902811756653\n",
      "Gradient Descent(24/99): loss=0.3895509328438159, gradient=0.012428995770532882\n",
      "Gradient Descent(25/99): loss=0.3895536226834096, gradient=0.016696655788712034\n",
      "Gradient Descent(26/99): loss=0.38956099682008694, gradient=0.013161780112175133\n",
      "Gradient Descent(27/99): loss=0.38957277352403313, gradient=0.01048704389192835\n",
      "Gradient Descent(28/99): loss=0.389570514327098, gradient=0.018577533329925826\n",
      "Gradient Descent(29/99): loss=0.38957545084500056, gradient=0.015280054887564322\n",
      "Gradient Descent(30/99): loss=0.38958006642674264, gradient=0.015126174364743566\n",
      "Gradient Descent(31/99): loss=0.38958557706999875, gradient=0.015035904867594697\n",
      "Gradient Descent(32/99): loss=0.3895938420650675, gradient=0.013885829272041477\n",
      "Gradient Descent(33/99): loss=0.3896036689292362, gradient=0.012392344260094145\n",
      "Gradient Descent(34/99): loss=0.38960579499146064, gradient=0.015279385029262835\n",
      "Gradient Descent(35/99): loss=0.3896099230186428, gradient=0.014998006023529464\n",
      "Gradient Descent(36/99): loss=0.3896201198614578, gradient=0.00999146943267751\n",
      "Gradient Descent(37/99): loss=0.3896262933814593, gradient=0.013236375980816523\n",
      "Gradient Descent(38/99): loss=0.3896324780531799, gradient=0.012844722235186217\n",
      "Gradient Descent(39/99): loss=0.3896370691335706, gradient=0.01607964294429376\n",
      "Gradient Descent(40/99): loss=0.38964026039937627, gradient=0.013493925653745712\n",
      "Gradient Descent(41/99): loss=0.38964883592720606, gradient=0.009965526452862845\n",
      "Gradient Descent(42/99): loss=0.3896541916189289, gradient=0.012940194631302039\n",
      "Gradient Descent(43/99): loss=0.3896550967896017, gradient=0.016417569067774324\n",
      "Gradient Descent(44/99): loss=0.3896643034944367, gradient=0.009562514062961965\n",
      "Gradient Descent(45/99): loss=0.3896686500150436, gradient=0.013126701853587044\n",
      "Gradient Descent(46/99): loss=0.3896755078621251, gradient=0.011620983054801906\n",
      "Gradient Descent(47/99): loss=0.38967508109086024, gradient=0.01675888656342979\n",
      "Gradient Descent(48/99): loss=0.38967947759940375, gradient=0.01499765076277259\n",
      "Gradient Descent(49/99): loss=0.3896877810884252, gradient=0.015135124276711872\n",
      "Gradient Descent(50/99): loss=0.3896877096239001, gradient=0.015332894652454687\n",
      "Gradient Descent(51/99): loss=0.38969640401698596, gradient=0.009707689570558838\n",
      "Gradient Descent(52/99): loss=0.3896973912406808, gradient=0.01605123145300519\n",
      "Gradient Descent(53/99): loss=0.3897049609202743, gradient=0.011089373320976582\n",
      "Gradient Descent(54/99): loss=0.38970683344372403, gradient=0.01536502769278853\n",
      "Gradient Descent(55/99): loss=0.38971282163615184, gradient=0.01345608852840375\n",
      "Gradient Descent(56/99): loss=0.38971968937036056, gradient=0.010985656211638587\n",
      "Gradient Descent(57/99): loss=0.3897273190578017, gradient=0.009847060021532818\n",
      "Gradient Descent(58/99): loss=0.3897292867764512, gradient=0.014369828469687061\n",
      "Gradient Descent(59/99): loss=0.3897357660730683, gradient=0.01090433448352\n",
      "Gradient Descent(60/99): loss=0.38973825439199145, gradient=0.014605855091396532\n",
      "Gradient Descent(61/99): loss=0.38973672866815534, gradient=0.015873030390552587\n",
      "Gradient Descent(62/99): loss=0.3897416732308408, gradient=0.013734687318311352\n",
      "Gradient Descent(63/99): loss=0.38975015667699703, gradient=0.01176057732835429\n",
      "Gradient Descent(64/99): loss=0.389751265245788, gradient=0.01481523583523734\n",
      "Gradient Descent(65/99): loss=0.3897585281242764, gradient=0.013798611586513984\n",
      "Gradient Descent(66/99): loss=0.389760787902222, gradient=0.01429414219872686\n",
      "Gradient Descent(67/99): loss=0.3897694807377587, gradient=0.010584482627031133\n",
      "Gradient Descent(68/99): loss=0.389770776019085, gradient=0.014131210845077995\n",
      "Gradient Descent(69/99): loss=0.38977215460793824, gradient=0.015724737366370218\n",
      "Gradient Descent(70/99): loss=0.38977728840156317, gradient=0.013525391969369797\n",
      "Gradient Descent(71/99): loss=0.3897838317032047, gradient=0.009875190249666424\n",
      "Gradient Descent(72/99): loss=0.3897804878799995, gradient=0.017076480317422947\n",
      "Gradient Descent(73/99): loss=0.3897858655069461, gradient=0.013055672991399472\n",
      "Gradient Descent(74/99): loss=0.3897914627596924, gradient=0.010287089648963868\n",
      "Gradient Descent(75/99): loss=0.3897979715623414, gradient=0.010652769252494289\n",
      "Gradient Descent(76/99): loss=0.3897995073197629, gradient=0.014130928446996406\n",
      "Gradient Descent(77/99): loss=0.38980629487454715, gradient=0.009877137390924074\n",
      "Gradient Descent(78/99): loss=0.3898116367108458, gradient=0.010544338313310307\n",
      "Gradient Descent(79/99): loss=0.38981000721381676, gradient=0.015892218245781735\n",
      "Gradient Descent(80/99): loss=0.3898151182614973, gradient=0.011495018613113006\n",
      "Gradient Descent(81/99): loss=0.38981924400013707, gradient=0.01190394178228843\n",
      "Gradient Descent(82/99): loss=0.38982386951134856, gradient=0.0101030234911708\n",
      "Gradient Descent(83/99): loss=0.38982482128448337, gradient=0.014489728117729141\n",
      "Gradient Descent(84/99): loss=0.3898277548855006, gradient=0.013374307099649761\n",
      "Gradient Descent(85/99): loss=0.38983175707814266, gradient=0.011094668964097684\n",
      "Gradient Descent(86/99): loss=0.3898387502789484, gradient=0.010025545479784608\n",
      "Gradient Descent(87/99): loss=0.3898413939701825, gradient=0.012816923893379345\n",
      "Gradient Descent(88/99): loss=0.38984125614956194, gradient=0.01641057439306735\n",
      "Gradient Descent(89/99): loss=0.38984077630415487, gradient=0.014888558247670087\n",
      "Gradient Descent(90/99): loss=0.38984719149947555, gradient=0.011068038329464456\n",
      "Gradient Descent(91/99): loss=0.3898466398146476, gradient=0.015996603777611588\n",
      "Gradient Descent(92/99): loss=0.3898536453905653, gradient=0.011804683338184317\n",
      "Gradient Descent(93/99): loss=0.38985405174171217, gradient=0.016180309114566367\n",
      "Gradient Descent(94/99): loss=0.389860479678198, gradient=0.015606020184754443\n",
      "Gradient Descent(95/99): loss=0.38986235909678013, gradient=0.01236794159304454\n",
      "Gradient Descent(96/99): loss=0.38986640042100323, gradient=0.012789142766182642\n",
      "Gradient Descent(97/99): loss=0.3898614085985029, gradient=0.017774400959574277\n",
      "Gradient Descent(98/99): loss=0.3898699545850483, gradient=0.00886598862338361\n",
      "Gradient Descent(99/99): loss=0.3898735910779688, gradient=0.01207705206404897\n",
      "Gradient Descent(0/99): loss=0.3909997785856887, gradient=0.01778163262562639\n",
      "Gradient Descent(1/99): loss=0.39100555353797817, gradient=0.014450885043809845\n",
      "Gradient Descent(2/99): loss=0.3910068508378966, gradient=0.016007184331055545\n",
      "Gradient Descent(3/99): loss=0.3910125030053426, gradient=0.014203951169532157\n",
      "Gradient Descent(4/99): loss=0.3910152186662916, gradient=0.015486675882772655\n",
      "Gradient Descent(5/99): loss=0.39101345489241046, gradient=0.017338733352862543\n",
      "Gradient Descent(6/99): loss=0.3910194830426583, gradient=0.013469866168859177\n",
      "Gradient Descent(7/99): loss=0.39102552417535724, gradient=0.012540428824139097\n",
      "Gradient Descent(8/99): loss=0.39102936578967057, gradient=0.012984174795457068\n",
      "Gradient Descent(9/99): loss=0.3910356530267392, gradient=0.012198176193593318\n",
      "Gradient Descent(10/99): loss=0.3910329048614999, gradient=0.01884462567820715\n",
      "Gradient Descent(11/99): loss=0.39103726541896, gradient=0.019286332999424064\n",
      "Gradient Descent(12/99): loss=0.39103520565816813, gradient=0.016202701219261678\n",
      "Gradient Descent(13/99): loss=0.39104127378519526, gradient=0.01319078527965817\n",
      "Gradient Descent(14/99): loss=0.3910426313703369, gradient=0.015334044689865158\n",
      "Gradient Descent(15/99): loss=0.3910497387300881, gradient=0.011151233554144747\n",
      "Gradient Descent(16/99): loss=0.3910556039864148, gradient=0.011671939189800236\n",
      "Gradient Descent(17/99): loss=0.39105201411055357, gradient=0.018048770600736598\n",
      "Gradient Descent(18/99): loss=0.3910579813304427, gradient=0.013226310030642572\n",
      "Gradient Descent(19/99): loss=0.3910596511186591, gradient=0.015413846109235586\n",
      "Gradient Descent(20/99): loss=0.3910587272471134, gradient=0.01724744791921016\n",
      "Gradient Descent(21/99): loss=0.3910580748488783, gradient=0.01679432760914985\n",
      "Gradient Descent(22/99): loss=0.3910643423435028, gradient=0.01133581358140545\n",
      "Gradient Descent(23/99): loss=0.391069554059316, gradient=0.013263387812576822\n",
      "Gradient Descent(24/99): loss=0.3910722537342946, gradient=0.013499240767276006\n",
      "Gradient Descent(25/99): loss=0.39107926001629273, gradient=0.010919035201617636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(26/99): loss=0.3910759314570441, gradient=0.017895056649328802\n",
      "Gradient Descent(27/99): loss=0.39108210063690596, gradient=0.013184779701391912\n",
      "Gradient Descent(28/99): loss=0.391083514333803, gradient=0.014559309496469065\n",
      "Gradient Descent(29/99): loss=0.39108692077628976, gradient=0.012541570293251066\n",
      "Gradient Descent(30/99): loss=0.39108995248395334, gradient=0.013561668030931891\n",
      "Gradient Descent(31/99): loss=0.39109014899028555, gradient=0.015836703582025335\n",
      "Gradient Descent(32/99): loss=0.3910927821540757, gradient=0.012867677468247926\n",
      "Gradient Descent(33/99): loss=0.3910985319792166, gradient=0.010760928470776324\n",
      "Gradient Descent(34/99): loss=0.39110250523213275, gradient=0.013302377401275219\n",
      "Gradient Descent(35/99): loss=0.3911033210463852, gradient=0.01514606208999543\n",
      "Gradient Descent(36/99): loss=0.3911067900019556, gradient=0.014586423617465065\n",
      "Gradient Descent(37/99): loss=0.39110683699496634, gradient=0.014973384204439966\n",
      "Gradient Descent(38/99): loss=0.39110737961895853, gradient=0.01601707261857496\n",
      "Gradient Descent(39/99): loss=0.39111470318874864, gradient=0.012228781260166879\n",
      "Gradient Descent(40/99): loss=0.3911186039493582, gradient=0.011125002497960909\n",
      "Gradient Descent(41/99): loss=0.3911205701655123, gradient=0.014592824493551697\n",
      "Gradient Descent(42/99): loss=0.39112252571144396, gradient=0.013994158968438312\n",
      "Gradient Descent(43/99): loss=0.39112630482113325, gradient=0.01621388169521274\n",
      "Gradient Descent(44/99): loss=0.3911220883874518, gradient=0.016703615413776504\n",
      "Gradient Descent(45/99): loss=0.3911270905680724, gradient=0.012786904536640492\n",
      "Gradient Descent(46/99): loss=0.3911324964340253, gradient=0.011279281941045605\n",
      "Gradient Descent(47/99): loss=0.3911332059332359, gradient=0.014918838685451822\n",
      "Gradient Descent(48/99): loss=0.391137754840192, gradient=0.014410039229115133\n",
      "Gradient Descent(49/99): loss=0.39113723118025, gradient=0.015357391502945739\n",
      "Gradient Descent(50/99): loss=0.39114366828663155, gradient=0.013862353017858101\n",
      "Gradient Descent(51/99): loss=0.39114143607742186, gradient=0.018177657028524023\n",
      "Gradient Descent(52/99): loss=0.3911439362501496, gradient=0.01860774557535291\n",
      "Gradient Descent(53/99): loss=0.3911451485156205, gradient=0.013714756145607389\n",
      "Gradient Descent(54/99): loss=0.39115194771301054, gradient=0.010751048200924625\n",
      "Gradient Descent(55/99): loss=0.3911516981686608, gradient=0.015293310941212758\n",
      "Gradient Descent(56/99): loss=0.3911560147979384, gradient=0.012444862336542604\n",
      "Gradient Descent(57/99): loss=0.3911607571569605, gradient=0.011566445056722255\n",
      "Gradient Descent(58/99): loss=0.39116034927860754, gradient=0.014140580970885256\n",
      "Gradient Descent(59/99): loss=0.39115754948175274, gradient=0.016709529671134236\n",
      "Gradient Descent(60/99): loss=0.3911603993927134, gradient=0.014256632891370285\n",
      "Gradient Descent(61/99): loss=0.39116585977509555, gradient=0.01145961217481793\n",
      "Gradient Descent(62/99): loss=0.39116840982521806, gradient=0.01266363059354494\n",
      "Gradient Descent(63/99): loss=0.39116853862244755, gradient=0.015334782297761255\n",
      "Gradient Descent(64/99): loss=0.3911744891096128, gradient=0.010763569159251968\n",
      "Gradient Descent(65/99): loss=0.3911734819849625, gradient=0.01586604778929782\n",
      "Gradient Descent(66/99): loss=0.39117800648560536, gradient=0.014322440826510267\n",
      "Gradient Descent(67/99): loss=0.3911793657606988, gradient=0.014235280999563298\n",
      "Gradient Descent(68/99): loss=0.3911802879786581, gradient=0.016947280765153037\n",
      "Gradient Descent(69/99): loss=0.3911793035408584, gradient=0.015713553712682158\n",
      "Gradient Descent(70/99): loss=0.39118457732762163, gradient=0.01455512200764173\n",
      "Gradient Descent(71/99): loss=0.39118405964215824, gradient=0.015768127174607956\n",
      "Gradient Descent(72/99): loss=0.39118772103374516, gradient=0.016529166535940165\n",
      "Gradient Descent(73/99): loss=0.39118672407534627, gradient=0.014482950737903707\n",
      "Gradient Descent(74/99): loss=0.39119163651987016, gradient=0.012616467768897152\n",
      "Gradient Descent(75/99): loss=0.39119376951210566, gradient=0.013062967824126685\n",
      "Gradient Descent(76/99): loss=0.39119535644238845, gradient=0.014005653976269343\n",
      "Gradient Descent(77/99): loss=0.39119620443005726, gradient=0.0144451582005207\n",
      "Gradient Descent(78/99): loss=0.39119972796896263, gradient=0.01116143582626828\n",
      "Gradient Descent(79/99): loss=0.3912021866360507, gradient=0.012970986147880392\n",
      "Gradient Descent(80/99): loss=0.3912032564296035, gradient=0.014102625187003637\n",
      "Gradient Descent(81/99): loss=0.39120790027043134, gradient=0.011068088355372383\n",
      "Gradient Descent(82/99): loss=0.3912111161495618, gradient=0.010849921045866873\n",
      "Gradient Descent(83/99): loss=0.39121136545068036, gradient=0.014320608253603118\n",
      "Gradient Descent(84/99): loss=0.3912098750828779, gradient=0.015912751738421216\n",
      "Gradient Descent(85/99): loss=0.39121487777187935, gradient=0.010403882312414093\n",
      "Gradient Descent(86/99): loss=0.3912171274944235, gradient=0.012818883168861506\n",
      "Gradient Descent(87/99): loss=0.39121607170824885, gradient=0.015182595603596132\n",
      "Gradient Descent(88/99): loss=0.3912183384012917, gradient=0.016450809130947958\n",
      "Gradient Descent(89/99): loss=0.39121736216232417, gradient=0.013652717318170908\n",
      "Gradient Descent(90/99): loss=0.3912230926786545, gradient=0.011422199150364406\n",
      "Gradient Descent(91/99): loss=0.391222152150949, gradient=0.015339429542957002\n",
      "Gradient Descent(92/99): loss=0.3912226502539045, gradient=0.016749055680428254\n",
      "Gradient Descent(93/99): loss=0.3912255393094037, gradient=0.011129720914324622\n",
      "Gradient Descent(94/99): loss=0.39123066516048677, gradient=0.01113612179293717\n",
      "Gradient Descent(95/99): loss=0.39122762604329075, gradient=0.01656780615488114\n",
      "Gradient Descent(96/99): loss=0.39123380231311494, gradient=0.011183506177833844\n",
      "Gradient Descent(97/99): loss=0.3912372773083373, gradient=0.010794917785637747\n",
      "Gradient Descent(98/99): loss=0.3912364129558201, gradient=0.01414974694860158\n",
      "Gradient Descent(99/99): loss=0.3912375431454648, gradient=0.013717968447300508\n",
      "Gradient Descent(0/99): loss=0.39159811572605346, gradient=0.015501980938518194\n",
      "Gradient Descent(1/99): loss=0.39159460400581686, gradient=0.01696932182839861\n",
      "Gradient Descent(2/99): loss=0.3915975994365478, gradient=0.013446315602295944\n",
      "Gradient Descent(3/99): loss=0.3915910834723469, gradient=0.019169342942007738\n",
      "Gradient Descent(4/99): loss=0.39159062242751896, gradient=0.0170936496799429\n",
      "Gradient Descent(5/99): loss=0.3915949709098725, gradient=0.013546551528137668\n",
      "Gradient Descent(6/99): loss=0.3915955956942485, gradient=0.01341065320480269\n",
      "Gradient Descent(7/99): loss=0.3915965460182103, gradient=0.014437000121596254\n",
      "Gradient Descent(8/99): loss=0.3915942360226798, gradient=0.016015545120817418\n",
      "Gradient Descent(9/99): loss=0.3915982513712691, gradient=0.013659206426936434\n",
      "Gradient Descent(10/99): loss=0.3915976910669411, gradient=0.01318250669520132\n",
      "Gradient Descent(11/99): loss=0.3915977921457329, gradient=0.014598574665943368\n",
      "Gradient Descent(12/99): loss=0.39159757872883144, gradient=0.013135026184057121\n",
      "Gradient Descent(13/99): loss=0.39159666529871434, gradient=0.014573308472839018\n",
      "Gradient Descent(14/99): loss=0.3915988251187469, gradient=0.0122599868894032\n",
      "Gradient Descent(15/99): loss=0.39159775619719484, gradient=0.014640308884476634\n",
      "Gradient Descent(16/99): loss=0.3915979311404167, gradient=0.012665388319644753\n",
      "Gradient Descent(17/99): loss=0.39159982712379837, gradient=0.01284644983214849\n",
      "Gradient Descent(18/99): loss=0.3915953025299375, gradient=0.01692082854355412\n",
      "Gradient Descent(19/99): loss=0.3915947352193503, gradient=0.01773685176307668\n",
      "Gradient Descent(20/99): loss=0.3915925607272172, gradient=0.013928608034630138\n",
      "Gradient Descent(21/99): loss=0.39159579715024206, gradient=0.012959238167298272\n",
      "Gradient Descent(22/99): loss=0.39159429792763706, gradient=0.014567733362672166\n",
      "Gradient Descent(23/99): loss=0.3915931379799966, gradient=0.015352576144338197\n",
      "Gradient Descent(24/99): loss=0.3915959115772641, gradient=0.01226117627391218\n",
      "Gradient Descent(25/99): loss=0.39159495921034754, gradient=0.014956387639499339\n",
      "Gradient Descent(26/99): loss=0.3915955739690074, gradient=0.01272845477161148\n",
      "Gradient Descent(27/99): loss=0.39159437676593456, gradient=0.015073916026448936\n",
      "Gradient Descent(28/99): loss=0.39159669857909357, gradient=0.012398817462358427\n",
      "Gradient Descent(29/99): loss=0.3915956061250032, gradient=0.014886257818722234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(30/99): loss=0.39159593694132705, gradient=0.01281982859841774\n",
      "Gradient Descent(31/99): loss=0.39159785239018235, gradient=0.01309948344283284\n",
      "Gradient Descent(32/99): loss=0.3915935809416166, gradient=0.016977303751740003\n",
      "Gradient Descent(33/99): loss=0.3915966782111113, gradient=0.014291812310185992\n",
      "Gradient Descent(34/99): loss=0.391596156228888, gradient=0.013059319170965245\n",
      "Gradient Descent(35/99): loss=0.391598427159895, gradient=0.012791299568286967\n",
      "Gradient Descent(36/99): loss=0.3915937233085575, gradient=0.01709877544688188\n",
      "Gradient Descent(37/99): loss=0.3915935616645082, gradient=0.017576559271972304\n",
      "Gradient Descent(38/99): loss=0.39158728869898246, gradient=0.018260317955258454\n",
      "Gradient Descent(39/99): loss=0.3915906324030862, gradient=0.015559750426417798\n",
      "Gradient Descent(40/99): loss=0.39158843291473505, gradient=0.016090531887322724\n",
      "Gradient Descent(41/99): loss=0.3915930214352895, gradient=0.01361824271911945\n",
      "Gradient Descent(42/99): loss=0.39158854654066927, gradient=0.01700544247699384\n",
      "Gradient Descent(43/99): loss=0.391591199032062, gradient=0.015439684244540963\n",
      "Gradient Descent(44/99): loss=0.39158900343275654, gradient=0.01593066091502909\n",
      "Gradient Descent(45/99): loss=0.39159335494065184, gradient=0.013612580467242624\n",
      "Gradient Descent(46/99): loss=0.3915906611425323, gradient=0.014842645198194582\n",
      "Gradient Descent(47/99): loss=0.3915934769043139, gradient=0.012562397261105303\n",
      "Gradient Descent(48/99): loss=0.39159151322331626, gradient=0.01543756988044686\n",
      "Gradient Descent(49/99): loss=0.39159522581598155, gradient=0.01359997349977315\n",
      "Gradient Descent(50/99): loss=0.391592166801373, gradient=0.014838914924619138\n",
      "Gradient Descent(51/99): loss=0.39159464488633167, gradient=0.012595958710517777\n",
      "Gradient Descent(52/99): loss=0.39159260167375154, gradient=0.015331773917969657\n",
      "Gradient Descent(53/99): loss=0.3915960946303641, gradient=0.013629917264896598\n",
      "Gradient Descent(54/99): loss=0.391593278358143, gradient=0.014692229563607877\n",
      "Gradient Descent(55/99): loss=0.39159081250546685, gradient=0.01616546432563166\n",
      "Gradient Descent(56/99): loss=0.3915891385998167, gradient=0.014544760907854512\n",
      "Gradient Descent(57/99): loss=0.39158927161253265, gradient=0.014941655537586964\n",
      "Gradient Descent(58/99): loss=0.3915916959292809, gradient=0.01254612300415984\n",
      "Gradient Descent(59/99): loss=0.39159178316424514, gradient=0.013381327603573237\n",
      "Gradient Descent(60/99): loss=0.3915944993492673, gradient=0.012632953928937837\n",
      "Gradient Descent(61/99): loss=0.3915900169734078, gradient=0.01718739415406392\n",
      "Gradient Descent(62/99): loss=0.3915937856458935, gradient=0.013914197624686223\n",
      "Gradient Descent(63/99): loss=0.39159310031225397, gradient=0.0133834282597161\n",
      "Gradient Descent(64/99): loss=0.3915932784492961, gradient=0.014716204588551985\n",
      "Gradient Descent(65/99): loss=0.39159117506958585, gradient=0.015676607915834154\n",
      "Gradient Descent(66/99): loss=0.3915947084772543, gradient=0.013976882045016448\n",
      "Gradient Descent(67/99): loss=0.39159406336227626, gradient=0.013244428975333324\n",
      "Gradient Descent(68/99): loss=0.391593934263964, gradient=0.014867805126837228\n",
      "Gradient Descent(69/99): loss=0.391591897993156, gradient=0.01558016123820539\n",
      "Gradient Descent(70/99): loss=0.39159533531381, gradient=0.01400478648801234\n",
      "Gradient Descent(71/99): loss=0.39159235175362006, gradient=0.014968129970571913\n",
      "Gradient Descent(72/99): loss=0.3915946456776879, gradient=0.013016358095335986\n",
      "Gradient Descent(73/99): loss=0.391593152991867, gradient=0.014856079329696927\n",
      "Gradient Descent(74/99): loss=0.39159251095605746, gradient=0.017768933252386977\n",
      "Gradient Descent(75/99): loss=0.3915883635356815, gradient=0.01547738058056662\n",
      "Gradient Descent(76/99): loss=0.3915912872414994, gradient=0.01340226052477022\n",
      "Gradient Descent(77/99): loss=0.3915902968918727, gradient=0.014950232431988363\n",
      "Gradient Descent(78/99): loss=0.39159153490525667, gradient=0.015706770226548518\n",
      "Gradient Descent(79/99): loss=0.3915916573303544, gradient=0.012706111027478938\n",
      "Gradient Descent(80/99): loss=0.39159389887627144, gradient=0.013054797264941773\n",
      "Gradient Descent(81/99): loss=0.3915925093518996, gradient=0.014923247775758998\n",
      "Gradient Descent(82/99): loss=0.3915933725113654, gradient=0.01569869686555607\n",
      "Gradient Descent(83/99): loss=0.39159318449428804, gradient=0.012734252925744876\n",
      "Gradient Descent(84/99): loss=0.3915953450496554, gradient=0.012938290349077461\n",
      "Gradient Descent(85/99): loss=0.39159148803140165, gradient=0.01662213809355344\n",
      "Gradient Descent(86/99): loss=0.3915946801180837, gradient=0.014136649552745201\n",
      "Gradient Descent(87/99): loss=0.3915943251200051, gradient=0.012761107716602667\n",
      "Gradient Descent(88/99): loss=0.3915965124094766, gradient=0.012825722119554196\n",
      "Gradient Descent(89/99): loss=0.3915923789615439, gradient=0.01674263533470332\n",
      "Gradient Descent(90/99): loss=0.391592636606208, gradient=0.01751891226508291\n",
      "Gradient Descent(91/99): loss=0.3915861312247431, gradient=0.018438991760022363\n",
      "Gradient Descent(92/99): loss=0.3915921333121415, gradient=0.013896783146967783\n",
      "Gradient Descent(93/99): loss=0.39158758633701457, gradient=0.017732495942116056\n",
      "Gradient Descent(94/99): loss=0.39159248771306604, gradient=0.013803023561875148\n",
      "Gradient Descent(95/99): loss=0.3915921169028971, gradient=0.013617914033985247\n",
      "Gradient Descent(96/99): loss=0.3915930010969977, gradient=0.014571936598494015\n",
      "Gradient Descent(97/99): loss=0.39159114960859015, gradient=0.015781709181588666\n",
      "Gradient Descent(98/99): loss=0.3915951623375798, gradient=0.013858203762611484\n",
      "Gradient Descent(99/99): loss=0.3915946209527001, gradient=0.013370457462783548\n",
      "Gradient Descent(0/99): loss=0.39119350881914966, gradient=0.016705085713111006\n",
      "Gradient Descent(1/99): loss=0.39118393788597344, gradient=0.016851528546964475\n",
      "Gradient Descent(2/99): loss=0.3911835471561549, gradient=0.01359772305005291\n",
      "Gradient Descent(3/99): loss=0.39117952731352157, gradient=0.0132941883499568\n",
      "Gradient Descent(4/99): loss=0.39117810984868623, gradient=0.013544252893454565\n",
      "Gradient Descent(5/99): loss=0.3911727317865166, gradient=0.016327545501848496\n",
      "Gradient Descent(6/99): loss=0.39117674521525175, gradient=0.012052977922954821\n",
      "Gradient Descent(7/99): loss=0.39117508244484717, gradient=0.01284248350287124\n",
      "Gradient Descent(8/99): loss=0.39117599486926413, gradient=0.012837017188398062\n",
      "Gradient Descent(9/99): loss=0.39117124463337083, gradient=0.016455566781544705\n",
      "Gradient Descent(10/99): loss=0.3911768953269852, gradient=0.011447285679806404\n",
      "Gradient Descent(11/99): loss=0.391174973443266, gradient=0.013245284300702076\n",
      "Gradient Descent(12/99): loss=0.39117258953912104, gradient=0.015200172669269532\n",
      "Gradient Descent(13/99): loss=0.39117334786783525, gradient=0.012971039612349548\n",
      "Gradient Descent(14/99): loss=0.39117745048372515, gradient=0.010051926743050723\n",
      "Gradient Descent(15/99): loss=0.3911742645579681, gradient=0.015716744014318364\n",
      "Gradient Descent(16/99): loss=0.3911763994669909, gradient=0.01457748226326003\n",
      "Gradient Descent(17/99): loss=0.3911709183974391, gradient=0.018427802186603677\n",
      "Gradient Descent(18/99): loss=0.39117227894610945, gradient=0.0161523924258648\n",
      "Gradient Descent(19/99): loss=0.39116974121945114, gradient=0.015642701885220035\n",
      "Gradient Descent(20/99): loss=0.39117433645177807, gradient=0.010308715928290541\n",
      "Gradient Descent(21/99): loss=0.39117672736646336, gradient=0.011631171699136125\n",
      "Gradient Descent(22/99): loss=0.39117972249366756, gradient=0.011276300370184263\n",
      "Gradient Descent(23/99): loss=0.39117452570006, gradient=0.017458598139022553\n",
      "Gradient Descent(24/99): loss=0.39117290045535275, gradient=0.016256151347198454\n",
      "Gradient Descent(25/99): loss=0.39117613691795017, gradient=0.017768035623776615\n",
      "Gradient Descent(26/99): loss=0.3911708487715717, gradient=0.01752740635927388\n",
      "Gradient Descent(27/99): loss=0.3911725554359457, gradient=0.01638499077899738\n",
      "Gradient Descent(28/99): loss=0.3911744849218557, gradient=0.011207522770549606\n",
      "Gradient Descent(29/99): loss=0.39117797735151616, gradient=0.012114604505079285\n",
      "Gradient Descent(30/99): loss=0.39117671001267396, gradient=0.015044886819449349\n",
      "Gradient Descent(31/99): loss=0.39118017440544905, gradient=0.013843575240294828\n",
      "Gradient Descent(32/99): loss=0.3911787840341147, gradient=0.013376563787936103\n",
      "Gradient Descent(33/99): loss=0.39117700985682113, gradient=0.015170498592863645\n",
      "Gradient Descent(34/99): loss=0.39118202905657695, gradient=0.009447289174114573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(35/99): loss=0.3911827658509988, gradient=0.0126117900227178\n",
      "Gradient Descent(36/99): loss=0.39118248992855076, gradient=0.013397651000362129\n",
      "Gradient Descent(37/99): loss=0.3911807306639619, gradient=0.015076282700695134\n",
      "Gradient Descent(38/99): loss=0.39118469421076235, gradient=0.010201005420117147\n",
      "Gradient Descent(39/99): loss=0.39118276440051963, gradient=0.01468819544456415\n",
      "Gradient Descent(40/99): loss=0.3911837995339439, gradient=0.015051237920882558\n",
      "Gradient Descent(41/99): loss=0.39118294653422997, gradient=0.012861291611330617\n",
      "Gradient Descent(42/99): loss=0.39118441337680404, gradient=0.012281720238326408\n",
      "Gradient Descent(43/99): loss=0.39118429624332224, gradient=0.01373324179485981\n",
      "Gradient Descent(44/99): loss=0.3911853487010449, gradient=0.01271623766762038\n",
      "Gradient Descent(45/99): loss=0.39118692310874, gradient=0.011760998196669833\n",
      "Gradient Descent(46/99): loss=0.3911885027620593, gradient=0.011997042938305344\n",
      "Gradient Descent(47/99): loss=0.39118389223831096, gradient=0.016755506360958808\n",
      "Gradient Descent(48/99): loss=0.3911853991836431, gradient=0.012384362684618755\n",
      "Gradient Descent(49/99): loss=0.39118647996539924, gradient=0.016096441952986654\n",
      "Gradient Descent(50/99): loss=0.39118178972543516, gradient=0.015374212291768387\n",
      "Gradient Descent(51/99): loss=0.39118173299374615, gradient=0.015350642696193592\n",
      "Gradient Descent(52/99): loss=0.3911835907537104, gradient=0.01270550054149562\n",
      "Gradient Descent(53/99): loss=0.3911900696759507, gradient=0.011087256141900282\n",
      "Gradient Descent(54/99): loss=0.3911868417933296, gradient=0.014867146737017936\n",
      "Gradient Descent(55/99): loss=0.39118358174645046, gradient=0.01632360227273312\n",
      "Gradient Descent(56/99): loss=0.39118767272708427, gradient=0.01025063543448875\n",
      "Gradient Descent(57/99): loss=0.3911900259813563, gradient=0.011505140673708864\n",
      "Gradient Descent(58/99): loss=0.39118858749584084, gradient=0.014325731457464413\n",
      "Gradient Descent(59/99): loss=0.3911903233568619, gradient=0.012207835883803272\n",
      "Gradient Descent(60/99): loss=0.391191246659095, gradient=0.012584790004653552\n",
      "Gradient Descent(61/99): loss=0.3911870949419755, gradient=0.016523473088139003\n",
      "Gradient Descent(62/99): loss=0.39119267934837276, gradient=0.011606947246680512\n",
      "Gradient Descent(63/99): loss=0.39118681003009154, gradient=0.01698031944376462\n",
      "Gradient Descent(64/99): loss=0.39118846731417867, gradient=0.015501682247275043\n",
      "Gradient Descent(65/99): loss=0.39118710641457355, gradient=0.01467401120749362\n",
      "Gradient Descent(66/99): loss=0.3911924635554787, gradient=0.010795774684176378\n",
      "Gradient Descent(67/99): loss=0.39118829568128205, gradient=0.015873708101021712\n",
      "Gradient Descent(68/99): loss=0.39118674927307046, gradient=0.015808396678750957\n",
      "Gradient Descent(69/99): loss=0.39118953132325124, gradient=0.01381341675016654\n",
      "Gradient Descent(70/99): loss=0.39119138752780086, gradient=0.010565916793299386\n",
      "Gradient Descent(71/99): loss=0.3911889302400104, gradient=0.015521873923518796\n",
      "Gradient Descent(72/99): loss=0.3911913588670144, gradient=0.01160427608044389\n",
      "Gradient Descent(73/99): loss=0.3911916944059826, gradient=0.013224529772310506\n",
      "Gradient Descent(74/99): loss=0.3911933276204234, gradient=0.012171405082171413\n",
      "Gradient Descent(75/99): loss=0.39118730635939053, gradient=0.020473183409817835\n",
      "Gradient Descent(76/99): loss=0.3911878299006869, gradient=0.01164653280203577\n",
      "Gradient Descent(77/99): loss=0.3911928775336451, gradient=0.010717902211666328\n",
      "Gradient Descent(78/99): loss=0.3911892649722815, gradient=0.016358975904346932\n",
      "Gradient Descent(79/99): loss=0.39119049960790947, gradient=0.014282949928176138\n",
      "Gradient Descent(80/99): loss=0.3911931998262952, gradient=0.010687826109201807\n",
      "Gradient Descent(81/99): loss=0.39119499077434894, gradient=0.011919033847049001\n",
      "Gradient Descent(82/99): loss=0.39119514336847444, gradient=0.015656677754990498\n",
      "Gradient Descent(83/99): loss=0.39118689166754456, gradient=0.018764947899018076\n",
      "Gradient Descent(84/99): loss=0.39119122115646665, gradient=0.012250153569486267\n",
      "Gradient Descent(85/99): loss=0.3911925973200947, gradient=0.01236356297367124\n",
      "Gradient Descent(86/99): loss=0.39119496319813796, gradient=0.011360429181632043\n",
      "Gradient Descent(87/99): loss=0.39119010095823076, gradient=0.016622291169352047\n",
      "Gradient Descent(88/99): loss=0.39119500951032066, gradient=0.009373213524964645\n",
      "Gradient Descent(89/99): loss=0.3911936248973787, gradient=0.014687646471158562\n",
      "Gradient Descent(90/99): loss=0.3911948556558168, gradient=0.012457381983759865\n",
      "Gradient Descent(91/99): loss=0.3911977802938731, gradient=0.01348066226928167\n",
      "Gradient Descent(92/99): loss=0.3911934313766214, gradient=0.015133240474392767\n",
      "Gradient Descent(93/99): loss=0.39119562817931003, gradient=0.012224570593021365\n",
      "Gradient Descent(94/99): loss=0.3911960292147752, gradient=0.012881560756129673\n",
      "Gradient Descent(95/99): loss=0.3911989902197269, gradient=0.013558489431841006\n",
      "Gradient Descent(96/99): loss=0.39119572262095714, gradient=0.014675057130852367\n",
      "Gradient Descent(97/99): loss=0.3911954581902536, gradient=0.014043039671290227\n",
      "Gradient Descent(98/99): loss=0.39120057780777334, gradient=0.010877239229513667\n",
      "Gradient Descent(99/99): loss=0.39119651437833725, gradient=0.014742026359787223\n",
      "Gradient Descent(0/99): loss=0.3908622101258695, gradient=0.02015626989189477\n",
      "Gradient Descent(1/99): loss=0.3908537633004448, gradient=0.01883212121740182\n",
      "Gradient Descent(2/99): loss=0.39085530422591525, gradient=0.014604157824331714\n",
      "Gradient Descent(3/99): loss=0.39084973515435206, gradient=0.015425379226615698\n",
      "Gradient Descent(4/99): loss=0.3908503115305339, gradient=0.012605136132933752\n",
      "Gradient Descent(5/99): loss=0.3908527558275702, gradient=0.010276658877484992\n",
      "Gradient Descent(6/99): loss=0.3908539451315374, gradient=0.013260637044436271\n",
      "Gradient Descent(7/99): loss=0.3908462898618329, gradient=0.01742239735265589\n",
      "Gradient Descent(8/99): loss=0.39084656283902025, gradient=0.015368300249970003\n",
      "Gradient Descent(9/99): loss=0.3908465774046802, gradient=0.012269881380411022\n",
      "Gradient Descent(10/99): loss=0.3908471712816517, gradient=0.013866177481864939\n",
      "Gradient Descent(11/99): loss=0.39084645734198925, gradient=0.014031744574946534\n",
      "Gradient Descent(12/99): loss=0.3908529756346706, gradient=0.01042328009255139\n",
      "Gradient Descent(13/99): loss=0.3908531730004315, gradient=0.011103383725778302\n",
      "Gradient Descent(14/99): loss=0.39085080712110315, gradient=0.014981173233455854\n",
      "Gradient Descent(15/99): loss=0.3908535776295905, gradient=0.012683096366027612\n",
      "Gradient Descent(16/99): loss=0.3908559671653639, gradient=0.008723398369682252\n",
      "Gradient Descent(17/99): loss=0.3908558134241051, gradient=0.012610513724474248\n",
      "Gradient Descent(18/99): loss=0.39085776778096115, gradient=0.010794759075682293\n",
      "Gradient Descent(19/99): loss=0.3908560712377746, gradient=0.014120513756810688\n",
      "Gradient Descent(20/99): loss=0.39085675943874504, gradient=0.01094798497207857\n",
      "Gradient Descent(21/99): loss=0.3908548538264723, gradient=0.014580971067385982\n",
      "Gradient Descent(22/99): loss=0.39085593029859844, gradient=0.011243595428351082\n",
      "Gradient Descent(23/99): loss=0.3908567454999817, gradient=0.013066560597567275\n",
      "Gradient Descent(24/99): loss=0.39085528692386456, gradient=0.0144525754597717\n",
      "Gradient Descent(25/99): loss=0.39085932234657705, gradient=0.012442764650351977\n",
      "Gradient Descent(26/99): loss=0.3908618701363529, gradient=0.008884329305199199\n",
      "Gradient Descent(27/99): loss=0.39086210281840145, gradient=0.013204026128909451\n",
      "Gradient Descent(28/99): loss=0.39086344827728203, gradient=0.011681586369035392\n",
      "Gradient Descent(29/99): loss=0.3908637682854133, gradient=0.014944138251006526\n",
      "Gradient Descent(30/99): loss=0.3908639881551004, gradient=0.011006267036202961\n",
      "Gradient Descent(31/99): loss=0.3908663645100151, gradient=0.011587146230381316\n",
      "Gradient Descent(32/99): loss=0.3908675100074823, gradient=0.011653071973472493\n",
      "Gradient Descent(33/99): loss=0.3908689218360934, gradient=0.014304882256550638\n",
      "Gradient Descent(34/99): loss=0.39086773844281353, gradient=0.012349223091011443\n",
      "Gradient Descent(35/99): loss=0.3908630548976944, gradient=0.016530289444265003\n",
      "Gradient Descent(36/99): loss=0.39086479014159814, gradient=0.010882663275761833\n",
      "Gradient Descent(37/99): loss=0.390867883470881, gradient=0.011150853895660073\n",
      "Gradient Descent(38/99): loss=0.39087115087966556, gradient=0.008660422768089504\n",
      "Gradient Descent(39/99): loss=0.3908658273602464, gradient=0.016744316333196407\n",
      "Gradient Descent(40/99): loss=0.39087100307924205, gradient=0.008377482181570885\n",
      "Gradient Descent(41/99): loss=0.39087297954002975, gradient=0.011077307753749857\n",
      "Gradient Descent(42/99): loss=0.3908760205425181, gradient=0.008432600118027978\n",
      "Gradient Descent(43/99): loss=0.39087332122558144, gradient=0.014611524027641274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(44/99): loss=0.3908749403599667, gradient=0.011644867267159298\n",
      "Gradient Descent(45/99): loss=0.3908766114828325, gradient=0.0111308752127292\n",
      "Gradient Descent(46/99): loss=0.3908757720662661, gradient=0.013220842905542604\n",
      "Gradient Descent(47/99): loss=0.3908790804934711, gradient=0.012692958395133099\n",
      "Gradient Descent(48/99): loss=0.39087854900933755, gradient=0.0114046567801408\n",
      "Gradient Descent(49/99): loss=0.3908783459771335, gradient=0.013477570922911995\n",
      "Gradient Descent(50/99): loss=0.3908792293613283, gradient=0.01194332461869919\n",
      "Gradient Descent(51/99): loss=0.3908825869442582, gradient=0.01252986513128465\n",
      "Gradient Descent(52/99): loss=0.3908800619899647, gradient=0.013511082027318709\n",
      "Gradient Descent(53/99): loss=0.39088117484624474, gradient=0.013467499077000726\n",
      "Gradient Descent(54/99): loss=0.39087747847981913, gradient=0.015647642187220912\n",
      "Gradient Descent(55/99): loss=0.39087961066171006, gradient=0.014465933249168393\n",
      "Gradient Descent(56/99): loss=0.39087967869895035, gradient=0.011561765808857377\n",
      "Gradient Descent(57/99): loss=0.3908850921122059, gradient=0.008672972822160311\n",
      "Gradient Descent(58/99): loss=0.39088629888033133, gradient=0.0115180967752618\n",
      "Gradient Descent(59/99): loss=0.39088412197319516, gradient=0.016481394468476716\n",
      "Gradient Descent(60/99): loss=0.3908795432924404, gradient=0.01584061610948785\n",
      "Gradient Descent(61/99): loss=0.390883876689265, gradient=0.011389704929636979\n",
      "Gradient Descent(62/99): loss=0.3908863492185424, gradient=0.010363713545096277\n",
      "Gradient Descent(63/99): loss=0.39088852081015885, gradient=0.011166147945911517\n",
      "Gradient Descent(64/99): loss=0.3908854418239578, gradient=0.014105167255462766\n",
      "Gradient Descent(65/99): loss=0.3908858932615048, gradient=0.012468979627405466\n",
      "Gradient Descent(66/99): loss=0.39089062266545926, gradient=0.0082421375414942\n",
      "Gradient Descent(67/99): loss=0.39088970612495677, gradient=0.013257007297029388\n",
      "Gradient Descent(68/99): loss=0.3908908034620608, gradient=0.011673228321622242\n",
      "Gradient Descent(69/99): loss=0.3908912757320746, gradient=0.014600510914487561\n",
      "Gradient Descent(70/99): loss=0.39089328525665745, gradient=0.00884717340521737\n",
      "Gradient Descent(71/99): loss=0.3908926530509238, gradient=0.013500543078909922\n",
      "Gradient Descent(72/99): loss=0.39089146318207185, gradient=0.01343864253949295\n",
      "Gradient Descent(73/99): loss=0.39089383115599596, gradient=0.014766671305785082\n",
      "Gradient Descent(74/99): loss=0.3908908674718797, gradient=0.013103117918088244\n",
      "Gradient Descent(75/99): loss=0.39088933968583645, gradient=0.015516218880501063\n",
      "Gradient Descent(76/99): loss=0.3908906747524189, gradient=0.012146222276782921\n",
      "Gradient Descent(77/99): loss=0.39089713923950525, gradient=0.009993281955111859\n",
      "Gradient Descent(78/99): loss=0.3908970750406663, gradient=0.010890432313937623\n",
      "Gradient Descent(79/99): loss=0.390893566544665, gradient=0.01582851371947166\n",
      "Gradient Descent(80/99): loss=0.39089458674717037, gradient=0.012001559237305549\n",
      "Gradient Descent(81/99): loss=0.39089860341964533, gradient=0.011891310172805558\n",
      "Gradient Descent(82/99): loss=0.3909005673639041, gradient=0.008791261668705596\n",
      "Gradient Descent(83/99): loss=0.39089718740615625, gradient=0.014939582486156372\n",
      "Gradient Descent(84/99): loss=0.39089849672611526, gradient=0.011655396099226759\n",
      "Gradient Descent(85/99): loss=0.3909004195388274, gradient=0.010664272110334553\n",
      "Gradient Descent(86/99): loss=0.39090304838984835, gradient=0.008408274882621618\n",
      "Gradient Descent(87/99): loss=0.3908994195812972, gradient=0.014989898530318839\n",
      "Gradient Descent(88/99): loss=0.39090183417085395, gradient=0.010335784951206703\n",
      "Gradient Descent(89/99): loss=0.3909027504195159, gradient=0.01183169980868213\n",
      "Gradient Descent(90/99): loss=0.3909026644048057, gradient=0.011162731575154312\n",
      "Gradient Descent(91/99): loss=0.3908955219367952, gradient=0.01770663871282128\n",
      "Gradient Descent(92/99): loss=0.39089918580487854, gradient=0.00886141227463047\n",
      "Gradient Descent(93/99): loss=0.390904535471791, gradient=0.008259508855198506\n",
      "Gradient Descent(94/99): loss=0.390898080371454, gradient=0.017509972660052418\n",
      "Gradient Descent(95/99): loss=0.39090153065516897, gradient=0.01302316658688712\n",
      "Gradient Descent(96/99): loss=0.3909037613231358, gradient=0.008992554284486615\n",
      "Gradient Descent(97/99): loss=0.39090655026875193, gradient=0.01063151433193512\n",
      "Gradient Descent(98/99): loss=0.39090486691899756, gradient=0.013643235300251886\n",
      "Gradient Descent(99/99): loss=0.3909048016565309, gradient=0.015193020179225615\n",
      "Gradient Descent(0/99): loss=0.3903155912586765, gradient=0.03442604956676652\n",
      "Gradient Descent(1/99): loss=0.3903410774633874, gradient=0.041748363512571\n",
      "Gradient Descent(2/99): loss=0.390390780819311, gradient=0.03686656630150645\n",
      "Gradient Descent(3/99): loss=0.3904220583883363, gradient=0.03598218599378689\n",
      "Gradient Descent(4/99): loss=0.390474347543911, gradient=0.03211151416865695\n",
      "Gradient Descent(5/99): loss=0.3904982479985293, gradient=0.038851609153345644\n",
      "Gradient Descent(6/99): loss=0.3905377902025503, gradient=0.03299593088660415\n",
      "Gradient Descent(7/99): loss=0.3905695533127821, gradient=0.035540066764362215\n",
      "Gradient Descent(8/99): loss=0.3906168049571453, gradient=0.031804906718514485\n",
      "Gradient Descent(9/99): loss=0.3906405502943348, gradient=0.03543449503802536\n",
      "Gradient Descent(10/99): loss=0.39065891394308105, gradient=0.03655035829015102\n",
      "Gradient Descent(11/99): loss=0.3907164307850714, gradient=0.027298871657468055\n",
      "Gradient Descent(12/99): loss=0.3907455515660514, gradient=0.03151622095622781\n",
      "Gradient Descent(13/99): loss=0.39075810671380706, gradient=0.03873104406331068\n",
      "Gradient Descent(14/99): loss=0.3908148080973125, gradient=0.027987250043317098\n",
      "Gradient Descent(15/99): loss=0.3908085732392032, gradient=0.04145161277323482\n",
      "Gradient Descent(16/99): loss=0.3908529218812286, gradient=0.029790396873599242\n",
      "Gradient Descent(17/99): loss=0.39087714050029804, gradient=0.03540467527527273\n",
      "Gradient Descent(18/99): loss=0.39090023771252724, gradient=0.03029016963275164\n",
      "Gradient Descent(19/99): loss=0.3909364226010584, gradient=0.02837887041244772\n",
      "Gradient Descent(20/99): loss=0.39098389069940903, gradient=0.025839554351558828\n",
      "Gradient Descent(21/99): loss=0.3910023814988919, gradient=0.03204978530683572\n",
      "Gradient Descent(22/99): loss=0.3910266826398757, gradient=0.030674670745507202\n",
      "Gradient Descent(23/99): loss=0.39106353319059, gradient=0.02842446602904845\n",
      "Gradient Descent(24/99): loss=0.39107613734764934, gradient=0.032656767131526654\n",
      "Gradient Descent(25/99): loss=0.39109383927652736, gradient=0.03360239282238986\n",
      "Gradient Descent(26/99): loss=0.39109537079279677, gradient=0.03759762986772747\n",
      "Gradient Descent(27/99): loss=0.39114074530202286, gradient=0.02800888313617004\n",
      "Gradient Descent(28/99): loss=0.39113434438072425, gradient=0.04150491404466528\n",
      "Gradient Descent(29/99): loss=0.3911713647868884, gradient=0.03136007615907025\n",
      "Gradient Descent(30/99): loss=0.39121286444824954, gradient=0.025345250192334638\n",
      "Gradient Descent(31/99): loss=0.39124219160415397, gradient=0.02753837792668134\n",
      "Gradient Descent(32/99): loss=0.39126072317099264, gradient=0.030824702741823414\n",
      "Gradient Descent(33/99): loss=0.39128771417595143, gradient=0.030448480814489455\n",
      "Gradient Descent(34/99): loss=0.39131489263063163, gradient=0.02598197114318221\n",
      "Gradient Descent(35/99): loss=0.39133026506036445, gradient=0.030890019725089698\n",
      "Gradient Descent(36/99): loss=0.3913523869801195, gradient=0.0324163077946034\n",
      "Gradient Descent(37/99): loss=0.3913731206625249, gradient=0.02695335458730662\n",
      "Gradient Descent(38/99): loss=0.3913941528946586, gradient=0.029977070639696433\n",
      "Gradient Descent(39/99): loss=0.3914118251352743, gradient=0.02973410639765019\n",
      "Gradient Descent(40/99): loss=0.39145190501747823, gradient=0.023715233442320165\n",
      "Gradient Descent(41/99): loss=0.39144210076255465, gradient=0.03707182556777408\n",
      "Gradient Descent(42/99): loss=0.39146796146173213, gradient=0.02871618888858996\n",
      "Gradient Descent(43/99): loss=0.39147397597410216, gradient=0.03531245513643256\n",
      "Gradient Descent(44/99): loss=0.3915003291164791, gradient=0.03041784004730261\n",
      "Gradient Descent(45/99): loss=0.39150379754255915, gradient=0.03645899162982289\n",
      "Gradient Descent(46/99): loss=0.39154401781553777, gradient=0.021994184819828882\n",
      "Gradient Descent(47/99): loss=0.3915641253992638, gradient=0.02905524261176347\n",
      "Gradient Descent(48/99): loss=0.39157561369776284, gradient=0.032341005094046894\n",
      "Gradient Descent(49/99): loss=0.39159726588069327, gradient=0.03296817740761829\n",
      "Gradient Descent(50/99): loss=0.3916198149415647, gradient=0.025479115831580905\n",
      "Gradient Descent(51/99): loss=0.3916584312076562, gradient=0.02090256347667066\n",
      "Gradient Descent(52/99): loss=0.391658659258425, gradient=0.03328168700659156\n",
      "Gradient Descent(53/99): loss=0.3916808762020547, gradient=0.026834842416462383\n",
      "Gradient Descent(54/99): loss=0.39171370772992875, gradient=0.021094759882275507\n",
      "Gradient Descent(55/99): loss=0.39172929309346466, gradient=0.027823578390650475\n",
      "Gradient Descent(56/99): loss=0.3917274928548154, gradient=0.03411493559829325\n",
      "Gradient Descent(57/99): loss=0.391718719010635, gradient=0.038870645865840006\n",
      "Gradient Descent(58/99): loss=0.39173071248380964, gradient=0.03229226462958187\n",
      "Gradient Descent(59/99): loss=0.39176597497501864, gradient=0.025388760374327527\n",
      "Gradient Descent(60/99): loss=0.39177156592188, gradient=0.034069861625992154\n",
      "Gradient Descent(61/99): loss=0.39179904816100264, gradient=0.02594765831001164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(62/99): loss=0.3918208483614189, gradient=0.028186493759361927\n",
      "Gradient Descent(63/99): loss=0.3918491250958401, gradient=0.02667003527070245\n",
      "Gradient Descent(64/99): loss=0.39187267968346307, gradient=0.024518452356387912\n",
      "Gradient Descent(65/99): loss=0.39187322488338433, gradient=0.03174096796260599\n",
      "Gradient Descent(66/99): loss=0.3918970161552483, gradient=0.02551013438809319\n",
      "Gradient Descent(67/99): loss=0.39192694957359336, gradient=0.025819297584815548\n",
      "Gradient Descent(68/99): loss=0.3919301086266136, gradient=0.029816768820425044\n",
      "Gradient Descent(69/99): loss=0.39193779429520526, gradient=0.03155405162171357\n",
      "Gradient Descent(70/99): loss=0.3919527537371119, gradient=0.02854637859178078\n",
      "Gradient Descent(71/99): loss=0.39196935657083176, gradient=0.0321781148382349\n",
      "Gradient Descent(72/99): loss=0.3919641135063177, gradient=0.036904173428324326\n",
      "Gradient Descent(73/99): loss=0.3919943804436136, gradient=0.030085719116347248\n",
      "Gradient Descent(74/99): loss=0.3919997378599916, gradient=0.0336793226499461\n",
      "Gradient Descent(75/99): loss=0.39202097533004354, gradient=0.03480255274122966\n",
      "Gradient Descent(76/99): loss=0.39201091143713657, gradient=0.04198918707689687\n",
      "Gradient Descent(77/99): loss=0.39204204206591997, gradient=0.033064445391673095\n",
      "Gradient Descent(78/99): loss=0.39205879927563203, gradient=0.025943320685788092\n",
      "Gradient Descent(79/99): loss=0.3920777926746095, gradient=0.027187145548027872\n",
      "Gradient Descent(80/99): loss=0.3920928734476058, gradient=0.027487898494304042\n",
      "Gradient Descent(81/99): loss=0.39211025164798124, gradient=0.02929307793667419\n",
      "Gradient Descent(82/99): loss=0.39211647637442054, gradient=0.03017149807121673\n",
      "Gradient Descent(83/99): loss=0.39213085357827776, gradient=0.03383947618835387\n",
      "Gradient Descent(84/99): loss=0.39211440909554957, gradient=0.044206319004389594\n",
      "Gradient Descent(85/99): loss=0.39213515522110504, gradient=0.0368108732328019\n",
      "Gradient Descent(86/99): loss=0.3921616749481246, gradient=0.021605187792064984\n",
      "Gradient Descent(87/99): loss=0.39217240165499845, gradient=0.02947699620973705\n",
      "Gradient Descent(88/99): loss=0.3921911358277511, gradient=0.026834733560259196\n",
      "Gradient Descent(89/99): loss=0.39219517080859195, gradient=0.03135253643403745\n",
      "Gradient Descent(90/99): loss=0.3922221400608184, gradient=0.02592793220574537\n",
      "Gradient Descent(91/99): loss=0.3922233315096428, gradient=0.031213136134252053\n",
      "Gradient Descent(92/99): loss=0.39222951121700067, gradient=0.030392486897658638\n",
      "Gradient Descent(93/99): loss=0.392254026505855, gradient=0.02621367374300987\n",
      "Gradient Descent(94/99): loss=0.39226255526002746, gradient=0.03095298506167603\n",
      "Gradient Descent(95/99): loss=0.39227390292590714, gradient=0.035969220503607686\n",
      "Gradient Descent(96/99): loss=0.39226734230459925, gradient=0.036801997568804305\n",
      "Gradient Descent(97/99): loss=0.3922810588577519, gradient=0.03731104148554838\n",
      "Gradient Descent(98/99): loss=0.3922821912029151, gradient=0.03298814959892096\n",
      "Gradient Descent(99/99): loss=0.3923121157210233, gradient=0.02736704365180682\n",
      "Gradient Descent(0/99): loss=0.3933557651501934, gradient=0.038978392489094384\n",
      "Gradient Descent(1/99): loss=0.39338040646228756, gradient=0.03216486143947795\n",
      "Gradient Descent(2/99): loss=0.3933897207587122, gradient=0.03040371770166627\n",
      "Gradient Descent(3/99): loss=0.3934019178035367, gradient=0.030982495824896643\n",
      "Gradient Descent(4/99): loss=0.39341743726317185, gradient=0.02706726902653265\n",
      "Gradient Descent(5/99): loss=0.3934414594277645, gradient=0.02627424087005485\n",
      "Gradient Descent(6/99): loss=0.39343436569913476, gradient=0.03725294779390151\n",
      "Gradient Descent(7/99): loss=0.3934643047623783, gradient=0.02904749487980288\n",
      "Gradient Descent(8/99): loss=0.39346798840921526, gradient=0.031288670325541675\n",
      "Gradient Descent(9/99): loss=0.393490027949956, gradient=0.029819590028848816\n",
      "Gradient Descent(10/99): loss=0.3934808562221273, gradient=0.04066464697688381\n",
      "Gradient Descent(11/99): loss=0.3934891033180678, gradient=0.042076256685755084\n",
      "Gradient Descent(12/99): loss=0.39350510601964644, gradient=0.024979390777591592\n",
      "Gradient Descent(13/99): loss=0.3935149324245627, gradient=0.030477489826442397\n",
      "Gradient Descent(14/99): loss=0.39351400907449324, gradient=0.033817671217724055\n",
      "Gradient Descent(15/99): loss=0.3935456293022048, gradient=0.024665643449529758\n",
      "Gradient Descent(16/99): loss=0.39354594863997006, gradient=0.03172582135120361\n",
      "Gradient Descent(17/99): loss=0.39355440092105154, gradient=0.03161174915372794\n",
      "Gradient Descent(18/99): loss=0.3935825644036085, gradient=0.024730862252144912\n",
      "Gradient Descent(19/99): loss=0.39360169582972687, gradient=0.02401334819146669\n",
      "Gradient Descent(20/99): loss=0.39358867398644226, gradient=0.03550763803270621\n",
      "Gradient Descent(21/99): loss=0.393594698709719, gradient=0.03240653628713532\n",
      "Gradient Descent(22/99): loss=0.39359133616827224, gradient=0.03741447916631637\n",
      "Gradient Descent(23/99): loss=0.3936168636540383, gradient=0.02402780738596562\n",
      "Gradient Descent(24/99): loss=0.3936307674468936, gradient=0.028582657983940708\n",
      "Gradient Descent(25/99): loss=0.39363498022913107, gradient=0.03126893160283315\n",
      "Gradient Descent(26/99): loss=0.39366715007725006, gradient=0.023057613965355817\n",
      "Gradient Descent(27/99): loss=0.3936371715818332, gradient=0.04086597963971411\n",
      "Gradient Descent(28/99): loss=0.3936685256865256, gradient=0.025818068661761895\n",
      "Gradient Descent(29/99): loss=0.393685490184891, gradient=0.026780721026972484\n",
      "Gradient Descent(30/99): loss=0.39369312891647, gradient=0.03319604925554374\n",
      "Gradient Descent(31/99): loss=0.3936940210536344, gradient=0.031152573490255762\n",
      "Gradient Descent(32/99): loss=0.3937241528581258, gradient=0.024516244250239388\n",
      "Gradient Descent(33/99): loss=0.3937111482716614, gradient=0.03723752692629645\n",
      "Gradient Descent(34/99): loss=0.3937388202076739, gradient=0.030414081655208546\n",
      "Gradient Descent(35/99): loss=0.39372844045221583, gradient=0.0343033445346593\n",
      "Gradient Descent(36/99): loss=0.39374709287513127, gradient=0.03090126763330925\n",
      "Gradient Descent(37/99): loss=0.39372092580622503, gradient=0.03965051688020959\n",
      "Gradient Descent(38/99): loss=0.3937519566726236, gradient=0.026198005998954412\n",
      "Gradient Descent(39/99): loss=0.3937621065458288, gradient=0.028927848483751996\n",
      "Gradient Descent(40/99): loss=0.3937778800388438, gradient=0.03131655956579052\n",
      "Gradient Descent(41/99): loss=0.39378180964420473, gradient=0.030753605431879057\n",
      "Gradient Descent(42/99): loss=0.39380964834171095, gradient=0.024607195108048468\n",
      "Gradient Descent(43/99): loss=0.39381819844829113, gradient=0.02739198558021552\n",
      "Gradient Descent(44/99): loss=0.39379612567689265, gradient=0.03831507982935094\n",
      "Gradient Descent(45/99): loss=0.39381940872070315, gradient=0.02661931753961039\n",
      "Gradient Descent(46/99): loss=0.39383594679954737, gradient=0.027736853619126013\n",
      "Gradient Descent(47/99): loss=0.3938475491262608, gradient=0.02682967776586558\n",
      "Gradient Descent(48/99): loss=0.39385709517567313, gradient=0.027653032848536242\n",
      "Gradient Descent(49/99): loss=0.3938761852208472, gradient=0.026451303077421204\n",
      "Gradient Descent(50/99): loss=0.39386993694849187, gradient=0.03217966280198258\n",
      "Gradient Descent(51/99): loss=0.3938754955316979, gradient=0.03514375477507782\n",
      "Gradient Descent(52/99): loss=0.39385193442005056, gradient=0.04498099251426467\n",
      "Gradient Descent(53/99): loss=0.3938788060121056, gradient=0.03348523686688591\n",
      "Gradient Descent(54/99): loss=0.3938833616993465, gradient=0.02815831018044709\n",
      "Gradient Descent(55/99): loss=0.3939043545767981, gradient=0.022450931182303444\n",
      "Gradient Descent(56/99): loss=0.39391344104869913, gradient=0.028749018776906016\n",
      "Gradient Descent(57/99): loss=0.3939247252695899, gradient=0.03039568590471198\n",
      "Gradient Descent(58/99): loss=0.3939117320134479, gradient=0.03715591483173963\n",
      "Gradient Descent(59/99): loss=0.39392999032447223, gradient=0.03218062253881286\n",
      "Gradient Descent(60/99): loss=0.3939410246383137, gradient=0.027606662711471157\n",
      "Gradient Descent(61/99): loss=0.3939597502927474, gradient=0.028357211802288593\n",
      "Gradient Descent(62/99): loss=0.3939519910709021, gradient=0.032616090103925605\n",
      "Gradient Descent(63/99): loss=0.3939739438257757, gradient=0.026503199165731295\n",
      "Gradient Descent(64/99): loss=0.39395636882611756, gradient=0.036039307705835764\n",
      "Gradient Descent(65/99): loss=0.393981715126419, gradient=0.025379406129497455\n",
      "Gradient Descent(66/99): loss=0.39399118941068634, gradient=0.02666495022241272\n",
      "Gradient Descent(67/99): loss=0.3940015797770739, gradient=0.027115979717048662\n",
      "Gradient Descent(68/99): loss=0.39397384345961173, gradient=0.03883492593425703\n",
      "Gradient Descent(69/99): loss=0.39399513324479507, gradient=0.028181022155417353\n",
      "Gradient Descent(70/99): loss=0.3940119178124975, gradient=0.024181256515626557\n",
      "Gradient Descent(71/99): loss=0.3940058456664147, gradient=0.03278603727241171\n",
      "Gradient Descent(72/99): loss=0.39401521424002867, gradient=0.02921037108922042\n",
      "Gradient Descent(73/99): loss=0.39401730532712353, gradient=0.03144446029052122\n",
      "Gradient Descent(74/99): loss=0.39403955251592004, gradient=0.026477752290045533\n",
      "Gradient Descent(75/99): loss=0.39403493796152117, gradient=0.03193548261505276\n",
      "Gradient Descent(76/99): loss=0.3940431784661785, gradient=0.030719158973959076\n",
      "Gradient Descent(77/99): loss=0.3940633310087512, gradient=0.026083886302334713\n",
      "Gradient Descent(78/99): loss=0.39405311511696, gradient=0.033332516013675054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(79/99): loss=0.39407442921535263, gradient=0.026595869063869577\n",
      "Gradient Descent(80/99): loss=0.39408595721625483, gradient=0.02628159276627858\n",
      "Gradient Descent(81/99): loss=0.3940950014062755, gradient=0.030436348886759626\n",
      "Gradient Descent(82/99): loss=0.39409867729257225, gradient=0.02594781498663609\n",
      "Gradient Descent(83/99): loss=0.3941115711353232, gradient=0.025297433656307627\n",
      "Gradient Descent(84/99): loss=0.3940945103324086, gradient=0.03468942432661924\n",
      "Gradient Descent(85/99): loss=0.39410478829044504, gradient=0.025225487934846455\n",
      "Gradient Descent(86/99): loss=0.3941180828015278, gradient=0.025699558305849683\n",
      "Gradient Descent(87/99): loss=0.39413111810218937, gradient=0.025087327354550795\n",
      "Gradient Descent(88/99): loss=0.3941212477851931, gradient=0.03510327701841532\n",
      "Gradient Descent(89/99): loss=0.394136432533364, gradient=0.031025455081850263\n",
      "Gradient Descent(90/99): loss=0.39412005316132515, gradient=0.036974730718444954\n",
      "Gradient Descent(91/99): loss=0.39414188956351076, gradient=0.031983393990103186\n",
      "Gradient Descent(92/99): loss=0.3941254841195013, gradient=0.03809994088365863\n",
      "Gradient Descent(93/99): loss=0.39415340445625585, gradient=0.0276580441235515\n",
      "Gradient Descent(94/99): loss=0.39414083511366704, gradient=0.033240478255614506\n",
      "Gradient Descent(95/99): loss=0.39414637111353873, gradient=0.030517816785809854\n",
      "Gradient Descent(96/99): loss=0.39417324685818983, gradient=0.022462078297258686\n",
      "Gradient Descent(97/99): loss=0.3941866736616651, gradient=0.023783470408224056\n",
      "Gradient Descent(98/99): loss=0.3941872567437394, gradient=0.029548330241937468\n",
      "Gradient Descent(99/99): loss=0.3941874027611589, gradient=0.02941630501060527\n",
      "Gradient Descent(0/99): loss=0.39467736808821086, gradient=0.026439520058609407\n",
      "Gradient Descent(1/99): loss=0.3946583137408978, gradient=0.03468469131793814\n",
      "Gradient Descent(2/99): loss=0.3946751285164433, gradient=0.02461162403314906\n",
      "Gradient Descent(3/99): loss=0.39467965211228684, gradient=0.028575697636429816\n",
      "Gradient Descent(4/99): loss=0.39466730940905276, gradient=0.03510455710314863\n",
      "Gradient Descent(5/99): loss=0.39466596400732384, gradient=0.03100215011986106\n",
      "Gradient Descent(6/99): loss=0.3946832599255232, gradient=0.026942742581562627\n",
      "Gradient Descent(7/99): loss=0.3946619629732242, gradient=0.0364939302573766\n",
      "Gradient Descent(8/99): loss=0.39468474389296165, gradient=0.025263751697188766\n",
      "Gradient Descent(9/99): loss=0.39469238627213, gradient=0.026120911527563486\n",
      "Gradient Descent(10/99): loss=0.3947008034651932, gradient=0.02647679056879674\n",
      "Gradient Descent(11/99): loss=0.39466417149083416, gradient=0.04028906874520076\n",
      "Gradient Descent(12/99): loss=0.39467983459384426, gradient=0.030976738031922166\n",
      "Gradient Descent(13/99): loss=0.39468750939922204, gradient=0.025494768973790657\n",
      "Gradient Descent(14/99): loss=0.39470044323065584, gradient=0.02609713637215991\n",
      "Gradient Descent(15/99): loss=0.3946966627687808, gradient=0.03046122740936439\n",
      "Gradient Descent(16/99): loss=0.3947118093595441, gradient=0.02382350102025789\n",
      "Gradient Descent(17/99): loss=0.39471915452781375, gradient=0.02592823195922706\n",
      "Gradient Descent(18/99): loss=0.39472502891148625, gradient=0.02656358247099535\n",
      "Gradient Descent(19/99): loss=0.39470480309963346, gradient=0.037210480258482705\n",
      "Gradient Descent(20/99): loss=0.39472613082205105, gradient=0.027587211435629814\n",
      "Gradient Descent(21/99): loss=0.39469527823557726, gradient=0.03885300923214577\n",
      "Gradient Descent(22/99): loss=0.39471426247649277, gradient=0.027724422514542116\n",
      "Gradient Descent(23/99): loss=0.3947184749025834, gradient=0.028921554390066915\n",
      "Gradient Descent(24/99): loss=0.3947284708040958, gradient=0.029971872739064757\n",
      "Gradient Descent(25/99): loss=0.39473255184190464, gradient=0.025367317539162697\n",
      "Gradient Descent(26/99): loss=0.3947466257197237, gradient=0.023792580889625686\n",
      "Gradient Descent(27/99): loss=0.3947356497830573, gradient=0.03172491933940552\n",
      "Gradient Descent(28/99): loss=0.3947484941711344, gradient=0.023888654751549422\n",
      "Gradient Descent(29/99): loss=0.39475488054565644, gradient=0.02560025722206459\n",
      "Gradient Descent(30/99): loss=0.3947588973374106, gradient=0.02673255789254278\n",
      "Gradient Descent(31/99): loss=0.3947556826182734, gradient=0.029412192547995715\n",
      "Gradient Descent(32/99): loss=0.3947509547650326, gradient=0.03407713284934274\n",
      "Gradient Descent(33/99): loss=0.39474781240543616, gradient=0.02680732311079671\n",
      "Gradient Descent(34/99): loss=0.3947532915127546, gradient=0.026103247109225687\n",
      "Gradient Descent(35/99): loss=0.3947581320727311, gradient=0.027970166162203262\n",
      "Gradient Descent(36/99): loss=0.3947683526199292, gradient=0.028361400423178983\n",
      "Gradient Descent(37/99): loss=0.3947503822865753, gradient=0.03404094730030832\n",
      "Gradient Descent(38/99): loss=0.3947700933160229, gradient=0.025190982919075955\n",
      "Gradient Descent(39/99): loss=0.39476361275679106, gradient=0.030104065341662548\n",
      "Gradient Descent(40/99): loss=0.3947721869716824, gradient=0.02625295678841456\n",
      "Gradient Descent(41/99): loss=0.3947831938096345, gradient=0.023967319422032084\n",
      "Gradient Descent(42/99): loss=0.39478333748078753, gradient=0.028476916603840753\n",
      "Gradient Descent(43/99): loss=0.394779891348462, gradient=0.029618107633937896\n",
      "Gradient Descent(44/99): loss=0.3947959971662199, gradient=0.02474333561122946\n",
      "Gradient Descent(45/99): loss=0.3947722008389092, gradient=0.034955444478031096\n",
      "Gradient Descent(46/99): loss=0.3947871115689823, gradient=0.024008977808883048\n",
      "Gradient Descent(47/99): loss=0.39476183916799346, gradient=0.040237571639395675\n",
      "Gradient Descent(48/99): loss=0.39478100714327374, gradient=0.03500224705713418\n",
      "Gradient Descent(49/99): loss=0.3947669815860284, gradient=0.03365654036529013\n",
      "Gradient Descent(50/99): loss=0.3947842559262688, gradient=0.027672058427129526\n",
      "Gradient Descent(51/99): loss=0.39478046133193634, gradient=0.030223932861951557\n",
      "Gradient Descent(52/99): loss=0.39478981179795847, gradient=0.02704853330496449\n",
      "Gradient Descent(53/99): loss=0.3947862426417956, gradient=0.030779731820629895\n",
      "Gradient Descent(54/99): loss=0.3947763653713671, gradient=0.03141109276348528\n",
      "Gradient Descent(55/99): loss=0.3947892862191145, gradient=0.026125993101154106\n",
      "Gradient Descent(56/99): loss=0.39480074158844464, gradient=0.02477497546278643\n",
      "Gradient Descent(57/99): loss=0.39480307112266544, gradient=0.02849813284573334\n",
      "Gradient Descent(58/99): loss=0.3948001774433433, gradient=0.03006344811757232\n",
      "Gradient Descent(59/99): loss=0.3948175592878406, gradient=0.025281908291844228\n",
      "Gradient Descent(60/99): loss=0.3948139147097974, gradient=0.02911209602832239\n",
      "Gradient Descent(61/99): loss=0.3948189785099412, gradient=0.02498176710675512\n",
      "Gradient Descent(62/99): loss=0.3948179802678442, gradient=0.029687457118026412\n",
      "Gradient Descent(63/99): loss=0.3948259745714878, gradient=0.02916891899878118\n",
      "Gradient Descent(64/99): loss=0.3948105804919868, gradient=0.03311481413724825\n",
      "Gradient Descent(65/99): loss=0.39482459598658876, gradient=0.027417281512975452\n",
      "Gradient Descent(66/99): loss=0.394816334171517, gradient=0.03064116744111662\n",
      "Gradient Descent(67/99): loss=0.39482979836149396, gradient=0.024281105855300748\n",
      "Gradient Descent(68/99): loss=0.3948173562039189, gradient=0.03310551913196317\n",
      "Gradient Descent(69/99): loss=0.39483064708249954, gradient=0.02878867507890444\n",
      "Gradient Descent(70/99): loss=0.3948085977957746, gradient=0.038387054830496856\n",
      "Gradient Descent(71/99): loss=0.39482969267721885, gradient=0.02852696336953934\n",
      "Gradient Descent(72/99): loss=0.3948374141104687, gradient=0.024508664228051247\n",
      "Gradient Descent(73/99): loss=0.3948270844307212, gradient=0.0320387473884695\n",
      "Gradient Descent(74/99): loss=0.3948225379143909, gradient=0.031008631618016588\n",
      "Gradient Descent(75/99): loss=0.39480874444253145, gradient=0.033844543288425\n",
      "Gradient Descent(76/99): loss=0.3948247921064204, gradient=0.029361885316840787\n",
      "Gradient Descent(77/99): loss=0.3948093531817277, gradient=0.037067665825392185\n",
      "Gradient Descent(78/99): loss=0.39482379107598453, gradient=0.034029040936648394\n",
      "Gradient Descent(79/99): loss=0.3948266388276358, gradient=0.026224075879726142\n",
      "Gradient Descent(80/99): loss=0.39483761376495896, gradient=0.026446747777557802\n",
      "Gradient Descent(81/99): loss=0.39483188194911545, gradient=0.03075931477166282\n",
      "Gradient Descent(82/99): loss=0.39484489816473617, gradient=0.024392953110563446\n",
      "Gradient Descent(83/99): loss=0.3948508288825943, gradient=0.026257625084131\n",
      "Gradient Descent(84/99): loss=0.39485481283394336, gradient=0.027098362370960714\n",
      "Gradient Descent(85/99): loss=0.3948471633747344, gradient=0.031141240363455825\n",
      "Gradient Descent(86/99): loss=0.39486233236649687, gradient=0.025505981481604382\n",
      "Gradient Descent(87/99): loss=0.39486931162965067, gradient=0.024354978334598175\n",
      "Gradient Descent(88/99): loss=0.3948667841057663, gradient=0.02922928901590313\n",
      "Gradient Descent(89/99): loss=0.3948620169740359, gradient=0.02979802684401719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(90/99): loss=0.3948672504154011, gradient=0.030841105237973107\n",
      "Gradient Descent(91/99): loss=0.39485120632311477, gradient=0.03337349018207371\n",
      "Gradient Descent(92/99): loss=0.39485379664915754, gradient=0.032959917963019815\n",
      "Gradient Descent(93/99): loss=0.394833280057089, gradient=0.03815387702167371\n",
      "Gradient Descent(94/99): loss=0.3948513613075725, gradient=0.032560216286642206\n",
      "Gradient Descent(95/99): loss=0.3948388200113547, gradient=0.033455770339629286\n",
      "Gradient Descent(96/99): loss=0.39485510144909464, gradient=0.028114597991453986\n",
      "Gradient Descent(97/99): loss=0.39484957320945574, gradient=0.030671884396999628\n",
      "Gradient Descent(98/99): loss=0.3948595974452795, gradient=0.026518016576323442\n",
      "Gradient Descent(99/99): loss=0.3948706682619647, gradient=0.02470165966585822\n",
      "Gradient Descent(0/99): loss=0.39444856603080414, gradient=0.034417580719077155\n",
      "Gradient Descent(1/99): loss=0.39445250107475394, gradient=0.024594365542097242\n",
      "Gradient Descent(2/99): loss=0.3944370444987854, gradient=0.03154019264828501\n",
      "Gradient Descent(3/99): loss=0.3944445910491519, gradient=0.02848190705939313\n",
      "Gradient Descent(4/99): loss=0.39443773861481796, gradient=0.029732817732945963\n",
      "Gradient Descent(5/99): loss=0.3944419204013231, gradient=0.029202400337531492\n",
      "Gradient Descent(6/99): loss=0.3944483522090515, gradient=0.024006345650348183\n",
      "Gradient Descent(7/99): loss=0.3944463574145735, gradient=0.02657095794958669\n",
      "Gradient Descent(8/99): loss=0.3944439168101646, gradient=0.029242447497518708\n",
      "Gradient Descent(9/99): loss=0.3944558686690281, gradient=0.02615507295824526\n",
      "Gradient Descent(10/99): loss=0.3944317982024603, gradient=0.03480814204070551\n",
      "Gradient Descent(11/99): loss=0.3944483319336419, gradient=0.025294981550700178\n",
      "Gradient Descent(12/99): loss=0.3944367626568712, gradient=0.030851398428535903\n",
      "Gradient Descent(13/99): loss=0.39444962949250434, gradient=0.02339732980207951\n",
      "Gradient Descent(14/99): loss=0.3944596917354737, gradient=0.023550533271054105\n",
      "Gradient Descent(15/99): loss=0.3944587872352503, gradient=0.0276799361428728\n",
      "Gradient Descent(16/99): loss=0.39445502826914886, gradient=0.028814528199695343\n",
      "Gradient Descent(17/99): loss=0.394466337835916, gradient=0.02579450717728077\n",
      "Gradient Descent(18/99): loss=0.3944355763872544, gradient=0.03860465711697879\n",
      "Gradient Descent(19/99): loss=0.394441190137004, gradient=0.032126978834515076\n",
      "Gradient Descent(20/99): loss=0.39443581428087365, gradient=0.03110684419237566\n",
      "Gradient Descent(21/99): loss=0.3944507955490133, gradient=0.026865593178433317\n",
      "Gradient Descent(22/99): loss=0.39444364019827416, gradient=0.03001601662016824\n",
      "Gradient Descent(23/99): loss=0.39445513584101716, gradient=0.024521228395369948\n",
      "Gradient Descent(24/99): loss=0.3944644707657748, gradient=0.02387516834675038\n",
      "Gradient Descent(25/99): loss=0.3944451421956837, gradient=0.03396404869752017\n",
      "Gradient Descent(26/99): loss=0.3944356906922637, gradient=0.03292997707355062\n",
      "Gradient Descent(27/99): loss=0.3944491842065238, gradient=0.024186442452466907\n",
      "Gradient Descent(28/99): loss=0.3944586060038554, gradient=0.024833781768695617\n",
      "Gradient Descent(29/99): loss=0.39445328349407593, gradient=0.029746631356456332\n",
      "Gradient Descent(30/99): loss=0.3944452765715679, gradient=0.031388651684713664\n",
      "Gradient Descent(31/99): loss=0.3944580907774924, gradient=0.026607937884403632\n",
      "Gradient Descent(32/99): loss=0.3944688944452133, gradient=0.023613274770405784\n",
      "Gradient Descent(33/99): loss=0.39446199410072846, gradient=0.030932980953912437\n",
      "Gradient Descent(34/99): loss=0.39447357386208864, gradient=0.02681590354947063\n",
      "Gradient Descent(35/99): loss=0.394476140753146, gradient=0.02614682320404997\n",
      "Gradient Descent(36/99): loss=0.3944633508685287, gradient=0.032628368193058414\n",
      "Gradient Descent(37/99): loss=0.39448323849125205, gradient=0.022474850680379063\n",
      "Gradient Descent(38/99): loss=0.3944864370996941, gradient=0.02467804973786439\n",
      "Gradient Descent(39/99): loss=0.3944844260002303, gradient=0.028387596683552823\n",
      "Gradient Descent(40/99): loss=0.39448054358912366, gradient=0.02909097387497618\n",
      "Gradient Descent(41/99): loss=0.3944957610255033, gradient=0.02431430414355025\n",
      "Gradient Descent(42/99): loss=0.39446800572072793, gradient=0.03529871449934765\n",
      "Gradient Descent(43/99): loss=0.3944765044262552, gradient=0.026081051322145426\n",
      "Gradient Descent(44/99): loss=0.39446821007263605, gradient=0.03185117039665463\n",
      "Gradient Descent(45/99): loss=0.3944856384583637, gradient=0.023865560005978646\n",
      "Gradient Descent(46/99): loss=0.3944784446394952, gradient=0.030009828333270823\n",
      "Gradient Descent(47/99): loss=0.3944860633762456, gradient=0.02570540409642106\n",
      "Gradient Descent(48/99): loss=0.3944905780378639, gradient=0.026014623976684966\n",
      "Gradient Descent(49/99): loss=0.3944951005834363, gradient=0.026332025901878848\n",
      "Gradient Descent(50/99): loss=0.39447880171691774, gradient=0.03519644357094054\n",
      "Gradient Descent(51/99): loss=0.3944909450925515, gradient=0.030056635683645277\n",
      "Gradient Descent(52/99): loss=0.3944704555124248, gradient=0.036542395239496706\n",
      "Gradient Descent(53/99): loss=0.39446400776941126, gradient=0.03641180009881634\n",
      "Gradient Descent(54/99): loss=0.3944818256283435, gradient=0.021248970207568857\n",
      "Gradient Descent(55/99): loss=0.39449452683760283, gradient=0.02318148263359898\n",
      "Gradient Descent(56/99): loss=0.3944743678603546, gradient=0.03579433338397453\n",
      "Gradient Descent(57/99): loss=0.39446770924524943, gradient=0.031134880669933158\n",
      "Gradient Descent(58/99): loss=0.39448398875930485, gradient=0.026156587077015844\n",
      "Gradient Descent(59/99): loss=0.3944856671958647, gradient=0.026661627665095947\n",
      "Gradient Descent(60/99): loss=0.394486784058018, gradient=0.028168686825748027\n",
      "Gradient Descent(61/99): loss=0.3944980001313702, gradient=0.026834692585991145\n",
      "Gradient Descent(62/99): loss=0.394494223406827, gradient=0.028554781912709735\n",
      "Gradient Descent(63/99): loss=0.3945009881848779, gradient=0.02286954596233113\n",
      "Gradient Descent(64/99): loss=0.39450329975633486, gradient=0.027043702038251516\n",
      "Gradient Descent(65/99): loss=0.3945065096217142, gradient=0.02983152495382404\n",
      "Gradient Descent(66/99): loss=0.3944787328977492, gradient=0.03899715584776631\n",
      "Gradient Descent(67/99): loss=0.3945013031155644, gradient=0.027476457822999237\n",
      "Gradient Descent(68/99): loss=0.39449160448550913, gradient=0.032046551031353135\n",
      "Gradient Descent(69/99): loss=0.39450805043130927, gradient=0.026526524738177657\n",
      "Gradient Descent(70/99): loss=0.3944985215731566, gradient=0.029022780994018203\n",
      "Gradient Descent(71/99): loss=0.39450456060095934, gradient=0.026268238663335\n",
      "Gradient Descent(72/99): loss=0.39451766314730474, gradient=0.025859109773349914\n",
      "Gradient Descent(73/99): loss=0.39449235523002685, gradient=0.03569493749383649\n",
      "Gradient Descent(74/99): loss=0.39451200801263003, gradient=0.023960719859579552\n",
      "Gradient Descent(75/99): loss=0.3944995816000762, gradient=0.033509434165220085\n",
      "Gradient Descent(76/99): loss=0.39450273730891217, gradient=0.03474756006970602\n",
      "Gradient Descent(77/99): loss=0.3944920083647874, gradient=0.03129902220177547\n",
      "Gradient Descent(78/99): loss=0.39449992579354265, gradient=0.028515670323146097\n",
      "Gradient Descent(79/99): loss=0.39450237381757947, gradient=0.030308819905243645\n",
      "Gradient Descent(80/99): loss=0.39451322744840295, gradient=0.021434076721906992\n",
      "Gradient Descent(81/99): loss=0.3944887446902971, gradient=0.03643827256014472\n",
      "Gradient Descent(82/99): loss=0.39449499401765464, gradient=0.027370755862668242\n",
      "Gradient Descent(83/99): loss=0.3945022458322586, gradient=0.023743189204767094\n",
      "Gradient Descent(84/99): loss=0.39450579028461547, gradient=0.027545781931250887\n",
      "Gradient Descent(85/99): loss=0.39450643002739205, gradient=0.027502677311546306\n",
      "Gradient Descent(86/99): loss=0.39451385965743535, gradient=0.02583267971483825\n",
      "Gradient Descent(87/99): loss=0.39451815014034614, gradient=0.026270420472778323\n",
      "Gradient Descent(88/99): loss=0.39452712850346777, gradient=0.023329411938928544\n",
      "Gradient Descent(89/99): loss=0.394520498398244, gradient=0.02882412705073802\n",
      "Gradient Descent(90/99): loss=0.39452321640733135, gradient=0.02655903003004227\n",
      "Gradient Descent(91/99): loss=0.3945164455128899, gradient=0.03006979880512148\n",
      "Gradient Descent(92/99): loss=0.3945159472998512, gradient=0.030441238738907285\n",
      "Gradient Descent(93/99): loss=0.3945032916854429, gradient=0.032897108166321076\n",
      "Gradient Descent(94/99): loss=0.3945172132602146, gradient=0.02659853382062385\n",
      "Gradient Descent(95/99): loss=0.3945119176251791, gradient=0.031758888005747984\n",
      "Gradient Descent(96/99): loss=0.39451658901409636, gradient=0.03461138242669377\n",
      "Gradient Descent(97/99): loss=0.3945003146481499, gradient=0.03366907788447503\n",
      "Gradient Descent(98/99): loss=0.39452823668716375, gradient=0.02147775189641882\n",
      "Gradient Descent(99/99): loss=0.3945028339347208, gradient=0.035990427889917916\n",
      "Gradient Descent(0/99): loss=0.3940997388160885, gradient=0.022129108529901728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1/99): loss=0.39410204521649916, gradient=0.0242618338049458\n",
      "Gradient Descent(2/99): loss=0.39409045063518505, gradient=0.030974714002158815\n",
      "Gradient Descent(3/99): loss=0.3941027891335214, gradient=0.025952606739694597\n",
      "Gradient Descent(4/99): loss=0.3940965833931915, gradient=0.02701751661293238\n",
      "Gradient Descent(5/99): loss=0.39408326837380914, gradient=0.03224869662460852\n",
      "Gradient Descent(6/99): loss=0.3940934831188835, gradient=0.02357273010619805\n",
      "Gradient Descent(7/99): loss=0.3940885554211192, gradient=0.029892626274411818\n",
      "Gradient Descent(8/99): loss=0.394089207326443, gradient=0.027674472122035672\n",
      "Gradient Descent(9/99): loss=0.39410564796953956, gradient=0.024224758537572458\n",
      "Gradient Descent(10/99): loss=0.39409757924584493, gradient=0.02988111338568968\n",
      "Gradient Descent(11/99): loss=0.3940981976797534, gradient=0.02735174114206932\n",
      "Gradient Descent(12/99): loss=0.3941067983099293, gradient=0.027231902729405108\n",
      "Gradient Descent(13/99): loss=0.3941223406619914, gradient=0.01901031177769182\n",
      "Gradient Descent(14/99): loss=0.39411163483556466, gradient=0.03073437957102139\n",
      "Gradient Descent(15/99): loss=0.3941057470482839, gradient=0.02907068413962869\n",
      "Gradient Descent(16/99): loss=0.3941298520142018, gradient=0.01993402301005601\n",
      "Gradient Descent(17/99): loss=0.39412232124214447, gradient=0.029005797832993025\n",
      "Gradient Descent(18/99): loss=0.3941112183863896, gradient=0.03143916608638784\n",
      "Gradient Descent(19/99): loss=0.3941148988882617, gradient=0.030093521155263037\n",
      "Gradient Descent(20/99): loss=0.3941076622714334, gradient=0.027021619065610212\n",
      "Gradient Descent(21/99): loss=0.39411896060775453, gradient=0.022242586503225276\n",
      "Gradient Descent(22/99): loss=0.39411839910409713, gradient=0.02847208630849835\n",
      "Gradient Descent(23/99): loss=0.39411899539471196, gradient=0.030905126674523987\n",
      "Gradient Descent(24/99): loss=0.39412622256151464, gradient=0.02193972310770933\n",
      "Gradient Descent(25/99): loss=0.39414786451172784, gradient=0.018192368590893737\n",
      "Gradient Descent(26/99): loss=0.39412029511625163, gradient=0.035712440433562\n",
      "Gradient Descent(27/99): loss=0.3941361001710132, gradient=0.02111373201874208\n",
      "Gradient Descent(28/99): loss=0.3941497995155788, gradient=0.02069136604573654\n",
      "Gradient Descent(29/99): loss=0.3941447986941521, gradient=0.028764852962891198\n",
      "Gradient Descent(30/99): loss=0.39413966855614646, gradient=0.029201292689398188\n",
      "Gradient Descent(31/99): loss=0.39415839912175954, gradient=0.0221700986179085\n",
      "Gradient Descent(32/99): loss=0.39411508863645767, gradient=0.04062083655485343\n",
      "Gradient Descent(33/99): loss=0.39413668748854125, gradient=0.02343175406829977\n",
      "Gradient Descent(34/99): loss=0.39411971916628363, gradient=0.03539185883886539\n",
      "Gradient Descent(35/99): loss=0.39413974213771585, gradient=0.023884016134889975\n",
      "Gradient Descent(36/99): loss=0.3941378177127913, gradient=0.027815680655087632\n",
      "Gradient Descent(37/99): loss=0.3941504244227858, gradient=0.02319079947903102\n",
      "Gradient Descent(38/99): loss=0.394153142869041, gradient=0.026668104884307098\n",
      "Gradient Descent(39/99): loss=0.39415717226882396, gradient=0.02653079306822403\n",
      "Gradient Descent(40/99): loss=0.39415684017127595, gradient=0.027714033135063397\n",
      "Gradient Descent(41/99): loss=0.3941753251603841, gradient=0.022839747320925346\n",
      "Gradient Descent(42/99): loss=0.3941435254323608, gradient=0.03682258343366082\n",
      "Gradient Descent(43/99): loss=0.3941451549137505, gradient=0.029339588936086753\n",
      "Gradient Descent(44/99): loss=0.3941260650505473, gradient=0.03759207699240864\n",
      "Gradient Descent(45/99): loss=0.39414784511433015, gradient=0.030787718163686768\n",
      "Gradient Descent(46/99): loss=0.39415021068401035, gradient=0.025087280812295738\n",
      "Gradient Descent(47/99): loss=0.39415748743662404, gradient=0.027089489686606145\n",
      "Gradient Descent(48/99): loss=0.39415539867323035, gradient=0.02850456255868697\n",
      "Gradient Descent(49/99): loss=0.39417333605180904, gradient=0.020378605033571758\n",
      "Gradient Descent(50/99): loss=0.3941699814680041, gradient=0.028430203325702265\n",
      "Gradient Descent(51/99): loss=0.39416676968617975, gradient=0.028274940726377013\n",
      "Gradient Descent(52/99): loss=0.39416532326427833, gradient=0.028260016756893495\n",
      "Gradient Descent(53/99): loss=0.3941794615724724, gradient=0.02499382639347807\n",
      "Gradient Descent(54/99): loss=0.3941519413739687, gradient=0.036083721238274406\n",
      "Gradient Descent(55/99): loss=0.39416295017100333, gradient=0.02571357340149554\n",
      "Gradient Descent(56/99): loss=0.39417688904534437, gradient=0.025342355251962503\n",
      "Gradient Descent(57/99): loss=0.3941651218532593, gradient=0.030988210183872\n",
      "Gradient Descent(58/99): loss=0.39416531398479837, gradient=0.02940293684333959\n",
      "Gradient Descent(59/99): loss=0.39417496067217495, gradient=0.02673630291942984\n",
      "Gradient Descent(60/99): loss=0.3941874489858187, gradient=0.022098735556037266\n",
      "Gradient Descent(61/99): loss=0.39419589505493763, gradient=0.0214926482120086\n",
      "Gradient Descent(62/99): loss=0.3941807218167448, gradient=0.033309407261928464\n",
      "Gradient Descent(63/99): loss=0.3941929750929872, gradient=0.026299315868485214\n",
      "Gradient Descent(64/99): loss=0.39419990780773045, gradient=0.02121601501511854\n",
      "Gradient Descent(65/99): loss=0.3941898296124979, gradient=0.03180656057785303\n",
      "Gradient Descent(66/99): loss=0.3941713418747716, gradient=0.03215021822902594\n",
      "Gradient Descent(67/99): loss=0.3941860297846024, gradient=0.02323375719675561\n",
      "Gradient Descent(68/99): loss=0.39419916220989515, gradient=0.022311988705996138\n",
      "Gradient Descent(69/99): loss=0.3942028351418594, gradient=0.02651802185385365\n",
      "Gradient Descent(70/99): loss=0.394192634259566, gradient=0.03142818950830629\n",
      "Gradient Descent(71/99): loss=0.39420482309532556, gradient=0.02556841387428038\n",
      "Gradient Descent(72/99): loss=0.39421026729577474, gradient=0.023661248670867698\n",
      "Gradient Descent(73/99): loss=0.3942175073117298, gradient=0.02198574973986676\n",
      "Gradient Descent(74/99): loss=0.3942026576874471, gradient=0.0331758629035758\n",
      "Gradient Descent(75/99): loss=0.3942186511086537, gradient=0.024815217430131195\n",
      "Gradient Descent(76/99): loss=0.39420637610204395, gradient=0.030765989631764096\n",
      "Gradient Descent(77/99): loss=0.39421553558524264, gradient=0.028101170014198266\n",
      "Gradient Descent(78/99): loss=0.39420051841350634, gradient=0.03212143148792964\n",
      "Gradient Descent(79/99): loss=0.39421712819050875, gradient=0.02086879205026199\n",
      "Gradient Descent(80/99): loss=0.3942058369189423, gradient=0.03125556264532214\n",
      "Gradient Descent(81/99): loss=0.39420992611001665, gradient=0.028620647800453123\n",
      "Gradient Descent(82/99): loss=0.39420986817479436, gradient=0.027958235332420603\n",
      "Gradient Descent(83/99): loss=0.3942224189856113, gradient=0.022669928850668597\n",
      "Gradient Descent(84/99): loss=0.3942247905158967, gradient=0.026067923109983654\n",
      "Gradient Descent(85/99): loss=0.3942249004770149, gradient=0.02688081502449941\n",
      "Gradient Descent(86/99): loss=0.39423177774055296, gradient=0.027498360703492934\n",
      "Gradient Descent(87/99): loss=0.39423011791128465, gradient=0.026264369970141623\n",
      "Gradient Descent(88/99): loss=0.3942299878022229, gradient=0.026751785352755554\n",
      "Gradient Descent(89/99): loss=0.39422069173453567, gradient=0.03401996859993826\n",
      "Gradient Descent(90/99): loss=0.39419106569109585, gradient=0.03547297830931997\n",
      "Gradient Descent(91/99): loss=0.394218020343106, gradient=0.019310559282009244\n",
      "Gradient Descent(92/99): loss=0.394214361514623, gradient=0.029885397965673587\n",
      "Gradient Descent(93/99): loss=0.39422884773337424, gradient=0.026277331858688587\n",
      "Gradient Descent(94/99): loss=0.39422620399076735, gradient=0.0259818455141497\n",
      "Gradient Descent(95/99): loss=0.39423476693235415, gradient=0.024865754075943526\n",
      "Gradient Descent(96/99): loss=0.3942303094499511, gradient=0.02803476106670523\n",
      "Gradient Descent(97/99): loss=0.394246497285335, gradient=0.020109807357027228\n",
      "Gradient Descent(98/99): loss=0.39424134352891677, gradient=0.02894751669496301\n",
      "Gradient Descent(99/99): loss=0.39424273368038, gradient=0.026972902157295277\n",
      "Minimum test error 0.3899261030668829 with lambda 0.00017782794100389227\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k_fold = 4\n",
    "lambdas = np.logspace(-9, -2, 20)\n",
    "\n",
    "mse_train = np.empty((len(lambdas), k_fold), float)\n",
    "mse_test = np.empty((len(lambdas), k_fold), float)\n",
    "\n",
    "w_initial = np.zeros(num_dim)\n",
    "max_iters = 100\n",
    "gamma = 0.1\n",
    "\n",
    "for l, lambda_ in enumerate(lambdas):\n",
    "    print('Lambda: {}'.format(lambda_))\n",
    "    k = 0\n",
    "    for train_split, test_split in k_fold_iter(y_train, tx_train, k_fold):\n",
    "        # Train\n",
    "        w, mse_tr = lasso_GD(train_split[1], train_split[0], w_initial, max_iters, gamma, lambda_)\n",
    "        mse_train[l, k] = mse_tr\n",
    "\n",
    "        # Test\n",
    "        mse_te = compute_loss_mse(test_split[1], test_split[0], w)\n",
    "        mse_test[l, k] = mse_te\n",
    "        \n",
    "        k += 1\n",
    "\n",
    "fig = cross_validation_visualization(lambdas, mse_train, mse_test)\n",
    "\n",
    "avg_mse_test = np.mean(mse_test, axis=1)\n",
    "lambda_opt = lambdas[np.argmin(avg_mse_test)]\n",
    "\n",
    "print('Minimum test error {} with lambda {}'.format(np.min(avg_mse_test), lambda_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Lasso regression using subgradient method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=0.3910522139442991, gradient=0.7635882956606208\n",
      "Gradient Descent(1/99): loss=0.38988267443507363, gradient=0.17363760573082376\n",
      "Gradient Descent(2/99): loss=0.3898234712378237, gradient=0.03948951044420328\n",
      "Gradient Descent(3/99): loss=0.3898200432782552, gradient=0.009001806310820783\n",
      "Gradient Descent(4/99): loss=0.389819872968603, gradient=0.002140012720010583\n",
      "Gradient Descent(5/99): loss=0.3898197870615962, gradient=0.0007871103400040745\n",
      "Gradient Descent(6/99): loss=0.3898197255101201, gradient=0.0006390141135528869\n",
      "Gradient Descent(7/99): loss=0.38981966153321257, gradient=0.0006255394740135875\n",
      "Gradient Descent(8/99): loss=0.38981959911637676, gradient=0.0006201215529258067\n",
      "Gradient Descent(9/99): loss=0.3898195372338913, gradient=0.0006152571013374492\n",
      "Gradient Descent(10/99): loss=0.3898194761038347, gradient=0.0006105300418308536\n",
      "Gradient Descent(11/99): loss=0.38981941566771994, gradient=0.0006059029504695377\n",
      "Gradient Descent(12/99): loss=0.389819355930696, gradient=0.0006013623098292613\n",
      "Gradient Descent(13/99): loss=0.3898192968836342, gradient=0.0005968987172934464\n",
      "Gradient Descent(14/99): loss=0.3898192385206611, gradient=0.0005925050054711683\n",
      "Gradient Descent(15/99): loss=0.38981918083507766, gradient=0.0005881755773805777\n",
      "Gradient Descent(16/99): loss=0.3898191238202411, gradient=0.0005839059854600249\n",
      "Gradient Descent(17/99): loss=0.3898190674693467, gradient=0.000579692640911784\n",
      "Gradient Descent(18/99): loss=0.38981901177547673, gradient=0.0005755326077702004\n",
      "Gradient Descent(19/99): loss=0.38981895673160033, gradient=0.0005714234539674013\n",
      "Gradient Descent(20/99): loss=0.3898189023305874, gradient=0.000567363141538499\n",
      "Gradient Descent(21/99): loss=0.38981884856522425, gradient=0.000563349944242772\n",
      "Gradient Descent(22/99): loss=0.38981879542822806, gradient=0.0005593823847721734\n",
      "Gradient Descent(23/99): loss=0.3898187429122639, gradient=0.0005554591862231679\n",
      "Gradient Descent(24/99): loss=0.3898186910099579, gradient=0.0005515792341425688\n",
      "Gradient Descent(25/99): loss=0.38981863971391134, gradient=0.0005477415465421156\n",
      "Gradient Descent(26/99): loss=0.3898185890167137, gradient=0.0005439452500057006\n",
      "Gradient Descent(27/99): loss=0.3898185389109537, gradient=0.0005401895605160319\n",
      "Gradient Descent(28/99): loss=0.38981848938923064, gradient=0.0005364737679762711\n",
      "Gradient Descent(29/99): loss=0.38981844044416253, gradient=0.0005327972236526528\n",
      "Gradient Descent(30/99): loss=0.38981839206839664, gradient=0.0005291593299439241\n",
      "Gradient Descent(31/99): loss=0.38981834425461465, gradient=0.0005255595320171547\n",
      "Gradient Descent(32/99): loss=0.38981829699554055, gradient=0.0005219973109485499\n",
      "Gradient Descent(33/99): loss=0.38981825028394723, gradient=0.0005184721780840005\n",
      "Gradient Descent(34/99): loss=0.3898182041126611, gradient=0.0005149836703912574\n",
      "Gradient Descent(35/99): loss=0.389818158474566, gradient=0.0005115313466221788\n",
      "Gradient Descent(36/99): loss=0.3898181133626085, gradient=0.0005081147841373274\n",
      "Gradient Descent(37/99): loss=0.3898180687698007, gradient=0.0005047335762743841\n",
      "Gradient Descent(38/99): loss=0.3898180246892227, gradient=0.000501387330163534\n",
      "Gradient Descent(39/99): loss=0.38981798111402705, gradient=0.0004980756649110523\n",
      "Gradient Descent(40/99): loss=0.3898179380374385, gradient=0.0004947982100866036\n",
      "Gradient Descent(41/99): loss=0.38981789545275763, gradient=0.0004915546044615927\n",
      "Gradient Descent(42/99): loss=0.3898178533533619, gradient=0.0004883444949550898\n",
      "Gradient Descent(43/99): loss=0.38981781173270647, gradient=0.00048516753575156025\n",
      "Gradient Descent(44/99): loss=0.38981777058432543, gradient=0.0004820233875616091\n",
      "Gradient Descent(45/99): loss=0.38981772990183355, gradient=0.00047891171700072674\n",
      "Gradient Descent(46/99): loss=0.3898176896789245, gradient=0.0004758321960666061\n",
      "Gradient Descent(47/99): loss=0.38981764990937395, gradient=0.0004727845016981593\n",
      "Gradient Descent(48/99): loss=0.38981761058703773, gradient=0.00046976831540288704\n",
      "Gradient Descent(49/99): loss=0.3898175717058532, gradient=0.00046678332294067655\n",
      "Gradient Descent(50/99): loss=0.38981753325983814, gradient=0.0004638292140552688\n",
      "Gradient Descent(51/99): loss=0.38981749524309184, gradient=0.00046090568224528686\n",
      "Gradient Descent(52/99): loss=0.38981745764979325, gradient=0.0004580124245680469\n",
      "Gradient Descent(53/99): loss=0.3898174204742017, gradient=0.0004551491414713329\n",
      "Gradient Descent(54/99): loss=0.38981738371065683, gradient=0.00045231553664830295\n",
      "Gradient Descent(55/99): loss=0.3898173473535764, gradient=0.00044951131691164524\n",
      "Gradient Descent(56/99): loss=0.3898173113974578, gradient=0.0004467361920844199\n",
      "Gradient Descent(57/99): loss=0.38981727583687553, gradient=0.0004439898749043241\n",
      "Gradient Descent(58/99): loss=0.3898172406664818, gradient=0.000441272080939654\n",
      "Gradient Descent(59/99): loss=0.3898172058810051, gradient=0.0004385825285150322\n",
      "Gradient Descent(60/99): loss=0.3898171714752497, gradient=0.0004359209386454718\n",
      "Gradient Descent(61/99): loss=0.38981713744409496, gradient=0.00043328703497716594\n",
      "Gradient Descent(62/99): loss=0.38981710378249435, gradient=0.00043068054373449587\n",
      "Gradient Descent(63/99): loss=0.3898170704854747, gradient=0.0004281011936718466\n",
      "Gradient Descent(64/99): loss=0.3898170375481351, gradient=0.0004255487160298994\n",
      "Gradient Descent(65/99): loss=0.3898170049656459, gradient=0.00042302284449535624\n",
      "Gradient Descent(66/99): loss=0.38981697273324895, gradient=0.00042052331516412864\n",
      "Gradient Descent(67/99): loss=0.38981694084625507, gradient=0.00041804986650684765\n",
      "Gradient Descent(68/99): loss=0.3898169093000443, gradient=0.00041560223933694246\n",
      "Gradient Descent(69/99): loss=0.38981687809006443, gradient=0.00041318017678068175\n",
      "Gradient Descent(70/99): loss=0.3898168472118303, gradient=0.00041078342424899565\n",
      "Gradient Descent(71/99): loss=0.3898168166609232, gradient=0.00040841172941061157\n",
      "Gradient Descent(72/99): loss=0.389816786432989, gradient=0.0004060648421667863\n",
      "Gradient Descent(73/99): loss=0.38981675652373815, gradient=0.0004037425146270588\n",
      "Gradient Descent(74/99): loss=0.3898167269289442, gradient=0.00040144450108600075\n",
      "Gradient Descent(75/99): loss=0.3898166976444437, gradient=0.00039917055800109093\n",
      "Gradient Descent(76/99): loss=0.3898166686661341, gradient=0.00039692044397102654\n",
      "Gradient Descent(77/99): loss=0.3898166399899738, gradient=0.000394693919715154\n",
      "Gradient Descent(78/99): loss=0.3898166116119805, gradient=0.0003924907480534066\n",
      "Gradient Descent(79/99): loss=0.38981658352823145, gradient=0.0003903106938866495\n",
      "Gradient Descent(80/99): loss=0.3898165557348617, gradient=0.0003881535241779487\n",
      "Gradient Descent(81/99): loss=0.38981652822806234, gradient=0.00038601900793394574\n",
      "Gradient Descent(82/99): loss=0.38981650100408216, gradient=0.0003839069161870417\n",
      "Gradient Descent(83/99): loss=0.3898164740592243, gradient=0.00038181702197760296\n",
      "Gradient Descent(84/99): loss=0.38981644738984694, gradient=0.000379749100336918\n",
      "Gradient Descent(85/99): loss=0.3898164209923618, gradient=0.00037770292827044786\n",
      "Gradient Descent(86/99): loss=0.38981639486323316, gradient=0.0003756782847411037\n",
      "Gradient Descent(87/99): loss=0.389816368998978, gradient=0.000373674950653253\n",
      "Gradient Descent(88/99): loss=0.38981634339616367, gradient=0.00037169270883688954\n",
      "Gradient Descent(89/99): loss=0.3898163180514089, gradient=0.00036973134403187475\n",
      "Gradient Descent(90/99): loss=0.38981629296138115, gradient=0.0003677906428727673\n",
      "Gradient Descent(91/99): loss=0.38981626812279774, gradient=0.00036587039387375877\n",
      "Gradient Descent(92/99): loss=0.38981624353242333, gradient=0.00036397038741373626\n",
      "Gradient Descent(93/99): loss=0.3898162191870702, gradient=0.0003620904157218932\n",
      "Gradient Descent(94/99): loss=0.3898161950835973, gradient=0.000360230272863196\n",
      "Gradient Descent(95/99): loss=0.38981617121890955, gradient=0.00035838975472436447\n",
      "Gradient Descent(96/99): loss=0.38981614758995675, gradient=0.00035656865899986757\n",
      "Gradient Descent(97/99): loss=0.3898161241937336, gradient=0.0003547667851782627\n",
      "Gradient Descent(98/99): loss=0.3898161010272782, gradient=0.00035298393452861716\n",
      "Gradient Descent(99/99): loss=0.3898160780876721, gradient=0.00035121991008703964\n",
      "Accuracy on evaluation set:  0.71908\n",
      "F1 Score on evaluation set: 0.6677704716400965\n"
     ]
    }
   ],
   "source": [
    "# Find optimal ridge regression model under lambda_opt\n",
    "max_iter = 1000\n",
    "gamma = 0.1\n",
    "w_lasso, mse_lasso = lasso_GD(y_train, tx_train, w_initial, max_iters, gamma, lambda_opt)\n",
    "\n",
    "# Test error on evaluation set\n",
    "y_eval_pred = predict_labels(w_lasso, tx_eval)\n",
    "\n",
    "acc_lasso = get_accuracy(y_eval_pred, y_eval)\n",
    "f1_lasso = get_f1_score(y_eval_pred, y_eval)\n",
    "\n",
    "print('Accuracy on evaluation set: ', acc_ridge)\n",
    "print('F1 Score on evaluation set:', f1_ridge)\n",
    "\n",
    "# Save current model predictions on test set\n",
    "y_test_pred = predict_labels(w_lasso, tx_test)\n",
    "create_csv_submission(ids_test, y_test_pred, '../data/lasso_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy = [acc_baseline, acc_ls, acc_GD, acc_SGD, acc_ridge, acc_lasso]\n",
    "labels = ['Baseline', 'Least-Squares', 'LS Gradient Descent', 'LS Stoch. Gradient Descent', 'Ridge Regression', 'Lasso Regression']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "ax.barh([i for i in range(len(accuracy))], accuracy)\n",
    "\n",
    "ax.set(title='Prediction Accuracy on Evaluation Set', xlabel='Accuracy',\n",
    "       yticks=[i for i in range(len(labels))], yticklabels=labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "ax.barh([i for i in range(num_dim)], w_ridge, label='Ridge, $\\lambda = 5.62 \\cdot 10^{-8}$')\n",
    "ax.barh([i+.5 for i in range(num_dim)], w_lasso, label='Lasso, $\\lambda = 1.78 \\cdot 10^{-4}$')\n",
    "\n",
    "ax.legend(fontsize=16, loc='center', bbox_to_anchor=(.5,1.05), ncol=2)\n",
    "\n",
    "ax.set(yticks=[], xlabel='model weight');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "c = np.corrcoef(tx_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "?np.corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: y: (250000,), x:(250000, 20)\n",
      "\n",
      "[-2.50270382e-15  4.48725856e-15  7.19034432e-15 -2.36302302e-14\n",
      " -3.20729487e-15  1.26038877e-14  2.86094082e-15 -6.98242575e-15\n",
      "  4.02186640e-15 -1.27422117e-14 -5.97875394e-15 -8.99406061e-15\n",
      " -6.01698247e-16 -4.92140906e-15  3.11615622e-15 -1.67606551e-15\n",
      " -9.40864542e-15  1.79148900e-14 -5.09692022e-15 -1.77026038e-15] [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "y_top, tx_top, _ = load_csv_data('../data/train_top_twenty.csv')\n",
    "\n",
    "# Normalise data\n",
    "tx_top, mean_tx_top, std_tx_top = standardise(tx_top)\n",
    "\n",
    "# Check shape of data\n",
    "print('Shape: y: {}, x:{}\\n'.format(y.shape, tx_top.shape))\n",
    "\n",
    "num_samples, num_dim_top = tx.shape\n",
    "\n",
    "# Check that data is normalised\n",
    "print(np.mean(tx_top, axis=0), np.std(tx_top, axis=0))\n",
    "\n",
    "# Split into train and evaluation set\n",
    "(tx_top_train, y_train), (tx_top_eval, y_eval) = train_eval_split(y, tx_top, split_ratio=.7, seed=SEED)\n",
    "\n",
    "# Load test data with reduced feature space\n",
    "_, tx_top_test, _ = load_csv_data('../data/test_top_twenty.csv')\n",
    "\n",
    "# Don't forget to standardise to same mean and std\n",
    "tx_top_test = standardise_to_fixed(tx_top_test, mean_tx_top, std_tx_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Compare performance of least-squares with reduced feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy LS on evaluation set:  0.7087733333333334\n",
      "F1 Score LS on evaluation set: 0.6516427432216906\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArgAAAGaCAYAAADkTDCPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABLwElEQVR4nO3dd3xO9///8WcSIUbRmDVao64gEWInZmLUilhRe9Wq9aVoKR/aokqH9qOKltLaRaJWa1ObWi1iVktsYkQSmef3h991fVxNVEQicvq4325ubc55n+u8zvtaz+t9vc+5HAzDMAQAAACYhGN6FwAAAACkJgIuAAAATIWACwAAAFMh4AIAAMBUCLgAAAAwFQIugHTHxVyA5x/P08Tok+cXARdIZ3v37pWbm1uS/zw8PFS9enV17txZS5YsUXx8fLrUeOXKFbm5ucnPz89uuZubm8qWLZvi27127ZqGDh2q/fv32y338/OTm5ubrly5kuLbTgunTp2y3Tdff/11epeDNBQaGvrI52VS//bu3ftM6urcubPc3Nz066+/PpP9We3cuVM9e/a0W2Z97erWrdszreVxLl26pIkTJ6pp06by8vJShQoVVL9+fY0YMUIHDhxIlX2Eh4dr/PjxWrlyZarcHlJfpvQuAMAD2bJlU7169eyWxcXFKSwsTAcOHNC+ffu0c+dO/fe//02nClPfiBEjtHPnTrVt2za9S0mWoKAgSVKWLFm0dOlS9erVSw4ODulcFdKav7//Y9vkzZv3GVSSPq5evaoePXqocOHC6V3KY23btk2DBg3S/fv3Vbx4cVWtWlVOTk46f/68goODFRwcrN69e2vo0KFPtZ/Jkyfrhx9+0MSJE1OpcqQ2Ai7wnHjxxRf1ySefJLkuJCREnTp10rp167RhwwY1aNDgGVeXtLVr1z5VwEtISEhy+dy5cxUbG/tchYa4uDitXLlSBQsWVJUqVbRq1Srt2rVLNWrUSO/SkMYe9bz8t3jU1/Cenp5au3atsmXL9owrStqdO3f01ltvKSEhQTNmzJCvr6/d+j179qhfv376+uuvVb58edWvXz/F+3rUaxeeH0xRADKAMmXK2EY5169fn87V/E/JkiVVokSJVL/dl19+WSVLllSmTM/PZ/CtW7fq5s2b8vHxUePGjSVJS5YsSeeqgPSTNWtWlSxZUi+99FJ6lyJJ2rRpk+7du6cWLVokCreSVL16dQ0ZMkQSz91/AwIukEEUKVJEkhQWFmZb5ufnp2rVqikkJEQBAQHy8PBQvXr1dOLECVubX375RT169FCVKlXk6ekpf39/zZ49WzExMUnuZ9WqVQoMDJSXl5dq1qypiRMnKiIiIsm2j5qDe/XqVX344Ydq0KCBPD09Va9ePY0cOVKhoaGS/je/cffu3ZKkLl26yM3Nzbb+UXNwb926pcmTJ+u1116Th4eHqlatqjfeeEPbt29PVMOIESPk5uamU6dOadmyZWrRooU8PT1VvXp1DR8+3Lav5AoODpYkvfbaa6pdu7Zy586tTZs26fr164/c5u7du/riiy/UpEkTlS9fXnXq1NGgQYN08uTJFLf9p3nP3bp1SzQf1Dpn8/Tp0+rYsaM8PDxUu3ZtW5/FxcVp8eLF6ty5s6pVqyZ3d3dVq1btkf0qSX/88YdGjx4tX19feXp66rXXXtOECRNsj80bN27I3d1dXl5eioqKSrR9bGysvL295eXlpcjIyEf2n9WlS5c0duxY+fn5ycPDQ97e3ho4cKB+++23RG2txxseHq5Zs2apcePGKleunGrVqqX333/f7vmTFtq0aSM3Nzft27cvyfWDBg2Sm5ubNmzYYFt29uxZjR49Wg0aNFD58uVVvnx5NWrUSJMnT9bdu3cfu89/mrM+atQoubm52abXWN2+fVuff/65WrRooYoVK9oeF8OHD9cff/xhazd16lTVqVNHknTx4kW5ubmpc+fOkv55Du7BgwfVv39/Va9eXR4eHvLz89N7772XZI1ubm5q1aqV7ty5o/fff1+1atVSuXLl1LhxY33zzTeKi4t7bB9I0s2bNyXpH79Vql+/vpo1ayYvL69E665cuaKxY8fK19dXHh4eqlmzpkaMGKELFy4kqnfZsmWSpJEjRz7TOdhIPgIukEGcOXNGkhKNlsTExKh37966f/++ateurUyZMqlkyZKSpGnTpqlXr17at2+fSpUqpdq1a+vGjRuaPHmyevbsmSjkfvrppxo2bJhOnjypKlWqyM3NTQsWLNCgQYOSXeeJEyfUqlUrfffdd3J0dFTdunWVPXt2BQUFqXXr1vrrr7+ULVs2+fv7K1++fJIkHx8f+fv7/+NXnefPn1dAQIBmz56t+/fv297Ud+/erZ49e+rLL79McrsvvvhCo0aNkqOjo2rXri0nJyetXLlSHTp00L1795J1TGFhYdq2bZvy5MmjmjVrytnZWU2bNlVcXJyWL1+e5DaXL19WYGCgvvrqK927d0916tRRwYIFtW7dOrVp00aHDh1KUduUGjBggC5cuKC6devK0dFRZcuWlWEY6t+/v8aOHavTp0/bgnWOHDm0Y8cO9erVSxs3brS7nV27dql169ZaunSpXnjhBdWtW1eS9P3336tt27YKCwtT3rx5VatWLUVGRibaXnrwoSssLEyNGjV67NfbR44cUfPmzbV48WI5OzvLz89PRYsW1fr169WuXTtb0Pi7ESNG6NNPP1Xu3LlVu3ZtRUZGauHCherRo0eanqwZEBAgSfrpp58Srbt37562bt2qXLly2ULjvn371KpVKy1dutS23NPTU6GhoZo9e7a6d++e6l+H37hxQ61bt9b06dMVGRkpHx8fVatWTdHR0Vq5cqXatm2ry5cvS3oQ5qxToqzPWx8fn3+8/QULFqhjx47auHGjXnnlFfn5+SlTpkxatGiRWrRooWPHjiXaJiIiQu3bt9eKFStksVhUpUoV/fXXX/rkk0/00UcfJeu4SpcuLenBXPkffvhB0dHRidq89NJL+vTTT9WvXz+75cePH1eLFi20ePFiZcmSRb6+vsqXL5+Cg4PVqlUruw9T/v7+evnllyVJXl5e8vf3f66mU+H/MwCkqz179hgWi8Xw9fV9ZJu9e/ca7u7uhsViMXbt2mVb7uvra1gsFiMwMNCIiYkxDMMw4uPjDcMwjJ07dxoWi8WoW7eucerUKds2ERERRt++fQ2LxWJ8+umntuVHjhwx3NzcDB8fH+Ps2bO25SdPnjSqV6+eZI0Wi8UoU6aM7e/4+HijefPmhsViMaZOnWokJCTY1k2dOtWwWCzGG2+8YVvWtWtXw2KxGHv27LG7XetxXb582TAMw0hISDBatmxpWCwW4/3337cdq7XuqlWrGhaLxdi2bZtt+TvvvGNYLBbD3d3d2LRpk215eHi40aRJE8NisRgLFy58ZJ8/bM6cOYbFYjEmTpxoW3b06FFbn1j7/GF9+vQxLBaLMWrUKLt6g4KCDIvFYjRq1ChFbf/e5w9Lqj87depkWCwWw8/Pz7h7965hGP97jKxdu9awWCzG66+/bkRFRdm2iY+PNyZMmGBYLBajW7dutuX37t0zatasaVgsFmPp0qW25XFxcca7775rWCwWY8yYMYZhGMaGDRsS3d9WAwcONCwWi7F3794kj8MqKirKtr+ZM2faPZ62bt1qlCtXznB3dzdOnDiR6HgrVapkHD582Lb8ypUrhre3d6LHyaNcuHDBsFgshsVieWzbh928edMoW7as4e3tbcTFxdmtCw4ONiwWi/Gf//zHtqxp06aGxWIxNmzYYNf2r7/+MqpUqWJYLBbj119/TXR8+/fvty37+/PlYdb7Zfny5bZl7733nmGxWIwPP/zQrk/Dw8ONdu3aGRaLxfjqq69syy9fvpzk89/62tW1a1fbsmPHjhmlS5c2ypcvb+zcudO2PD4+3vYa4Ovra0RHR9vWWfu5ZcuWxpUrV2zLra9h7u7uRnh4eKJj+7uEhATbc8BisRgVKlQw+vTpY8yePdv47bffknyeGoZhREdHG/Xq1TMsFosxb948u3XBwcGGm5tbopqT6lc8XxjBBZ4Tt27d0rBhw+z+DRo0SM2bN1fnzp0VGxurTp06ydvbO9G27du3l7OzsyTJ0fHB03r27NmSpNGjR6tUqVK2ttmyZdOECRPk4uKiBQsW2EZxlyxZIsMwNGjQILt5tRaLJdkjuIcOHdKJEyfk4eGhAQMG2H1V+Oabb6p06dKKi4t75PSIR9m/f7+OHTumkiVLatSoUbZjlR6c6DJixAi7Y35Yo0aN7C5vliNHDtsom3VU/HGsX++2atXKtszd3V2lS5fWxYsXtWPHDrv2V69e1ZYtW5QvXz6NGTPGrt6WLVuqZs2aypUrl27cuPFEbZ9Gy5Yt9cILL0j632MkISFBfn5+GjZsmFxcXGxtHR0dFRgYKOnB9ACrTZs26dq1a3rttdfUpk0b23InJye9/fbbevnll21fqdepU0d58uTRrl277Gq/c+eONm/erKJFi6pKlSr/WPNPP/2ka9euqWbNmurdu7fd46lOnTrq3bu3YmNj9d133yXatmPHjipfvrzt7wIFCthOKkru/W71uEuEPTwa6Orqqpo1a+rmzZuJpimsWbNGktS8eXNJD0Z0PTw81LZt20QnPL388suqXr26JNlGU1PLiy++qFq1amngwIF2fZojRw41a9bsqfY5b948JSQk6M0337Qb6XV0dNSAAQNUtWpVXbx4UWvXrk207bBhw1SgQAHb3z4+PipevLhiY2P1559/PnbfDg4O+uqrr9SxY0c5OzsrMjJSW7Zs0aRJk9SmTRv5+Pjo/fffT/Rc2rBhgy5cuKAGDRqoU6dOdutatGihhg0b6uLFi8/V+Q94vOfnDA7gXy4yMlKrVq2yW5Y5c2a5urrK19dXrVq1UsOGDZPc1vrVnFV8fLztOpnVqlVL1N7V1VVly5bVwYMHdfz4cVWoUMF2LdratWsnal+vXj299957jz0G6xt6Uid4ODk56ccff3zsbSTFWlvDhg3l5OSUaH2jRo00atQoHTx4UPHx8XZtHg45VtavE5Mz//PYsWM6efKk3N3dZbFY7Na1bt1aEyZM0JIlS+z6zTofr1atWsqcOXOi23w4iFuvo5mctk/j748RSWratKmaNm1qtywyMlJnzpzRL7/8IunBfFmrf7p/c+XKZTev1NnZWc2bN9ecOXO0evVq2zzNNWvWKDY2Vi1atHjsFTis93ujRo2SXN+kSRNNnTo10XWUpaTvd+uUmOTc7w973GXCPDw87P4OCAjQ1q1btXbtWtsH0lu3bmn37t0qUqSIKlWqJOlBoPz71++GYejSpUs6fvy4be7nw/dBakjqA2tYWJhOnjxpe91I6T6t94X1RMy/a9q0qfbt26f9+/erRYsWdus8PT0Ttc+XL5/OnTuX5FzupGTLlk1jxozRgAEDtGnTJu3atUv79+/X9evXdevWLS1cuFCrV6/W7NmzbfuzPl+Teq2UHjw3161bp3379tk+AOD5R8AFnhOFCxfW5s2bU7Rtrly57P6+ffu27t+/L0m2N9NHuXz5sipUqKBr165Jkt0IilX+/PntRhYfxXrCVcGCBZNVd3JZa3vUdTizZs0qV1dXXb9+XXfu3JGrq6ttnXXU8mHWAGwk41eIrKO3N27csJ1cY2U9+W7r1q26evWqre+epB/Sqs/+7u+PEau7d+9q8eLF2r59u/744w/b6FZS4fNJa23VqpXmzJmjlStX2gLuihUr5ODgkCjcJMV6v1tPsPw76/KkRrdz5syZaNmT3O8Pe9LLhPn5+SlHjhxav369xo4dq0yZMmndunWKjY1Vs2bNEvXt/v379cMPP+j48eM6f/687RsOa7snrTc5zp8/r/nz5+vgwYM6d+6cbT760+7zcc/VR91njo6OypEjR6L21vvsSechu7q6KjAw0PZNxNmzZ7Vp0ybNnTtXN2/e1KBBg7R+/XplzpzZNlo9fvx4jR8//pG3+bz98Az+GQEXMAHrV85W1pNosmbN+thrPVpHtR73xpacS3Yl92znJ5WcN1vrG+DfR0Gf5jq9MTExWr16taQH0w6uXr2aZLu4uDgtW7ZM/fv3l6QnOokpNU94+qfbSqofTp06pa5du9pODCtXrpxKliypsmXL6pVXXlHr1q3t2j/p/WuxWFSuXDn9/vvvOnv2rJycnHTkyBFVrVr1kaH1YY+73x91n6c3FxcXNWzYUEFBQdq9e7dq1aplm55gnR5jNXbsWC1evFhOTk4qU6aM/P39VapUKXl5eWnJkiWJrn7wpJIKhqtWrdI777yj+Ph4FStWTLVr11bJkiVVrlw5XblyRWPGjEnx/h53n1kfo6n5PLXu98SJE7p7926SI7ElS5ZUyZIl1bx5cwUEBOjy5cs6cOCAvL29bX3k4+OjPHnyPHIfr7766lPViGeLgAuYUO7cueXs7Ky4uDhNmjQpya/1/y5//vz6888/denSJRUtWtRuXXh4eLK+IrSG5UeNdGzdutV21nbu3LkffyAP1SbpkZf2unfvnsLCwuTi4pLkKFBKbdq0Sbdv35a3t7fmzp2bZJsNGzZowIABWrZsmd588005OjrapkA8qh/279+vy5cvq1q1ak/UtkCBAnJwcFB8fLwMw0gUCsLDw5/o+MaNG6ewsDD1798/0XzMpC5lZr1/HxX016xZI2dnZ9WuXds2p7dVq1b6/ffftX79etvj8OG5zP/kcfe79Sv8fwol6SUgIEBBQUH6+eefVbp0af3666/y8PCwm9++b98+LV68WEWKFNHs2bNVrFgxu9tI7vQU6/2W1AeQv19mLCIiQmPHjpWjo6OmT59uu5qD1bx585K1z0fJnz+/QkNDFRoaqldeeSXReut9mRb32euvv66YmBjt2bPnka8vBQsWVI0aNbRmzRrduXNH0v8e1y1atEj0AQQZFyeZASaUOXNmlS9fXrGxsbZrzT4sJiZGrVq1UocOHWxvONYTQjZt2pSovXU+5uNUrFhRkhKddCU9GGEZP368hg4dansjTu6ojfVkpA0bNiQ5Srlu3ToZhqGqVasm6/aSy3rt27/PU31YnTp1lDt3bl26dMnWT9ZrbO7evTvJ0PHf//5Xw4cP19WrV5+orSTbZbX+/hXvvXv3nvjkKeulj/r27Zvovti5c6ck+xFA6/2b1OPh/v37GjVqlEaNGmX3gapZs2bKkiWLNmzYoM2bNytbtmx67bXXklWf9X7/+eefk1xvvRRXat/vqaFatWp66aWXtG3bNq1fv14JCQm2k8usjhw5IunBXOK/h9uoqCgdPHhQ0uO/nn/UYyI+Pl6///673bKzZ88qIiJC7u7uicKtlPT9/iSjq+l1nzk4OKh8+fIyDEMLFy78x7bnzp2TJNvJt5UrV5b06Ne5zz//XAEBAfrhhx/s9ofnGwEXMKmuXbtKevAV6KlTp2zL4+LiNG7cOB07dkyRkZG2r4o7dOggZ2dnTZs2ze5N8cKFC/r444+TtU9vb28VL15chw4d0rfffmu37quvvtKFCxfk4+NjG7XMkiWLpMePPFatWlVly5bV2bNnNWHCBLsTYI4eParJkydLenDmfGq5du2aduzYIWdn50ee3Cc9+DBhPaFm8eLFkqTixYurRo0aunz5siZNmmQXylesWKF9+/apePHiKleu3BO1lWQ70e3hkbaYmBiNHTv2iU8Mss6l/fuHmq1bt2rq1KmSZHct0SZNmih37tz66aef7K7zGhcXpwkTJigqKkqNGze2m6+dM2dONWjQQMeOHdPhw4eTde1bq8aNGyt//vzasWOHvv76a7uvv3/55RfNmjVLzs7Oev3115/ouJ8FBwcHNWvWTNevX9fXX3+tTJkyJTpByXpN6507d9r1c3h4uIYNG2YLrEldz/VhDz8mrH2UkJCgzz77LNFou/U+P3XqlN0PGMTHx2v69OnasmVLon1apxNEREQ8dgpCp06d5OTkpOnTp9t9uDYMQ19++aX279+vwoULJ3mi4tOyfoMydepUzZgxI1G/RUZGavz48Tp+/LhtWob04ANsvnz5tHr1ai1YsMBum+3bt2v27Nk6efKk7TkoJf+1C+mHKQqASTVs2FBdu3bVd999p1atWsnDw0N58+bV0aNHdfnyZbm6uuqzzz6ztS9VqpTeeecdTZgwQe3atVP16tXl7Oys3bt3q1SpUo/8Wvphjo6O+uyzz9StWzdNmjRJQUFBKlGihM6ePaszZ84ob968+vDDD23trV9hvv/++1q5cqWGDh2a5NeaDg4O+uyzz9S1a1ctWLBAmzdvlqenp27duqUDBw4oPj5e/fr1s/3oQGpYsWKF4uPjVatWrUeeoGUVEBCgRYsW6ZdfftGVK1dUsGBBjR8/Xh07dtT333+vzZs3y93dXRcvXtTRo0eVNWtWff7557ZRoCdp2717dx06dEgzZ87Ujh07VLhwYR06dEj379+Xr6+vLaAkR7du3fTee+9pyJAhmj9/vvLkyWO7r1566SU5ODjo7t27iomJUebMmZUjRw59/PHHGjBggAYPHqzZs2erUKFCtjP+S5QoobfffjvRflq3bq3Vq1fLMAy1bNky2fVlzZpVX3zxhXr37q1PP/1Uy5cvV+nSpXXlyhUdPnxYmTJl0tixY1WmTJlk32ZKDBs27LFtqlSpkihoBwQE6JtvvtGVK1dUu3btRF/L+/r66uWXX9axY8dUv359lS9f3jZyGxkZqVdffVVnzpx57CXiOnfurHXr1mn16tUKCQnRq6++qmPHjunq1atq3Lix3YeR/Pnzq0mTJlq7dq38/f1VtWpVZcqUSb/99puuX7+e5D5z586t3Llz6/bt22rfvr3KlSunUaNGJVmLh4eHRo4cqQkTJqhbt27y8vJSgQIFdOLECf35559ydXXV559/rqxZsz62T5+Uj4+PPvjgA33wwQeaMmWKZs6cqfLly9tqP3LkiCIjI+Xu7m73od36HOvTp48++OADfffddypVqpRu3Lihw4cPS3rwwyEPP86sr1PTpk3TgQMH1LVr18ee0ItnixFcwMTeffddTZs2TVWqVNHZs2f1yy+/yMXFRZ07d9aKFSvs5gNKD94ov/76a1WsWFGHDh3SoUOH1KRJE82aNSvZX8mVLVtWQUFBCgwMVHh4uDZv3qw7d+6oVatWWrZsmd1VGvr06aO6desqPDxcO3fu/MdrXRYvXlzBwcHq3r27nJ2dtXnzZp09e1a1a9fW3Llz9X//938p6qNHsU5PaNKkyWPbenl5qVixYoqPj9fSpUslSYUKFdLy5cttVw/YvHmzQkND1ahRIy1dutTusl1P0va1117T9OnT5eXlpbNnz2rv3r3y8vLSsmXLnvgkmPbt22vy5MkqW7asQkJCtHXrViUkJKhnz55asWKFqlWrpri4OLuvbmvXrq2lS5eqSZMmunTpkjZt2qT4+Hh16dJFixYtSnIOdIUKFeTo6Jisa9/+XcWKFRUcHKy2bdsqOjpamzZt0qVLl9SsWTMtXrxYbdu2faLbS4lVq1Y99l9SvzZXqlQp288q/316giRlz55d8+bNU8uWLZUpUyZt3bpVx44dU8WKFfXNN9/YQtjjPrRUqFBB3333ne2bgB07dqhYsWJauHBhkidcffjhhxowYIAKFiyo3bt3a8+ePSpYsKDGjBmj4OBg5cqVS4cPH7b9rLGDg4MmT56s4sWL6+jRo4+tp3Pnzpo/f758fX117tw5bd68WYZhqFu3blqxYkWSlwNLLYGBgVq7dq3eeOMNFS9eXCdOnNDGjRt18uRJeXp6aty4cVq6dGmiObqVK1fWihUrFBgYqJiYGG3btk2XLl2yvb50797drn3btm3VvHlzxcXFafv27Tp9+nSaHRNSxsFIi+uPAADw/61YsULvvPOOBg8erDfffDO9ywHwL8AILgAg1UVHR8swDF24cEFTp06Vs7Oz3a+fAUBaYg4uACDVBQcH204INAxDvXr1sl2OCQDSGgEXAJDqXn31Vbm4uChr1qxq2bKlBg8enN4lAfgXYQ4uAAAATIU5uAAAADAVpijA5uDBg2lybUKziY6Otl3kG49GPz0efZQ89FPy0E/JQz8lT0bop+joaFWoUCHJdQRc2Dg4OKT5BdPNICQkhH5KBvrp8eij5KGfkod+Sh76KXkyQj+FhIQ8ch1TFAAAAGAqBFwAAACYCgEXAAAApkLABQAAgKkQcAEAAGAqBFwAAACYCgEXAAAApkLABQAAgKkQcAEAAGAqBFwAAACYCgEXAAAApkLABQAAgKk4GIZhpHcReD4cPXZMHu7u6V0GAAB4jt2PjZeLs1N6l6GQkBCVKVMmyXWZnnEteI45OTrKcdiq9C4DAAA8xxI+8U/vEh6LKQoAAAAwFQIuAAAATIWACwAAAFMh4AIAAMBUCLgAAAAwFQIuAAAATIWACwAAAFMh4AIAAMBUCLgAAAAwFQIuAAAATIWACwAAAFMh4AIAAMBUCLgAAAAwFQIuAAAATIWACwAAAFMh4AIAAMBUCLgAAAAwFQIuAAAATIWACwAAAFMh4AIAAMBUCLgAAAAwFQIuAAAATIWACwAAAFMh4AIAAMBUCLgAAAAwFQIuAAAATIWACwAAAFMh4AIAAMBUCLgAAAAwFQIuAAAATIWACwAAAFMh4AIAAMBUCLgAAAAwFQIuAAAATIWACwAAAFMh4AIAAMBUCLgAAAAwFQIuAAAATIWACwAAAFMh4AIAAMBUCLgAAAAwFQIuAAAATIWACwAAAFMh4AIAAMBUCLgAAAAwFQIuAAAATIWACwAAAFMh4AIAAMBUCLgAAAAwFQIuAAAATIWACwAAAFMh4AIAAMBUCLgAAAAwFQIuAAAATIWACwAAAFMh4AIAAMBUCLgAAAAwFQIuAAAATCVTehfwsBEjRig4OFibNm1SkSJF/rFtQkKClixZouDgYJ05c0YJCQkqXLiw6tevr549e+qFF1545LadO3fWvn37HlvPgAEDNHDgwCc+juQIDQ1VvXr1ktU2Of0BAACAB56rgPsk3n77ba1du1aNGzeWv7+/HB0ddfToUc2aNUs///yzFi9erBdffDHJbfv27as2bdrY/t6wYYM2bNigvn37qkSJErblbm5uaVa/q6urJk+ebLds4sSJkqSRI0cmagsAAIDkyZAB9+DBg1q1apVGjBih7t27262rXbu2Bg8erFmzZmn48OFJbl+jRg27v8+fP68NGzbIx8dH1apVS7O6H5YtWzYFBATYLfviiy8kKdFyAAAAJF+GnIN76NAhSYmDqiQ1btxY+fPn1+HDh59xVQAAAHgeZMiAmz17dknSDz/8oISEhETrN23apAULFqTKvjZu3Kh27drJ09NTlStXVt++fXXixAm7Nm5ubvrqq680c+ZM1axZU15eXurRo4dCQkJSpYbo6GhNmTJFfn5+8vDwUL169fTFF18oJibG1iYoKEhubm46ceKEhg4dqipVqsjLy0v9+vVTaGhoqtQBAACQEWTIgNuwYUPlypVL8+bNU/369TVp0iT98ssvioyMlCRlzpw5VfazYMEC9e/fX7GxsXrrrbfUrVs3/fbbb2rfvr1+++03u7ZLly7VrFmz1K5dO1sI7tixo/7444+nqiE+Pl59+vTRnDlz5Ofnp1GjRql69eqaMWOGBg0aJMMw7Nq/+eabunPnjoYMGaJ27dpp69atGjx48FPVAAAAkJFkyDm4rq6u+uabbzR06FBduHBB3377rb799ls5OzurZs2a6tevnzw9PZ9qH7du3dLHH38sT09PLViwwBaaW7RooWbNmumDDz7QsmXLbO2vXLmiZcuWyd3dXZJUv359NW/eXF9++aU+++yzFNfx448/avfu3Zo1a5Zq1aplW+7p6akxY8Zo06ZNql+/vm25h4eHpk6davs7MjJSixcv1p9//qlixYqluA4AAACr1PqWOq1kyIArSeXLl9fPP/+sHTt2aPPmzdq5c6dCQ0O1ZcsWbdu2TZMnT5a/v3+Kb3/37t2KiopS9+7d7UaEixQpoubNm2vJkiW6du2a8ufPL+nBfGBruJWkkiVLqlatWtq6dasSEhLk6JiywfL169fL1dVV7u7uCgsLsy2vU6eOnJyctHXrVruA27hxY7vty5QpI0m6ceMGARcAAKQKa75IT/8UsjNswJWkTJkyqW7duqpbt64k6Y8//tDChQs1b948jR8/Xg0aNJCLi0uKbts6b/Xhy4ZZlSxZUpJ06dIlW8B99dVXE7UrVqyYtmzZotu3b6f4Ul/nz59XWFiYvL29k1x/+fJlu7//fmk0aziPj49P0f4BAAAymgwZcL/88ksVKFBAgYGBdstLlCih0aNHKzY2VosXL9aZM2fk4eGR6vu3znt1dna2LXv4/62soTKlo7fW2yhWrJjGjh2b5PqcOXPa/f00+wIAADCDDBlwV6xYIUlq06aNHBwcEq23WCySpKxZs6Z4H4ULF5b0YFS4dOnSduusJ44VLFjQtuz8+fOJbuOvv/5S7ty5lTt37hTXUaRIER09elTVq1e3C6+xsbHasGGDXQ0AAADIoFdR8Pf314ULFzRz5sxE66Kjo7VixQoVK1YsyekFyeXj46MsWbJozpw5dpfjunLlilatWiVPT0/lyZPHtnzz5s26ePGi7e9Tp05px44datiwYYprkCQ/Pz/dvn1bixYtslu+ePFiDRkyRLt3736q2wcAADCb53IEd8qUKbZr3T6scePG8vb2Vp8+fbR3715NmTJFW7duVb169eTq6qrLly9r1apVunLlir799tskR3eT68UXX9Rbb72liRMnqn379vL391dERIQWLVqkhIQEjR492q69g4OD2rdvr86dOys2NlbfffedXF1dNXDgwBTXIEmBgYEKDg7WuHHjdOzYMXl6eurUqVNasmSJ3N3d1apVq6e6fQAAALN5LgPu6tWrk1xeokQJeXt7y8XFRd9//70WLVqkn376SbNmzVJERIRcXV3l4+OjPn36qHjx4k9dR7du3ZQ/f359++23+uyzz5Q1a1ZVrVpVAwYMkJubm13bxo0bq2jRopo1a5YSEhJUo0YNDR8+3HYSWkplzpxZc+fO1bRp07Ru3TqtXLlS+fPnV/v27dW/f/+nmoYBAABgRg7G338pAE/Mzc1NLVu21EcffZTepTyVkJAQuc8+k95lAACA51jCJym/DGtqCgkJeeTlyjLkHFwAAADgUQi4AAAAMBUCLgAAAEzluTzJLKM5efJkepcAAACA/48RXAAAAJgKARcAAACmQsAFAACAqRBwAQAAYCoEXAAAAJgKARcAAACmQsAFAACAqRBwAQAAYCoEXAAAAJgKARcAAACmQsAFAACAqRBwAQAAYCoEXAAAAJgKARcAAACmQsAFAACAqRBwAQAAYCoEXAAAAJgKARcAAACmQsAFAACAqRBwAQAAYCoEXAAAAJgKARcAAACmQsAFAACAqRBwAQAAYCoEXAAAAJgKARcAAACmQsAFAACAqRBwAQAAYCoEXAAAAJgKARcAAACmQsAFAACAqRBwAQAAYCoEXAAAAJgKARcAAACmQsAFAACAqRBwAQAAYCoEXAAAAJgKARcAAACmQsAFAACAqRBwAQAAYCoEXAAAAJgKARcAAACmQsAFAACAqRBwAQAAYCoEXAAAAJgKARcAAACmQsAFAACAqRBwAQAAYCoEXAAAAJgKARcAAACmQsAFAACAqRBwAQAAYCoEXAAAAJhKpvQuAM+P+IQEJXzin95lAACA59j92Hi5ODuldxn/iBFc2MTGxKR3CRlCSEhIepeQIdBPj0cfJQ/9lDz0U/LQT8nzT/30vIdbiYALAAAAkyHgAgAAwFQIuAAAADAVAi4AAABMhYALAAAAUyHgAgAAwFQIuAAAADAVAi4AAABMhYALAAAAUyHgAgAAwFQIuAAAADAVAi4AAABMhYALAAAAUyHgAgAAwFQIuAAAADAVAi4AAABMhYALAAAAUyHgAgAAwFQIuAAAADAVAi4AAABMhYALAAAAUyHgAgAAwFQIuAAAADAVAi4AAABMhYALAAAAUyHgAgAAwFQIuAAAADAVAi4AAABMhYALAAAAUyHgAgAAwFQIuAAAADAVB8MwjPQuAs+Ho8eOycPdPb3LAAAAGdT92Hi5ODs9k32FhISoTJkySa7L9EwqQIbg5Ogox2Gr0rsMAACQQSV84p/eJUhiigIAAABMhoALAAAAUyHgAgAAwFQIuAAAADAVAi4AAABMhYALAAAAUyHgAgAAwFQIuAAAADAVAi4AAABMhYALAAAAUyHgAgAAwFQIuAAAADAVAi4AAABMhYALAAAAUyHgAgAAwFQIuAAAADAVAi4AAABMhYALAAAAUyHgAgAAwFQIuAAAADAVAi4AAABMhYALAAAAUyHgAgAAwFQIuAAAADAVAi4AAABMhYALAAAAUyHgAgAAwFQIuAAAADAVAi4AAABMhYALAAAAUyHgAgAAwFQIuAAAADAVAi4AAABMhYALAAAAUyHgAgAAwFQIuAAAADAVAi4AAABMhYALAAAAUyHgAgAAwFQIuAAAADAVAi4AAABMhYALAAAAUyHgAgAAwFQIuAAAADAVAi4AAABMhYALAAAAUyHgAgAAwFQIuAAAADAVAi4AAABMhYALAAAAUyHgAgAAwFQIuAAAADAVAi4AAABMhYALAAAAUyHgAgAAwFQIuAAAADAVAi4AAABMhYALAAAAU3lswB0xYoTc3NwUGhr6LOp5YjExMbp69Wqy2u7cuVO9evVStWrV5OHhIV9fX40cOVJ//vln2hYJAACAZyZDj+BevHhR/v7+2rlz52PbLl26VD169FBERIR69eqlMWPGqEmTJtq8ebMCAgJ05MiRZ1AxAAAA0lqm9C7gaYSGhiZr9DUqKkqTJ0+Wn5+fpk+fbreuXbt2CggI0Pjx47V06dI0qhQAAADPSoYewU2uM2fO6O7du6pRo0aidUWLFpWvr69CQkIUHR2dDtUBAAAgNaVqwD1z5oz69++vypUrq3z58mrXrp22b9+eqN1PP/2kTp06qVKlSvLw8JCfn58mT56smJgYW5uYmBhNmDBB9erVk4eHh+rUqaP3339fd+7ckSQFBQWpS5cukqSRI0fKzc3tkXVlz55dkrR69Wrdu3cv0fqPPvpIR48eVZYsWWzLwsLCNHLkSFWvXl2VKlXSyJEjtXPnTrm5uWnv3r22Gh7+2yqp5X/99Zfeeecd1a5dWx4eHqpatar69u2r06dPJ9pu3bp18vPzU/ny5TV16lRJUnR0tKZMmSI/Pz95eHioXr16+uKLL+z6TJLWrVun1q1by8vLS5UqVVL37t114MCBR/YNAACA2aTaFIWTJ0+qQ4cOyps3r/r06SNnZ2etXr1avXv31qeffqomTZpIejAXdvTo0fLz89OwYcMUGxurDRs2aPbs2ZKkt99+W5L0wQcfaPXq1erSpYuKFi2q06dPa8GCBfrrr7/07bffqkqVKurbt69mzJih119/XZUqVXpkbSVKlFDFihV18OBB+fr6qn79+qpZs6aqVaumvHnzytnZ2a59dHS0OnXqpPPnz6tz584qUKCAli1bpn379qWob27cuKG2bdsqR44c6tSpk1588UWFhITohx9+0LFjx7R582a7GkaNGqVOnTopR44cqlChguLj49WnTx8dPHhQbdu2VcmSJXX06FHNmDFDISEhmj59uhwcHLRv3z4NGTJEtWvXVmBgoKKiojR//nx1795da9asUdGiRVNUPwAAQEaSagF3/PjxcnV1VXBwsLJlyyZJ6tSpk7p27aoJEyaofv36ypw5s7799lt5eXnpq6++koODgySpQ4cOqlevnrZv324LuKtWrVLr1q311ltv2faRLVs2bd++XRERESpatKh8fHw0Y8YMVahQQQEBAf9Y33//+18NHz5cu3fvVlBQkIKCguTg4KBy5cqpW7duatq0qa3tDz/8oLNnz+qjjz5Sy5YtJUlt2rRR8+bNU9Q3QUFBunPnjhYuXKiSJUvalmfPnl1ff/21Tp06JXd3d9vypk2bavDgwXbb7969W7NmzVKtWrVsyz09PTVmzBht2rRJ9evX19q1a+Xi4mILvJLk4+OjQYMG6dixYwRcAACQ5kJCQtK7hNQJuLdu3dK+ffvUuXNn3b9/X/fv37eta9CggSZOnKjff/9dlSpV0sqVKxUVFWULYJJ08+ZN5cyZU5GRkbZlBQsW1Nq1a+Xh4aH69esrZ86cGjx4sF3wexL58uXT3Llz9fvvv2v9+vXauXOnjh8/rt9++01vvfWW9u3bp/fff1+StHXrVuXOndsu0ObIkUMdO3bU5MmTn3jfvXv3VuvWrZUnTx7bsvv378vR8cEMkYePW5KqVKli9/f69evl6uoqd3d3hYWF2ZbXqVNHTk5O2rp1q+rXr6+CBQsqIiJC48ePV4cOHVSyZEnblAcAAIBnoUyZMs9kP/8UpFMl4F64cEGSNG/ePM2bNy/JNpcvX5YkOTs7a//+/Vq9erX++OMPnT9/Xjdv3pQkFS5c2Nb+vffe0+DBgzVy5Ej95z//UYUKFdSgQQO1bt1aL7zwQoprLVeunMqVK6ehQ4fq5s2bWrVqlaZOnarFixerVatWKl++vC5fvqxChQrJycnJbtuHR1+fVGxsrKZMmaJjx47p/PnzCg0NVXx8vCQpISHBru3DQViSzp8/r7CwMHl7eyd529a+7dSpk3bs2KH58+dr/vz5KlKkiHx9fdWmTRuVLl06xbUDAABkJKkScK1BrWPHjqpfv36SbV599VVJ0rhx4zR//nyVLVvWNrXAy8tL48aNswU1SfL29taWLVts/3bu3KmJEydq7ty5CgoKkqura7Lr+/HHH3X27Fm76Q7SgyDZrVs35c+fX0OGDNGBAwdUvnx5SZJhGIlu5+GT0P6JtT+sfv31V73xxhvKli2bfHx81Lp1a5UtW1bnz5/XBx98kGh768juw7dXrFgxjR07Nsn95cyZU9KDUeb58+fr8OHD2rhxo3755RfNmzdPCxYs0OTJk+Xv75+s+gEAADKyVAm41pFXJycn+fj42K07c+aMQkNDlTVrVl28eFHz589XQEBAoq/6b9y4Yfv/mJgYhYSEqGDBgmratKmaNm2qhIQEzZkzR5MnT9aaNWvUuXPnZNe3d+9eLV++XG3btlWRIkUSrS9VqpQkycXFRZL0yiuvaP/+/YqLi1OmTP/rovPnz9ttZw2if7+SwcPHIj2Y/+vi4qI1a9bYBfMZM2Ykq/4iRYro6NGjql69ul34tZ6gV7BgQUnSuXPnFB4ergoVKqhChQoaNmyYzpw5o44dO2rOnDkEXAAA8K+QKpcJy58/vzw8PBQcHGz3s7mxsbF69913NWjQIMXFxdku8WUdzbXatm2b/vzzT8XFxUl6MKf39ddf18yZM/9XqKOjypUrZ/t/SbYpBH//iv/vrMFu/PjxSV7rdsmSJXJyclLdunUlSa+99prCw8PtfvghNjZWixYtstsuX758kuzngMTFxWn9+vV27W7fvi1XV1e7cBseHq7g4GBJiUd8/87Pz0+3b99OtP/FixdryJAh2r17t+34+vXrp4iICFubEiVKKGfOnIlGhQEAAMwq2SO4U6ZMsV1P9mGNGzeWt7e3Ro8era5du6p169Zq3769cufOrTVr1ujIkSMaOnSoXnzxRWXPnl2FChXSjBkzFB0drYIFC+q3335TcHCwsmTJYgtmBQoUkL+/vxYuXKioqCh5eXnp9u3bmj9/vvLmzavGjRtLkl588UVJ0sqVK2UYhlq2bGk34mrl7e2t7t27a86cOWrSpImaNWumokWLKjw8XJs3b9a+ffs0YsQIFSpUSNKDQBwcHKxx48bp9OnTKlGihG3O8MOqVq2qfPny6auvvlJ0dLTy5MmjH3/8MdFJY7Vr19Y333yj//u//1PNmjV1/fp1LVu2zDbS+3AgTUpgYKCtnmPHjsnT01OnTp3SkiVL5O7urlatWkmSunfvrl69eqljx45q0aKFsmTJoo0bN+r8+fOaNGnSY+9jAAAAM0h2wF29enWSy0uUKCFvb295eXlp0aJFmjp1qubMmaO4uDgVL17c7lJbmTNn1tdff62PPvpI33//vQzD0Msvv6x3331XcXFxmjBhgo4ePSoPDw+NGzdORYsW1Zo1a7RmzRplzZpV3t7eGjJkiG0ktGTJkurcubOCgoL0+++/q1q1anr55ZeTrHPEiBGqWrWqfvjhBy1fvly3b99W9uzZ5enpqdmzZ6tmzZq2tk5OTpoxY4Y+//xzrVq1SlFRUfL19VXDhg3tgqKzs7NmzZqljz76SLNmzVK2bNnUrFkzNWzYUJ06dbK1GzhwoOLj47V27Vpt2bJF+fPnl4+Pj3r06KGmTZtqz549atCgwSP7PnPmzJo7d66mTZumdevWaeXKlcqfP7/at2+v/v37K2vWrJKkmjVravr06Zo5c6YtdJcqVUqfffaZ3WXQAAAAzMzBSOpsKiQpKChII0eO1Pfff69q1aqldzmpLiQkRO6zz6R3GQAAIINK+OTZne8TEhLyyEuSMTETAAAApkLABQAAgKkQcAEAAGAqqXId3H+LVq1a2a5YAAAAgOcTI7gAAAAwFQIuAAAATIWACwAAAFMh4AIAAMBUCLgAAAAwFQIuAAAATIWACwAAAFMh4AIAAMBUCLgAAAAwFQIuAAAATIWACwAAAFMh4AIAAMBUCLgAAAAwFQIuAAAATIWACwAAAFMh4AIAAMBUCLgAAAAwFQIuAAAATIWACwAAAFMh4AIAAMBUCLgAAAAwFQIuAAAATIWACwAAAFMh4AIAAMBUCLgAAAAwFQIuAAAATIWACwAAAFMh4AIAAMBUCLgAAAAwFQIuAAAATIWACwAAAFMh4AIAAMBUCLgAAAAwFQIuAAAATIWACwAAAFMh4AIAAMBUCLgAAAAwFQIuAAAATIWACwAAAFMh4AIAAMBUCLgAAAAwFQIuAAAATIWACwAAAFMh4AIAAMBUCLgAAAAwFQIuAAAATIWACwAAAFMh4AIAAMBUCLgAAAAwFQIuAAAATIWACwAAAFMh4AIAAMBUCLgAAAAwlUzpXQCeH/EJCUr4xD+9ywAAABnU/dh4uTg7pXcZjODif2JjYtK7hAwhJCQkvUvIEOinx6OPkod+Sh76KXnop+RJaT89D+FWIuACAADAZAi4AAAAMBUCLgAAAEyFgAsAAABTIeACAADAVAi4AAAAMBUCLgAAAEyFgAsAAABTIeACAADAVAi4AAAAMBUCLgAAAEyFgAsAAABTIeACAADAVBwMwzDSuwg8Hw4fPqwsWbKkdxkAAACPFR0drQoVKiS5joALAAAAU2GKAgAAAEyFgAsAAABTIeACAADAVAi4AAAAMBUCLgAAAEyFgPsvcOHCBQ0YMEBVq1ZV1apV9fbbbyssLCzNtsuoUuN4//Of/6hz585pVOHzIaX9tH37dnXo0EHly5eXl5eXunXrpsOHD6d9wekkpf20e/dutW/fXl5eXqpVq5YmTJigiIiIZ1Bx+kiN592JEyfk4eGhqVOnplGV6S+l/dSmTRu5ubkl+jdo0KBnUPWzldI+CgsL0+jRo+Xj46OKFSuqc+fOvDY9JDQ0NMnH0MP/9u7d+wyPIHm4TJjJ3bp1S61bt1ZMTIy6dOmi+Ph4zZ49W4ULF9bSpUuVOXPmVN0uo0qN4126dKlGjx6tqlWrat68ec+g6mcvpf20b98+denSRaVKlVLr1q0VFxenhQsX6tq1a1q4cKE8PT2f8ZGkrZT20+7du9WjRw+5u7urZcuWunz5sr7//nu5u7trwYIFcnQ015hEajzv4uLiFBgYqOPHj2vAgAEaOHDgM6j82UppPxmGoYoVK8rHx0cNGza0W1e4cGFVrlz5WZT/TKS0j+7du6fAwEBdu3ZN3bp1U86cObVgwQJdvXpVS5culcViecZHkrZS0k+RkZHasGFDouXR0dEaN26c8uTJox9//FG5cuV6FoeQfAZM7bPPPjPKlCljnDlzxrZs586dhsViMZYsWZLq22VUT3O8cXFxxtSpUw03NzfDYrEYnTp1Suty001K+ykgIMCoW7euERkZaVt2/fp1o0qVKka3bt3StOb0kNJ+atmypeHr62tERUXZls2fP9+wWCzG1q1b07Tm9JAarzNffvml4e7ublgsFuO///1vWpWarlLaT+fPnzcsFouxfPnyZ1Fmunqa9zo3Nzdj3759tmXXrl0zPD09jeHDh6dpzekhNd/bx48fb5QuXdrYv39/apeZKsw1HIBE1qxZo6pVq6pkyZK2ZT4+PipevLjWrFmT6ttlVCk93ujoaLVs2VJTp05VQECAChQo8CzKTTcp6ac7d+7oxIkTatSokbJmzWpbnjdvXlWpUkWHDh1K87qftZT0U3R0tF588UW1bdtWLi4utuVVq1aVJJ08eTJti04HT/s6c/LkSU2fPl39+vVLyzLTXUr76cyZM5Jkt51ZpaSPDMNQcHCw6tatqypVqtiW58uXT2+//bapRritUuu9/eTJk5o/f75atmz53PYTAdfE7ty5owsXLsjd3T3ROnd3dx07dixVt8uonuZ4o6Ojde/ePU2ZMkWTJk1SpkyZ0rLUdJXSfsqRI4d+/vlndevWLdG6W7duycnJKbVLTVcp7acsWbJo9uzZ6tu3r93ykJAQSVKhQoVSv9h09LSvM3FxcRo5cqRq1Kih5s2bp1WZ6e5p+un06dOS/hdwIyMj06bIdJbSPgoNDdXVq1fl4+Mj6UHgtc5379ixo9q2bZt2RaeD1HxvnzJlilxcXDR48OBUrDB1EXBN7OrVq5KU5Khivnz5FB4ervDw8FTbLqN6muPNkSOH1q9fryZNmqRpjc+DlPaTk5OTihUrlmi7EydO6ODBg/Ly8kqbgtNJaj1/Ll68qKCgIE2YMEEWi0UNGjRI9VrT09P20zfffKO//vpL77//fprV+Dx4mn46ffq0smfProkTJ8rLy0teXl6qX7++6b6FS2kf/fXXX5KkPHnyaNKkSapcubIqVqyoBg0aaPPmzWlbdDpIrdemEydOaMuWLWrXrp3y58+f6nWmFgKuiVk/iT78tbBVlixZJCX9iT6l22VUT3O8jo6Oph61fVhqPi4iIiL0zjvvSJJ69+6dShU+H1Kjn27fvi0/Pz+NHDlS0dHRGj16tG1bs3iafjp9+rSmTZumd955RwULFky7Ip8DT9NPZ86cUUREhMLDwzV58mR9+OGHyp49u9566y2tWLEizWp+1lLaR3fv3pUkffHFF9q2bZtGjRqlSZMmycXFRf3799euXbvSsOpnL7VewxctWiQnJyd16tQpdQtMZf+Od+Z/KSMZF8hwcHBIte0yqn/b8aZUavVTVFSU3nzzTZ04cUJ9+vSxzTE1i9ToJwcHB02ZMkUxMTGaN2+eunfvrilTpui1115LrTLTXUr7KT4+XiNGjFClSpVM9xVyUp7m8dS2bVslJCSoY8eOtmVNmzZVs2bN9PHHH8vf398UU4RS2kcxMTGSHgTddevW2a4C4OfnpwYNGujTTz+1TV8wg9R4bbp//75WrlwpPz8/FS5cOLVKSxOM4JpYtmzZJD2YJ/p31mU5cuRIte0yqn/b8aZUavTT3bt31aNHD+3du1etW7fWkCFDUr/QdJYa/ZQrVy41adJELVq00IIFC1SoUCFNnDgx9YtNRyntp9mzZ+vkyZMaOnSowsLCFBYWZhuJi4qKUlhYmBISEtKw8mfraR5P7du3twu3kuTi4qKAgADduHHDdhJaRve073UNGza0u8RVzpw55efnp2PHjpnqGtSp8dq0d+9eRUZGqlGjRqlfYCoj4JqY9aSU69evJ1p37do15cyZ0/aAT43tMqp/2/Gm1NP2082bN9WlSxcdPHhQr7/+uiZMmGDKkfHUfjy5uLiobt26unz5sql+aCWl/bR9+3bFxsYqMDBQ3t7e8vb2VsuWLSU9CL/e3t66dOlS2hb/DKXF65Orq6sk80w1S2kfWeeiWvvjYa6urjIMwzR9JKXOY2nbtm3KnDmz6tatmxYlpiqmKJhYzpw5VaRIkSTPjDx+/Lg8PDxSdbuM6t92vCn1NP107949vfHGGwoJCVG3bt00cuTItCw1XaW0n86ePatevXrpjTfeSDTqFhERIQcHB1P9wEpK++mdd96xjdha3bhxQ8OHD1dAQIBatGihfPnypUnN6SGl/XT16lX16NFDjRs31oABA+zWnTt3TpJUpEiR1C84HaS0j0qVKqXMmTMnOZIdGhqqLFmyJBl+M6rUeK87ePCgPDw8MsS3mozgmlzDhg21e/dunT171rZs165dOnfu3D+e+Z/S7TKqf9vxplRK++mDDz5QSEiIunTpYupwa5WSfnrllVcUHh6uxYsX2+YGSg+uprBu3TpVqVIlQ7ypPImU9JOHh4d8fHzs/lWsWFGSVLRoUfn4+JjuhLyU9FOBAgV09+5dLV26VPfu3bMtv3TpkoKCglStWjVTfRBISR9ly5ZNfn5+2rp1q+2SatKDn7LdvHmz6tWrZ4o5yg97mve62NhYnTlzRmXLlk3rMlMFP9VrcmFhYWrWrJmcnJzUo0cPRUdHa9asWXr55Ze1ePFiZc6cWRcuXNDBgwdVsWJFFS1aNNnbmUlK++nvrBPvzfpTvSnpp7Nnz6pJkybKmTOnRo4cmeQbRkBAQDocTdpJ6ePpxx9/1Ntvv60KFSqoefPmunXrlhYsWKDY2FgtXLjQdD8bmlrPu9DQUNWrV8+0P9Wb0n7auHGj+vfvr1KlSikwMFARERG2x9OiRYtM9QMQKe2j0NBQBQYGSpK6dOkiZ2dnff/994qKilJQUNAjH3MZ1dM8586fP68GDRpo+PDh6tmzZzoeRTKlx8+n4dk6e/as0bNnT6NChQpG9erVjXfeece4efOmbf3y5cuT/DnHx21nNintp4f5+vqa+qd6DePJ+2nhwoWGxWL5x39mlNLH05o1a4yWLVsa7u7uRpUqVYyBAwcaf/zxx7Mu/5lJjefdhQsXTP1TvYaR8n7asGGD0aZNG8PDw8OoXLmy0a9fP7ufaTWTlPbR+fPnjYEDBxqVKlUyKlasaPTu3du0fWQYKe+nI0eOGBaLxVi0aNGzLjlFGMEFAACAqTAHFwAAAKZCwAUAAICpEHABAABgKgRcAAAAmAoBFwAAAKZCwAUAAICpEHABAABgKgRcADChnj17ys3NTb169UrvUgDgmSPgAoDJXL9+Xbt27VLWrFm1Y8cOXblyJb1LAoBnioALACazatUqxcfHq2fPnkpISNCyZcvSuyQAeKYIuABgMitWrFCuXLnUs2dPvfDCCwoKChK/yg7g34SACwAmcuLECZ08eVLe3t5ycXFR/fr1dfHiRe3cuTPJ9suXL1dgYKC8vLxUo0YNvfnmmzpx4sQTtwsKCpKbm5vmzp2baNvOnTvLzc1Nd+/elSTt3btXbm5uWrhwod566y15enqqZs2aOnDggCTp4sWLGjt2rOrXr69y5crJy8tLrVq10qJFixLddnx8vObMmaPmzZurQoUKqlOnjoYPH64LFy5Ikn799Ve5ublp2LBhSR5//fr1VbduXSUkJPxzxwLIUDKldwEAgNSzYsUKSVKTJk1s/w0ODtbSpUtVs2ZNu7ZjxozRkiVLVLhwYQUEBCg2NlarV6/Wnj17tGjRIpUuXfqJ2j2padOmKVu2bOrUqZPOnDkjd3d3hYaGqk2bNoqKilKDBg300ksv6erVq1q3bp3ee+89xcfHq1OnTpKkhIQE9enTR9u3b9err76qNm3a6NatW1q7dq327NmjZcuWqVKlSipSpIg2bdqkqKgoZc2a1bb/gwcP6sKFC+rdu7ccHRnvAcyEgAsAJhEfH6/Vq1cre/bsqlu3riTJx8dHefLk0aZNmxQWFiZXV1dJ0u7du7VkyRJVrlxZM2fOVI4cOSRJrVu3VocOHfT5559rxowZyW6XEhEREVqxYoXy5ctnW/b111/r1q1bmjNnjnx8fGzLO3XqpMDAQK1evdoWcIOCgrR9+3Y1atRIH3/8sTJnzixJ8vX11dChQ/XNN99o9OjRCggI0LRp07RlyxZb8JcezFWWpICAgBTVD+D5xUdWADCJnTt36vr162rQoIGyZMkiScqUKZMaNWqk2NhY/fjjj7a2a9askSQNHTrUFlolqWLFinrrrbfk6+v7RO1SomLFinbhVpKaN2+uDz/80C7cSpKnp6dcXFx08+bNRMfw7rvv2sKtJDVt2lR9+/ZVxYoVJUktWrSQ9L9AK0mxsbH66aef5O7urldffTXFxwDg+cQILgCYhDXANm3a1G65v7+/FixYoGXLlql79+6SHszVdXJyUrly5RLdTu/evW3/n9x2KVGkSJFEyypXrqzKlSvr9u3bCgkJ0fnz53Xu3DkdPnxY0dHRio+Pt6utUKFCKlCggN1tODg4aMiQIba/X375ZVWsWFHbt2/XnTt3lCtXLu3YsUO3bt1S3759n+oYADyfCLgAYAL37t3Txo0bJemRP+5w5swZHTx4UBUrVtTdu3eVJUsWOTs7/+PtJrddSlhHmR92584dTZw4UatXr1ZsbKwcHBxUuHBhVa9eXcePH09UW968eZO1rxYtWujgwYNav369AgMDtXLlSmXKlEnNmjVLlWMB8Hwh4AKACfz888+6f/++ypUrp7JlyyZaf+7cOe3bt09Lly5VxYoVlS1bNkVHRysuLk6ZMtm/FTx8MlZy2zk4OEhSkpcji4qKSvZxDB8+XNu2bVO7du0UEBAgi8Vimxrx8BQDa20RERFJ3k5kZKSyZctm+7tx48aaMGGCfvrpJzVv3lxbtmxRjRo1kh2QAWQsBFwAMAHr9IQRI0aocuXKidZfunRJ9erV088//6xRo0bJYrEoJCREx48fl6enp13bfv366ffff9f27duT3c46whsZGWnXxjAM2yW7Hufu3bvatm2bPDw89P7779utCw0NVXR0tF2AtlgsOnDggK5fv55oLm+LFi3k4OCgdevWSZJy5swpPz8/bdy4URs3blRUVBQnlwEmxklmAJDBXbx4Ufv371fhwoVVqVKlJNsUKlRI1atXV2RkpNasWaPmzZtLkqZMmaL79+/b2h06dEj79u2Tl5eXsmbNmux2JUqUkCRt377dbp7swoULdfv27WQdh7OzsxwdHXX37l3FxMTYlt+/f1/jxo2T9ODkMKvmzZvLMAx98skndvv86aef9Ndff8nb29vu9lu0aKHY2Fh9+umnyp49u+rVq5esugBkPIzgAkAG9+OPP8owDPn7+9umCiSlVatW2rVrl5YuXaply5apdevWWr58uQICAlSrVi1FRERozZo1yp49u8aMGSNJqlmzZrLalS1bVu7u7jp06JA6dOigKlWq6OTJk9qzZ4/Kly+vI0eOPPY4smbNqgYNGmjdunUKDAxUjRo1FBkZqS1btujGjRvKlSuXwsPDlZCQIEdHR7Vp00br16/XihUrdPLkSVWrVk1Xr17V+vXrVaRIEbsTzazHkjdvXl28eFGtWrWSi4vLU/Q6gOcZI7gAkMFZpydYR1sfpUGDBnrhhRf0+++/6+TJk5owYYLGjh0rFxcXLVmyRBs2bFDt2rW1aNEiFS1a1LZdctvNnDlTLVu21J9//qn58+crKipK3333ncqXL5/sY/nwww/VtWtXhYeHa/78+dq+fbvKlSunRYsWqUWLFrp//7727t0rSXJyctL06dM1ePBg3b9/XwsWLNCePXvk7++vhQsXKleuXHa3nSlTJjVo0EAS174FzM7B4AfKAQD/Eu3atdPVq1e1efPmfxztBpCxMYILAPhX2LFjhw4dOqTWrVsTbgGTYwQXAGBqEyZM0IEDB3Ty5Em98MIL+vnnn5U7d+70LgtAGmIEFwBgavnz59e5c+dUvHhxTZ8+nXAL/AswggsAAABTYQQXAAAApkLABQAAgKkQcAEAAGAqBFwAAACYCgEXAAAApkLABQAAgKn8P1Ri/lNv5yXDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get linear least-squares model\n",
    "w_ls_top, mse_ls_top = least_squares(y_train, tx_top_train)\n",
    "\n",
    "# Test error on evaluation set\n",
    "y_eval_pred = predict_labels(w_ls_top, tx_top_eval)\n",
    "\n",
    "acc_ls_top = get_accuracy(y_eval_pred, y_eval)\n",
    "f1_ls_top = get_f1_score(y_eval_pred, y_eval)\n",
    "\n",
    "print('Accuracy LS on evaluation set: ', acc_ls_top)\n",
    "print('F1 Score LS on evaluation set:', f1_ls_top)\n",
    "\n",
    "# Save current mode predictions on test set\n",
    "y_test_pred = predict_labels(w_ls_top, tx_top_test)\n",
    "create_csv_submission(ids_test, y_test_pred, '../data/least_squares_top_ten_submission.csv')\n",
    "\n",
    "accuracy = [acc_ls, acc_ls_top]\n",
    "labels = ['Least Squares', 'LS Top Ten']\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "ax.barh([i for i in range(len(accuracy))], accuracy)\n",
    "\n",
    "ax.set(title='Prediction Accuracy on Evaluation Set', xlabel='Accuracy',\n",
    "       yticks=[i for i in range(len(labels))], yticklabels=labels);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
